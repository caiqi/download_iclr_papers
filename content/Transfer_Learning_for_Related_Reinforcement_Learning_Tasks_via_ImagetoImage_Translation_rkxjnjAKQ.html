<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Transfer Learning for Related Reinforcement Learning Tasks via Image-to-Image Translation | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Transfer Learning for Related Reinforcement Learning Tasks via Image-to-Image Translation" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rkxjnjA5KQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Transfer Learning for Related Reinforcement Learning Tasks via..." />
      <meta name="og:description" content="Deep Reinforcement Learning has managed to achieve state-of-the-art results in learning control policies directly from raw pixels. However, despite its remarkable success, it fails to generalize, a..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rkxjnjA5KQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Transfer Learning for Related Reinforcement Learning Tasks via Image-to-Image Translation</a> <a class="note_content_pdf" href="/pdf?id=rkxjnjA5KQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019transfer,    &#10;title={Transfer Learning for Related Reinforcement Learning Tasks via Image-to-Image Translation},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rkxjnjA5KQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Deep Reinforcement Learning has managed to achieve state-of-the-art results in learning control policies directly from raw pixels. However, despite its remarkable success, it fails to generalize, a fundamental component required in a stable Artificial Intelligence system. Using the Atari game Breakout, we demonstrate the difficulty of a trained agent in adjusting to simple modifications in the raw image, ones that a human could adapt to trivially. In transfer learning, the goal is to use the knowledge gained from the source task to make the training of the target task faster and better. We show that using various forms of fine-tuning, a common method for transfer learning, is not effective for adapting to such small visual changes. In fact, it is often easier to re-train the agent from scratch than to fine-tune a trained agent. We suggest that in some cases transfer learning can be improved by adding a dedicated component whose goal is to learn to visually map between the known domain and the new one. Concretely, we use Unaligned Generative Adversarial Networks (GANs) to create a mapping function to translate images in the target task to corresponding images in the source task. These mapping functions allow us to transform between various variations of the Breakout game, as well as between different levels of a Nintendo game, Road Fighter. We show that learning this mapping is substantially more efficient than re-training. A visualization of a trained agent playing Breakout and Road Fighter, with and without the GAN transfer, can be seen in \url{<a href="https://streamable.com/msgtm}" target="_blank" rel="nofollow">https://streamable.com/msgtm}</a> and \url{https://streamable.com/5e2ka}.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Transfer Learning, Reinforcement Learning, Generative Adversarial Networks, Video Games</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a method of transferring knowledge between related RL tasks using visual mappings, and demonstrate its effectiveness on visual variants of the Atari Breakout game and different levels of Road Fighter, a Nintendo car driving game.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_H1xIHS96hQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting method to improve transfer learning between related tasks. The motivation is strong, explanations are intuitive, technical parts are solid, experiments are sufficient. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkxjnjA5KQ&amp;noteId=H1xIHS96hQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper739 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper739 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper propose an intermediate stage before transfer learning on playing new games that is with slight visual change. The intermediate stage is basically a mapping function to translate the images in the new game to old game with certain correspondence. The paper claims that the adding of intermediate stage can be much more efficient than re-train the model instead. 
Then the paper compares different baselines without the mapping model. The baselines are either re-trained from scratch or (partially) initialized with trained model. The learning curves show that fine-tuning fails to transfer knowledge from the source domain to target domain. The mapping model is constructed based on unaligned GAN. And the experiments are setup and results are shown.

Pros:
+ The paper makes a very good start from analogizing human being adjusting himself between similar tasks. 
+ The paper demonstrates strong motivation on improving the existing transfer learnings that are either fail or take too much time to train from scratch.
+ The paper clearly illustrate the learning curve of multiple approaches for transferring knowledge across tasks.
+ The paper proves detailed analysis why using unaligned GAN to learn the mapping model, and gives
+ I also like the experiment section. It is well written, especially the discussions section answer all my questions. 

Questions:
1.	Why fine-tuning from a model that is trained from related task does not help, even decelerate the learning process? Could you explain it more?
2.	Could you please also include in figure 2 the proposed transfer learning curve with the mapping model G? I’m curious how much faster it will converge than the Full-FT. And I suppose the retrain from scratch can be extremely slow and will exceed the training epoch scope in the figure.
3.	In dataset collection, you use untrained agent to collect source domain image. Will it improve the results if you use well trained agent, or even human agent, instead? 
4.	I hope, if possible, you can share the source code in the near future.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HyerEyRB6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' response to Reviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkxjnjA5KQ&amp;noteId=HyerEyRB6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper739 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper739 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your time in reviewing our paper.

1. Our paper discusses tasks in which the inputs of the model in each step are raw pixels. The A3C model learned the detail and noise in the training images to the extent that small and insignificant modifications create data that is unrecognizable to the model, which prevent the model from following the policy it learned and making optimal decisions.

2. Figure 2 presents the results of the RL agents during training. In that matter, our approach is a zero-shot transfer approach in which there is no need for any additional RL training. The number of GAN iterations needed for the maximum scores and # of iterations for each task is presented in Table 1, 2.

3. Yes, if the agent is allowed to collect images from later stages of the game it needs to transfer to, results improve a little. However, we did not consider this a realistic scenario, because the untrained agent cannot reach this stages. The idea of human demonstration is an interesting one, and we expect it could work well.

4. We will share source code upon publication.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkeqDrUTnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>TRANSFER LEARNING FOR RELATED REINFORCEMENT LEARNING TASKS VIA IMAGE-TO-IMAGE TRANSLATION</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkxjnjA5KQ&amp;noteId=rkeqDrUTnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper739 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper739 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper seeks to generalize the reinforcement learning agents to related tasks. The authors first show the failure of conventional transfer learning techniques, then use GANs to translate the images in the target task to those in the source task. It is an interesting attempt to use the style-transferred images for generalization of RL agents. The paper is well written and easy to follow.
Pros:
1.	It is a novel attempt to use GANs to generate pictures that help RL agents transfer the policies to other related environments.
2.	It is an interesting viewpoint to use the performance of RL agent to evaluate the quality of images generated by GANS.
Cons:
1.	The pictures generated by GANs can be hardly controlled, and extra noise or unseen objects might be generated, and may fool the RL agent during training.

Other feedback:
In Figure 2, it seems the fine-tuning methods also achieve comparable results (Full-FT and Partial-FT), such as Figure 2(b) and Figure 2(c). Besides, the plot is only averaged over 3 runs, whereas the areas of standard deviation still overlap with each other. It may not be convincing enough to claim the failure of fine-tuning methods.

Minor typos:
1.	In 2.1, second paragraph: 80x80 -&gt; $80 \times 80$
2.	In 2.1, second paragraph: chose -&gt; choose
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJEql0HT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' response to Reviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkxjnjA5KQ&amp;noteId=rJEql0HT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper739 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper739 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your time in reviewing our paper.

* Although the results between the runs are different, in all cases the fine-tuning results don’t outperform training from scratch. We would expect a generalized model to adjust quickly to small modifications in the images since most pixels are the same as in the original games and the dynamics didn’t change. Instead, the model behaves as if the modified games are completely new tasks.

* Thank you for the corrections, will change the paragraph accordingly.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJx6ByVYhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Problematic qualitative results, generic unsupervised domain adaptation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkxjnjA5KQ&amp;noteId=HJx6ByVYhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper739 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper739 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value"># Summary

This paper proposes to improve the sample efficiency of transfer learning for Deep RL by mapping a new visual domain (target) onto the training one (source) using GANs. First, a deep RL policy is trained on a source domain (e.g., level 1 of the Atari Road Fighter game). Second, a GAN (e.g. UNIT or CycleGAN) is trained for unsupervised domain adaptation from target images (e.g., level 2 of Road Fighter) to source ones. Third, the policy learned in the source domain is applied directly on the GAN-translated target domain. The experimental evaluation uses two Atari games: i) transfer from Breakout to Breakout with static visual distractors inpainted on the screen, ii) from one Road Fighter level to others. Results suggest that this transfer learning approach requires less images than retraining from scratch in the new domain, including when fine-tuning does not work.


# Strengths

Controlled toy experiments of Deep RL generalization issues:
The experiments on Breakout quantify how badly A3C overfits in this case, as it shows catastrophic performance degradation even with trivial static visual input perturbations (which are not even adversarial attacks). The fine-tuning experiments also quantify well how brittle the initial policy is, motivating further the importance of the problem studied by the paper.

Investigating the impact of different GANs on the end task:
The experiments evaluate two different image translation algorithms: one based on UNIT, the other based on CycleGAN. The results suggest that this choice is key and depends on the target domain. This suggests that the adaptation is in fact task dependent, confirming the direction pursued by others in task-specific unsupervised domain adaptation (cf. below).


# Weaknesses

Discrepancy between quantitative and qualitative results:
The good quantitative results (accumulated rewards) reported in the experiments are not reflected in the qualitative results. As can be seen from the videos, these results seem more to be representative of a bias in the data. For instance, in the Road Fighter videos, one can clearly see that the geometry of the road (width, curves) and dynamic obstacles are almost completely erased in the image translation process. The main reasons the quantitative results are good seem to be i) in the non-translated case the agent crashes immediately, ii) the "translated" image is a wide straight road identical to level 1 where the policy just keeps the car in the middle (thus crashing as soon as there is a turn or a collision with an obstacle). Even in the breakout case, there are catastrophic translation failures for some of the studied variations although the domain gap is static and small. The image translation results look underwhelming compared to state of the art GANs used for much more complex tasks and environments (e.g., the original CycleGAN paper and follow-up works, or the ICLR'18 progressive growing of GANs paper). This might be due to a hyper-parameter tuning issue, but it is unclear why the adaptation results seem not on par with previous results although the paper is in a visually simpler domain (Atari games).

Does not address the RL generalization issues:
Although it is the main goal of the paper, the method is fundamentally side-stepping the problem as it does not improve in any way the policy or the Deep RL algorithm (they are left untouched). It is mapping the target environment to the source one, without consideration for the end task besides tuning GAN hyper-parameters. If the initial policy is very brittle (as convincingly shown in section 2), then just mapping to the source domain does not improve the generalization capabilities of the Deep RL algorithm, or even improves transfer learning: it just enables the policy to be used in other contexts that can be reduced to the training one (which is independent of the learning algorithm, RL or otherwise). So it is unclear whether the main contribution is the one claimed. The contribution seems instead an experimental observation that it might be easier to reduce related domains to the training one instead of retraining a new (specialised and brittle) policy. Existing works have actually gone further, learning jointly the image translation and task network, including for very challenging problems, e.g. in unsupervised sim-to-real visual domain adaptation (e.g., Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks from Bousmalis et al at CVPR'17, which is not cited here).

Experimental protocol:
The experimental conclusions are not clear and lack generality, because the optimal methods (e.g., choice of GAN, number of iterations) vary significantly depending on the task (cf. Table 3 for instance). Furthermore, the best configurations seem selected on the test set for every experiment.

Data efficiency vs actual training efficiency:
The main claim is that it is better to do image translation instead of fine-tuning or full re-training. The basis of that argument is the experimentally observed need for less frames to do the image translation (Table 2). However, it is not clear that training GANs for unsupervised image translation is actually any easier / faster. What about training instability, mode collapse, hyper-parameter tuning, and actual training time comparisons on the same hardware?



# Recommendation

Using image translation via GANs for unsupervised domain adaptation is a popular idea, used in the context of RL for Atari games here. Although the experiments show that mapping a target visual domain to a source one can enable reusing a deep RL policy as is, the qualitative results suggest this is in fact due to a bias in the data used here and the experimental protocol does not yield general insights. Furthermore, this approach is not specific to RL and its observed generalization issues. It does not improve the learning of the policy or improve its transferability, thus having only limited new insights compared to existing approaches that jointly learn image translation and target task-specific networks in much more challenging conditions.

I believe this submission is at the start of an interesting direction, and requires further work on more challenging tasks, bigger domain gaps, and towards more joint training or actual policy transfer to go beyond this first set of encouraging but preliminary results.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkg8xwAHT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' response to Reviewer1 - part 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkxjnjA5KQ&amp;noteId=Bkg8xwAHT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper739 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper739 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your time in reviewing our paper.

Discrepancy between quantitative and qualitative results:
While we agree that the transferred agent does not perform perfectly, we disagree that it succeeds “only due to biases in the data”. We note that (a) staying in the middle of the road is, in fact, a learned policy that took the RL agent many (a few million) game interaction frames to acquire; and (b) the agent does more than just driving at the middle of the road: it also occasionally steers to avoid obstacles, and, when hitting an obstacle such as a car, it does not immediately crash but actually succeeds to recover from the hit and keep driving in many cases. This latter behavior (recovering from hits) is also a learned skill that was discovered by the RL policy when training on the first level of the game, and which again took the agent millions of frames (and tens of thousands of crashes) to acquire. We do not consider these behaviors as “biases in the data” but rather behaviors needed to play the game. We note that, as stated in the paper, RL training requires 10M-15M iterations to achieve 5000 points in levels 2 and 4, and substantially more than that in level 3. Using our method the agent is able to apply the abilities it learned during training on level 1 (achieving scores of 5350, 5350 and 2050) after only hundred of thousands of GAN iterations and very few additional game interactions -- a significant improvement to the millions or tens of millions interaction frames needed when using RL training. Moreover, as can be seen in the videos that when using fine-tuning the agent completely fails - it crashes when hitting the sideways, even when the road is wide. Therefore, a method that overcomes this obstacle and others is considered a success. We also note that visually translating a narrow road to a wide one is, in fact, a very good strategy, provided that the position of the car on the road is correctly kept. Dealing with curve roads is more challenging, as the original agent indeed hardly observed any in its training.

Regarding the deficiencies in GAN training: in Breakout, we train the model to translate images where most bricks exist and it successfully accomplishes that for similar images during testing. The problem occurs when the generator is introduced with images where many bricks are missing, images that are different from the ones it has seen during training. More generally, we indeed observed that the CycleGAN is much harder to train on seemingly easy tasks such as the breakout transfer compared to results on natural images (or even for the road-fighter game). We consistently find that such tasks are harder for GANs. The lack of diversity in our game-based datasets makes it easier for the model to overfit comparing to the datasets used in the original papers. Moreover, the fine-grained details are both much more important in the game-transfer setting *and* are easier to notice for a human observer, compared to the natural “horse to zebra” transfer where deficiencies in the background are easier to miss and easier to forgive. 

Does not address the RL generalization issues:
Our paper demonstrates how the generalization problem exists in model-free deep RL algorithms due to an extreme reliance on visual details, and proposes a way to decouple the visual reliance to some extent by performing a visual mapping between related tasks. This helps to transfer the obtained non-visual knowledge across tasks. Our approach isn’t meant to solve the generalization problem, but given an overfitted model it’s a novel way to still be able to benefit from previous learning when learning a new task. We pointed out the issue of generalization and overfitting to explain the motivation for transfer approaches in this field.

With regards to works such as Bousmalis et al (<a href="https://arxiv.org/abs/1612.05424)" target="_blank" rel="nofollow">https://arxiv.org/abs/1612.05424)</a> - we note that the RL setting is different than the static classification one. We did try to improve the GAN generation by adding a loss component comparing the A3C classifier results on the original and translated image, and several other approaches. These additions did not improve the results and in some cases, the results were better without them. In RL tasks, in comparison to the tasks mentioned in the Bousmalis et al, the examples are revealed only as the agent improves and goes further in the task. For this reason, we can only collect data from the early stages of the game where the optimal actions are, for example, to stay in the middle of the road and drive fast (in Road Fighter) - the exact abilities we achieved without it. The obstacles only appear in further stages, which is why this and similar domain adaptation works would not benefit to the zero-shot transfer we aim to gain. We will mention Bousmalis et al in the paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ryx5jL0STm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' response to Reviewer1 - part 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkxjnjA5KQ&amp;noteId=ryx5jL0STm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper739 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper739 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Experimental protocol:
We agree that selecting configurations based on the test set is far from ideal, but we also note that this is the de-facto standard in video game-playing RL works, so we do not believe our work is any worse than others in the literature in this regard. 
Regarding “optimal methods (e.g., choice of GAN, number of iterations) vary significantly depending on the task” - indeed, in current GANs works, configurations (and even pre-processing in many cases) change between tasks. Given the amount of interest in GAN research, we expect this aspect to improve over time. However, we stress that in the Experiments section (Section 4) we test our approach using UNIT-GAN *only*. We test different GANs only in Section 5 as we evaluate them by comparing their results on our tasks.

Data efficiency vs actual training efficiency:
The training process of the actor-critic algorithms mainly depends on CPU where the GAN is trained using GPU therefore, you cannot compare the time on the same hardware. We agree that GANs today still suffers from many issues and are not stable enough, we also mention some of their limitation in Section 4. As noted above, we expect these aspects of GANs to improve. Despite the limitations, this method still manages to succeed in most tasks and clearly shows how transfer can be achieved by a visual mapping.

More generally, we presented a novel transfer approach for model-free RL that decouples the visual transfer from the policy transfer, by using unaligned GANs. While the approach is not perfect (and we explicitly discuss many of the points raised by rev1 in the paper), the method is effective, and, to our knowledge, has not been proposed or demonstrated to work before in the context of RL. Rev1 seems to dislike the fact that we did not address the “core” problems of model-free RL directly, but rather proposed a “workaround” in the form of GAN-based mapping which is external to the RL process. In contrast, we see precisely this separation as the main idea and strength of our proposal: we let the agent re-use its learned policy by helping it map the new environment to its “previous experiences”. Furthermore, rev1 dismisses our reported quantitative results because, in their opinion, they are not reflected in the videos. However, we argue that both videos reflect the success of our approach -- the Breakout video clearly shows how the agent follows the learned policy perfectly and the Road Fighter video demonstrate how the agent applies the learned techniques from level 1 in each of the successive levels. It is possible that future iterations of the idea will improve on our method with more complex machinery, and we look forward to seeing others expand on our research.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJxwOUCrpm" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkxjnjA5KQ&amp;noteId=BJxwOUCrpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper739 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>