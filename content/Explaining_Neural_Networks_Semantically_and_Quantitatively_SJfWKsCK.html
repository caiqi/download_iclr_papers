<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Explaining Neural Networks Semantically and Quantitatively | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Explaining Neural Networks Semantically and Quantitatively" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SJfWKsC5K7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Explaining Neural Networks Semantically and Quantitatively" />
      <meta name="og:description" content="This paper presents a method to explain the knowledge encoded in a convolutional neural network (CNN) quantitatively and semantically. How to analyze the specific rationale of each prediction made..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SJfWKsC5K7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Explaining Neural Networks Semantically and Quantitatively</a> <a class="note_content_pdf" href="/pdf?id=SJfWKsC5K7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019explaining,    &#10;title={Explaining Neural Networks Semantically and Quantitatively},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SJfWKsC5K7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">This paper presents a method to explain the knowledge encoded in a convolutional neural network (CNN) quantitatively and semantically. How to analyze the specific rationale of each prediction made by the CNN presents one of key issues of understanding neural networks, but it is also of significant practical values in certain applications. In this study, we propose to distill knowledge from the CNN into an explainable additive model, so that we can use the explainable model to provide a quantitative explanation for the CNN prediction. We analyze the typical bias-interpreting problem of the explainable model and develop prior losses to guide the learning of the explainable additive model. Experimental results have demonstrated the effectiveness of our method.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Network interpretability, deep learning, knowledge distillation, convolutional neural networks</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">This paper presents a method to explain the knowledge encoded in a convolutional neural network (CNN) quantitatively and semantically.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_H1e4CKdc3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting area but quite misleading on explanation vs. interpretation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJfWKsC5K7&amp;noteId=H1e4CKdc3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper420 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 18 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper420 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=H1e4CKdc3m" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
Summary:
=========
The paper proposes a technique to explain the embedding space quantitatively and semantically by distilling learned representations into an additive explainable model. Explanation generation for deep neural network models is a growing research area especially for mission critical applications that decision without explanation is unacceptable such as security and medical applications. More importantly, the focus on semantic explanations instead of (in addition to) the usual ‘visual attention or pixel level correlation as explanation’ is interesting. Although, the proposed method is interesting, I have several concerns. I have laid out my comments on the strength of the paper and my concerns as follows.

Strength:
======== 
- Distilling the representation into a separate model to explain rather than incorporating the explanation generation into a single model helps minimize performance degradation due to the additional constraint of generating explanation.

- The introduction of semantic explanations is interesting, although the ‘explanation’s are not human-like explanations. See below for further discussion on this.

Weakness:
=========
- The abstract could use a bit simplification and clarification to make it concise at the same time be inclusive of the key points of the proposed method. More specifically, the second sentence could be simplified to make it easy to follow.

- The need for explanation was not strongly motivated in the introduction in the context of existing literature. Existing explanation generating and/or interpretable networks should be discussed and the proposed method should be presented with what gap in existing literature it is trying to address in a more concise form. It is a bit lengthy in current form.

- Need to cite related literature on making claims such as “…Distilling knowledge from a pre-trained neural network into an additive model usually suffers from the problem of bias-interpreting.”

- The paper mixes interpretation with explanation. Interpretability and explainability are two different things. Invoking attributes of explanations in human cognition, generated explanations should mimic human explanations in that they should be human-understandable, concise, and at the right level of detail. What the introduction promised as semantic explanation was not delivered in the experiments. Explanations for why the network predicts a particular face as attractive are not laundry list of other attributes with positive and negative correlations. Explanations are composed. The authors are learning the inter-attribute relationship (correlations) and this is presented as ‘explanation’. This is disappointing. The title and other earlier discussions need to be revised not to confuse simple interpretation with explanation.

- The ‘explanations’ are not as generic as they were advertised earlier. They are in fact specific to specific tasks. It is not obvious from the experiments on object parts and facial attributes tasks how this interpretation using correlation could be generalized to any task learned through a CNN.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HygFgXuOn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Some question about the weight for the i-th visual concept.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJfWKsC5K7&amp;noteId=HygFgXuOn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018</span><span class="item">ICLR 2019 Conference Paper420 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1. I am wondering what the range of \alpha_i(I) is.  Do you use the softmax or sigmoid after the network g?
2. As you shown in the paper, you network is very simple to do classification. But you use a 152 layers to estimate weights for g visual concept? It's so strange here.

Hope you answer me asap. Thank you.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJx0WYKunm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJfWKsC5K7&amp;noteId=rJx0WYKunm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper420 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018</span><span class="item">ICLR 2019 Conference Paper420 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for your comments.

Q1.  I am wondering what the range of \alpha_i(I) is.  Do you use the softmax or sigmoid after the network g?

A: A good question. Please find the answer in the paragraph just before Section 3.1.

We have introduced two typical application cases in Section 3.1 and Equation (4). In the first case, we do not need to ensure \alpha_i to be positive, i.e., there may exist a negative relationship between the target output and a certain visual concept. In this case, \alpha_i is simply the output of an FC layer.

In the second case, we need to ensure positive relationships between the target output and all certain visual concepts. Then, we set \alpha= log[1 + exp(x)] (Please see the paragraph just before Section 3.1).

Theoretically, there is no reason to limit the range of alpha to a small range, e.g., 0&lt;alpha&lt;1 and -1&lt;alpha&lt;1. It is because we did not normalize output scores of visual concepts.


Q2: As you shown in the paper, your network is very simple to do classification. But you use a 152 layers to estimate weights for g visual concept? It's so strange here.

A: A good question. It is not strange that the network g for explanation is much more complex than the network for classification. It is because explaining the rationale of a network is usually much more difficult than using the network for classification. Compared to the classification net, the explainer network g uses much restricted input data to mimic the logic of the classification net.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1eWu1OU27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The content must be re-organized and a deeper evaluation is needed</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJfWKsC5K7&amp;noteId=B1eWu1OU27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper420 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper420 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value"> 
=============================
SUMMARY
=============================

The paper proposes a method to address the task of model explanation for deep convolutional neural networks (CNNs). More specifically, given a (performer) model to be explained, it proposes to use pre-trained detectors of visual concepts as means to justify the the predictions made by the performer through an additive (explainer) model. This additive model operates on top of the response, i.e. the score, produced by the concept detectors and its weights are learned also through a deep model, i.e. a ResNet-152. 

 The proposed method is evaluated on two specific explanation tasks. The first task is related to explaining the occurrence of an object in an image (object recognition) based on object parts. Experiments on this task are conducted on the Pascal-Parts dataset and cover various standard CNN architectures including AlexNet and various variants of VGG (VGG-S, VGG-M and VGG-16). The second task is related to the prediction of face attributes based on other attributes. Experiments on this task are conducted on the CelebFaces Attributes dataset and are based on the VGG-16 CNN architecture.
 Performance on both experiments is reported in terms of prediction accuracy and relative deviation, which compare the performance of the performer model with that of the explainer model.
 
=============================
STRENGHTS
=============================

The manuscript proposes to produce explanations derived from semantic concepts. Adding semantic concepts to visual explanations is a desirable characteristic in order to reduce possible ambiguity that may occur when highlighting image regions with strong influence on the decision made by the model. 

The related work section of the manuscript seems is quite detailed and covers a good amount of work from the literature.

=============================
WEAKNESSES
=============================

The content of the manuscript is not properly balanced. On the one hand, the starting sections are quite detailed and extended. On the other hand, the last sections of the are quite reduced. This affects the flow of the manuscript, which has a good start but results in an abrupt end.

Similar to the network dissection method (Bau et al, CVPR'17), the proposed method has the requirement of a set of visual concepts to be pre-defined. A suboptimal selection of these concepts will result in a set not representative or not sufficiently general concepts which will produce meaningless explanations. That is the possible set of explanations is bounded by the set of selected concepts.
In addition, the proposed has the strong requirement of pre-trained detector for each of these visual concepts.

In several parts of the manuscript, i.e Section 1 and 5, it is claimed that the proposed method allows to provide a semantic explanation of the logic followed by the CNNs in order to make predictions. At this point it is not clear to me how the proposed method effectively mimics the decision-making process followed by the performer models to reach a particular decision.

In the experiment from Section 4.1 it is stated that several CNN types (AlexNet, VGG-S, VGG-M and VGG-16) are considered. However, besides depth and spatial resolution of some of the layers, all these models are feed-forward CNN architectures. I would be more convinced if CNNs and perhaps another type of architecture, e.g. a RNN, were considered.

In Section 3.1, details regarding the computation of prior weights are provided. However, a closer analysis of this Section in conjunction with Section 4 suggest that the computation of this priors is tailored for the specific explanation tasks covered in the experiments conducted in Sections 4.1 and 4.2.
In addition, for the explanation tasks of Sections 4.1 and 4.2, a very suitable set of visual concepts were pre-defined.
Perhaps I am missing something, but these two points make wonder about the generability of the proposed method.

In the quantitative evaluation from Section 4.3 the proposed method is only compared against a single baseline, i.e. the distillation loss to learn the explainer. 
This evaluation seems quite limited when taking into account the significant amount of visual explanation methods that have been proposed recently and the fact that distillation loss used in the compared baseline is not necessarily designed for model explanation.
In addition, the discussion of results covered in Section 4.3 is quite reduced. The manuscript would definitively benefit from an extended discussion of its inner-workings (an ablation  study) and an extended comparison w.r.t. recent methods.
Furthermore, I would recommend complementing the presented experiments with evaluations following protocols proposed in existing work, e.g. a) occlusion analysis (Zeiler et al., ECCV 2014, Samek et al.,2017), a pointing experiment (Zhang et al., ECCV 2016), or c) a measurement of explanation accuracy by feature coverage (Oramas et al. arXiv:1712.06302).

Finally, there are several parts of the manuscript pointing towards the appendix. Some of these pointers, e.g. Sec. 4.3, link toward important content that should be on the main manuscript. Having inspected the appendix, it seems a good amount of experiments were conducted and significant amount of qualitative results are available for the proposed method. In my opinion the manuscript would benefit significantly from moving some of this content to the main manuscript and discussing it adequately.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ryxWIxNk27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>a question</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJfWKsC5K7&amp;noteId=ryxWIxNk27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Nancy_Lu1" class="profile-link">Nancy Lu</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018</span><span class="item">ICLR 2019 Conference Paper420 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">it is interesting, can this method also explains object segmentation and image generation?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BklZ7stu3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJfWKsC5K7&amp;noteId=BklZ7stu3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper420 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018</span><span class="item">ICLR 2019 Conference Paper420 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you. As introduced in the second paragraph, we assume the output of the network is a scalar in this study. Nevertheless, we can still extend the proposed method to networks with high-dimensional outputs, such as networks for object segmentation and image generation, without loss of generality. In this case, alpha will not be a vector, but a tensor with the same size of the output of the target network. Thus, the norm in Equation (2) is a Frobenius norm of a tensor.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SyxDp1dviX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting task and simple approach but experiments are not formulated well and are not convincing</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJfWKsC5K7&amp;noteId=SyxDp1dviX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper420 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper420 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This work proposes to explain classification decisions via quantitative estimates of the contributions of individual visual concepts to the classification score. It learns an explainer model on top of a performer model (as distillation); the explainer returns the contributions of different concepts. To prevent the model from only selecting a few concepts for the explanation, the authors use a prior-enforcing loss. 

The approach is clear but evidence that the proposed work will have an impact in practice is not convincing. The quantitative results don't show the proposed method is useful. Table 1 shows entropy over concept contributions, but the authors don't convince the reader that this is the right metric to use. They argue using too few concepts (which of course leads to low entropy) is undesirable, but there is no evidence/proof why this is undesirable. Table 2 uses prediction accuracy, which does not seem to be in line with explainability. The qualitative figures are somewhat interesting, but they are not sufficient. It is easy to look at a qualitative figure and say "sure, this makes sense" but this does not mean the proposed method is useful, especially when there is no result from a baseline, whose output might also make perfect sense. Finally, the authors claim pixel-level visualizations aren't useful, but there's no proof of why that is. A user study of what's useful, or a quantitative evaluation of the usefulness of different visualizations for downstream tasks, would be good. 

In terms of writing, the first few pages tend to be repetitive yet vague about what exactly will be done (generally ok for introduction, but a bit too vague). It is not clear what "subjectively visualizing" means. Finally, I found the end of second para in Sec. 3 unclear.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>