<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Adaptive Mixture of Low-Rank Factorizations for Compact Neural Modeling | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Adaptive Mixture of Low-Rank Factorizations for Compact Neural Modeling" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=r1xFE3Rqt7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Adaptive Mixture of Low-Rank Factorizations for Compact Neural..." />
      <meta name="og:description" content="Modern deep neural networks have a large amount of weights, which make them difficult to deploy on computation constrained devices such as mobile phones. One common approach to reduce the model..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_r1xFE3Rqt7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Adaptive Mixture of Low-Rank Factorizations for Compact Neural Modeling</a> <a class="note_content_pdf" href="/pdf?id=r1xFE3Rqt7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019adaptive,    &#10;title={Adaptive Mixture of Low-Rank Factorizations for Compact Neural Modeling},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=r1xFE3Rqt7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Modern deep neural networks have a large amount of weights, which make them difficult to deploy on computation constrained devices such as mobile phones. One common approach to reduce the model size and computational cost is to use low-rank factorization to approximate a weight matrix. However, performing standard low-rank factorization with a small rank can hurt the model expressiveness and significantly decrease the performance. In this work, we propose to use a mixture of multiple low-rank factorizations to model a large weight matrix, and the mixture coefficients are computed dynamically depending on its input. We demonstrate the effectiveness of the proposed approach on both language modeling and image classification tasks. Experiments show that our method not only improves the computation efficiency but also maintains (sometimes outperforms) its accuracy compared with the full-rank counterparts.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Low-Rank Factorization, Compact Neural Nets, Efficient Modeling, Mixture models</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a simple modification to low-rank factorization that improves performances (in both image and language tasks) while still being compact.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rylcGdES6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Small improvement but breaks a single efficient computation to multiple computation segments (no wall-clock time reported)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1xFE3Rqt7&amp;noteId=rylcGdES6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1472 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1472 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes an input-dependent low rank approximations of weight matrices in neural nets with a goal of accelerating inference. Instead of decomposing a matrix (a layer) to a low rank space like previous work did, the paper proposes to use a linear combination/mixture of multiple low rank spaces. The linear combination is dynamic and data dependent, making it some sense of non-linear combination. The paper is interesting, however,
I doubt its significance at three aspects:
(1) computation efficiency: the primary motivation of this paper is to accelerate inference stage; however, it might not be wise to break computation in a single low-rank space to segments in multiple low-rank spaces. In the original low-rank approximation, only two matrix-vector multiplications are needed, but this paper increases it to 2*K plus some additional overheads. Although the theoretical FLOPs can be cut, but modern hardware runs much faster when a whole bulk of data are computed all together. Because of this, the primary motivation in this paper wasn't successfully supported by wall-clock time;
(2) low-rank approximation: low-rank approximation only makes sense when matrices are known and redundant, otherwise, no approximation target exists (i.e., what matrix is it approximating?). Because of this, low-rank neural nets [1][2] start from trained models, approximate it and fine-tune it, while this method trains from scratch without an approximation target. Although, we can fit the method to approximate trained matrices, then decomposing a matrix to a mixture of low-rank spaces is equivalent to decomposing to one single low-rank space (the only difference is the combination is data dependent). Therefore, I view this paper more in a research line of designing compact neural nets, which brings me to a concern in (3).
(3) efficient architecture design: essentially, the paper proposes a class of compact neural nets, at each layer of which there are K "low-rank" branches with a gating mechanism to select those branches. However, branching and gating is not new [3][4]. I like the results in Table 2, but, to show the efficiency, it should have been compared with more SOTA compact models like CondenseNet. 
 
Clarity:
How FLOPs reduction are exactly calculated? I am not convinced by FLOPs reduction in the LSTM experiments, since in LSTM (especially in language modeling), FLOPs in the output layer are large because of a large vocabulary size (650x10000 in the experiments). However, output layer is not explicitly mentioned in the paper.

Improvement:
(1) Accuracy improvement in Table 1 is not statistically significant, but used more parameters. For example, an improvement of 93.01% over 92.92% is within an effect of training noise;
(2) It is a little hacking to conclude that a random matrix  P_random has a small storage size because we can storage a seed for recovery. When we deploy models across platforms, we cannot guarantee they use the same random generator and has a consistent implementation;
(3) sparse gating of low-rank branches may make this method more computation efficient.

[1] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. In Advances in Neural Information Processing Systems (NIPS). 2014.
[2] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up convolutional neural networks with low rank expansions. In Proceedings of the British Machine Vision Conference (BMVC), 2014.
[3] Szegedy, Christian, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. "Going deeper with convolutions." In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9. 2015.
[4] Huang, Gao, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Q. Weinberger. "Multi-scale dense networks for resource efficient image classification." arXiv preprint arXiv:1703.09844 (2017).</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1e1aJRdTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarifications: our method does not break a single efficient computation to multiple computation segments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1xFE3Rqt7&amp;noteId=S1e1aJRdTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1472 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1472 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the time and detailed and valuable comments. Please find our response below.


- computation efficiency: in the original low-rank approximation, only two matrix-vector multiplications are needed, but this paper increases it to 2*K plus some additional overheads. Although the theoretical FLOPs can be cut, but modern hardware runs much faster when a whole bulk of data are computed all together.

Thanks for the detailed analysis. We would like to point out that the reviewer’s analysis is based on a specific type of implementation of our method, where K matrix-vector multiplications are conducted. However, this implementation can be easily optimized (especially when we set K to rank where the rank is small): we can still use two matrix-vector multiplications, with a smaller matrix-vector multiplications to compute the mixing weights, and a vector-vector multiplication for weighting bottleneck. This avoids 2*K matrix multiplications. The proposed implementation supports massive parallel computing and also has good data locality, thus it is very efficient to compute.

We willingly admit that the current paper mainly focus the FLOPs, as the actual inference time depends on implementation, and also include runtime by other factors (such as final softmax layer in RNN language modeling). Nevertheless, we conducted some preliminary experiments with non-optimized implementation on RNN (without final softmax), and measure the actual inference time with CPU as shown in the table below. We observed that the proposed method still provides similar speedup as regular low-rank (while getting significantly better accuracy/perplexity).

-----------------------------------------------------------------
  Method      |      low-rank ratio; time in ms
-----------------------------------------------------------------
                      |    1   |  1/2  |  1/4  | 1/8 | 1/16 
-----------------------------------------------------------------
Full  Rank     | 10.8 | N/A  | N/A | N/A | N/A
regular LR    | N/A  | 13.1 | 6.6  | 3.3   | 2.0
adaptive LR  | N/A | 16.5  | 8.6  | 4.5   | 2.8
-----------------------------------------------------------------

We would like to improve our implementation of the adaptive low-rank to further speed up the actual inference time, and conduct more run-time comparisons in the future.


- low-rank approximation: decomposing a matrix to a mixture of low-rank spaces is equivalent to decomposing to one single low-rank space (the only difference is the combination is data dependent).

While we appreciate that the reviewer’s careful observation, especially on that our method can also be applied to the scenarios where the weight matrices are pre-trained. But we would like to point out a inaccurate assertion that our method is equivalent to regular low-rank factorization in [1][2]. Essentially, as shown in proposition 1, our method is non-linear transformation while the regular low-rank is linear. They are not equivalent theoretically, and in practice, as explicitly demonstrated in the toy example (Figure 1), the proposed method is much more expressive than regular low-rank, meaning it can approximate better with around the same amount of parameters and computation. Our experiments on RNN and CNN also show that adaptive low-rank enjoys better expressiveness than conventional low-rank decomposition.


- efficient architecture design: branching and gating is not new [3][4]. I like the results in Table 2, but, to show the efficiency, it should have been compared with more SOTA compact models like CondenseNet. 

Thanks for the multi-angular analysis. In our understanding, [3] proposes GoogleNet using static branching, and [4] proposes a dynamic network based on input samples. These two papers are orthogonal to our method. We aim to propose an simple yet expressive low-rank decomposition method to speed up the inference of matrix multiplication, which is the fundamental operation in modern neural networks. Therefore, we can apply the method to any existing network architectures.

Regarding empirical evaluation, we aim to show our adaptive low-rank is better than regular low-rank method in our experiments, so we used a very similar yet powerful architecture MobileNet to exclude the interference of other factors, making sure it is a fair comparison. We aim to improve low-rank decomposition itself that is general but not to design a specific compact network architecture. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkg8CkAupQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our clarification (cont'd)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1xFE3Rqt7&amp;noteId=Bkg8CkAupQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1472 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1472 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">- In LSTM (especially in language modeling), FLOPs in the output layer are large because of a large vocabulary size (650x10000 in the experiments). However, output layer is not explicitly mentioned in the paper.

Thanks for point this out. We only calculate the FLOPs of LSTM layer, and do not count the FLOPs in output softmax layer. This is because we only apply the low-rank factorization on the LSTM layers to demonstrate adaptive low-rank enjoys better expressiveness.  We will explicit mention it in the revision. The output softmax is unarguably computation hungry, and can be quite challenging to tackle by itself (there are papers like SVD-softmax trying to tackle this problem), thus we do not include its discussion in this paper.


- Accuracy improvement in Table 1 is not statistically significant, but used more parameters. 

CIFAR dataset is a relatively small and simple dataset, which does not need huge network capacity. Our experiments also show that when the rank is high, both method suffers from little accuracy drop. However, when the rank is low, our method significantly outperforms regular low-rank. Furthermore, the effectiveness of our method is also tested on larger and more convincing dataset ImageNet (in Table 2), where the performance is even more significant.


- When we deploy models across platforms, we cannot guarantee they use the same random generator and has a consistent implementation.

Yes, we agree the random seed generation can be different across platforms. However, the random projection is just a simple yet interesting comparison for us to better understand the problem. Our main results focus on the one using learned mixing weight computation.


- sparse gating of low-rank branches may make this method more computation efficient.

Yes, we agree. And we would like to note that our default setting is to set the number of mixtures to the rank so currently it can also be seen as gating, computational wise.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_r1lHGXH0hX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The paper introduces a low rank factorization strategy for neural network compression. They propose a data adaptive model to approximate the weights as a learned mixture of low rank factorizations. The method is novel and the results look promising. A limitation of the method presented is that its is applicable only to weights arising as mode-2 tensors (matrices).</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1xFE3Rqt7&amp;noteId=r1lHGXH0hX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1472 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1472 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Some suggested improvements follow below

1. It is claimed (page 2, last paragraph) that the proposed method leads to a 3.5% and 2.5% improvement in top-1 accuracy over the mobilenet v1 and v2 models. However the results in table 2 indicate 2.5% and 1.4% improvement. This should be corrected.
2. The authors should include the performance of the full rank CNN for the toy example in Figure 1. A Neural Net with 2 neurons in the hidden layer can not learn the XOR/XNOR efficiently . So its rank-1 factorization can only perform as good as the original CNN.
3. In (1), the dimensions of U^k and V^k should be mentioned explicitly.
4. The choice of “k” in (1) should be discussed. How does it relate to the overall accuracy / compression of the CNN?
5. The paper addresses low rank factorization for “MLP”, RNN/LSTM and “pointwise” convolutions. All of these have weights in the form of matrices (mode 2 tensors). The extension to mode-3 and and mode-4 tensors which are more common in CNNs is not straightforward.
6. In the imagenet experiment, the number of mixtures (k) is set to the rank (d). How is the rank computed for every layer?
7. In Fig 7, row 0 and row 8 look identical. Is this indicative of something?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJeU9eAupQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1xFE3Rqt7&amp;noteId=SJeU9eAupQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1472 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1472 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank the reviewer for the time and valuable comments. Please find our response below.


- The results in table 2 indicate 2.5% and 1.4% improvement. This should be corrected.

Thanks for pointing out our typos on the improvement rates. The correct ones should be (1) (70.5-68.8)/68.8=2.5% and (2) (73.1-71.7)/71.7=1.95%. We will update them in the revision.


- The authors should include the performance of the full rank CNN for the toy example in Figure 1. A Neural Net with 2 neurons in the hidden layer can not learn the XOR/XNOR efficiently . So its rank-1 factorization can only perform as good as the original CNN.

We’d like to clarify the toy example in Figure 1, the input data point is 2-dimensional, and a MLP (not CNN) is used as function to classify the labels, i.e. P(y|x) = softmax(W’σ(Wx)), where W ∈ R^{2×2}. This is the original full rank model, which is able to effectively learn the synthetic XOR/XNOR task. However, when we factorize W using two 2×1 matrices, i.e. W = UV^T, the induced linear bottleneck largely degenerates the performance (Figure 1b). After applying the proposed method, the performance can be largely improved (Figure 1c).


- In (1), the dimensions of U^k and V^k should be mentioned explicitly.

Thanks for the suggestion, we will mention it in the revision.


- The choice of “k” in (1) should be discussed. How does it relate to the overall accuracy / compression of the CNN?

The discussion of k is presented in the experiments. We tested different K, and found that using more mixtures generally leads to better results, although the performance starts to plateau when the number of mixtures is large enough. However, to obtain a larger compression rate and speedup, the rank-d we use in the low-rank factorization can be already small, thus the extras of using different number of mixtures may not differ too much. For this reason, we can just set K as rank-d.


- The extension to mode-3 and and mode-4 tensors which are more common in CNNs is not straightforward.

Thanks for the questions. We willingly acknowledge that we only considered low-rank factorization of 2d matrices in the scope of this work, which has already found applications in many deep neural network scenarios. For CNNs (which was targeted in this work), we apply our method with widely-used compact depth-separable convolution layers such that it does not require a direct mode-3 tensor factorization. 

We also believe it could be straightforward to extend this framework to high-order tensor factorization with minor adjustments. For example, we could apply our method to CP decomposition (<a href="https://en.wikipedia.org/wiki/Tensor_rank_decomposition)," target="_blank" rel="nofollow">https://en.wikipedia.org/wiki/Tensor_rank_decomposition),</a> simply by extending each mixture from two low-rank vector products to three low-rank vector products.


- In the imagenet experiment, the number of mixtures (k) is set to the rank (d). How is the rank computed for every layer?

To ensure a fair comparisons with MobielNets, we simply followed the setting in the original MobileNetV2 paper by setting the number of channel of bottleneck to be ⅙ of the output channels.


- In Fig 7, row 0 and row 8 look identical. Is this indicative of something?

In Fig 7, the labels for each row are in the same order as in original CIFAR-10 dataset, namely "airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck". The row 0 and the row 8 correspond to the class of airplane and the class of ship respectively, which suggests the learned mixtures are class discriminative.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1g0zh_7hQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice intuitive paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1xFE3Rqt7&amp;noteId=B1g0zh_7hQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1472 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1472 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this paper, the authors propose a compression technique to reduce the number of parameters to learn in a neural network without losing expressiveness.
The paper nicely introduces the problem of lack in espressiveness with low-rank factorizations, a well-known technique to reduce the number of parameters in a network.
The authors propose to use a linear combination of low-rank factorizations with coefficients adaptively computed on data input. Through a nice toy example based on XNOR data, they provide a good proof of concept showing that the accuracy of the proposed technique outperforms the classical low-rank approach.
I enjoyed reading the paper, which gives an intuitive line of reasoning providing also extensive experimental results on multilayer perceptron, convolutional neural networks and recurrent neural networks as well.
The proposal is based on an intuitive line of reasoning with no strong theoretical founding. However, they provide a quick theoretical result in the appendix (Proposition 1) but, I couldn’t understand very well its implications on the expressiveness of proposed method against classical low-rank approach.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HyxyQ-C_6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for your feedback!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1xFE3Rqt7&amp;noteId=HyxyQ-C_6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1472 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1472 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank the reviewer for the time and valuable feedback. We also like to add on the implication of proposition 1, which demonstrates that the mixture of low-rank factorizations are actually learned non-linear transformation, which is more expressive than the linear one of regular low-rank factorization, i.e. the former cannot be approximated by the latter. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>