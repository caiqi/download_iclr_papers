<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Advanced Neuroevolution: A gradient-free algorithm to train Deep Neural Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Advanced Neuroevolution: A gradient-free algorithm to train Deep Neural Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=r1g5Gh05KQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Advanced Neuroevolution: A gradient-free algorithm to train Deep..." />
      <meta name="og:description" content="In this paper we present a novel optimization algorithm called Advanced Neuroevolution. The aim for this algorithm is to train deep neural networks, and eventually act as an alternative to..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_r1g5Gh05KQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Advanced Neuroevolution: A gradient-free algorithm to train Deep Neural Networks</a> <a class="note_content_pdf" href="/pdf?id=r1g5Gh05KQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019advanced,    &#10;title={Advanced Neuroevolution: A gradient-free algorithm to train Deep Neural Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=r1g5Gh05KQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">In this paper we present a novel optimization algorithm called Advanced Neuroevolution. The aim for this algorithm is to train deep neural networks, and eventually act as an alternative to Stochastic Gradient Descent (SGD) and its variants as needed.We evaluated our algorithm on the MNIST dataset, as well as on several global optimization problems such as the Ackley function. We find the algorithm performing relatively well for both cases, overtaking other global optimization algorithms such as Particle Swarm Optimization (PSO) and Evolution Strategies (ES).
</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Evolutionary Algorithm, Optimization, MNIST</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A new algorithm to train deep neural networks. Tested on optimization functions and MNIST.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">13 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Hye8WzS927" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Poorly motivated, vague, and low-quality.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1g5Gh05KQ&amp;noteId=Hye8WzS927"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1292 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1292 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: The authors propose a type of black box (gradient free) optimization algorithm for training neural networks. The method is a type of genetic algorithm (similar to particle swarm optimization), with particular choices for how to generate new proposals (perturbations) and how to combine them at every iteration (generation). The authors apply their method to some standard non-convex low-dimensional test functions, as well as to train a CNN on MNIST.

Major concerns: There are serious issues with the quality, clarity, originality and significance of this work.

First and foremost, the ideas are vague and poorly motivated. In the abstract, the authors propose that their algorithm will be used to train deep networks, and "eventually act as an alternative to Stochastic Gradient Descent (SGD)". However, this overlooks theory in the optimization literature on how convergence rates for first-order (gradient) methods are *independent* of the parameter dimension. This amazing fact is why they are so powerful for solving high-dimensional optimization problems. On the other hand, gradient-free search methods are not independent of the parameter dimension, and slow down considerably when the effective parameter dimension is large. From the abstract, and parts of the text, it seems as if the authors think that gradient-free algorithms can be used to train neural networks even when gradient information is available (e.g. for training image recognition models, like their example application of training a CNN on MNIST). However, there are fundamental theoretical reasons why this is not a good idea, which are glossed over or ignored by the paper.

If the authors want to target problems which are non-differentiable (they reference a couple of examples where gradient-free optimization has been used for non-differentiable problems such as in reinforcement learning or architecture search), then they should apply their algorithm directly to these problems. Related to this point, after referencing papers on gradient-free algorithms applied to reinforcement learning, the authors state that this "reflects the awareness within the broader research community about ... the need for alternatives to SGD." I strongly disagree with this reading of that literature, rather, those papers point out that gradient-free algorithms are useful/competitive with other techniques for a particular domain (RL) which contains non-differentiable optimization problems. Equating that with the field at large needing alternatives to SGD is an over-generalization.

Additionally, it is unclear if the authors are proposing an optimization algorithm particularly for neural networks, or something more general. The state that they are interested in training neural networks, but then why compare their algorithm on low-dimensional test functions (such as Rastrigin or Ackley) which have very a different loss landscape compared to that of neural networks?

When it comes to the algorithm itself, the presentation is vague, introduces unnecessary complexity, and fails to reference significant related work.  The algorithm is introduced using a lot of colloquial language (Elites, Anchors, Probes, and Blends) which are hard to follow. It seems as if, at a high level, the authors are proposing a genetic algorithm--one where there is a procedure to generate new proposal iterates, and a method for combining them to form the next generation. A lot of the particular choices for how to generate these proposals and combine them seem arbitrary (a lot of hyperparameters are introduced). Finally, it is unclear how many function evaluations are required to evaluate one step (generation) of their algorithm, which is critical to comparing performance to other algorithms.

When it comes to comparing performance, there are fundamental flaws with the study. One, the authors compare do not tune hyperparameters for the algorithms they compare against (going as far as to state that "tuning those parameters is beyond the scope of this paper"), but presumably tune the hyperparameters for their algorithm. This results in unfair comparisons. Two, the authors report performance in terms of "generations", but they should be reporting performance in terms of the number of function evaluations, since different algorithms may require more function evaluations (and thus take more time) per iteration/generation. This is particularly problematic for the comparison against SGD, where every step of SGD only requires one forward/backward pass but presumably their algorithm requires many forward passes per "generation". This makes the comparisons that are currently in the paper meaningless.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">1: Trivial or wrong</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJl32NFkpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1g5Gh05KQ&amp;noteId=BJl32NFkpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1292 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1292 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">First, thank you for the time invested in producing a high-quality review. We appreciate the effort it took to analyze and go over the paper. There has been grave misunderstandings and misinterpretations which are detrimental to a fair review. Please allow us to address some of the concerns and comments you raised.

1) We agree 100% that gradient-based methods are independent of the search dimension. Hence thus far they are the main optimization technique used to train deep neural networks (with up to 100s of millions of parameters). This enormous search space is difficult for any algorithm that accounts for each parameter as independent, such as Evolutionary algorithms. However, that is precisely why we introduce our algorithm; to facilitate the navigation of high-dimensional search spaces efficiently (we train a 4.7M parameter CNN). This is challenging, but there are challenges associated with SGD as well, for example, Exploding and Vanishing Gradients.

2) Again, we agree 100% that SGD should be used where it is effective. We have enormous respect for the incredible efforts the research community has put in, and there is no proposition to brush that aside. Where the gradient space is informative, such as in ImageNET, NLP, etc.. SGD should definitely be used. We do in fact state that SGD can be used in tandem with our algorithm as appropriate. For example, if one is training a robot, it would make sense to train the perception module using SGD, and train the control policy using our algorithm. SGD is challenged when applied to some domains such as RL and robot control. That is where we see the application of an algorithm such as ours.

3) It is necessary first to verify our algorithm against simple, well-studied problems before using it on RL problems. That is the reason we decided to run experiments against select Global Optimization problems and MNIST. Each experiment has a different goal. The experiments reveal some areas where we can improve the algorithm, as well as some areas where it is attractive. As an introduction, we think this is not unreasonable. After all, the community didn't attempt to solve RL problems such as Atari games and Mujoco environments using SGD before first solving MNIST.

4) In regards to our interpretation of the literature. There doesn't seem to be any disagreement between our interpretation and yours. We state clearly that "We don’t see our algorithm replacing SGD, especially in fields where it is already quite successful such as Computer Vision".  We do see a need for alternatives only where SGD is challenged, exactly as you mentioned. Thus, the fallacy of "over-generalization" is not true.

5) We developed the algorithm specifically to train deep neural networks. The optimization functions are the Solution Space not the Search Space. The solver, i.e. algorithm, trains the network to predict the location of the global optimum. The different functions reflect different conditions which are challenging for a solver. For example, the Easom function will not give the solver a clue which direction to go to find the optimum, since it is almost flat everywhere. This is a scenario that will be difficult for SGD; we refer to them as saddle points. We wanted to test if the algorithm can train networks to solve those functions, with the imposed constraints such as limited populations.

6) We will review and update citations, particularly with regards to evolutionary algorithms.

7) We agree there are many hyper parameters. In future iterations, those may be simplified.

8) Each generation requires exactly 50 evaluations, this is the same for all the algorithms. The number of evaluations is determined by the pool size. We fix the pool size to 50, for all the experiments. For SGD, we use a batch size of 50. Thus we do compare the number of evaluations fairly. For the MNIST experiment, our algorithm takes 2333 evaluations while SGD takes only 320 evaluations to converge to the same loss. Thus, it is 7.29x slower than SGD on this problem.

9) We did not tune our algorithm for each problem. We use the exact same set of hyper parameters for all the functions, AND the MNIST experiments. We did not change the setting for the library implementations of the algorithms, nor for the SGD implementation in PyTorch (eg. learning rate). Those implementations are not adversarial, however. That is, usually the algorithms are implemented according to the respective publication, or according to a setting that is known to work well in general, empirically derived. Thus, we neither aid nor hamper the compared algorithms.

Again, we thank you most genuinely for the review. It reflects domain knowledge and the effort you've put. When taken at large, we don't see many disagreements between your view and our work; just miscommunications. We take full ownership of this, and will attempt to rectify it by adjusting the narrative in some parts of the paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HygErxL9aQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1g5Gh05KQ&amp;noteId=HygErxL9aQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1292 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1292 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your thorough reply. Responding to your comments:

(1) This is poorly motivated. We have good methods for training large CNNs. If you expect your method to overcome difficulties with exploding or vanishing gradients, you need to test it on problems where exploding and vanishing gradients make training with gradient descent difficult. Moreover, you need to compare your method with other standard methods for dealing with these gradient issues (e.g. gradient clipping). 

(2) Again, the experiments in the paper should address the domain where you expect your method to have the most applicability. Without experiments demonstrating your method in RL, it is harder to verify your claim that the method would be applicable there. You say that this is "where we see the application of an algorithm such as ours"---well, show it!

(3) There are plenty of simple, well-studied problems in RL where you could have applied your algorithm. Applying it to low-dim. global optimization problems and CNNs on MNIST make the motivation and justification for your method unclear. Specifically, it is unclear what specific optimization issues your algorithm overcomes--by applying it to every problem under the sun, you are implicitly giving vague motivation for why someone ought to use this method.

(4) My specific complaint was with the claim that the broader research community needs "alternatives to SGD". It's the way you phrased it, it's too general. There may be specific situations where SGD fails due to poor quality gradients. Then, you need to elucidate exactly how/why the gradients are low quality (for a specific set of problems) and why your method alleviates those issues. Stating that the field broadly needs "alternatives to SGD" is too general.

(5) "We developed the algorithm specifically to train deep neural networks" -- Then why did you test against problems in Table 1, none of which involve neural networks? 

(7) It is difficult to assess the viability of your algorithm without a clear understanding of the hyperparameter tuning required.

(8) The graphs in the paper plot performance against the number of "generations", which mask the fact that in terms of function evaluations, your method is slower (as it should be! for these problems, gradient information is very useful!). In the future, please report performance against the number of function evaluations.

(9) Are these hyperparameters reported anywhere?

The authors did not significantly address any of my concerns with their rebuttal. I do not see the need to raise my score.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_HyleI4NqhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An evolutionary algorithm that performs much worse than SGD</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1g5Gh05KQ&amp;noteId=HyleI4NqhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1292 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1292 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper describes an evolutionary algorithm, with the goal of providing an alternative to SGD. Unfortunately, the only comparison to SGD provided is an experiment where the proposed approach is orders of magnitude *worse* than SGD. And that is the only machine learning experiment in the paper. Thus, the results obviously do not warrant publication at ICLR; in fact, I am wondering why the paper was submitted to this conference.

Some more details about the one machine learning experiment reported in the paper:
Here the authors train a ConvNet on a subset of MNIST. While MNIST is by now a toy problem, it is OK to start with it. Only reaching a validation accuracy of 90% on it sets off an alarm in my head (since the state of the art is beyond 99.5%), but even the time to reach this performance is worse than that of standard SGD (with unspecified, and possibly untuned, learning rate, momentum, etc). And not just a little worse, but requiring almost 10x the number of "generations", with a generation of the authors' method performing 50 evaluations and a generation of SGD performing 1 evaluation. So in total the authors' method is about 500 times slower.

I do not expect the authors to be able to modify this paper to be accepted at a machine learning venue, but the experiments for the blackbox functions appear more promising. I am wondering whether these positive results would hold up when comparing to good implementations of the other algorithms. Therefore, I encourage the authors to evaluate their algorithm on the benchmark functions in the BBOB challenge (<a href="https://bbcomp.ini.rub.de/);" target="_blank" rel="nofollow">https://bbcomp.ini.rub.de/);</a> if they can beat the algorithms that participated in that challenge they should be able to publish their results at one of the evolutionary search conferences, such as GECCO. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">1: Trivial or wrong</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SylfKeOJT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1g5Gh05KQ&amp;noteId=SylfKeOJT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1292 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1292 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the high-quality review. We appreciate the time spent on reading and analyzing the paper. Please allow us to address some of your concerns and comments. There has been grave misinterpretations and misunderstandings that are detrimental to a fair review.

1) In the paper we clearly state that we do not want to replace SGD. In fact, we go on as to state that they ought to be used together as appropriate. SGD works brilliantly for some problems such as NLP, ImageNet, MSCOCO, etc... While intriguing, it would perhaps be redundant to try to use our algorithm to solve those problems. However, we aim --eventually-- to address areas where SGD is challenged, such as RL. As an introduction, we worked with a toy problem such as MNIST to gauge our algorithm. If it can't solve that simple problem, it would not be able to solve more challenging ones. 

2) Our algorithm does not perform orders of magnitude worse than SGD in the experiment. What we show is the true performance, as described in section 4. SGD takes 320 evaluations (ie. forward passes) to reach the outlined loss, while ours takes 2333 evaluations in total; averaged over 5 trials. Thus, our algorithm is 7.29x slower than SGD, on this problem, and with the imposed conditions.

3) This is a new optimization algorithm, designed to optimize deep neural networks. It is tested on a variety of problems, as a means of verification to its proficiency. The experiments reflect varying conditions which are commonly encountered when training neural networks such as sparse rewards. That is the reason we submitted our work to ICLR. Over the years, the community developed techniques to aid in training when using SGD, such as Dropout, Batch Normalization, Residual Connections, etc... We expect that once our algorithm is ready to be used in practice, the community will also develop techniques to aid in the training process.

4) In using SGD, we did not change the setting from the vanilla implementation in PyTorch. That is not a handicapped setting but it is also not specifically tuned to solve MNIST either, thus the comparison is fair. In turn, we also did not tune our own algorithm for MNIST. We used the same set of hyperparameters as we did for the Global Optimization problems, even the same population size. If our goal was to showcase SOTA performance on solving MNIST we would have taken a different approach, as stated in the paper. Instead, we wanted to demonstrate that the exact same algorithm that solves those GO problems, approximately solves this ML problem (and in a much higher-dimensional search space) comparably to SGD.

5) We appreciate your suggestions regarding publishing in GECCO. However, please understand the goal for this paper. This is not an attempt to find a better evolutionary algorithm in general. We are building optimization algorithms to train deep neural networks, under certain conditions such as limited populations. There are many Global Optimization algorithms, but they are not used to train DNNs. The BBOB challenge is not the proper metric for our algorithm.

We thank you again for your review. We hope the points raised in our reply will allow you to see the paper in a new light. In addition, we take full ownership of the misunderstandings arisen. We shall attempt to rectify them by adjusting the narrative in some parts of the paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SklXViu16Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your reply</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1g5Gh05KQ&amp;noteId=SklXViu16Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1292 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1292 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your quick response. Here are my replies to your individual points:

1) Indeed, I agree that gradient-free methods can be complementary to SGD. This has, e.g., already been demonstrated convincingly in work by OpenAI from last year <a href="https://arxiv.org/abs/1703.03864" target="_blank" rel="nofollow">https://arxiv.org/abs/1703.03864</a> and in several follow-up works. I would encourage you to study some RL problems where the gradient signals are weak and the likelihood is higher that you can actually outperform SGD. (An experiment on MNIST can still serve as a first step.)

2) Thanks for the clarification, but indeed in Section 4.2 you stated twice that your algorithm requires 2333 *generations*, not *evaluations*. I believed that since the population size is 50, so aren't there 50x more evaluations than generations? Also, you stated that SGD's evaluations use batch size 50 and you use batch size 2000, and that you require 16.000x more inferences. From this I assume your speed is actually 16.000x slower than that of SGD, which would be even higher than my initial estimate.

3) I agree that it is interesting to develop an algorithm specifically to handle the types of loss surfaces typically encountered in neural networks. However, I fail to see how exactly your method does that. I encourage you to make this clearer in future versions of your work, e.g. using ablation studies on problems with different types of known loss surfaces.

4) I appreciate that you did not tune your algorithm's hyperparameters for MNIST and provided an unbiased comparison. 

5) I understand your goals, but I would've thought that the BBOB challenge also covers various different loss landscapes of relevance. Even if you do not like those loss landscapes, I encourage you to benchmark against implementations of algorithms created for that competition (rather than vanilla implementations from some library). 

I wish you all the best in future versions of this paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1lF2Qay67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Follow-up</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1g5Gh05KQ&amp;noteId=B1lF2Qay67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1292 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1292 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your response. It seems our comments addressed some of your concerns. Please allow us to clarify some points which still seem improperly communicated.

1) We compared our algorithm against vanilla Evolution Strategies, and the results were favorable, under the imposed constraints. We are most concerned with having high-efficiency under extremely limited populations. This not something OpenAI were concerned about. This is a major reason why we introduced a new algorithm.

2) SGD and our algorithm function quite differently. We compare them in two ways. The first is the number of optimization steps, ie. weight updates, taken. In SGD an optimization step is performed per batch. In our algorithm an optimization step is performed per generation. We report this in the paper, in Figure 1, and in section 4.2. Our algorithm takes, on average, 2333 optimization steps to converge to the target validation loss. SGD takes 320 optimization steps. Thus, our algorithm takes 7.29x more steps to converge. The second metric is the amount of data seen. Per optimization step, our algorithm performs 50 evaluations over 100% of the dataset (since we don't use batch training). Per optimization step, SGD performs 1 evaluation over 2.5% of the dataset. It is clear why SGD is an absolute winner in this case. Thus, our algorithm sees ~14,600x more images in the absolute. We report both metrics in the paper.

3) The larger the neural network, the greater the number of dimensions are in the search space. Our method has to be competent in navigating high-dimensional search spaces, while utilizing a constrained pool size. The power of neural networks are in their representational capacity, and to utilize that, the networks need to be large, and deep. This challenges the current generation of optimization algorithms, and the reason gradient-based methods are used. We demonstrate that our method can navigate high-dimensional search spaces (4.7M param CNN) with a smooth loss surface, as well as low-dimensional search spaces (128-wide FC DNN) with challenging loss surfaces. Performing ablation studies would definitely be valuable.

4) Thank you. We do not shy away from the fact that our algorithm needs work. But we also need to manage expectations because SGD (and its variants such as ADAM) took decades of research to arrive to its current, usable format. The same can be said for ES. This is a first iteration.

5) The BBOB was developed as a metric for a certain branch of optimization. Indeed, some of the functions we chose are borrowed from BBOB such as Rosenbrock, and Rastrigin. Others, not included in BBOB, are more relevant to us, such as the Eggholder function and the Easom function. We value your suggestions, however.

We are extremely grateful for this discussion, and the chance to elucidate our arguments. We hope in this new understanding you will review your rating for the paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_Bye8Rj5wnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting concept, not thoroughly backed up by experiments.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1g5Gh05KQ&amp;noteId=Bye8Rj5wnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1292 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1292 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a new, evolutionary, gradient-free algorithm, aimed at training deep networks. The algorithm is compared against other evolutionary algorithms and against Stochastic Gradient Descent.

High level comments:
* Clarity: The paper is written very clearly and easy to follow.
* Quality: The introduced idea is interesting, but overall the paper quality is quite low, mainly because the experiments do not support the claims of the paper, and there are several improvements that can be made to the writing.
* Originality: I am not familiar with the literature in evolutionary methods, therefore I cannot evaluate this fairly. 
* Significance: Based on the presented experiments, I cannot see a use case for choosing this method instead of SGD or other competitors.

Pros:
-	The introduced concept is interesting
-	The language in the paper is simple, making it easy to follow.
-	Several of the decisions in the paper are explained intuitively. 
-	I appreciate the honest comparison when specifying that the competitors’ implementation is suboptimal, and in the comparison with SGD.

Cons:
-	The experiments do not entirely support the utility of the method, even as proof of concept (e.g. not varying the pool size, no results on computation speed comparison, competitors are not optimized, MNIST experiments have not converged).
-	Some details are missing (e.g. what is the loss function? Are there any constraints on choosing it?)
-	Some choices of parameters in the paper that are not justified (e.g. numbers in equation 1, loose stopping criteria)
-	There are some minor typos and editing issues that need to be fixed.

Detailed comments:
•	Major issues:
   1.	Part of your motivation in the intro states that your algorithm is not replacing SGD, but “complementing” it. Can you please describe more specifically how this would be done? Give examples. When should one use your algorithm, and how do you use it “in tandem” with SGD. It is not clear from the paper when/how your algorithm should be used.
   2.	You mention that: “Our algorithm should be able to handle sparse-rewards, exploration, exploitation, high-dimensionality, computational efficiency, pool-efficiency and sample-efficiency”. None of these are proved by experiments:
      a.	high-dimensionality is not addressed
      b.	computational efficiency is mentioned in the conclusions, but there are no experiments comparing run times, and no discussion about algorithmic complexity.
      c.	pool-efficiency: indeed you use a small pool, but you didn’t mention how this is for other competitors, and you didn’t vary the size of this pool to see what happens.
      d.	sample-efficiency: you run MNIST on a subset of samples, but the accuracy you get is nowhere near state-of-the-art. The accuracy you get is the same as SGD, but in Figure 1,  none of the two losses has converged (see downward trend  of the curve), so maybe SGD would be better at convergence.
   3.	RL vs not RL: often times in discussion you mention “agents” and RL (e.g. entire section 1.1), but none of the experiments are about RL. While RL may be a natural application, since it is not evaluated here, I suggest reformulating the paper using supervised learning terms.
   4.	From the comparison with SGD in terms of number inference calls it sounds like your algorithm would be very expensive on large networks. Because of that, it would be good to see come computation time plots on larger networks. However, I appreciate that you did bring this comparison up!

•	Minor issues:
   1.	The entire section 2 relies on picking samples based on some performance measure. However, you never mention what this measure should be. Is it a loss function? If so, are there any constraints on this?
   2.	“The historical elite is the best performing sample across all generations”. This is very vague. How is this quantified? It would help to introduce some mathematical notation for the loss, and describe how the elite is chosen in terms of this loss.
   3.	The experimenter has to choose several constants in your model. Is there some intuition how to do that?
   4.	Section 1.1, paragraph 2: RL doesn’t necessarily need labeled data. What are other potential issues with simulations and how is this related to the proposed method?
   5.	“It is typical that any optimization algorithm is sensitive to the choice of parameters. However, tuning those parameters is beyond the scope of this paper.”  -- not true. If  you don’t make a sensible choice of  step size in SGD, the comparison will not be fair.  The fairest thing to do is to compare these methods, each using its best hyperparameters.
   6.	Section 3.2: “The test is terminated once the algorithm achieves a loss of 0.15 or lower”. You don’t mention what loss is used to be able to tell if 0.15 is  small enough. From the plot it seems the models have not converged.
   7.	Table 1 caption does not specify the metric of the numbers in the table.
   8.	The conclusion mentions new results on a RL experiment, which are not discussed in the paper.
   9.	Please briefly define what are evolutionary optimization algorithms in the intro.

•	Typos and writing style:
   1.	The citation style is most often used incorrectly. Put the reference in brackets when is not part of the sentence (e.g.  in first paragraph: Object Detection (Liu et. Al, 2016), … ), and without brackets when you specifically refer to that paper title.
   2.	Typos: last paragraph of background (“also use computer vision is a starter”),  Section 3 second paragraph (“That is, the models for copied into each”).

Final remarks and advice: 
It seems that the authors pose this work more as a proof-of-concept, than a new algorithm ready to be used in practice. While the idea definitely has value, I believe the experiments are not sufficiently convincing or adequately chosen, to prove what you want to show. If sample complexity is your selling point, gear the experiments more in that direction and show what happens for various sample sizes and pool sizes, for you and competitors. If it is computationally efficient (as you say in the conclusion), show experiments comparing run times with other competitors. If complexity is the selling point, tell us exactly how much computation is required in every generation, compared to other methods. Also, if SGD is your competitor, find a scenario where SGD doesn’t work but your method does. Moreover, for the presentation of the algorithm in section 2, an algorithm box summarizing the method would be of great help. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJlIz7yx6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1g5Gh05KQ&amp;noteId=HJlIz7yx6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1292 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1292 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">First, thank you for your in-depth review. The quality of your review reflect the amount of time, effort and attention to detail that you have invested. And we are grateful for that. There may be some misunderstandings and misinterpretations that are detrimental to a wholesome review. Please allow us to communicate our response to some of your comments and concerns.

1) Per the other reviews, we noted a strong necessity to rephrase parts of the narrative and how the results are communicated.

2) The significance of this work is not in its immediate use-case for the researcher. SGD in its current format took decades of research, and similarly the other optimization algorithms. The significance, as we see it, is in the insights we introduce, the results we present (highlighting where the algorithm is attractive and where it needs improvement), and in the potential of building upon it to train neural networks on challenging problems.

3) Performing ablation studies would be valuable, no doubt. We do not regard algorithmic complexity, computational/wall-clock *time* to be relevant criteria for evaluating our algorithm. Both are definitely important, but at this stage, come as a lower priority.

4) The loss function is well-known for MNIST, it is the CrossEntropy, ie. classification, loss for the validation set. For the global optimization functions it is the value of the function. The loss is the error between the function value at the predicted location (x, y) and the global optimum. This is in section 3.1 of the paper.

5) Using only 2000 training images, the validation loss doesn't decrease much beyond 0.15 for both SGD and our algorithm. For that reason, to get out of diminishing returns, we decided upon this stopping criteria. This is also the case for the global optimization problems. For example, if the global optimum is 0; we decide that the stopping criterion is when the solver reaches +-0.06, that is a marginal error. It is not a "loose" criterion.

6) Some hyper parameters are derived empirically, others intuitively. For example, after a few generations the entropy in the models, ie. samples, decreases (it becomes less random). In that case, it is unlikely that perturbing more than 10% of the weights will lead to a positive outcome. Neural networks are trained with small updates to the weights. This is an insight. We use such insights to shape equations 1 and 2. An explicit parameter search may be helpful, however.

7) As an example, when one is training a robot SGD can be used to train the perception module and our algorithm to train the control policy. SGD is highly successful in some domains such as NLP, CV. It definitely should be used wherever the gradient signal is informative. The community developed an array of techniques such as Skipped Connections, Dropout, Batch Normalization to aid in such problems, and we have enormous respect for those contributions.

8) 
- High-dimensionality is addressed in the search space (we train a 4.7M parameter CNN). When one is using gradient-free methods, each single parameter is an independent dimension in the search space. This is challenging to gradient-free optimization algorithms, and the root for gradient-based training of DNNs.
- Our algorithm can fit a single Titan V GPU, and requires nothing else.
- We fix the pool size for ALL the competitors, 50. With this relatively small number, most evolutionary algorithms do not perform well. This is an important differentiator for our method.
- SGD is certainly better for MNIST. The gradient space is extremely smooth. The algorithms reach diminishing returns after this stage, and the gap in performance keeps will keep on widening. We were not trying to out-perform SGD in that problem. We did not even tune the parameters (or the CNN architecture) for this purpose. The experiment demonstrates high-dimensional search spaces, on a more relevant neural network problem than the GO functions. For the GO problems, our algorithm performs better than the other algorithms.

9) We will take your suggestions about the typos, the writing, and the RL phrasing when revising the paper.

10) Samples are ranked according to the loss/cost function. There are no constraints on it.

11) The Elite is the best-performing sample in a generation. The Elite of gen 1 may be better, or worse, than the Elite of gen 2; and so on. The "historical" Elite, is the best-performing sample across generations (1 to N).</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJxN8XJg6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1g5Gh05KQ&amp;noteId=HJxN8XJg6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1292 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1292 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">12) There are many hyper parameters in our algorithm. In future iterations that number may be reduced. They are chosen intuitively as we illustrated and empirically. However, note that in our experiments we do NOT tune our algorithm's parameters for each problem separately. We use the exact same set of parameters over ALL the problems, including MNIST. This means that the *exact* same solver is used to solve all those problems simultaneously. That is another reason why we don't tune the params of other algorithms. The vanilla implementations are not optimal, but they are not adversarial either. Usually, they are based on the respective publications or empirically set to values known to work well in general. We didn't change the default setting in PyTorch for SGD, eg. learning rate, and similarly for the other algorithms. Thus the comparison between our algorithm and others is fair.

13) Take MuJoCo environments for example. While the environment is solved many times over (ie. Humanoids can acquire physical skills), the community is still challenged to solve Physical RL problems such as Grasping and Locomotion. Our method aims, eventually, to enable Physical Learning (on-line learning from experience). This is a first step in that direction, that attempts to account for sample-efficiency, and other criteria as mentioned.

14) The metric in Table 1 is Number of Generations. It is shown in the table.

15) The main "selling-point" of the paper, is that we propose a new optimization algorithm designed to train DNNs that works better than others in low-dimensional search spaces (128-wide FC DNNs) with challenging loss surfaces. In addition, the algorithm navigates a high-dimensional search space (4.7M param CNN) with smooth loss surfaces, while performing 7.29x more optimization steps. These results translate into findings on how the mechanisms we introduce can improve, as well as identify areas of attractiveness.

We are sorry for the lengthy reply. However, your in-depth review merited that we put in the effort to address all your concerns. Thank you again for the feedback and suggestions, which will definitely be put to use. We hope our response will motivate you to see the paper in a different light.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_rkeCSBdZqX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Hollow</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1g5Gh05KQ&amp;noteId=rkeCSBdZqX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1292 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">"However, we aim to improve upon the shortcomings of other algorithms with the mechanisms we introduce here. Our
algorithm should be able to handle sparse-rewards, exploration, exploitation, high-dimensionality,
computational efficiency, pool-efficiency and sample-efficiency" -- Big Claims , no supporting evidence

" That is, the agent usually takes a relatively large number of samples, possibly in the order of ...." -- The authors do not use even a single RL task or say how many samples were needed in comparison to any RL algorithm. Then the authors state "It solves the assigned problems, though it takes longer than other approaches." 

"We see this paper as a starting point, for the whole Robotics community to engage in developing of
alternative optimization for DNNs used in Robotics and RL." - Yet there was no bench-marking on mujoco/roboschool/atari or any standard benchmarks and comparison with other optimization/RL techniques is completely missing.

Comparison with state of the art algorithms missing for MNIST.

More than half the conclusion section talks about comparisons with RL which is NOT a part of this paper. the authors are drawing conclusions on some future paper they might or might not write ! 

Whimsical / No mathematical basis of any of the content.  Arbitrary intuitions to get some results on mathematical functions.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkeqe5yQcX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your comments.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1g5Gh05KQ&amp;noteId=Bkeqe5yQcX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1292 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1292 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">“Our algorithm should be able to handle sparse-rewards, exploration, exploitation, high-dimensionality, computational efficiency, pool-efficiency and sample-efficiency"-- Big Claims , no supporting evidence”

These are not “claims”. These are constraints that any evolutionary algorithm used to train deep neural networks ought to meet. 

Regarding pool efficiency: Up to this point, that has not been a concern for evolutionary algorithms. Typically pool sizes on the order of hundreds are employed to solve the optimization problem. We use only 50 in all our experiments.

Regarding Exploration vs. Exploitation: We test our algorithm’s ability to handle the trade-off between exploration and exploitation using the global optimization problems. The Easom function is flat everywhere except for a small region compared to the search space. If the algorithm can not explore sufficiently, and quickly, it will not be able to solve it. The Bukin function has a sharp ridge where the global optimum lies. The algorithm must explore sufficiently the search space, and then exploit the ultra-narrow region where the global optimum lies, else it will never converge.

We will include in the appendix of a revised version illustrations of the global optimization problems. This will visually corroborate the narrative.

Regarding High-dimensionality: The global optimization problems are solved in 2D using single hidden-layer fully-connected network of size 128. Each weight in this network is treated as an independent variable in our search space (the space of all possible real-valued weight configurations). We test our algorithms capacity to handle higher-dimensions by training a 4.7M parameter network. That is amongst the largest ever trained by an evolutionary algorithm.

Regarding computational efficiency: We noticed several other papers propose evolutionary algorithms that require thousands of CPU cores to train. We thus developed this algorithm with an eye for implementation with limited computational resources. It works using only 1 GPU, and in our case we extend the implementation to 4 GPUs for Data Parallelism. This ties in to the pool efficiency aspect discussed earlier.

-- “ That is, the agent usually takes a relatively large number of samples, possibly in the order of ....”

We were stating that traditional RL using gradient-descent as an optimization step requires the mentioned number of samples. This is corroborated by the citations, and is well-known in the community.

-- “The authors do not use even a single RL task or say how many samples were needed in comparison to any RL algorithm. Then the authors state "It solves the assigned problems, though it takes longer than other approaches."”

In the paper, we never stated we were going to use RL benchmarks. In fact, we explicitly stated it is premature to expressively benchmark a new optimization algorithm against RL tasks.

Regarding that last statement, the paragraph as a whole says: 
- “Finally, while not presented in this work, preliminary tests of our algorithm on RL tasks have been promising. It solves the assigned problems, though it takes longer than other approaches. We aim to improve upon the algorithm and the strategies employed in order to achieve competitive results on RL and Robotics tasks.”

We were alluding to some very preliminary results after some initial testing. This puts the paper in context of the larger research direction. Even so, it takes nothing away from the paper itself if we removed it entirely. We will take your comments in mind for the revised version.

Finally regarding mathematical modelling. We would have introduced mathematical models where we deemed there was a need. However, we invite you to do so should you feel obliged to.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>