<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Transfer Learning via Unsupervised Task Discovery for Visual Question Answering | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Transfer Learning via Unsupervised Task Discovery for Visual Question Answering" />
        <meta name="citation_author" content="Hyeonwoo Noh" />
        <meta name="citation_author" content="Taehoon Kim" />
        <meta name="citation_author" content="Jonghwan Mun" />
        <meta name="citation_author" content="Bohyung Han" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rkelDoCqFX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Transfer Learning via Unsupervised Task Discovery for Visual..." />
      <meta name="og:description" content="We study how to leverage off-the-shelf visual and linguistic data to cope with out-of-vocabulary answers in visual question answering. Existing large-scale visual data with annotations such as..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rkelDoCqFX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Transfer Learning via Unsupervised Task Discovery for Visual Question Answering</a> <a class="note_content_pdf" href="/pdf?id=rkelDoCqFX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=shgusdngogo%40postech.ac.kr" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="shgusdngogo@postech.ac.kr">Hyeonwoo Noh</a>, <a href="/profile?email=carpedm20%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="carpedm20@gmail.com">Taehoon Kim</a>, <a href="/profile?email=choco1916%40postech.ac.kr" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="choco1916@postech.ac.kr">Jonghwan Mun</a>, <a href="/profile?email=bhhan%40snu.ac.kr" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="bhhan@snu.ac.kr">Bohyung Han</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We study how to leverage off-the-shelf visual and linguistic data to cope with out-of-vocabulary answers in visual question answering. Existing large-scale visual data with annotations such as image class labels, bounding boxes and region descriptions are good sources for learning rich and diverse visual concepts. However, it is not straightforward how the visual concepts should be captured and transferred to visual question answering models due to missing link between question dependent answering models and visual data without question or task specification. We tackle this problem in two steps: 1) learning a task conditional visual classifier based on unsupervised task discovery and 2) transferring and adapting the task conditional visual classifier to visual question answering models. Specifically, we employ linguistic knowledge sources such as structured lexical database (e.g. Wordnet) and visual descriptions for unsupervised task discovery, and adapt a learned task conditional visual classifier to answering unit in a visual question answering model. We empirically show that the proposed algorithm generalizes to unseen answers successfully using the knowledge transferred from the visual data.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HkgTXvOYp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkelDoCqFX&amp;noteId=HkgTXvOYp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper232 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper232 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HkgoKpPT2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Novel model on zero-shot VQA, however, a lot of details is not clear in the paper. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkelDoCqFX&amp;noteId=HkgoKpPT2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper232 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper232 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">[Summary]
This paper study how to leverage off-the-shelf visual and linguistic data to cope with out-of-vocabulary answers in visual question answering. The authors tackle the problem in two steps, 1) learning a task conditional visual classifier based on unsupervised task discovery, and 2) transferring and adapting the task conditional visual classifier to visual question answering model. With these two steps, the VQA model is formulated as p(a|v(I,q), eta_(q)) where v(I,q) is the visual feature with attention and eta_(q) is the task specification vector. During the pre-training stage, the embedding of task description can be obtained from two sources, word embedding of the name of the answer set, or caption with &lt;blank&gt; which replace the answers which encoded by a GRU. The authors verify the proposed method by re-construct the VQA 2.0 dataset, which the out-of-vocabulary answer appears during pre-training. The experiment shows that the proposed method is better than the baseline methods. 

[Strength]
1. The proposed model is the first work on zero-shot VQA model which can handle out-of-the-vocabulary answers. 

2. Experiment results show the proposed method is effective on the proposed splits. 

[Weakness]
1. There are several parts of the paper is super unclear, can the authors answer my following questions? 

- What is the model for each module proposed in the paper? There is no model figure or description at all, which make the proposed method is hard to understand or replicate the results. 

- Page 4, weakly supervised task regression, what is the new indirect loss, and how to calculate it, could the authors explain more? 

- Page 5, wordnet, how to select which node of the WordNet hierarchy to use as the task specification name t_s? 

- Page 6, visual description, how to get the visual description? coco caption annotation or generated by some image captioning model? 

- Page 6, how to combine these two linguistic knowledge source? 

2. Page 5, footnote mention that the ambiguity of the reference is usually resolved by attention model. what if the attention model attends the wrong parts? 

3. Given the WordNet answer set W_ts, how to select the answer from this set? will this differ from the visual description?

4. It seems the dataset split is delicately constructed for the proposed method, will the proposed method apply on normal VQA split? 

5. What is the performance with standard VQA 2.0 split, such as train on train and test on val. the novel answers could be obtained by enlarging the VQA answer number. Although the novel answer maybe only just a small portion of the test set, it will be worth checking the performance on this split and compare with other methods. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJgzmS9c2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Approach to “zero-shot” VQA which decouples task prediction from answer prediction for generation; requires additional and more careful experimental evaluation.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkelDoCqFX&amp;noteId=BJgzmS9c2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper232 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper232 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper looks at the task of extending VQA to a novel answer space, for which there are no corresponding Questions-Image-Answer Triples available at training time, but separate Text and Image classification information exists for these answers. The paper proposes a model which decouples task prediction from answer prediction to allow the model to generalize to novel answers.

Strength:
1.	Interesting direction of extending supervised VQA to a novel answer space.
2.	Model successfully allows transfer from different task data.
3.	Overall novel model which aims to decouple task prediction from answer prediction.

Weaknesses:
4.	Related work: 
4.1.	The paper mentions the zero-shot VQA work from Teney &amp; Hengel, however, given that is one of the most related works, I expect a more thorough discussion of the similarity in dataset and method. Is it possible to run the approach from Teney &amp; Hengel on the dataset of the authors or run the authors approach on the zero-shot answer split?
4.2.	The paper is related to zero-shot recognition, which is an established field in computer vision, however this line is not discussed sufficiently, see e.g. Xian et al. for a discussion on the topic.
4.3.	While the paper discusses the setting of Agrawal et al. (2018), it misses to discuss how this works related to the GVQA model proposed by Agrawal et al, which has a several similar aspects to the model proposed in this work (although it is overall different).
5.	Evaluation
5.1.	The paper only evaluates on the novel classes, but it is unclear what happens if the model encounters known “classes”, i.e. has to answer with known answers from the training set. I think it is critical to also evaluate this scenario. For image classification this is sometimes referred to as “Generalized zero shot”, see e.g. Xian et al. CVPR 2017, or for just evaluating including known classes as distractors see Rohrbach CVPR 2010.
5.2.	It also would be good to include a baseline/ablation which does not look at the question, i.e. without task prediction.
5.3.	The paper evaluation consists of a plot which shows the performance over training iterations. Instead the paper should pick the iteration on a validation set and report results on a test set as a single number, best in a table, which allows clear comparison for future work.
5.4.	The paper uses a “test-validation” set which includes the novel answers, which are also in the test set. However, as the validation set is part of model optimization, i.e. training of the model, any validation set, including “test-validation” should not include novel answers included in the test set. Instead I suggest the dataset should split a “test-validation” which does not overlap with test nor with the training set, to do proper validation without conflict of the test set.


Conclusion: Overall a great direction and interesting approach but requires more careful experimental setup and evaluation and discussion of related work for acceptance.


References: 
Xian, Y., Schiele, B., &amp; Akata, Z. (CVPR 2017). Zero-shot learning-the good, the bad and the ugly. 
And/or the TPAMI version Zero-shot learning - A comprehensive evaluation of the good, the bad and the ugly; 2018

Rohrbach, M., Stark, M., Szarvas, G., Gurevych, I., &amp; Schiele, B. (CVPR 2010). What helps where â?? and why? Semantic relatedness for knowledge transfer.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJezdAGvhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting Work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkelDoCqFX&amp;noteId=rJezdAGvhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper232 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper232 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">==========
Strengths:
==========

- Out-of-vocabulary VQA is an interesting and timely task. 

- The presented approach leverages region annotations (Visual Genome attributes and objects) and descriptions (COCO Captions) to learn a task-conditioned visual classifier. This seems fairly intuitive and an efficient reuse of existing data to learn visual groundings for out-of-vocabulary words. 

- Overall the qualitative and quantitative results support the proposed method's effectiveness. 

- I really appreciate the extended results in the supplement.

- The paper is generally written well with clear descriptions and useful figures. Thanks!

- Thank you for using multiple random seeds and reporting variance. 

==========
Concerns:
==========

[A] How is the initial answer space of the task-conditioned classifier set? The dataset construction paragraph is somewhat vague about what a 'pretrained visual word' is. Is it the 3000 objects and 1000 attributes from Visual Genome referenced in the pretraining paragraph?

[B] The task construction for visual descriptions could be clearer. From my understanding, you select a caption and randomly remove a word, replacing it with the &lt;blank&gt; symbol. This word becomes the answer and the RNN encoded blanked-caption is the task. How is the word selected? 

[C] Minor point: I think the VQA score listed in 5.1 is incomplete. If I understand correctly, the calculation should be run over all 10 choose 9 sets of answers and the mean reported.

[D] Small request: Could the authors add a list of answers in train and test in the supplement? Aside from academic curiosity, I would also like to get a sense for how many of these answer refer to novel concepts / entities / relationships as opposed to being synonyms for existing ones in train. Any attempt to quantify this would also be appreciated!

==========
Overview:
==========

I'm overall positive on this paper. I think the problem is interesting and the approach leverages existing source of visual grounding in a exciting way. The results in the main paper and supplement paint a convincing picture of the method's efficacy. There are a couple of things that I would like more clarity on in the submission.


</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>