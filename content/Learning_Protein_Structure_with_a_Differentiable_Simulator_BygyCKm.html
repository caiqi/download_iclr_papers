<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learning Protein Structure with a Differentiable Simulator | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learning Protein Structure with a Differentiable Simulator" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Byg3y3C9Km" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learning Protein Structure with a Differentiable Simulator" />
      <meta name="og:description" content="The Boltzmann distribution is a natural model for many systems, from brains to materials and biomolecules, but is often of limited utility for fitting data because Monte Carlo algorithms are unable..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Byg3y3C9Km" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning Protein Structure with a Differentiable Simulator</a> <a class="note_content_pdf" href="/pdf?id=Byg3y3C9Km" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019learning,    &#10;title={Learning Protein Structure with a Differentiable Simulator},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Byg3y3C9Km},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">The Boltzmann distribution is a natural model for many systems, from brains to materials and biomolecules, but is often of limited utility for fitting data because Monte Carlo algorithms are unable simulate it in available time. This gap between the expressive capabilities and sampling practicalities of energy-based models is exemplified by the protein folding problem, since energy landscapes underlie contemporary knowledge of protein biophysics but computer simulations are still unable to fold all but the smallest proteins from first-principles. In this work we bridge the gap between the expressive capacity of energy functions and the practical capabilities of their simulators by using an unrolled Monte Carlo simulation as a model for data. We compose a neural energy function with a novel and efficient simulator based on Langevin dynamics to build an end-to-end-differentiable model of atomic protein structure given amino acid sequence information. We introduce techniques for stabilizing backpropagation under long roll-outs and demonstrate the model's capacity to make multimodal predictions and to generalize to unobserved protein fold types when trained on a large corpus of protein structures.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">generative modeling, simulators, molecular modeling, proteins, structured prediction</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We use an unrolled simulator of a neural energy function as an end-to-end differentiable model of protein structure and show it can hierarchically generalize to unseen fold types.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Skx-IPBvTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting paper - not clear and mature enough </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byg3y3C9Km&amp;noteId=Skx-IPBvTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1020 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1020 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a new end-to-end training framework for computational prediction of protein structure from sequence. 
This is a very important problem and any progress due to new data and/or methods for utilizing may have high impact. 

The paper presents several technical contributions in the modelling and training procedure - for example, automatic transformation between Cartesian and angular coordinates, using Langevin dynamics, and imputation method to get fine atomic coordinates. 

The overall breadth and depth of the methods presented in the paper are impressive. The paper describes a quite complicated systems with multiple modules interacting between them. The paper doesn't describe the system built in enough details, although many of the details are given in the appendix.  
Figure. 6 presents a scheme of the entire system, but it lacks details about the different modules, and it is not clear how they interact and how their training together is performed. 
The pseudo-code boxes describing Algorithms 1-4, and Table 2 describing the representation are informative and helpful, and more descriptions of this type would help. 
For example:    - In Algorithm 3, what do 'CartesianStep' and 'ClippedInternalStep' mean? where are they described? (should have their own boxes/description).  
		- I didn't see an Algorithm describing the atomic imputation part. 
		- It would be good to add a high-level pseudo-code for the entire end-to-end training algorithm. In it there could be calls to Algorithms 1-4 when needed.  

There is also no single place where all the parameters used by the authors to achieve their empirical results are presented 
(e.g. learning rates, Gaussian kernel widths, how are random time steps for enforcing Lipschitz condition chosen etc.). 
In addition, the empirical loss defined in eq. (8) is a sum of 6 different losses. It is not clear how are these very different losses scaled to the same 'units', which ones are more important, 
if and how are constants multiplying them chosen to give lower/higher weights to some of the losses etc. - I guess these choices will have a large effect on the training. 

The authors present generalization results of their trained model in predicting 3D structures from CATH at different generalization level
(i.e. different similarity levels to the training set proteins). It is not clear to me how good are these results, except that they are shown 
to be better than a baseline simple model. How well does the author's model compare to other recently suggested end-to-end models? 
(the authors mention AlQuraishi, Anand&amp;Huang, papers). How do they compare to state-of-the art structure prediction programs? (e.g. CASP winners)? 
I realize giving an automatic end-to-end solution is interesting even if performance is below that of best programs, but still it would be good to know gaps.
If such comparisons are less meaningful/not practical to perform this should be argued convincingly. 
It would also be useful to add some metrics of running time - it is not clear how computationally heavy and scalable is the author's model and training, compared to other methods. 


There are many typos and inconsistent notations which makes it harder for the reader to understand the paper. 
For example, 'Figure ??' in multiple locations, wrong Figure referenced, using s vs. S for sequence - S is defined as an L*20 matrix but in the appendix there are
3 indices: s_{i,l,j} and it looks like different sequences in alignment should be denoted s_i. 
Equation for M_{l,j} isn't clear: j is used both as fixed index and index in summation. 
The indexing in 'orientation vectors' v-hat_ij definition seems off (the formula of base vectors gives 0/0)



</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1lRoQMLTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byg3y3C9Km&amp;noteId=B1lRoQMLTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1020 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1020 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Overall this is an important piece of work that deserves publication at ICLR. I recommend to the authors revise their manuscript to make it more accessible to the machine learning community and that they provide better context to allow them to assess the relative quality of the work compared to state of the art results.

# Quality

The hypothesis that the authors set out to resolve is whether there is an advantage in using an energy function sampled by Langevin dynamics versus simply using a neural network to regress shape from sequence. They construct a flexible deep energy model where the sequence and structure dependent parts are separated in such a way that fast rollouts are possible. They also adapt the learning algorithm to  ensure that long rollouts can be carried out and present a clever trick for integrating internal coordinates efficiently on a GPU. 

The only criticism in terms of quality of work is that it somewhat lacks putting in context with results from the larger community, for example how well does the model compare in terms of speed and accuracy with co-evolutionary approaches? I realise it will not be possible to give a completely fair like to like comparison, but it will help readers put the results in context if they understood, for example, what the average TM score for CASP12 results was, as summarized in this paper for example: <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/prot.25423." target="_blank" rel="nofollow">https://onlinelibrary.wiley.com/doi/full/10.1002/prot.25423.</a> Similarly, it would be useful to compare the baseline - at least qualitatively - with the results from AlQuraishi et. al. whose model seems very similar in spirit.

# Clarity

I think in terms of clarity, the paper could be improved a little to take into account the audience of ICLR. In particular:

* It may be useful to add a sentence of how profiles have been found to improve secondary structure prediction greatly. Currently the text makes it sound as though they constitute a sort of 'data augmentation', whereas in my opinion they add information compared to the sequence alone. In fact a brief explanation of the importance of homology might help the reader understand the relevance of the hierarchical approach taken to splitting the training set.

* Fig. 2 caption. Could add some information to explain what panel B is showing. I think this would go a long way to explain why both cartesian and internal coordinates are important.

* Fig. 4 second panel. The x axis should be labeled fraction or be numbered 0-100.

* Fig 4. caption. The figure does not have a caption explaining what the graphs are showing. This would be a good place to explain that the colors refer to test sets that overlap with the training set in the full CATH code (black), overlap only in the CAT code (orange) etc. I admit I had found the explanation of the test/train/validation split rather confusing. It is not clear what the validation set is used for, i.e. which hyper-parameters have been tuned on it etc.

* The nature of the loss. The appendix does a good job in describing each term in the loss function, but does not explain how the empirical loss function and the log-likelihood terms are mixed together. 

# Originality

The work is original and is references the relevant literature.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hkgk3b343m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea with some weaknesses in the evaluation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byg3y3C9Km&amp;noteId=Hkgk3b343m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1020 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1020 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents an end-to-end differentiable model (NEMO) for protein structure prediction. I found this paper very interesting and the idea of training the network through the sampling procedure promising. The authors present the challenges and techniques (damping, Lyapunov regularization etc) in detail.

The paper is clearly written, however the description of the method can be confusing. This stems in part from the many components of the network as well as the fact that the protein is represented using various coordinate systems and features, so that it is not easy to follow which applies at each stage. Fig. 6 in the appendix helps, however it would be better to have a (perhaps more concise) overview in the main text.

In the evaluation, the NEMO method is compared to a baseline approach using RNNs. While NEMO trained on profile features performs best, the baseline is trained on sequences only. However, it outperforms the NEMO model trained on sequence-only in every category. Therefore, it would be interesting to see whether NEMO outperforms a baseline trained on profile features. Otherwise, I am not certain whether I can follow the conclusion that "NEMO generalizes more effectively". Beyond that, it would be interesting to see some generated atomic substructures from the imputation network, in particular an analysis of how diverse the generated atom positions are and whether they depend on the local environment.

Overall, I appreciate the general idea and find the proposed approach very interesting. The contribution could have been stronger with a more detailed evaluation and better presentation.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hyl_nFNX6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Quick clarification: RNN baselines are also trained on profiles</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byg3y3C9Km&amp;noteId=Hyl_nFNX6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1020 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1020 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review and positive words about the idea and approach. While we will respond in full later, we wanted to briefly clarify that all RNN models and NEMO results of Fig 4,5 were trained on profiles. The sentence "We report the results of a sequence-only model in Table 1 and Figure 4" is a figure-link typo and should instead read "We also report the results of a sequence-only NEMO model in Table 1 and Figure 9." We apologize for the confusion and will make these points clearer upon revision.

In the meantime, we hope that this can clarify that our main claim about generalization is based on comparing profile-based NEMO to profile-based baselines.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SJlUZjqEim" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byg3y3C9Km&amp;noteId=SJlUZjqEim"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1020 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1020 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes an end-to-end neural architecture for learning protein structures from sequences. The problem is highly important. The method proposes to use a Langevin simulator to fold the protein ‘in silico’ from some initial state, proposes numerous tricks for the optimisation, and proposes neural networks to extract information from both the sequence and the fold state (energy function). The system works on internal coordinates, which are conditioned and integrated on the fly. The method seems to perform very well, improving upon their baseline model considerably.

In spite of the paper being an outstanding work, I have two criticisms about the accessibility and impact of the paper on the broader ICLR audience. In its current form and complexity, the paper feels accessible mostly to a narrow audience.

First, the framework proposed in the paper is massive, containing a large amount of components, neural networks, simulators, integrators, optimisation tricks, alignments, profiles, stabilizations, etc. The amount of work done in the manuscript is staggering, but the method is also difficult to understand from reading the main manuscript alone. The 10+ page appendix is critical for understanding (for instance, the appendix reveals that MSA is used to generate more data), and even with it the method is difficult to grasp as a whole. This paper should be presented in a journal form with a presentation not hindered by page limits, while currently one needs to jump between the main text and appendix to get the whole picture. I also wonder if some parts of the system have already been published, and perhaps the presentation could be condensed that way. 

Second, the introduction lists numerous competing methods both on the protein modelling side and on the MCMC vs optimisation side. The paper does not compare to any of these, which is strange, and makes it difficult to assess how much this paper improves upon state-of-the-art. Right now its unclear what is state-of-the-art in general. No bigger context of protein folding is given either, for instance, how well the method fares against purely alignment based approaches, or against purely physics-based simulators. Finally, the experimental section poorly describes how all the pieces of the system affect the final predictions. The discussion on the exploding gradients and dampening is excellent however. The only baseline is one with the simulator replaced by an RNN. There does not seem to be any running time analyses. As such, it is hard to interpret the current system, and it feels like a black box.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1eOXxVb9X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Confused about testing and interpretation of results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byg3y3C9Km&amp;noteId=B1eOXxVb9X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1020 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I've really enjoyed reading your paper, but I'm left very confused about the testing procedures and the quality of the results obtained. The key issue when reviewing papers on protein structure prediction is whether or not there is any overlap between the training/validation data sets and testing data. Here it seems that you have tried to be very stringent about this by splitting your dataset of domains along CATH boundaries - so two domains with different CATH codes should in theory be unrelated and thus have no detectable sequence similarity. Likewise two proteins with different CAT codes should have different folds, CA different architectures and C having different classes. Having trained and validated on the domains in CATH release 4.1 you then tested on domains in CATH release 4.2, but only those that are unrelated to the training set (different CATH numbers) or have different folds (different CAT numbers). Assuming that was done without bugs, then I find it hard to understand the middle panel of Figure 4. Unless I’m misunderstanding what’s going on there (the x-axis scale is wrong by the way), it seems to suggest that a large proportion of your testing set is actually quite similar to your training set. In some cases identical (100% identical protein sequence). Does that not indicate that there is sequence overlap between training and test data. Of course it’s impossible to know whether the overlap is with the validation set rather than the training set, but that would still be problematic.

If those highly similar sequences were included in the statistics shown in Table 1, for example, it would make the results there very difficult to interpret.

Looking at some of the specific examples of folded domains shown, it would have been useful to know what the sequence similarity is between the target and the most similar protein domain in the training/validation set. For example, I note that domain 2oy8A03 shares 100% sequence identity with domain 3ckgA02 (one is simply a deletion mutant relative to the other), which was already present in CATH release 4.1 and so must have been either in the training set or at least the validation set. If this is true, then the network has simply recapitulated what it has already seen and hasn’t actually predicted anything. Other examples shown have similar issues e.g. 4ykaC00, which is identical to 2wa9B00 in CATH 4.1 and indeed has the exact same CATH code, and so I don’t think should be included in the test set at all. Probably I have just misunderstood the exact way you’ve effected your training/test split, so I’d welcome any clarification you can give.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Ske1rVBZcX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>To clarify: Training set is hierarchically purged</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byg3y3C9Km&amp;noteId=Ske1rVBZcX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1020 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1020 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your question about the training and test splits; we will do our best to clarify briefly here as well as update the manuscript at the next opportunity.

We very much agree that careful analysis of test and train overlap is one of the key issues when interpreting the results on protein structure prediction and we created our dataset in an attempt to frame this problem in terms of widely used (ie CATH) fold classifications.

To clarify we *did not* train on all of CATH 4.1 but rather intentionally hierarchically purged the training set at multiple levels of A, T and H. So, after collecting all folds in 4.1 (subject to max length 200 and class 1,2,3), we then randomly selected a subset of A classifications and purged all folds descending from these into the A-level validation set. We repeated this process two more times for T and H. While the specific domain-level splits are contained in a large table not feasible for attaching to an openreview submission, we intend to make these available and will try to summarize the high level held out classifications at next update to the manuscript.

Regarding the middle panel of Figure 4, the x axis is correct and our test set from CATH 4.2 does (naturally) contain some folds of very high (sometimes complete) sequence overlap. However, because we purged the training set (as just described above), the test set also contains many sequences of low/random sequence overlap (left cluster in middle panel) and low classification overlap. The color coding on this scatter plot indicates how close the given test domain is to the training set, where blue means that the ATH classifications were not present in train (column C in table 1), magenta means TH were not present in train (column A), orange means H was not present in train (column T) and gray (column H) means that the full CATH classification *was* present in train. The showreel plot contains only folds from the A and T columns (magenta and orange). 

We hope that hierarchical purging approaches to evaluation such as this will be more widely used in the future because they allow testing fold generalization more systematically across thousands of domains (rather than only doing temporal purging).

To conclude, our main claim is that the model is able to (sometimes) produce reasonable (TM&gt;0.5) predictions for these difficult ATH, TH, and H problems created by our purging process and that it does so more accurately than a baseline that predicts angles directly without a folding process.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByeS8UUb5X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Still a bit unclear on the purging</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byg3y3C9Km&amp;noteId=ByeS8UUb5X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1020 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the clarifications.

Minor point, but that Fig. 4B x-axis is definitely incorrect. The scale goes from 0-1 and the axis label states that the units are % sequence ID (i.e. the maximum value is 100).

"Hierarchical purging" like that is commonly practised in the comp. biol. community but it's unclear how that purging process has been extended to the _test_ set, which is based on CATH 4.2. I understand that you have split your _train_ and _validation_ sets according to a randomly selected subset of C, CA, CAT numbers found in CATH 4.1 - that's fine - I get that bit - but after training with that validation set, you cannot then have the same purged CAT numbers present in your test set because you will have fitted your model to your training set and then selected a model which does well on the domains in the validation set.

In reality you need to produce three splits of the CATH classification - train, validation and test. The test CAT codes should not have been used for either training or validation.  Possibly this is exactly what you did, but it really could do with some clarification in the text.

However, if that is what you did, then I don't understand how you can end up with the plot in Fig. 4B - because that would suggest that you have a lot of protein domains which have _different_ topologies (T in CATH) but which still have very high sequence similarity. That just can't be right. There are virtually no known examples of proteins with high sequence similarity which do not have the same topology. Even a sequence identity of just 30% is typically enough to guarantee that the two structures will have the same topology (TM-score &gt; 0.5).

From what you say, though,  it sounds like your test set was simply all the new domains added to CATH 4.2, and that some happen to overlap with the training set domains and some don't. That would explain what I see in Figure 4B alright, but it would pretty much invalidate your final results as your test set would be contaminated. Surely you don't mean that?

Sorry to bang on, but if it wasn't such a potentially interesting paper I wouldn't care enough to ask!
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryeM3Sm4im" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Test set is stratified</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byg3y3C9Km&amp;noteId=ryeM3Sm4im"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1020 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1020 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for catching the fraction (0-1) / percent (0-100) mislabeling, we will fix it.

It is correct that we split train and validation based on CATH 4.1 (hierarchical split) and tested on everything that was new to CATH 4.2 (temporal split), with the test set stratified into subsets of varying difficulty, from the very easy to the very difficult, and clearly labelled as such. While we currently stratify by CATH similarity between {test} and {train} (to evaluate more fold types), we can also include results that stratify by CATH similarity between {test} and {train+validation} (at the cost of reduced fold diversity). Since we subject all models and baselines to the same splits either way, all of these are interpretable measures of generalization.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>