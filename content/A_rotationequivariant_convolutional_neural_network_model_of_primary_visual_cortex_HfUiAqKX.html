<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>A rotation-equivariant convolutional neural network model of primary visual cortex | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="A rotation-equivariant convolutional neural network model of primary visual cortex" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1fU8iAqKX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="A rotation-equivariant convolutional neural network model of..." />
      <meta name="og:description" content="Classical models describe primary visual cortex (V1) as a filter bank of orientation-selective linear-nonlinear (LN) or energy models, but these models fail to predict neural responses to natural..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1fU8iAqKX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A rotation-equivariant convolutional neural network model of primary visual cortex</a> <a class="note_content_pdf" href="/pdf?id=H1fU8iAqKX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 13 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019a,    &#10;title={A rotation-equivariant convolutional neural network model of primary visual cortex},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=H1fU8iAqKX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=H1fU8iAqKX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Classical models describe primary visual cortex (V1) as a filter bank of orientation-selective linear-nonlinear (LN) or energy models, but these models fail to predict neural responses to natural stimuli accurately. Recent work shows that convolutional neural networks (CNNs) can be trained to predict V1 activity more accurately, but it remains unclear which features are extracted by V1 neurons beyond orientation selectivity and phase invariance. Here we work towards systematically studying V1 computations by categorizing neurons into groups that perform similar computations. We present a framework to identify common features independent of individual neurons' orientation selectivity by using a rotation-equivariant convolutional neural network, which automatically extracts every feature at multiple different orientations. We fit this rotation-equivariant CNN to responses of a population of 6000 neurons to natural images recorded in mouse primary visual cortex using two-photon imaging. We show that our rotation-equivariant network not only outperforms a regular CNN with the same number of feature maps, but also reveals a number of common features shared by many V1 neurons, which deviate from the typical textbook idea of V1 as a bank of Gabor filters. Our findings are a first step towards a powerful new tool to study the nonlinear computations in V1.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">rotation equivariance, equivariance, primary visual cortex, V1, neuroscience, system identification</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A rotation-equivariant CNN model of V1 that outperforms previous models and suggest functional groupings of V1 neurons.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">11 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Skx-rp2qnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting contribution to V1 modeling</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1fU8iAqKX&amp;noteId=Skx-rp2qnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper177 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper177 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this interesting study, the authors show that incorporating rotation-equivariant filters  (i.e. enforcing weight sharing across filters with different orientations) in a CNN model of the visual system is a useful prior to predict responses in V1. After fitting this model to data, they find that the RFs of model V1 cells do not resemble the simple Gabor filters of textbooks, and they present other quantitative results about V1 receptive fields. The article is clearly written and the claims are supported by their analyses. It is the first time to my knowledge that a rotation-equivariant CNN is used to model V1 cells.

The article would benefit from the following clarifications:

1. The first paragraph of the introduction discusses functional cell types in V1, but the article does not seem to reach any new conclusion about the existence of well-defined clusters of functional cell types in V1. If this last statement is correct, I believe it is misleading to begin the article with considerations about functional cell types in V1. Please clarify.

2. For clarity, it would help the reader to mention in the abstract, introduction and/or methods that the CNN is trained on reproducing V1 neuron activations, not on an image classification task as in many other studies (Yamins 2014, etc). 

3. “As a first step, we simply assume that each of the 16 features corresponds to one functional cell type and classify all neurons into one of these types based on their strongest feature weight.” and “The resulting preferred stimuli of each functional type are shown in Fig. 6.“
Again, I think these statements are misleading because they suggest that V1 cells indeed cluster in distinct functional cell types rather than form a continuum. However, from the data shown, it is unclear whether the V1 cells recorded form a continuum or distinct clusters. Unless this question is clarified and the authors show the existence of functionally distinct clusters in their data, they should preferably not mention "cell types" in the text.

Suggestions for improvement and questions (may not necessarily be addressed in this paper):

4. “we apply batch normalization”
What is the importance of batch normalization for successfully training the model? Do you think that a sort of batch normalization is implemented by the visual system? 

5. “The second interesting aspect is that many of the resulting preferred stimuli do not look typical standard textbook V1 neurons which are Gabor filters. ”
OK but the analysis consists of iteratively ascending the gradient of activation of the neuron from an initial image. This cannot be compared directly to the linear approximation of the V1 filter that is computed experimentally from doing a spike-triggered average (STA) from white noise. A better comparison would be to do a single-step gradient ascent from a blank image. In this case, do the filters look like Gabors?

6. Did you find any evidence that individual V1 neurons are themselves invariant to a rotation?

7. The article could be more self-contained. There are a lot of references to Klindt et al. (2017) on which this work is based, but it would be nice to make the article understandable without having to read this other article.

Typo: Number of fearture maps in last layer 

Conclusion:
I believe this work is significant and of interest for the rest of the community studying the visual system with deep networks, in particular because it finds an interesting prior for modeling V1 neurons, that can probably be extended to the rest of the visual system. However, it would benefit from the clarifications mentioned above.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkeaZwLuaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thoughtful review with good suggestion that we address</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1fU8iAqKX&amp;noteId=SkeaZwLuaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper177 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper177 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your thoughtful and constructive review. Below we respond to your seven comments:

1. [Functional cell types]
Finding out whether V1 is organized in distinct, well-defined clusters of functional cell types is indeed the big biological question we’re after. As you point out correctly, we do not answer this question in the present paper. The contribution of the present paper is not the biological finding that there are such well-defined cell types, but instead the development and verification of methods that allow us to address this question. With current methods (i.e. Klindt et al. 2017) we could not answer this question, because two Gabor filters with identical parameters except orientation would be considered two different cell types, which is undesirable from a biological perspective. The methods presented in our paper overcome this gap and let us treat preferred orientation as a nuisance just like receptive location. Therefore, we think it is appropriate to start the paper with these considerations, as they put our work in context by stating the long-term goals. We rephrased the introduction (third paragraph) to state the contributions more clearly. Please let us know if this revision addresses your concern or if you think that a more substantial revision of the introduction is necessary.

2. [CNN trained on neural data, not image categorization] 
Thank you. We revised the abstract (2nd sentence), introduction (third paragraph) and Section 3 (last sentence of first paragraph). Let us know if it’s still not clear.

3. [Functional cell types #2]
You have a point. We changed the wording to “functional groups” wherever we describe what we did. The only place where we refer to functional cell types is the introduction where we provide the background/context of our work.

4. [Batch norm]
Batch normalization serves two purposes here: (1) it helps training and (2) it ensures the features of the last CNN layer have unit variance, which is useful given the L1 penalty on the readout weights. Note that at test time, it does not have an effect. The normalization constants can be fully absorbed into the linear weights. In other words, if we gave you a trained model, you would not be able to tell whether it was trained with batch normalization or without, because they are indistinguishable at test time.

5. [Non-standard filters]
We agree that they are not directly comparable. The point is that this model-based procedure reveals deviations from the linear models. It not only shows that spike-triggered average from white noise is insufficient for characterizing V1 neurons, but also provides a means for characterizing them. Regarding your question about a single gradient step from a blank image: these indeed tend to look more similar to standard Gabor filters (we performed such a comparison in a different project not using rotation equivariance and not published yet). We can create a new Figure analogous to Fig. 6 but using a single gradient step and add it to the final version of the paper.

6. [Rotation-invariant neurons]
No, except for the trivial ones that have circularly symmetric center-surround receptive fields (or preferred stimuli), we did not find any evidence for rotation invariant neurons.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HkesEK4KaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A rigorous rebuttal</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1fU8iAqKX&amp;noteId=HkesEK4KaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper177 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper177 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I would like to thank the authors for addressing thoroughly all my concerns. These clarifications confirm to me the rigor and quality of the work. 

"Regarding your question about a single gradient step from a blank image: these indeed tend to look more similar to standard Gabor filters (we performed such a comparison in a different project not using rotation equivariance and not published yet). We can create a new Figure analogous to Fig. 6 but using a single gradient step and add it to the final version of the paper."

I think this would be a valuable addition to the paper because it shows how the model can also account for our old conception of V1 RFs (gabor-like RFs obtained from white noise stimulation, which is the filter corresponding to the best linear approximation of the cell's response). It might also help addressing the concern of Rev. 1: "Many of the receptive fields in Figure 6 look pathological (overfitted?) compared to typical V1 receptive fields in the literature."

To justify the mathematical equivalence of a one-step gradient ascent from a blank image to STA in response to a perturbative white-noise stimulus, I think you could cite:

Melinda E. Koelling and Duane Q. Nykamp. Computing linear approximations to nonlinear neu-
ronal response.Network (Bristol, England), 19(4):286–313, 2008. ISSN 1361-6536. doi:10.1080/09548980802503139

Odelia Schwartz, Jonathan W. Pillow, Nicole C. Rust, and Eero P. Simoncelli. Spike-triggered neural
characterization.Journal of Vision, 6(4):13, July 2006. ISSN 1534-7362. doi: 10.1167/6.4.1</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_HklYxH1ShX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Does rotation equivariance add neuroscientific insight?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1fU8iAqKX&amp;noteId=HklYxH1ShX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper177 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper177 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">﻿This paper applies a rotation-equivariant convolutional neural network model to a dataset of neural responses from mouse primary visual cortex. This submission follows a series of recent papers using deep convolutional neural networks to model visual responses, either in the retina (Batty et al., 2016; McIntosh et al., 2016) or V1 (Cadena et al., 2017; Kindel et al., 2017; Klindt et al., 2017). The authors show that adding rotation equivariance improves the explanatory power of the model compared to non-rotation-equivariant models with similar numbers of parameters, but the performance is not better than other CNN-based models (e.g. Klindt et al., 2017). The main potential contributions of the paper are therefore the neuroscientific insights obtained from the model. However, I have concerns about the presented data and the validity of rotation equivariance in modeling visual responses in general (below). Together with the fact that the model does not provide better explanatory power than other models, I cannot recommend acceptance. I am open to discussions with the authors, but do not anticipate a major change in the rating.

1. As noted by the authors, the finding that “Feature weights are sparse” (page 6) could be due to the sparsity-inducing L1 penalty. The fact that a model without L1 penalty performs worse does not mean that there is sparsity in the underlying data. For example, the unregularized model could be overfitting. A more careful model selection analysis is necessary to show that the data is better fit by a sparse than a dense model. 

2. The finding that there are center-surround or asymmetric (non-gabor) RFs in mouse V1 is not novel and not specific to this model (e.g. Antolik et al., 2016). 

3. Many of the receptive fields in Figure 6 look pathological (overfitted?) compared to typical V1 receptive fields in the literature. I understand that sensitivity to previously undetected RF features is a goal of the present work. However, given how unusual the RFs look, more controls are necessary to ensure they are not an artefact of the method, e.g. the activation maximization approach with gradient preconditioning, the sparsity constraints, or overfitting. Perhaps a comparison of RFs learned on two disjoint subsets of the training set would help to determine which features are reproducible.

4. Should orientation be treated as a nuisance variable? Natural image statistics are not rotation-invariant. In the visual system, especially in mice, it is not clear whether orientation is completely disentangled from other RF properties. The orientation space is not uniformly covered, and some directions have special meaning (e.g. cardinal directions), such that it might be invalid to assume that the visual system is equivariant to rotation. (The same concern applies to the translation equivariance assumed when modeling visual RFs with standard CNNs.) Of course, there is a tradeoff between model expressiveness and the need to make assumptions to fit the model with realistic amounts of data. However, this concern should at least be discussed.

5. Some more details about the neural recordings would be good. What calcium indicator? How was the recording targeted to V1? Perhaps some example traces.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkl4DDIda7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Careful review, but the score seems too harsh or based in part on misunderstandings</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1fU8iAqKX&amp;noteId=Bkl4DDIda7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper177 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper177 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your careful review. We would like to start by clarifying the contributions of our paper before providing responses to your five specific points.

As also mentioned in the response to AnonReviewer3, our long-term goal is to find out whether V1 is organized in distinct, well-defined clusters of functional cell types. However, this is a complex biological question that will require additional, very careful and extensive data analysis as well as potentially further direct experimental verification. The contribution of the present paper are therefore not the biological insights, but instead the development and verification of methods that will allow us to address this question. Specifically, this means (1) adapting rotation-equivariant CNNs to the problem of predicting neural responses, (2) showing that it can successfully be trained using a steerable basis for the filters, (3) showing that it outperforms conventional CNNs, and (4) showing that it does so with substantially fewer features than previous methods. We revised the introduction to make this point more explicit.

In addition, on re-reading your comments, we believe there may be another misunderstanding regarding performance that we would like to clarify. You write:

“The authors show that adding rotation equivariance improves the explanatory power of the model compared to non-rotation-equivariant models with similar numbers of parameters, but the performance is not better than other CNN-based models (e.g. Klindt et al., 2017).”

This sentence mentions “non-rotation-equivariant models” and “other CNN-based models (e.g. Klindt et al., 2017)” as if they were two different entities. However, the non-rotation-equivariant model we use is exactly that of Klindt et al. 2017 (more or less literally, their code is public). So our model *does perform better* than that of Klindt et al. 2017. The only reason why the numbers are not better is because it is a different (larger) dataset.

We hope this clarifies the contribution. Now to your comments about the biological findings:

1. [Sparsity] 
We agree that the fact that a model without L1 penalty performs worse does not mean that there is sparsity in the underlying data and we are open to suggestions on how to better substantiate that point. The unregularized model is clearly overfitting, and by using cross-validated regularization we can only get better. However, the fact that L1 regularization leads to such a strong improvement, does suggest that the sparsity assumption is pretty good. We would be grateful if you could expand on what you mean by “a more careful model selection analysis.”

2. [Novelty of center-surround/asymmetric RFs]
You are right that these have of course been observed before (also long before Antolik 2016). However, what’s different is the prevalence of such non-Gabor RFs. For instance, if you look at Fig. 3 of Antolik 2016, you will notice that most RFs that are discernible are actually quite clean Gabors, while for the non-linear neurons (pink boxes in his Fig. 3) he does not obtain good RFs (unsurprisingly).

3. [Pathological RFs in Fig. 6]
Thank you – this is a good suggestion. We will make sure to include such an analysis in the final version. Given our experience with activity maximization approaches, we expect them to look very similar overall, except for some of the noisy features in the surround (but certainly not more variable than across different neurons within the same group, which have different receptive field locations and have therefore seen different stimuli; see also response to AnonReviewer2). 

4. [Should orientation be treated as a nuisance variable?]
We agree that the visual system is neither equivariant to rotation nor to translation. However, that does not undermine the usefulness of equivariant representations at all. If your concern was valid and, for instance, horizontal filters looked completely different from vertical ones, then our rotation-equivariant network would not perform better than a regular CNN, because it would require a large number of features. But that’s not what we find. We find that the assumption of rotation equivariance (1) *does* lead to a substantial improvement over a regular CNN (see also our clarifications in the other comment) and (2) does so with fewer features, showing that it’s a good assumption to make. We added a sentence discussing this point (2nd sentence in the discussion). We will also add a plot showing the distribution of orientations being used at the readout stage for each feature, which will address the question of how rotation-equivariant the representation in mouse V1 actually is.

5. [Experimental details]
We added those details (Section 4, subsection “Neural data”) and will show example traces in the final version.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HkxrJkvlAX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Mainly concerned about sparsity and RF structure</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1fU8iAqKX&amp;noteId=HkxrJkvlAX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper177 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper177 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I acknowledge that I misunderstood the performance comparison. Providing a model with better fit quality is a considerable contribution.

Regarding my other comments:

1. Sparsity: One obvious control would be to use L2 instead of L1 regularization. Are L2-regularized models statistically significantly worse than L1-regularized models? Or did I miss a reason for why trying L2 regularization is not possible?

2. Novelty of center-surround RFs: Perhaps this could be discussed in the text as you did in your response above.

3. RF structure: Thanks, I am looking forward to the new analyses. Just to add more detail, one reason why I am skeptical is that many of the RF features, e.g. in 12 and 13 in Figure 6, look very small. What is the size of these features (in degrees of visual space)? A scale bar in the RF plots would be helpful. How does this compare to published values for the resolution of the mouse visual system (e.g. both Niell and Stryker, 2008, and Marshel et al., 2011, report a mean preferred spatial frequency of about 0.04 cycles per degree)?

4. Orientation as a nuisance variable: Thanks for adding the additional discussion.

5. Thanks for adding additional details.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJgAi-CC3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Rotation-equivariant model does beat regular CNN</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1fU8iAqKX&amp;noteId=rJgAi-CC3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper177 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper177 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for reviewing our paper. We would like to make a quick clarification right away, which we hope will change your assessment. We will provide a more detailed response to the other comments later.

There seems to be a misunderstanding about the performance of the model. As shown in Table 1, our rotation-equivariant CNN does outperform a regular CNN (Klindt et al. 2017). 

A couple of more detailed points to also keep in mind in this respect:
- We are quite conservative with the model comparison: Table 1 shows the rotation-equivariant model with 16 features, which is not even the best-performing one among all the rotation-equivariant ones we tested (Fig. 2).
- Related to above, the regular CNN has been subjected to an equally rigorous hyperparameter search, with the range of hyperparameters taken from Klindt et al (2017). Thus, the comparison is as fair as we can make it.
- The performance in absolute numbers is lower than that in Klindt et al. (2017), but these numbers are not comparable because different datasets are used. There is quite some variability across datasets (see, e.g., Table 1 in Klindt et al. 2017).</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HkerUYEy67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Performance was comparable to earlier studies</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1fU8iAqKX&amp;noteId=HkerUYEy67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper177 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper177 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the comment. A fair assessment, given the variability between datasets, would be that your model performs similarly to previous studies, as you stated in the text ("Performance was comparable to earlier studies modeling V1 responses with similar stimuli (Klindt et al., 2017; Antolík et al., 2016).

I agree that you made a very rigorous effort in comparing the models (e.g. the hyperparameter search for control models).

My thinking was that an improvement in model fit quality cannot be considered a main contribution of the paper, because the difference is within measuring variability. In any case, I don't think it is essential to show a performance improvement. The insights gained from the model are more important. This is what my main comments are about.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1gsFfq1Tm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>One should not compare performance across studies</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1fU8iAqKX&amp;noteId=H1gsFfq1Tm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper177 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper177 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your quick reply and the clarification. We will address the other points shortly. Just one more quick comment about performance, because we think it's important to be on the same page.

For a given dataset, the difference in performance between our model and a conventional CNN is *not* within measuring variability. We report the SD of the best models in Table 1, and the difference is several SDs between the two models.

The difference in performance *across datasets* is not very meaningful, because the absolute numbers depend on factors such as the signal-to-noise ratio of the recordings, the brain state of the animal and the number of repeats per image. We should have avoided that one sentence about performance being comparable to earlier studies for this very reason.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_SylAnad4om" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting work on matching CNN filters to Neurons</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1fU8iAqKX&amp;noteId=SylAnad4om"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper177 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper177 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper analyses the data collected from 6005 neurons in a mouse brain. Visual stimuli are presented and the responses of the neurons recorded. In the next step, a rotational equivariant neural network architecture together with a sparse coding read-out layer is trained to predict the neuron responses from the stimuli. Results show a decent correlation between neuron responses and trained network. Moreover, the rotational equivariant architecture beats a standard CNN with similar number of feature maps. The analysis and discussion of the results is interesting. Overall, the methodological approach is good.

I have trouble understanding the plot in Figure 4, it also does not print well and is barely readable on paper.

I have a small problem Figure 6 where "optimal" response-maps are presented. From my understanding, many of those feature maps are not looking similar to feature maps that are usually considered. Given the limited data available and the non-perfect modeling of neurons, the computed optimal response-map might include features that are not present in the dataset. Therefore, it would be interesting to compare those results with the stimuli used to gather the data. E.g. for a subset of neurons, one could pick the stimulus that created the maximum response and compare that to what the stimulus with the maximum response of the trained neuron was. It might be useful to include the average correlation of the neurons belong to each of the 16 groups(if there are any meaningful differences), especially as the cut-off of "correlation 0.2 on the validation set" is rather low.

Note: I am not an expert in the neural-computation literature, I am adapting the confidence rating accordingly.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJgB5uLO6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We will address the comments about Fig. 6 with additional analyses</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1fU8iAqKX&amp;noteId=rJgB5uLO6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper177 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper177 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for reviewing our paper and providing constructive comments.

Regarding the optimal stimuli presented in Fig. 6: Thank you for this suggestion. We will investigate whether an analysis of the optimal stimulus in the training set is helpful. Unfortunately, this may not be the case, because neural responses are highly variable and therefore the stimulus that happened to elicit the strongest response may not actually be the one the drives the neuron most on average. In addition, we have done such analysis in the past on networks trained for large-scale object recognition and found that one needs several hundred thousand image patches to find examples close to the optimum (and these networks do not have observation noise). Note, however, that the variability across neurons within one group is a good indicator of which aspects of the optimal stimuli are reproducible vs. noise, because the neurons have different receptive field locations and have therefore seen different stimuli during the experiment. See also the suggestion by AnonReviewer1 to split the data in two halves, which we will perform for the final version.

We will add the average correlations of the neurons shown in Fig. 6 as you suggested.

Regarding your trouble with Fig. 4: We will make it full width, so it is more legible. Overall, the point of the figure is to show that for a large fraction of neurons the weights are very sparse. We are open to any suggestions on how to improve the Figure.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>