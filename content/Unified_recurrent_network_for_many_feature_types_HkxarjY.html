<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Unified recurrent network for many feature types | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Unified recurrent network for many feature types" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Hkxarj09Y7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Unified recurrent network for many feature types" />
      <meta name="og:description" content="There are time series that are amenable to recurrent neural network (RNN) solutions when treated as sequences, but some series, e.g. asynchronous time series, provide a richer variation of feature..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Hkxarj09Y7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Unified recurrent network for many feature types</a> <a class="note_content_pdf" href="/pdf?id=Hkxarj09Y7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019unified,    &#10;title={Unified recurrent network for many feature types},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Hkxarj09Y7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=Hkxarj09Y7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">There are time series that are amenable to recurrent neural network (RNN) solutions when treated as sequences, but some series, e.g. asynchronous time series, provide a richer variation of feature types than current RNN cells take into account. In order to address such situations, we introduce a unified RNN that handles five different feature types, each in a different manner. Our RNN framework separates sequential features into two groups dependent on their frequency, which we call sparse and dense features, and which affect cell updates differently. Further, we also incorporate time features at the sequential level that relate to the time between specified events in the sequence and are used to modify the cell's memory state. We also include two types of static (whole sequence level) features, one related to time and one not, which are combined with the encoder output. The experiments show that the proposed modeling framework does increase performance compared to standard cells.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">sparse, recurrent, asynchronous, time, series</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We introduce a unified RNN that handles five different feature types, each in a different manner.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BJg382TuTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Modified LSTM cell updates considering different feature types</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkxarj09Y7&amp;noteId=BJg382TuTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper127 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper127 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a new type of recurrent neural network which takes into account five different features: in addition to the prevalent dense features, the author(s) also consider(s) sparse features, time features, global static and changing features. The differences in feature types are reflected in the cell state or output state update rules. Experiments on a modified UCI dataset and a proprietary dataset show that the proposed model outperforms the time-variant LSTM (TLSTM).

Pros:
1. By decomposing the cell state into different components for different feature types (dense vs sparse, short term vs long term) and update them in different manners, the model takes advantage of the feature type information.
2. By updating sparse feature related cell states only when sparse features are present, it could be potentially computationally cheaper than treating everything as dense (although in the paper due to more parameters the proposed model is actually slower).

Cons:
1. The contributions are not significant. It seems that TLSTM already "accounts for asynchronous feature sampling" (Sec 1) and the novelty here lies most in how sparse features are treated.
2. The presentation is sometimes confusing. For example, in Figure 5 which presents the main results, what's the "relative change in F1 score" and what's the unit in the plot? If it's percentage the gains seem to be too small and could be potentially due to the additional parameters. Besides, what does "group sampling" mean exactly? Furthermore, legend seems to be missing.
3. Crucial implementation details are missing. The paper mentions that "the number of features grows" in the proposed model. Are sparse features and static features used or not in TLSTM? 
4. What's the difference between a static decay feature (Sec 3.5) and decay features (Sec 3.2)? Isn't the static decay feature varying with time as well?

Minor comments:
1. Figure 1 and 5 are too small and hard to read.
2. Sec 3.3, "updated based on Equation 1 and Equation 2", but none of the equations are numbered in this paper.
3. Some discussions on the proprietary dataset seem to be irrelevant. I'd rather see how are sparse features generated for the UCI dataset.
4. The decay function $g= 1 / log (e + \alpha^T x_t^{\delta})$, how can we make sure that as time passes it decreases as time passes?


Overall, I think explicitly taking into account different feature types in the LSTM cell update rules is interesting, but the contributions of this paper compared to TLSTM are not significant enough for acceptance, and the presentation can be made more clear.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1lzLxgbAm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: Modified LSTM cell updates considering different feature types</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkxarj09Y7&amp;noteId=S1lzLxgbAm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper127 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper127 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the review and helpful comments. We have responded to your concerns below.

Con 1: TLSTM accounts more for irregular time between samples as opposed to asynchronous feature sampling, where different features are sampled at every time step.  The modification of TLSTM updates is an expansion on that work, but the inclusion of sparse features tackles a completely different problem than TLSTM and in our opinion is novel.

Con 2: The plots do show the percentage gains in score as you guessed. We can modify the figures to make this clearer. We could also include a legend showing which line corresponds to which feature, but it’s not clear how much value this would provide. While there is a disparity in the number of parameters between the models, additional experiments showed that the base model did not improve by adding more parameters. We will add this information to the text. All information on sequence processing for the power consumption data set can be found in the Appendix.


Con 3: The quoted part should read “the number of sparse features grows” and will be updated. Sparse features are used in TLSTM but are simply treated as dense features. We can clarify this in the text. In Section 4.1 we mention that static features are present for all experiments, with the exception of the Table 1b which investigates their effectiveness.

Con 4: Static decay and decay features are very similar, however as stated in Section 3.5 “the static decay features apply to the sequence as a whole rather than an individual time step.” A static decay feature does vary with time and could be the time elapsed between the final time step of the sequence and the prediction time being used for that sequence.

Minor comment 1: We can make the figure/fonts bigger as the length restrictions allow.

Minor comment 2: There are labels for groups of equations on Page 4.

Minor comment 3: More details can be found in the Appendix.

Minor comment 4: We restrict this with \alpha \geq 0 so that the function is non-increasing. This restraint will be added to the text.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Skeo_ksLaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>a study of time series feature types for RNN models</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkxarj09Y7&amp;noteId=Skeo_ksLaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper127 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper127 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper addresses limitations of the LTSM method for modeling time series. The work is motivated by applications where multiple time series need to be combined while they may get updated in an asynchronous fashion. Authors mention IoT applications in the Intro and give examples from a power consumption data set and a churn prediction application in their numerical experiments sections. 
The paper's main contribution is to shrink down time series into 5 different categories which have different sampling schemes: (1) dense (2) sparse (3) decay (4) static decay (3) static standard, and proposing building blocks to incorporate these in a unified recurrent mechanism. 
The paper's motivation for introducing sparse input to LTSM was rather straightforward and convincing, and the churn prediction application is an excellent motivation for this. I had a harder time following why the 3 other types of features needed to be included as well at this point. Perhaps a more problem oriented explanation with concrete examples could have helped. Or those features should not yet be used as generalizations but kept for future work.
Overall I think the paper has several interesting ideas. Even though not all are as convincing, I think the paper is thought provoking and may interest the ICLR community.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyxE3UJb0Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: a study of time series feature types for RNN models</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkxarj09Y7&amp;noteId=SyxE3UJb0Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper127 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper127 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the comments, we are glad that you found the paper thought provoking. In light of your comments about the motivation, we will add explicit examples for each subsection in Section 3.5 to make this clearer, but our churn dataset did in fact support all five data types. Examples of each type for the power consumption dataset can be found in the Appendix. Although some of these features were synthetically created to mirror our churn dataset, they can still give some insight to the need for each feature. A decay feature such as the time between timesteps allows us to take into account the nonuniformity between events. For a static decay feature we used the time between the last event and the prediction time. The intention is to remove short term contributions in the prediction when there is a long-time gap after the final event. Static decay features for the power consumption data set are day of week, day of month, and time of day as these feature are informative but are relevant on a sequence level but not on a time step level.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SkgQlMh92m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting problem often faced in practice</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkxarj09Y7&amp;noteId=SkgQlMh92m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper127 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper127 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary
========
The paper addresses the problem of irregular spacing in sequential data with five different irregularity types. The solution is based on modifying LSTM cell.

Comment
========
Irregular and multiple spacing presents in many real world applications with time-stamped events. The problem is therefore important to address properly. However, I found the evaluation of the proposed solution is less convincing. The main results in Figure 5 are not informative. 

There have been related works addressing irregular and multiple spacing (not just missing data as cited in the paper):

- Pham, T., Tran, T., Phung, D., &amp; Venkatesh, S. (2016, April). DeepCare: A deep dynamic memory model for predictive medicine. In Pacific-Asia Conference on Knowledge Discovery and Data Mining (pp. 30-41). Springer, Cham.
- Koutnik, J., Greff, K., Gomez, F., &amp; Schmidhuber, J. (2014). A clockwork RNN. arXiv preprint arXiv:1402.3511.
- Chen, C., Kim, S., Bui, H., Rossi, R., Koh, E., Kveton, B., &amp; Bunescu, R. (2018, October). Predictive Analysis by Leveraging Temporal User Behavior and User Embeddings. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management (pp. 2175-2182). ACM.



</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1xJivtxAX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: Interesting problem often faced in practice</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkxarj09Y7&amp;noteId=B1xJivtxAX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper127 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper127 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would appreciate receiving further comments and details regarding why the proposed solution is not convincing, and why the main results are not informative? Especially since we see our model performing better under most circumstances.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ryeOGpb5hX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Brings together existing work and adds a bit, somewhat limited experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkxarj09Y7&amp;noteId=ryeOGpb5hX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper127 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper127 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">[Relevance] Is this paper relevant to the ICLR audience? yes

[Significance] Are the results significant? somewhat

[Novelty] Are the problems or approaches novel? reasonably

[Soundness] Is the paper technically sound? yes, I think. I did not thoroughly check the equations.

[Evaluation] Are claims well-supported by theoretical analysis or experimental results? somewhat

[Clarity] Is the paper well-organized and clearly written? yes, except the experiments

Confidence: 2/5

Seen submission posted elsewhere: No (but I did find it on arXiv after writing the review)

Detailed comments:

In this work, the authors propose a new type of memory cell for RNNs which account for multiple types of time series. In particular, the work combines uniformly sampled observations (“normal” RNN input), time-decaying observations, non-decaying observations which may change, and static features which are not time-dependent. Empirically, the proposed approach outperforms an RNN with TLSTM cells in some cases.

=== Comments

I found the proposed approach for incorporating the different types of time series reasonable. This work definitely leans heavily on ideas from TLSTM and others, but, to the best of my knowledge, the specific combination and formulation is novel, especially concerning the “non-decaying time series” observations.

 However, I had difficult coming up with an example of a “static decay feature”. It would be helpful to give a concrete example of one of these in the main text. (It is also not clear to me why the difference in time between the time of the last event in a sequence and the prediction time for that sequence would be considered a “static decay” feature rather than just a “static” feature.)

My main concern with the paper is that the experimental design and results are not especially easy to follow; consequently, they are not as convincing as they might be. First, the sparsity mechanism is rather simple. In many domains (e.g., the medical domain considered in several of the cited papers), missingness is non-uniform and is often meaningful. While “meaningfulness” may be difficult to simulate, burstiness (non-uniformity) could be simulated. Second, for the groups, it is not clear whether all combinations of, e.g., 2 (informative) feature were sparsely sampled or if only one group of 2 was chosen. If the former, then some measure of variance should be given to help estimate statistical significance. Third, the particular classification task here is, essentially, forecasting one of the input variables. While that is certainly a relevant problem, many other time series classification or regression problems are not tied so directly to observations. It is not clear if these results are relevant in that setting.

=== Typos, etc.

The plots and figures in the paper are very difficult to read. Larger versions, or at least versions with increased fonts, should be used.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJxFHKFxA7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: Brings together existing work and adds a bit, somewhat limited experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkxarj09Y7&amp;noteId=rJxFHKFxA7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper127 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper127 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the review and your detailed comments. Below are our responses to some of your concerns.

Comment: However, I had difficult coming up with an example of a “static decay feature”. It would be helpful to give a concrete example of one of these in the main text. (It is also not clear to me why the difference in time between the time of the last event in a sequence and the prediction time for that sequence would be considered a “static decay” feature rather than just a “static” feature.)

It seems that giving examples of each feature type in the subsections of Section 3 would help explain the motivation for incorporating each type of a feature. There is one ‘static decay’ feature mentioned in the Appendix, and as you mentioned it is the time between the last event and the prediction time. This is treated differently from ‘static’ features for the same reason ‘decay’ features are not ‘dense’ features, i.e., they are used to decompose the output of the RNN network (or the memory state for ‘decay’ features) into short and long-term components. The intention is to remove short term contributions in the prediction when there is a long-time gap.

Comment: First, the sparsity mechanism is rather simple. In many domains (e.g., the medical domain considered in several of the cited papers), missingness is non-uniform and is often meaningful. While “meaningfulness” may be difficult to simulate, burstiness (non-uniformity) could be simulated.

There are actually two sampling mechanisms for the power consumption dataset, random and group sampling, which are detailed in the appendix. We agree that the random method is rather simple and so we also included group sampling to simulate the ‘burstiness’ you mention. While this may still be missing some information that cannot be simulated easily, we can say that our churn dataset did exhibit natural non-uniformity.

Comment: Second, for the groups, it is not clear whether all combinations of, e.g., 2 (informative) feature were sparsely sampled or if only one group of 2 was chosen. If the former, then some measure of variance should be given to help estimate statistical significance. 

It was only one subset of the two, determined by which features most improved predictions individually that are treated as sparse. This clarification will be added to the manuscript. 

Comment: Third, the particular classification task here is, essentially, forecasting one of the input variables. While that is certainly a relevant problem, many other time series classification or regression problems are not tied so directly to observations. It is not clear if these results are relevant in that setting.

While we cannot share much detail on the churn dataset, we can reveal that the labels are not tied directly to the input features. In churn, the prediction is if a customer is likely to churn while the features are for example customer interactions with the company. So the predictions are completely decoupled from input features.  Despite this we still saw performance gains for our model over the base model.  The tie mentioned in your comment is present in the power dataset since the data does not include other possible labels. 


</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>