<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learning Grounded Sentence Representations by Jointly Using Video and Text Information | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learning Grounded Sentence Representations by Jointly Using Video and Text Information" />
        <meta name="citation_author" content="Patrick Bordes" />
        <meta name="citation_author" content="Eloi Zablocki" />
        <meta name="citation_author" content="Laure Soulier" />
        <meta name="citation_author" content="Benjamin Piwowarski" />
        <meta name="citation_author" content="Patrick Gallinari" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJe8niAqKX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learning Grounded Sentence Representations by Jointly Using Video..." />
      <meta name="og:description" content="Visual grounding of language is an active research field aiming at enriching text-based representations with visual information. In this paper, we propose a new way to leverage visual knowledge for..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJe8niAqKX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning Grounded Sentence Representations by Jointly Using Video and Text Information</a> <a class="note_content_pdf" href="/pdf?id=BJe8niAqKX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=patrick.bordes%40lip6.fr" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="patrick.bordes@lip6.fr">Patrick Bordes</a>, <a href="/profile?email=eloi.zablocki%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="eloi.zablocki@gmail.com">Eloi Zablocki</a>, <a href="/profile?email=laure.soulier%40lip6.fr" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="laure.soulier@lip6.fr">Laure Soulier</a>, <a href="/profile?email=benjamin.piwowarski%40lip6.fr" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="benjamin.piwowarski@lip6.fr">Benjamin Piwowarski</a>, <a href="/profile?email=patrick.gallinari%40lip6.fr" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="patrick.gallinari@lip6.fr">Patrick Gallinari</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Visual grounding of language is an active research field aiming at enriching text-based representations with visual information. In this paper, we propose a new way to leverage visual knowledge for sentence representations. Our approach transfers the structure of a visual representation space to the textual space by using two complementary sources of information: (1) the cluster information: the implicit knowledge that two sentences associated with the same visual content describe the same underlying reality and (2) the perceptual information contained within the structure of the visual space. We use a joint approach to encourage beneficial interactions during training between textual, perceptual, and cluster information. We demonstrate the quality of the learned representations on semantic relatedness, classification, and cross-modal retrieval tasks.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">multimodal, sentence, representation, embedding, grounding</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a joint model to incorporate visual knowledge in sentence representations</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">5 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HyePcSFgaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJe8niAqKX&amp;noteId=HyePcSFgaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper712 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper712 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1xcSHYlTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>General comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJe8niAqKX&amp;noteId=r1xcSHYlTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper712 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper712 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank all the reviewers for their interesting remarks that will help us to enhance the paper. We withdraw the paper but we would like to provide some elements of answer to clarify our motivations and detail some elements that needed more thorough explanation in our submission.

In our paper, "visual grounding of language" corresponds to the frameworks that leverage visual information to enhance text comprehension and NLP tasks. Visual information stands here as common sense information, not necessarily aligned with textual information. We particularly investigate visual grounding within the scope of sentence representations, which is referred as "visual grounding of sentences".
The evaluation of visual grounding is difficult since the effectiveness of such approaches can only be captured by a series of indirect tasks (semantic relatedness, classification, paraphrase identification, image-sentence retrieval). An analysis of effectiveness metrics and structural measures shows that cluster and perceptual information perform better than the text-only, projection and sequential baselines.

Moreover, we would like to precise some particular points regarding the model and the evaluation.

- In our semantic similarity evaluations, we evaluate our representations with simple testing of cosine distance and we do not train models unlike [Kiros et al. 2015].
- In section 2.3, we write &lt;u,v&gt; for the dot product between vectors u and v; thus, the dot product between the sum of word embeddings and the projected frame representation is a softmax weight.
- Concerning the MSVD video captioning dataset: it contains 1970 videos, and each video is described by an average of 41 sentences. For a given video, each caption describes the totality of the video (not just isolated frames). Furthermore, MSVD contains approximately 80,000 sentences, which represent only 0.1 percent of the 74 million sentences in the Toronto BookCorpus dataset. Thus, the text available in the vision side is indeed orders of magnitude smaller to the text in the textual side.
- In the video model R, we represent each video of the MSVD corpus with a random vector. As a result, the average cosine similarity between two videos is 0. The loss function imposes that the textual similarities should be close to 0 when sentence describe distinct videos. I.e., it drives away visually different sentences in the textual space, which can explain the boost in performance compared to the text-only baseline. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1lbgrj937" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Idea not convincing enough</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJe8niAqKX&amp;noteId=H1lbgrj937"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper712 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper712 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a method to use videos paired with captions to improve sentence embeddings. They report improved performance on several semantic similarity benchmarks. I do not find the ideas significant. The cluster hypothesis and the perceptual hypothesis should both be naturally captured by projections between visual and textual space. The improvements over projections are also minimal. 

I was curious about how the model performance compares to the best model on STS (not just among multimodal baselines).   
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SygAfNW5hm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Text-and-video based sentence representations</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJe8niAqKX&amp;noteId=SygAfNW5hm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper712 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper712 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This submission proposes a model for sentence learning sentence representations that are grounded, based on associated video data. The model consists in a loss function composed of three terms. The first one is a text-only loss, for instance skip-thought. The second one pushes closer together representations of sentences that are describing the same video. The third one enforces a good correlation between similarity scored computed from text and video representations. The obtained representations are evaluated on standard transfer tasks for NLP and caption and image retrieval on COCO.

- First of all, the proposed model has very weak performance, bellow a simple baseline such as average fasttext vector + logistic regression on tasks that are part of senteval. This raises doubts about the importance of grounding, as the baseline sentence representation does not even beat such a trivial baseline. Even for caption and image retrieval, better results could probably be obtained with a CCA between average word vectors and say VGG features.

- Results in Table 4 are hard to parse. Please avoid reporting all possible variants of the model, and move these comparison to a separate ablation study. Moreover, the table lacks many simple baselines, which beat the proposed approach with no visual information whatsoever. 

- The related work section does not mention works on image or video captioning. Here is a loose list of at least a few models that cope with two modalities and would be worth mentioning, while the related work states that "visual grounding of sentences is quite new":
- Barnard, Kobus, Pinar Duygulu, David Forsyth, Nando de Freitas, David M. Blei, and Michael I. Jordan. "Matching words and pictures." Journal of machine learning research 3, no. Feb (2003): 1107-1135.
- Hodosh, Micah, Peter Young, and Julia Hockenmaier. "Framing image description as a ranking task: Data, models and evaluation metrics." Journal of Artificial Intelligence Research 47 (2013): 853-899.
- Regneri, Michaela, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal. "Grounding action descriptions in videos." Transactions of the Association of Computational Linguistics 1 (2013): 25-36.
- Socher, Richard, Andrej Karpathy, Quoc V. Le, Christopher D. Manning, and Andrew Y. Ng. "Grounded compositional semantics for finding and describing images with sentences." Transactions of the Association of Computational Linguistics 2, no. 1 (2014): 207-218.

Overall, I think that the model proposed in this work is barely novel and the experimental results are very weak. The proposed approach can be outperformed by a very simple word-vector-based baseline. Moreover, the submission lacks a good discussion of work on joint modeling of visual and textual domains. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1eXVRJF3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice idea but I am not sure about the empirical evaluation.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJe8niAqKX&amp;noteId=r1eXVRJF3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper712 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper712 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a method for improving text-based sentence embeddings through a joint multimodal framework. 

Different from typical multimodal work, the proposed model DOES NOT project text and vision derived representations into a single space. Instead the proposed model uses a joint loss function that allows text-derived representations and the vision-derived representations to influence each other, while remaining in their separate representation spaces. In particular the loss functions ensure the following:

Sentences that describe the same image should have similar text-derived representations. The similarity of text-derived representations of a pair of sentences should correlate with similarity of the representations of their corresponding images.

The paper then describes multiple experiments that test whether the vision-based loss functions lead to improvements over text-only representations and if this method of incorporation vision information is better than projection based methods for multimodal representations. 

Strengths:

1. The problem is well motivated and the approach is interesting in that it tries to avoid a difficult problem of projecting text and visual features into a single space. These modalities don’t often have 1-1 correspondence in the kinds of information they contain. 
2. The main idea can be implemented in a simple fashion using loss functions, without requiring any changes to the original models that compute the representations.

Issues:

The main issue I had is with respect to the empirical evaluations. In general I found the presentation hard to follow and the results don’t look convincing for an end application. Here are some follow up questions:

1. The experimental results in Table 4 seem to suggest that there is no single setting works consistently better than the projection method across all tasks. 

2. Skipthought’s spearman rho numbers in [Kiros et al., 2015] is much higher (~77 whereas whats reported in Table 1 here is only 52). Is this due to simple testing of the cosine distance as opposed to training?

3. Temporal grounding discussion was unclear for me. There appears to be an attention model that somehow picks out relevant frames using the word embeddings and the frame representation. There seem to be two corresponding vectors that are used but how is this turned into a softmax weight? Is there a dot product that is missing somewhere?

4. Are the sentences associated with each frame treated separately (or) all sentences for a video simply treated as multilabels?

5. Can you argue whether the gains arise due to vision related information or because this setting simply introduces some additional training data . It would be cleaner to control for this if possible (ensuring that the text only and grounded models have same amounts of training somehow). Maybe this is a non issue because the size of the text available from the vision side (i.e. the # of sentences associated with the videos) is orders of magnitude smaller to what you get for text alone. I suspect this is the case but it would be helpful to point this out or control for it. 

6. I did not understand the explanation for random frame choice . What do you mean by “even when anchors bear no perceptual semantics”?

To summarize, I liked the main idea behind the paper. It is simple and avoids a tricky issue of grounding different modalities in one place. The empirical evaluation is a bit hard to follow because of missing details and the results with respect to end tasks does not look convincing.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>