<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HkElFj0qYQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="PPD: Permutation Phase Defense Against Adversarial Examples in Deep..." />
      <meta name="og:description" content="Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HkElFj0qYQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning</a> <a class="note_content_pdf" href="/pdf?id=HkElFj0qYQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019ppd:,    &#10;title={PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HkElFj0qYQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Deep neural networks have demonstrated cutting edge performance on various tasks including classification. However, it is well known that adversarially designed imperceptible perturbation of the input can mislead advanced classifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a novel method to resist adversarial attacks. PPD combines random permutation of the image with phase component of its Fourier transform. The basic idea behind this approach is to turn adversarial defense problems analogously into symmetric cryptography, which relies solely on safekeeping of the keys for security. In PPD, safe keeping of the selected permutation ensures effectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10 datasets yielded state-of-the-art robustness against the most powerful adversarial attacks currently available.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">permutation phase defense, adversarial attacks, deep learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Permutation phase defense is proposed as a novel method to guard against adversarial attacks in deep learning.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">28 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_B1xMkk49am" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>FFT after performing a permutation?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=B1xMkk49am"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper416 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">It doesn't make intuitive sense why you would perform an FFT after permuting the pixels. This seems like it would destroy all spatial locality making the FFT useless.

Do you instead mean that you *first* perform a FFT and then *second* permute the pixels?</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1gIr-d0hX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>No Title</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=r1gIr-d0hX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper416 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper416 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The Paper is written rather well and addresses relevant research questions.
In summary the authors propose a  simple and intuitive method to improve the defense on adversarial attacks by combining random permutations and using a 2d DFT. The experiments with regards to robustness to adversarial attacks I find convincing, however the overall performance is not very good (such as the accuracy on Cifar10). 

My main points of critique are:

1. The test accuracy on Cifar10 seems to be quite low,  due to the permutation of the inputs. This 
makes me question  how favorable the trade-off between robustness vs performance is. 

2. The authors state "We believe that better results on clean images automatically translate to better results on adversarial examples"

I am not sure if this is true.   One counter argument is  that better results on clean images can be obtained by memorizing more structure of the data (see [1]). But if more memorizing (as opposed to generalization) happens, the classifier is more easily fooled (the decision boundary is more complicated and exploitable).



[1] Zhang, C., Bengio, S., Hardt, M., Recht, B., &amp; Vinyals, O. (2016). Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJedRiS5hX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting approach.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=HJedRiS5hX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper416 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper416 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes Permutation Phase Defense (PPD), a novel image hiding method to resist adversarial attacks. PPD relies on safekeeping of the key, specifically the seed used for permuting the image pixels. The paper demonstrated the method on MNIST and CIFAR10, and evaluates it against a number of adversarial attacks. The method appears to be robust across attacks and distortion levels.

The idea is clearly presented and evaluated. 

*Details to Improve*
It would be interesting to see how performance degrades if the opponent trains with an ensemble of random keys.

It would be great to see this extended to convolutional networks.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJlPpLn_2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good idea but has limited use case</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=HJlPpLn_2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper416 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper416 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper explores the idea of utilizing a secret random permutation in the Fourier phase domain to defense against adversarial examples. The idea is drawn from cryptography, where the random permutation is treated as a secret key that the adversarial does not have access to. This setting has practical limitations, but is plausible in theory.

While the defense technique is certainly novel and inspired, its use case seems limited to simple datasets such as MNIST. The permuted phase component does not admit weight sharing and invariances exploited by convolutional networks, which results in severely hindered clean accuracy -- only 96% on MNIST and 45% on CIFAR-10 for a single model. While the security of a model against adversarial attacks is important, a defense should not sacrifice clean accuracy to such an extent. For this weakness, I recommend rejection but encourage the authors to continue exploring in this direction for a more suitable scheme that does not compromise clean accuracy.

Pros:
- Novel defense technique against very challenging white-box attacks.
- Sound threat model drawn from traditional security.
- Clearly written.

Cons:
- Poor clean accuracy makes the technique very impractical.
- Insufficient baselines. While the permutation is kept as a secret, it is plausible that the adversary may attempt to learn the transformation when given enough input-output pairs. Also, the adversary may attack an ensemble of PPD models for different random permutations (i.e. expectation over random permutations). The authors should introduce an appropriate threat model and evaluate this defense against plausible attacks under that threat model.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rklxM4To9Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Can the secret permutation be extracted?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=rklxM4To9Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">This method develops a fixed permutation to use for all images. The results show that if an attacker learns the permutation, the method is insecure. It is therefore important that it is not possible for the attacker to learn the permutation. Have you thought at all about if this attack might be possible? 

In crypto (which the paper gives as inspiration for a secret key), papers dedicate a significant amount of effort to demonstrating that the secret key is not leaked. Even under a chosen plaintext attack, it should not be possible to learn anything about the key that is being used.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HyxIf9wC9Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=HyxIf9wC9Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper416 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your interesting question. So far, we have not been able to come up with an attack that can learn the permutation. We know that there are huge possible permutations (784! &gt; 10^1500) for MNIST for example. This rules out the possibility of random guess, but does not preclude the possibility of leaking information. We don't know at the moment if this kind of attack is possible.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_r1g-LRjic7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A couple of concerns</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=r1g-LRjic7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">- Your CIFAR-10 accuracy is &lt;45%. This is half (!) of what a SOTA neural network achieves, and is *lower* than the black-box adversarial accuracy of SOTA models.

- With an L2 distortion of 4 on MNIST, it is often possible to actually change the underlying digit to a human, yet the paper claims 96%+ accuracy. What is your accuracy at a L2 distortion of larger values (6? 8?)? This is sufficiently high to modify any digit to any other digit, and should be 0%. In particular, the mean minimum distance between an MNIST digit and a different digit in a different class is about 7.5.

- Similarly, with a CIFAR-10 distortion of 0.2 (that is, 51/255), it is possible to make images completely unrecognizable to humans, yet you still claim only a two percentage point loss in accuracy. This is highly suspicious. For each test sample, it would be useful to try 10,000 different values of uniform random noise with mangitude 0.2 as suggested by Athalye et al. (2018) as a way to verify your defense is not just breaking the optimizers. You do report "On CIFAR-10, for l_\infty perturbations less than 0.1, adversarial attacks are not more effective than random noise distortion" but this does not repeatedly trying random noise for the same input sample, which is often much more effective.

- Have you tried performing an attack using Expectation over Transforms (Athalye et al. 2018) where you take the expectation over the different random seeds? </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HylHzT-sqX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>It seems to me that your fundamental assumption is invalid.  Or did I miss something?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=HylHzT-sqX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">A neural network is fed with input images that could be clean or adversarial which is unknown.
Input images should go through the same pipeline.  In your case the pipeline is PPD in Figure 1.  

Adversarial images generated by whatever algorithms in traditional ways (i.e., based on clean images) 
or any benign test images sent to the network will go through the same permutation and Pixel2Phase in your PDD.  
Therefore, attackers don't need to know your secret permutation at all.

It seems that your fundamental assumption in section 4.2 (the "Adversaryâ€™s Knowledge" paragraph) 
that "yet a different permutation to craft adversarial examples" is invalid.  Or did I miss something?

</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryxP9XMicQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Assumption is valid</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=ryxP9XMicQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper416 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your comment. Input image by itself is not enough to craft adversarial images. Adversary requires both input image and a classifier to generate adversarial examples. However, hiding permutation seed stops adversary from using the true model (note that hiding permutation stops adversary's access to the gradient of loss function with respect to the input). So, adversary cannot push the image towards the decision boundaries of the classifier.

As an alternative, adversary has to use a substitute model to craft adversarial images. Assuming that the permutation is not revealed, adversary may use a similar model trained with a different permutation which is shown to be ineffective.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hyeb7Rhi57" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>I feel that you did not really address my comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=Hyeb7Rhi57"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your quick response.  However, can you spend more time to read and think about my comments?  No matter an adversary uses or does not use a similar/same model to generate adversarial images, they don't need to perform permutation and Pixel2Phase by themselves.  Again, the raw inputs to your or any other network could be clean or adversarial images and the fact is unknown (otherwise, no problem needs to be addressed).   The inputs will go through the same pipeline and whatever secret or public permutations that you use.  The attackers don't need to know the secret permutation.  In other words, the "encryption" concept is not applicable here.  I won't further comment on your paper unless you or other readers convincingly dispute my comments or point out what I really missed in your paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryxazwD0qQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: I feel that you did not really address my comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=ryxazwD0qQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper416 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Encryption is not happening in the inference stage. It is happening in building the adversarial images. Maybe the word "encryption" has been misleading. Sorry for that. Let me explain in other words: 

I understand that when you are given an image for inference you do not know whether it is adversarial or clean and you just feed it to the classifier. But the question is can adversary really generate adversarial images? That's where we are stopping adversary not in the inference stage. Adversary requires two elements to generate adversarial examples: (a) input space, and (b) classifier. But how PPD stops adversary? You can think about it in two ways:

1. If you consider the neural network as the classifier, PPD is hiding the input space.
2. If you consider the whole pipeline as the classifier, PPD is hiding the classifier.

I hope you find this explanation helpful.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1gvH3hZim" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>"can adversary really generate adversarial images?"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=r1gvH3hZim"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">It has been shown repeatedly that the adversary does not need access to the gradients to construct images. A black-box attack that fools an ensemble of models would definitely be "adversarial" to your defense, I think. Worth a shot?

<a href="https://arxiv.org/abs/1611.02770" target="_blank" rel="nofollow">https://arxiv.org/abs/1611.02770</a> may be worth a read if you disagree :)</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_H1eNFs7j97" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Blackbox attacks, universal adversarial perturbations, etc</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=H1eNFs7j97"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">How does the defense fare against blackbox attacks and attacks such as universal adversarial perturbations? It seems to me that attacks that rely on the current model may fail (because of the hidden permutation) but other attacks such as the ones mentioned may succeed (accuracy similar to or less than that of Madry et al.)</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1xKozVj9X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Will be tested</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=B1xKozVj9X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper416 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">If I understood correctly, you mean: 
"attacks that query the true model and construct a substitute model based on the query results without any permutation or pixel2phase block. Then, use the substitute model to craft adversarial examples."
Another comment also mentioned such a scenario based on this paper:  <a href="https://arxiv.org/abs/1602.02697." target="_blank" rel="nofollow">https://arxiv.org/abs/1602.02697.</a> We agree that this is a valid scenario for adversarial attack. It will be tested and the results will be added to the revision.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1xTlHni57" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Simpler attacks can reveal things about your defense</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=r1xTlHni57"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Nope. I meant an even simpler attacks -- 

Blackbox: use FGSM with a large-ish epsilon on a model without any defenses to generate adversarial examples and test your model+defense against that. This is a sure sign that the defense is based on gradient obscuring.

Universal Adversarial Perturbations: <a href="https://arxiv.org/abs/1610.08401" target="_blank" rel="nofollow">https://arxiv.org/abs/1610.08401</a>

Compare both vs Madry et al. and see if you notice any improvement.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_r1xuz7PZ5X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Does PPD work in "gray box" settings?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=r1xuz7PZ5X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I understand that PPD is meant to work when the permutation is kept secret.
Does it still work if the attacker gets to send queries to the model and observe the output? Or can the attacker reverse-engineer the function represented by the network with few queries?
(e.g. would it stand up to this attack? <a href="https://arxiv.org/abs/1602.02697" target="_blank" rel="nofollow">https://arxiv.org/abs/1602.02697</a> )</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJeQJMZi97" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Will be tested</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=BJeQJMZi97"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper416 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for bringing this attack to our attention. We will test and add results to the revision.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SJxT0bPbcm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Checking for gradient masking</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=SJxT0bPbcm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Many proposed defenses actually achieve only "gradient masking": they break the optimizer used for traditional attacks, but don't actually move the decision boundary.

- Have you checked that your attacks reach ~100% success rate against the target model (the one with the known permutation)? If the attacks do not mostly succeed here, this suggests that the attack fails because the new operations make optimization difficult, not because the permutation key is unknown. Sorry if this is in the paper and I've missed it.

- Have you tested your model by running ~1000 noisy perturbations for each example and picking the most damaging noisy permutation? This is essentially random search for adversarial examples and it sometimes works in cases where gradient-based search fails due to gradient masking.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkemB1Zs57" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>PPD robustness is because of unknown permutation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=rkemB1Zs57"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper416 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">-The blue curves in Figures 4 and 5 show the accuracy for the target model (the one with the known permutation). As seen in these figures, some attacks such as MIM and CW have decreased the accuracy of the target model to below 20% for MNIST. However, they have not been successful on models with hidden permutation (red dashed curves). This shows that robustness is actually achieved by unknown permutation and not gradient masking.

- I'm not sure if I understood your second part of comment. Do you mean the following? 
"Adversary trains ~1000 PPD models and craft adversarial examples for each of those models. Then, tests adversarial examples of each of those models against the unknown PPD model to see which PPD model out of the 1000 models was the most damaging to the unknown PPD model?"

</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJl20qQjq7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Gradient Masking vs Obscurity</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=rJl20qQjq7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Do you then agree that the defense purely relies on obscurity? It has been shown that some other defenses based on randomized perturbations of the input can be broken by adaptive attacks. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJlwltEs9X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>PPD is not just obscurity, it completely changes the input space</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=rJlwltEs9X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper416 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Randomized perturbation of the input proposed in previous work does not completely change the input space. For example, the methods proposed in this paper <a href="https://openreview.net/forum?id=SyJ7ClWCb" target="_blank" rel="nofollow">https://openreview.net/forum?id=SyJ7ClWCb</a> can be broken because they just change some parts of the input through randomization. In fact, they have to keep a large portion roughly unchanged to retain accuracy. However, PPD completely changes the input space. Note that the role of pixel2phase block is to build the input image of the neural network out of the relational positions of the pixels while the relational positions are hidden using the permutation.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJgdGN2i97" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Partial vs Complete Obscurity?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=SJgdGN2i97"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The defense only works if you obscure how exactly you are changing the input space. What is the important of parts of the input vs the whole input?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HklMPJPC5X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An example for clarification</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=HklMPJPC5X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper416 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Maybe the following example helps to clarify partial vs complete obscurity:

One method proposed in <a href="https://openreview.net/forum?id=SyJ7ClWCb" target="_blank" rel="nofollow">https://openreview.net/forum?id=SyJ7ClWCb</a> is total variance minimization. This approach randomly selects a "small" set of pixels, and reconstructs the simplest image that is consistent with the selected pixels. Why small set of pixels? Because the neural network wants to decide based on the reconstructed image and if a lot of pixels are changed, it will fail to make a good decision. This is what I mean by saying that important parts of the image (or in other words a large portion of the image) are preserved in total variance minimization.

However, PPD completely obscures the input space by random permutation + pixel2phase. Why are we saying completely hides important pixels of the image? Consider an MNIST image of digit 2. Clearly, important pixels are the white pixels in the image. If we just use random permutation (which is a partial obscurity), we are changing positions of these white pixels. So, if adversary attacks white pixels in the unpermuted image of 2 (by converting them to grey pixels), it has successfully attacked a permuted image no matter what permutation is used, because grey pixels of unpermuted image remain gray pixels in the permuted domain as well. But adding pixel2phase block after the permutation block solves this issue. Fourier transform captures frequency of change in pixel values. So, instead of looking at what pixels are white in the permuted image of digit 2, it looks at the change in pixel values of neighbors (and this is hidden from adversary due to the random permutation). So severity of attack is mitigated in this way</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_ryeiMlwW5X" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=ryeiMlwW5X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper416 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJg9Hyvbqm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Abstract should be revised</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=BJg9Hyvbqm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The abstract says that the defense provides "state-of-the-art robustness against the most powerful adversarial attacks currently available." This should be revised to say "the most powerful black box adversarial attacks" because all of the evaluations are against black box attacks. These are not the most powerful; white box attacks are more powerful.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hkxk5Fgic7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Term "black box" usually describes models that hide everything</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=Hkxk5Fgic7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper416 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The word "black box" is not used because PPD only requires to hide the random permutation seed. Everything else can be revealed. However, the term "black box" is usually meant for models that hide structure, parameters, training dataset, etc., and can only be queried.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HklbH57i9X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>At best it can be called "grey box"?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=HklbH57i9X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">If the permutation is known, the paper itself shows that the accuracy can be brought down. How then does this work as a defense against the "most powerful attacks" currently available? 

This argument of hiding the permutation is tantamount to security through obscurity.  Perhaps "most powerful grey-box attacks" is the right term, so as to not over-claim the results? 

Also, the comparison with Madry is unfair. Only meaningful attacks are those designed for the specific defense. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_H1gfa0IZ5Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Typo</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=H1gfa0IZ5Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Section 2 title should be "Preliminaries"</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkgx0tkj97" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Will be fixed in the revision</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkElFj0qYQ&amp;noteId=rkgx0tkj97"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper416 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Oct 2018</span><span class="item">ICLR 2019 Conference Paper416 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for pointing that out. It will be fixed in the revision.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>