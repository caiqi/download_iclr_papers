<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>GradMix: Multi-source Transfer across Domains and Tasks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="GradMix: Multi-source Transfer across Domains and Tasks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1xL_iR9Km" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="GradMix: Multi-source Transfer across Domains and Tasks" />
      <meta name="og:description" content="The machine learning and computer vision community is witnessing an unprecedented rate of new tasks being proposed and addressed, thanks to the power of deep convolutional networks to find complex..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1xL_iR9Km" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>GradMix: Multi-source Transfer across Domains and Tasks</a> <a class="note_content_pdf" href="/pdf?id=H1xL_iR9Km" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019gradmix:,    &#10;title={GradMix: Multi-source Transfer across Domains and Tasks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=H1xL_iR9Km},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">The machine learning and computer vision community is witnessing an unprecedented rate of new tasks being proposed and addressed, thanks to the power of deep convolutional networks to find complex mappings from X to Y. The advent of each task often accompanies the release of a large-scale human-labeled dataset, for supervised training of the deep network. However, it is expensive and time-consuming to manually label sufficient amount of training data. Therefore, it is important to develop algorithms that can leverage off-the-shelf labeled dataset to learn useful knowledge for the target task. While previous works mostly focus on transfer learning from a single source, we study multi-source transfer across domains and tasks (MS-DTT), in a semi-supervised setting. We propose GradMix, a model-agnostic method applicable to any model trained with gradient-based learning rule. GradMix transfers knowledge via gradient descent, by weighting and mixing the gradients from all sources during training. Our method follows a meta-learning objective, by assigning layer-wise weights to the source gradients, such that the combined gradient follows the direction that can minimize the loss for a small set of samples from the target dataset. In addition, we propose to adaptively adjust the learning rate for each mini-batch based on its importance to the target task, and a pseudo-labeling method to leverage the unlabeled samples in the target domain. We perform experiments on two MS-DTT tasks: digit recognition and action recognition, and demonstrate the advantageous performance of the proposed method against multiple baselines.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Transfer Learning, Domain Adaptation, Multi-source Learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a gradient-based method to transfer knowledge from multiple sources across different domains and tasks.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_S1euK9och7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A paper with some problems</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xL_iR9Km&amp;noteId=S1euK9och7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper355 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper355 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes to combine the gradients of source domains to help the learning in the target domain.

According to Eq. (1), it seems that the proposed model is not so flexible as claimed in the introduction since an implicit assumption behind Eq. (1) is that different tasks share the same network architecture. If the source and target tasks are from different domains, how to adjust the weights associated with the input layers since different tasks have different feature representations with different dimensionalities. Similar situation happens to the output layer for different types of tasks as the label spaces are different.

Authors claim that problem (5) is a constrained non-linear optimization problem. However, in my opinion, this is a linear programming problem with an analytical and trivial solution. I don't know where the nonlinearity comes.

The use of pseudo labels is very common in semi-supervised learning. I cannot see any significant contribution of the proposed methods compared with existing approaches.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1lrmn9I6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xL_iR9Km&amp;noteId=H1lrmn9I6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper355 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper355 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the valuable feedback. We would revise the paper and re-submit. 

We would like to clarify that Equation (5) is a non-linear problem, because the cosine similarity for two vector is their dot product normalized by their magnitudes. The normalization introduces non-linearity. 

In terms of network sharing, we assume that the source and target data should be of the same format (image), so that after resizing to the same dimensionality they can share the same convolutional layers. In terms of the output layer, we assume that one source should share the same task as the target. We understand that is a limitation and would improve on it.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HyeD7F6F37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good Idea but major issues with formulation and experimental setup.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xL_iR9Km&amp;noteId=HyeD7F6F37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper355 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper355 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper is proposing a method which is aimed for multi-source domain adaptation with weak supervision. In addition to the classical setup, paper also consider the case in which some of the source domains have a possibly non-overlapping label space with the target task. Proposed idea is rather simple and effective. The paper consider a model in which shared parameters are updated with convex combination of gradients, computed for available sources. The mixture weights for the convex combination is chosen such that the resulting update decreases the loss function of the few labelled examples from the target domain. This defines a bilevel optimization problem and paper uses a simple proxy which is minimising the cosine distance between the mixture and the gradient for the few labelled examples.

The biggest strength of the paper is its effectiveness in large set of experiments. Considering that it is also very easy to implement, I would expect the idea to be useful for the deep learning community. Idea of using adaptive weights for each batch and using different mixture weights for different layers are interesting and novel. Having said that, the paper is not ready to be published yet in my honest opinion as I summarize the issues below.

MAJOR ISSUES:
- Experimental Setup: Although the method is largely compared with domain adaptation methods, the setup is almost identical to few shot learning. In few shot learning, the test(target) and train(source) distributions typically have different label distribution. Hence, all few-shot learning methods are valid baselines for this paper. I think recent ones should be included in the experimental study. I would also suggest to use few-shot datasets like OmniGlot and CARS for the experiments since the performance of few-shot learning algorithms on them is already widely presented in the literature.

- Related Work: Although the paper does a good job comparing with related work in domain adaptation, it misses two recent work which poses a bilevel optimization problem for meta-learning in order to solve a similar problem. These references are: [Bilevel Programming for Hyperparameter Optimization and Meta-Learning, ICML 2018] and [Deep Bilevel Learning, ECCV 2018]. Although these papers addresses slightly different problems with different optimization methods, fundamental idea is closely related. Authors should discuss these works.

- Correctness: I think the way the optimization problem (defined in Equation 5) is solved is NOT correct. First of all, the inner product is a linear operator; hence the problem is a linear program in terms of w_{s_i}^l. Hence, the statement that it is a non-linear optimization problem is not correct. Second of all, as long as any of the inner-products is positive, the optimal value is infinity as the w is not bounded. Even if it was bounded, it would be a one hot vector such that the weight corresponding to largest inner product is 1 and others are zero. I do not think the paper solves (eq 5) correctly. Please correct me if I misunderstood some part.

MINOR ISSUES
-Implementation Details: Paper is missing majority of implementation details. For example, how the parameters are shared between different domains and tasks? This is rather crucial and should be discussed in details.

SUMMARY: I think the fundamental idea in the paper is interesting although a similar ideas have been used for similar problems already. I think the paper would still be useful to the community even after considering the limited novelty. However, I do not think the paper is correct. Moreover, I think it needs a stronger empirical study comparing with few-shot learning methods. Hence, I suggest authors to fix the correctness issue. extend the experiments with few-shot learning setup, and re-submit.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1e9_n9IaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xL_iR9Km&amp;noteId=S1e9_n9IaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper355 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper355 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the valuable feedback. We would revise the paper and re-submit. 

We would like to clarify that Equation (5) is non-linear, because the cosine similarity for two vector is their dot product normalized by their magnitudes. Although the dot product is linear, the normalization operation introduces non-linearity. For the same reason, the optimal value would not go to infinity. 

We agree with the reviewer that few-shot learning methods should be evaluated. We would improve the paper to incorporate the reviewer’s suggestions.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BklzV5CIam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for the clarification</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xL_iR9Km&amp;noteId=BklzV5CIam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper355 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper355 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for clarifying the non-linearity. I think I got confused because &lt;,&gt; notation is typically used for inner product. Although you explicitly mentioned that it is cosine similarity, I missed that line while reading. It might be a good idea to use something other than &lt;,&gt; notation since it might confuse other people.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_SJgPTu5FhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Incremental over previous literature. Contributions not thoroughly validated by experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xL_iR9Km&amp;noteId=SJgPTu5FhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper355 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper355 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper is essentially a combination of a modified meta-learning reweighting framework plus self-training. The proposed reweighting framework is largely similar to Ren et al. ICML18, with certain changes including cosine similarity based meta-learning objective, layer-wise weighting, and adaptive learning rate. 

Clarity:
- The paper is well-written with good clarity.

Concerns:
- There are several significant weaknesses in the paper that points to a clear rejection of this work:

1) The proposed methods seems incremental, given the existence of Ren et al. ICML18. The idea of self-training is not new either. The authors also failed to cite several highly related works that used pseudo-label/self-training for domain adaptation:
Zou et al., Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training, ECCV18
Inoue et al., Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation, CVPR18
Lee, Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks, ICML13 Workshop

2) With all proposed modifications on top of Ren et al. ICML18, there is not any theoretical or empirical justification to support these modifications except the ablation study on adaptive learning rate. The proposed method is not even compared with Ren et al. ICML18, or other meta-learning methods. 

3) The proposed GradMix module doesn't seem to outperform many previous literature in the experiment. The only part that gives a very clear gain happens to be the pseudo-label module, which has been investigated widely by many previous works and is not a major contribution of this work.

Summary:
- Overall, it is hard to see substantial contributions of this work given the incremental improvements, limited justifications, and the limited performance gain from the main technical method.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1xFzT9Lpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xL_iR9Km&amp;noteId=B1xFzT9Lpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper355 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper355 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the valuable feedback. We would improve on the weakness identified by the reviewer, strengthen the novelty of this paper, and re-submit. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>