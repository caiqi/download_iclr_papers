<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Knowledge Representation for Reinforcement Learning using General Value Functions | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Knowledge Representation for Reinforcement Learning using General Value Functions" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rygvZ2RcYm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Knowledge Representation for Reinforcement Learning using General..." />
      <meta name="og:description" content="Reinforcement learning (RL) is a very powerful approach for learning good control strategies from data. Value functions are a key concept for reinforcement learning, as they guide the search for..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rygvZ2RcYm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Knowledge Representation for Reinforcement Learning using General Value Functions</a> <a class="note_content_pdf" href="/pdf?id=rygvZ2RcYm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019knowledge,    &#10;title={Knowledge Representation for Reinforcement Learning using General Value Functions},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rygvZ2RcYm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Reinforcement learning (RL) is a very powerful approach for learning good control strategies from data. Value functions are a key concept for reinforcement learning, as they guide the search for good policies. A lot of effort has been devoted to designing and improving algorithms for learning value functions. In this paper, we argue that value functions are also a very natural way of providing a framework for knowledge representation for reinforcement learning agents. We show that generalized value functions provide a unifying lens for many algorithms, including policy gradient, successor features, option models and policies, and other forms of hierarchical reinforcement learning. We also demonstrate the potential of this representation to provide new, useful algorithms.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Reinforcement Learning, General Value Functions, Policy Gradient, Hierarchical Reinforcement Learning, Successor Features</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rJgZHIEIpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good insights on unifying ideas under GVFs; but questions on algorithmic improvements.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygvZ2RcYm&amp;noteId=rJgZHIEIpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1179 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1179 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors analyze various RL algorithms under the purview of Generalized Value Functions (Sutton et al. 2011). Specifically, the successor feature vector, the policy-gradient theorem, the option-value function, and the gradients for manager/worker in FeUdal Networks are all represented crisply in terms of GVFs. The detailed proofs in the Appendix and the mathematical rigor are appreciated.

A major motivation for unifying ideas under GVFs is to facilitate development of new algorithms, but I find the paper slightly less convincing on this front. With respect to different sections:

•	[GVF for PG]: Rewriting the policy-gradient theorem with 2 GVFs, the authors propose to improve on the baseline algorithm with bootstrapping on both the critic and the actor-gradients. Although the results on a small domain look interesting, I would have enjoyed some discussion on the scalability aspects and integration/comparison with more recent actor-critic algorithms. 

•	[GVF for Options]: The authors claim that “The GVF view highlights the fact that the option-value function depends .... and a long-term signal summarizing the performance of other options”. I believe that the definition of the option-value defined in Bacon et al. (2017) makes this dependence quite clear – it has an expectation over the option-value of all options. Therefore, I’m unable to appreciate the importance of writing option-value in GVFs and/or deriving an algorithmic improvement out of that. 

•	[GVF for FuN]: Using the GVF view, the authors propose to use a different prediction variable (v) for the policy – i.e. instead of difference in state representation at (t+c) and (t) as used in FuN, they use a discounted (by gamma_hat) sum of these differences. Is this interpretation correct? If yes, could you provide some intuition for how this amounts to an algorithmic improvement over FuN? The Atari results definitely don’t show the improvement empirically.  </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkxTYSwohX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Very interesting paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygvZ2RcYm&amp;noteId=rkxTYSwohX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1179 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1179 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors present an interesting analysis of generalized value functions (GVFs) and demonstrate that several RL algorithms including policy gradient, successor features, options, etc. are special cases of GVFs. The paper is reasonably well-written, and empirical studies demonstrate the benefits of using the GVF formulation to tweak existing algorithms. 

Pros:
1. Very interesting analysis of the various algorithms and unification into the single GVF framework. 
2. Especially ike the fact that these connections help discover flaws in existing algorithms (like the mode-collapse issue in FuN)

Cons:
1. The options experiments (and comparison to FuN) are done on simple Atari games, which do not benefit as much from hierarchical policies. It would be good to perform empirical studies on more suitable environments (like 3D mazes / Montezuma's revenge).
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJxau3-Fh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>GVFs for Knowledge</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygvZ2RcYm&amp;noteId=SJxau3-Fh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1179 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1179 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper advocates general value functions (gvfs) as a unifying and binding technology for building AI systems, and points out how existing ideas like successor representations, policy gradient, and options can be well-thought of as gvfs. The paper uses these connections to develop new ideas for policy gradient and feudal networks, illustrating these ideas with experiments in Atari and a gridworld environment.

I am leaning to reject this paper. On the one hand I agree with the authors on the importance of gvfs and their potential is spread across numerous papers making it difficult, currently to make these connections. On the other hand many of the insights here are simple and in many cases well-known, but perhaps not yet formally written down. The current paper is not particularly well organized and the experiments are a bit confusing, and not particularly convincing in the case of the Atari results. All taken together, the paper needs a bit more work to achieve the ambitious goals it sets for itself. It seems another round of editing, tightening, and improving the results could indeed make this a very solid paper.

Explanation for rating. This paper is trying to achieve something fairly difficult: elevate the importance of gvfs as a unifying concept for AI systems. This is needed because the Horde and Nexting papers did not provide or attempt to spell out all the connections to other things that can be well-thought of as a collection of gvfs, while later work that used these ideas, like the UNREAL paper, did a poor job of assigning credit and clearly discussing how the gvf idea was key. To do this well the writing of the paper must proceed very carefully, and this submission falls a little flat here. For example the abstract states “In this paper, we argue that value functions are also a very natural way of providing a framework for knowledge representation for reinforcement learning agents.“ This was the whole objective of the Horde paper, so one has to explain what is missing without sounding redundant. The introduction has similar problematic sentences of this form: “In this paper, we highlight the fact that value functions can in fact be used as the main building block for knowledge representation in RL agents.“  To do this well the paper has to be very precise, and sometimes it fails to do so. For example: “Sutton et al (2011)…illustrates in fact the successful large-scale learning of GVFs about a diversity of signals and at many different time scales, in parallel, from a single stream of data.“. This is not correct, scale was demonstrated in the Nexting paper, not the Horde paper. Furthermore, the related work section is strange because it talks about two architectures that use the gvf formalism. These seem to be examples supporting your argument, not things to contrast against. In fact, it is not clear what the related work section of such a paper should be given that this paper is primarily a review (with unifications that should always be the outcome of an extensive and insightful review) and position paper.

In the end I am not sure this paper has a clear identify because of the somewhat distracting focus on new algorithms and experiments trying to illustrate their utility—but the experiments don’t make a serious attempt at illustrating a contribution as discussed below. This could all be addressed with a change in the pitch and tone of the paper, but the authors should ask themselves “what are we adding beyond what can be found in the work of Sutton et al (2011), Modayil et al (2013), White (2015), and Sutton’s numerous writings on Predictive approaches to AI?”

The connection between nexting (and thus gvfs) and Successor features is well known, while theorem 1 follows directly from the original policy gradient theorem. This is not to say these connections are not interesting and should not be talked about, but they are true by construction and that is indeed why using gvfs for knowledge is such a good idea. There must be a way to discuss these things without pitching the resulting new algs as the main contribution of the paper. It seems the paper is trying to do too many things at once. If the new algs are interesting write a paper about just them and provide clearer evidence of their utility rather than hiding it inside a large narrative about gvfs for AI.

The experiments are difficult to understand. Section 4.1 appears to describe an experiment, but in the end is describing an algorithm described in Fig 1. I think the alg requires rollouts which is not ideal, but this is mentioned in passing. There is an algorithm block in the paper but it is never referenced: I can’t quite see how it connects to experiment #1. Section 4.3 finally describes the experiment (after the the main figure, which does not indicate the task or what the baselines are). What is being learned here: the 4 rooms problem was originally designed for option learning. Are you learning policies for each room, it is never stated. What is the behavior policy here? I don’t understand the y-axis of these graphs it says “value function” but the caption mentions L1 norm. I assume up is good but I have no context to understand if this is good final performance and fast learning. A well explained baseline would help. It is really great that you show all the parameter combinations and their performance, good job.

The second experiment compares against Kondas AC calling it a a policy gradient method for larger or continuous environments. This was not explained further, and it was evaluated on a small discrete domain. I cannot tell if this experiment was fair, even consulting the plots in the appendix. It raises several questions:  How did you decide the ranges of the meta parameters to sweep? The AC baseline’s performance gets better and better with increasing alphas. What about even larger alphas?  Did that make the two approaches tie.  I don’t understand why the baseline totally fails in the larger gridworld. Is this expected or perhaps it is harder to get the baseline working on the larger domain? Because the paper does not comment on the outcome, the reader is left to wonder.

The results of the 3rd experiment don’t paint a clear picture either. Sometimes the new approach helps a small amount, sometimes it hurts—basically the same overall. Perhaps this—atari—was not a good domain to illustrate the merit of this idea.

Overall the paper is too rushed, and too unpolished. I look forward to the author respond so that I can refine both my understanding and assessment of this work. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>