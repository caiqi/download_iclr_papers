<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>On the loss landscape of a class of deep neural networks with no bad local valleys | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="On the loss landscape of a class of deep neural networks with no bad local valleys" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HJgXsjA5tQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="On the loss landscape of a class of deep neural networks with no..." />
      <meta name="og:description" content="We identify a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss which provably have no bad local valley, in the sense that from any point in..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HJgXsjA5tQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>On the loss landscape of a class of deep neural networks with no bad local valleys</a> <a class="note_content_pdf" href="/pdf?id=HJgXsjA5tQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019on,    &#10;title={On the loss landscape of a class of deep neural networks with no bad local valleys},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HJgXsjA5tQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=HJgXsjA5tQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We identify a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss which provably have no bad local valley, in the sense that from any point in parameter space there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero. This implies that these networks have no sub-optimal strict local minima.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">loss landscape, local minima, deep neural networks</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Hkg3N5wTnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>good progress; but simulation requires some work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJgXsjA5tQ&amp;noteId=Hkg3N5wTnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper607 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper607 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper shows that a class of deep neural networks have no spurious local valleys –--implying no strict local-minima. The family of neural networks studied includes a wide variety of network structure such as (a variant of) DenseNet. Overall, this paper makes some progress, improving previous results on over-parametrized networks. 

Pros: The flexibility of the network structure is an interesting point.
Cons: CNN was covered in previous related works (so weight sharing is not a new contribution); DenseNet is not explicitly covered in this work (I mean current DenseNet does not have N skip-connections to output; correct me if wrong). 
  The simulation part is not that clear, and I have a few questions that I hope the authors can answer. 

Some comments/suggestions:
1) Training error needs to be discussed.
   Page 8 says “This effect can be directly related to our result of Theorem 3.3 that the loss landscape of skip-networks has no bad local valley and thus it is not difficult to reach a solution with zero training error”. This relation is not justified. The implication of Thm 3.3 is that getting zero training error is easier, but the tables are only for test error. Showing training error is the only way to connect to Thm 3.3. I expect to see a high training error for C-10, original VGG and sigmoid activation functions, and zero training error for both skip-SGD (rand) and skip-SGD (SGD). 
    This paper has no theory on generalization, thus if a whole section is just about “investigating generalization error”, then the connection to theoretical parts is weak --btw, one connection is the comparison of two algorithms, which fits the context well, and thus interesting (though comparison result itself probably not surprising).   

2) Data augmentation.
  “Note that the rand algorithm cannot be used with data augmentation in a straightforward way and thus we skip it for this part.” Why? 
   With data augmentation, is M still larger than N? If yes, then the number of added skip connection is different for C-10 and C-10-plus, which is not mentioned in the instruction of Table 2. 

3)It may be better to mention explicitly that "it is possible to have bad local min" –perhaps in abstract and/or introduction. 
  --Although “no sub-optimal strict local minima” is mentioned, readers, especially non-optimizers, might not notice "strict".
  --In fact, in the 1st round read, I do not have a strong impression of "strict". Later I realized it. Mentioning this can be helpful. 

4) Some references I suggest to include:
   [R1] Yu, X. and Chen, G. On the local minima free condition of backpropagation learning. 1995.  --related work. 
   [R2] Lu, H., Kawaguchi, K. Depth creates no bad local minima. 2017. --also deep nets.
   [R3] Liang, S., Sun, R., Li, Y., &amp; Srikant, R. "Understanding the loss surface of neural networks for binary classification." 2018. --Also study SoftPlus neurons.
   [R4] Nouiehed, M., &amp; Razaviyayn, M. Learning Deep Models: Critical Points and Local Openness. 2018. --also deep nets. 

Minor questions:
  --Exact 10% test accuracy for a few cases. Why exact 10%?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkeL8Gi26Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJgXsjA5tQ&amp;noteId=BkeL8Gi26Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper607 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper607 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the feedback. Below are answers to your comments/questions by their numbering.

1) We agree with the reviewer about the training error matter. Thus we have added Section F in the appendix to discuss training error in details. As expected, the training error is zero except the case where sigmoid activation is used with original VGGs from Table 2 or original CNN13 from Table 1.
Moreover, we show in this section that adding skip-connections to the output is also helpful for training extremely deep (narrow) networks with softplus activation. This together show that skip-connections are helpful for training deep networks with both sigmoid and softplus activation. In Section E in the appendix, we provide a visual example of the loss landscape of a small network, before and after adding skip-connections, where one can see that adding skip-connections to the output layer help to smooth the loss surface and get rid of bad local valleys, which is helpful for local search algorithms like SGD to succeed.

2) As described in our experiments, the number of skip-connections is fixed to M=N in both cases (with and without data-augmentation), where N is the size of the original data set. We quote the following sentence from our experimental section for the convenience of the reviewer:
"...we aggregate all neurons of all the hidden layers in a pool and randomly choose from there a subset of N neurons to be connected to the output layer...".
In the setting of data-augmentation, at each training iteration the network uses additional examples (randomly) generated from the original dataset, and thus it is not clear in this case how the number of training samples should be defined. That's why we fixed the number of skip-connections in both cases to be the size of the original data set.

3) We agree that this might be overlook by non-optimizers. Nevertheless we want to keep our abstract short and precise. Thus we have added the following sentence in the introduction to make this further clear: 
"We note that this implies the loss landscape has no strict local minima, but theoretically non-strict local minima can still exist."

4) We have included the references suggested by the reviewer, and can add more detailed comparisons if the reviewer think that it's necessary.

Regarding 10% test accuracy, we added a discussion on this issue under Section F in the appendix. Briefly, the reason, as observed in our experiments, is that the network converges quickly to a constant zero classifier (i.e. the output of last hidden layer converges quickly to zero), and thus the training/test accuracy converge to 10% and the cross-entropy loss in Equation (2) converges to − log(1/10). We realized later that this is actually a known issue of sigmoid activation when training plain networks with depth &gt; 5, as pointed out earlier by Glorot &amp; Bengio [1].

[1] Understanding the difficulty of training deep feedforward neural networks. Xavier Glorot, Yoshua Bengio. ICML 2010.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Ske7SH5q37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>a breakthrough paper on the loss landscape of neural networks</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJgXsjA5tQ&amp;noteId=Ske7SH5q37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper607 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper607 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper analyzes the loss landscape of a class of deep neural networks with skip connections added to the output layer. It proves that with the proposed structure of DNN, there are uncountably many solutions with zero training error, and the landscape has no bad local valley or local extrema. 

Overall I really enjoy reading the paper. 
The assumptions to aid the proof are very natural and much softer than the existing literature. As far as I’m concerned, the setting is very close to real deep neural networks and the paper is a breakthrough in the area. The experiments also consolidate that the theoretical settings are natural and useful, namely, with enough skip connections and specially chosen activation functions. 
The presentation of the paper is intuitive and easy to follow. I’ve also checked all the proof and think it’s brilliantly and elegantly written. 

My only complaint is about the experiments. As we all know that both VGG and the sigmoid activation are commonly used DL tools, and why do they fail to generalize when used together? Does the network fail to converge or is it overfitting? The authors should try tuning the parameters and present a proper result. With that said, since the paper is more about theoretical findings, this issue doesn’t influence my recommendation to accept the paper.


Minor issues:
I think it’s better to formally define “bad local valley” somewhere in the paper. From what I read, the definition of “bad local valley” is implied by the abstract and in the proof of Theorem 3.3(2), but I did not find a formal definition anywhere else. 
In proof number 4 (of Theorem 3.3), the statement should be “any *principle* submatrices of negative semi-definite matrices are also NSD”, and it’s not true otherwise. But this typo doesn’t influence the proof. 
Also, it seems the proof of 3 is somewhat redundant, since local minimum is a special case of your “bad local valley”. 
It seems the analysis could not possibly be extended to the ReLU activation, since it will break the analytical property of the function. Just out of curiosity, do the authors have some further thoughts on non-differentiable activations?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HkgeJDjn6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJgXsjA5tQ&amp;noteId=HkgeJDjn6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper607 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper607 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for the support. Below are our answers to your comments/questions in the order that they appear.

Regarding the failure of original VGG with sigmoid activation, we have added a discussion on this issue under Section F in the appendix (please see also our response to AnonReviewer3 on the 10% accuracy matter).
Basically, we have observed that the network in this case converges to a constant zero classifier, regardless of our effort in tuning the learning rate. This behavior is actually not restricted to the specific architecture of VGG, but has been shown before as an issue of sigmoid activation when training plain networks with depth &gt; 5, see e.g. [1].

Answers to minor issues:
Actually the definition of bad local valleys has previously appeared just above Theorem 3.3 in the text. However we follow the reviewer's suggestion by putting this in a formal definition 3.3 now.

"In proof number 4 (of Theorem 3.3), the statement should be “any *principle* submatrices of negative semi-definite matrices are also NSD”, and it’s not true otherwise. But this typo doesn’t influence the proof."
Yes, the reviewer is completely right. We fixed this typo. Thanks!

"Also, it seems the proof of 3 is somewhat redundant, since local minimum is a special case of your “bad local valley”."
We agree. We keep it there as we wanted to make all our statements and results become clear and as rigorous as possible.

"It seems the analysis could not possibly be extended to the ReLU activation, since it will break the analytical property of the function. Just out of curiosity, do the authors have some further thoughts on non-differentiable activations?"
Thank you for an interesting question. At the moment, we do not really have a clear clue how to extend the result to general non-differentiable activations, so this could be an interesting question for future research. 
For ReLU, we think that it might be possible to exploit the fact that softplus can approximate ReLU arbitrarily well, and so perhaps a limiting argument on their corresponding loss functions can be helpful..

[1] Understanding the difficulty of training deep feedforward neural networks. Xavier Glorot, Yoshua Bengio. ICML 2010.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJx5sWN5hX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting experimental results, but less significant theoretical contribution</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJgXsjA5tQ&amp;noteId=BJx5sWN5hX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper607 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper607 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a class of neural networks that does not have bad local valleys. The “no bad local valleys” implies that for any point on the loss surface there exists a continuous path starting from it, on which the loss doesn’t increase and gets arbitrarily smaller and close to zero. The key idea is to add direct skip connections from hidden nodes (from any hidden layer) to the output.

The good property of loss surface for networks with skip connections is impressive and the authors present interesting experimental results pointing out that
* adding skip connections doesn’t harm the generalization.
* adding skip connections sometimes enables training for networks with sigmoid activation functions, while the networks without skip connections fail to achieve reasonable performance.
* comparison of the generalization performance for the random sampling algorithm vs SGD and its connection to implicit bias is interesting.

However, from a theoretical point of view, I would say the contribution of this work doesn’t seem to be very significant, for the following reasons:
* In the first place, figuring out “why existing models work” would be more meaningful than suggesting a new architecture which is on par with existing ones, unless one can show a significant performance improvement over the other ones.
* The proof of the main theorem (Thm 3.3) is not very interesting, nor develops novel proof techniques. It heavily relies on Lemma 3.2, which I think is the main technical contribution of this paper. Apart from its technicality in the proof, the statement of Lemma 3.2 is just as expected and gives me little surprise, because having more than N hidden nodes connected directly to the output looks morally “equivalent” to having a layer as wide as N, and it is known that in such settings (e.g. Nguyen &amp; Hein 17’) it is easy to attain global minima.
* I also think that having more than N skip connections can be problematic if N is very large, for example N&gt;10^6. Then the network requires at least 1M nodes to fall in this class of networks without bad local valleys. If it is possible to remove this N-hidden-node requirement, it will be much more impressive.

Below, I’ll list specific comments/questions about the paper.
* Assumption 3.1.2 doesn’t make sense. Assumption 3.1.2 says “there exists N neurons satisfying…” and then the first bullet point says “for all j = 1, …, M”. Also, the statement “one of the following conditions” is unclear. Does it mean that we must have either “N satisfying the first bullet” or “N satisfying the second bullet”, or does it mean we can have N/2 satisfying the first and N/2 satisfying the second?
* The paper does not describe where the assumptions are used. They are never used in the proof of Theorem 3.3, are they? I believe that they are used in the proof of Lemma 3.2 in the appendix, but if you can sketch/mention how the assumptions come into play in the proofs, that will be more helpful in understanding the meaning of the assumptions.
* Are there any specific reasons for considering cross-entropy loss only? Lemma 3.2 looks general, so this result seems to be applicable to other losses. I wonder if there is any difficulty with different losses.
* Are hidden nodes with skip connections connected to ALL m output nodes or just some of the output nodes? I think it’s implicitly assumed in the proof that they are connected to all output nodes, but in this case Figure 2 is a bit misleading because there are hidden nodes with skip connections to only one of the output nodes.
* For the experiments, how did you deal with pooling layers in the VGG and DenseNet architectures? Does max-pooling satisfy the assumptions? Or the experimental setting doesn’t necessarily satisfy the assumptions?
* Can you show the “improvement” of loss surface by adding skip connections? Maybe coming up with a toy dataset and network WITH bad local valleys will be sufficient, because after adding N skip connections the network will be free of bad local valleys.

Minor points
* In the Assumption 3.1.3, the $N$ in $r \neq s \in N$ means $[N]$?
* In the introduction, there is a sentence “potentially has many local minima, even for simple models like deep linear networks (Kawaguchi, 2016),” which is not true. Deep linear networks have only global minima and saddle points, even for general differentiable convex losses (Laurent &amp; von Brecht 18’ and Yun et al. 18’).
* Assumption 3.1.3 looked a bit confusing to me at first glance. You might want to add some clarification such as “for example, in the fully connected network case, this means that all data points are distinct.”</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByeYv_JTaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer1. Part 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJgXsjA5tQ&amp;noteId=ByeYv_JTaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper607 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper607 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Answers to specific comments/questions:
* "Assumption 3.1.2 doesn’t make sense. Assumption 3.1.2 says “there exists N neurons satisfying…” and then the first bullet point says “for all j = 1, …, M”. Also, the statement “one of the following conditions” is unclear. Does it mean that we must have either “N satisfying the first bullet” or “N satisfying the second bullet”, or does it mean we can have N/2 satisfying the first and N/2 satisfying the second?"

We apologize for the typo and confusion. Please check our revision now where we have rephrased this a bit. It is possible to have mixed skip-connections as the reviewer mentioned, but for simplicity at the moment we just require that all the neurons with skip-connections have the same activation functions which satisfy one of our conditions.

* "The paper does not describe where the assumptions are used...but if you can sketch/mention how the assumptions come into play in the proofs, that will be more helpful in understanding the meaning of the assumptions."

As the reviewer noted, these assumptions are used in the proof of Lemma 3.2, and hence in our main result Theorem 3.3 (though not directly used here). Basically in proving Lemma 3.2, we used our conditions on activation functions to prove that there exists a set of parameters so that the matrix Psi has full rank. Then we use the analytic property of the activation functions together with Lemma A.1 to establish the result on the measure-zero set property. The condition on the training data is used to guarantee that the value of each hidden unit can be chosen to be non-identical for different training samples.

* "Are there any specific reasons for considering cross-entropy loss only? Lemma 3.2 looks general, so this result seems to be applicable to other losses..."

The reviewer is right. Indeed our result holds for other convex loss functions. Please check our extension to this setting in Section C in the appendix. The reason why we presented our main result with cross-entropy loss in the beginning is because we wanted to keep everything simple, and also because this is the loss actually used in practice.

* "...Figure 2 is a bit misleading because there are hidden nodes with skip connections to only one of the output nodes."

Yes, they are connected to all the hidden units. We apologize for the confusion in Figure 2 as we thought it might look a bit too dense. Please check our revision now where we have updated the figure. 

* "For the experiments, how did you deal with pooling layers in the VGG and DenseNet architectures? Does max-pooling satisfy the assumptions? Or the experimental setting doesn’t necessarily satisfy the assumptions?"

It depends. In general, max-pooling can be used above all the neurons with skip-connections in the network. However as the main goal of the experiments is to find out the generalization performance of skip-networks, we did not want to include this part in the paper. Nevertheless, we have added Section G in the appendix to treat this question separately. 

* Can you show the “improvement” of loss surface by adding skip connections? Maybe coming up with a toy dataset and network WITH bad local valleys will be sufficient, because after adding N skip connections the network will be free of bad local valleys.

Yes. Please check our Section E in the appendix now, where we provide a visual example of the loss landscape of a small network, before and after adding skip-connections. One can easily see that skip-connections to the output help to smooth the loss landscape and get rid of bad local valleys. 

* "In the Assumption 3.1.3, the $N$ in $r \neq s \in N$ means $[N]$?"
Yes. We fixed the typo. Thanks!

* "In the introduction, there is a sentence “potentially has many local minima, even for simple models like deep linear networks (Kawaguchi, 2016),” which is not true...."

The reviewer is right. It's actually an english issue as we meant non-convexity which previously appears before this term. We removed it now in our revision. 

* "Assumption 3.1.3 looked a bit confusing to me at first glance. You might want to add some clarification such as “for example, in the fully connected network case, this means that all data points are distinct.”"

Thanks for another helpful comment. We have updated/improved the statement of this condition a bit. In particular, we require now only the distinctness between the input patches at the same location across different training samples. This is just a subtle change and the current proof of Lemma 3.2 is not affected by this modification. We follow your suggestion by adding the following sentence right below Equation (3): 
"The third condition is always satisfied for fully connected networks if the training samples are distinct. For CNNs, this condition means that the corresponding input patches across different training samples are distinct."</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkeg9UyaTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer1. Part 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJgXsjA5tQ&amp;noteId=rkeg9UyaTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper607 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper607 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for the detailed feedbacks. Below are answers to your comments/questions in the order that they appear.

* "In the first place, figuring out “why existing models work” would be more meaningful than suggesting a new architecture which is on par with existing ones, unless one can show a significant performance improvement over the other ones."

We absolutely agree that understanding why existing models work is what one desires to achieve in the end. But to reach that point, one has to start somewhere, and make progress continually. This is the reason for the existence of a bunch of recent work on this topic:

A. Choromanska, M. Hena, M. Mathieu, G. B. Arous, and Y. LeCun. The loss surfaces of multilayer networks. 2015.
I. Safran and O. Shamir. On the quality of the initial basin in overspecified networks. 2016.
B. D. Haeffele and R. Vidal. Global optimality in neural network training. 2017.
H. Lu, K. Kawaguchi. Depth creates no bad local minima. 2017.
M. Hardt and T. Ma. Identity matters in deep learning. 2017.
C. Yun, S. Sra, and A. Jadbabaie. Global optimality conditions for deep neural networks. 2017.
D. Soudry and E. Hoffer.  Exponentially vanishing sub-optimal local minima in multilayer neural networks. 2017.
M. Nouiehed and M. Razaviyayn. Learning Deep Models: Critical Points and Local Openness. 2018.
T. Laurent and J. H. von Brecht. The Multilinear Structure of ReLU Networks. 2018.
S. Liang, R. Sun, J. D. Lee, and R. Srikant. Adding one neuron can eliminate all bad local minima. 2018.

At the moment, we are not aware of any previous work which can prove directly strong theoretical results on the loss landscape of "existing models" which actually work in practice. Moreover in this paper, we show that the presented class of networks enjoy both strong theoretical properties and good empirical performance. We do not make great claim about the result, but we believe that this is a significant contribution to the literature, especially w.r.t. the recent great effort of the community in trying to make progress on theoretical understanding of deep learning models.

* "The proof of the main theorem (Thm 3.3) is not very interesting, nor develops novel proof techniques. It heavily relies on Lemma 3.2, which I think is the main technical contribution of this paper. Apart from its technicality in the proof, the statement of Lemma 3.2 is just as expected and gives me little surprise, because having more than N hidden nodes connected directly to the output looks morally “equivalent” to having a layer as wide as N, and it is known that in such settings (e.g. Nguyen &amp; Hein 17’) it is easy to attain global minima."

The proof of our main result is simple and elegant, as also noted by AnonReviewer2. Simple proofs are often generalizable better to complex models. Thus we think that it is actually an advantage of this work.  
Can the reviewer elaborate on why the statement of Lemma 3.2 is just as expected? Given that said, does the reviewer have in mind an easier proof for this lemma? - which we would be very happy to know We would like to note that the class of networks analyzed in this Lemma is quite general and hence the mathematical proof is non-trivial.  We agree that one can view the N skip-connections as an implicit wide layer, but this is just an intuition and very weak argument to conclude that the statements are just as expected. There are things that might look "intuitive" and "as expected" but it's completely wrong, for instance, a deep linear network with N skip-connections to the output does not satisfy our conditions and results if the training data has very low rank.

* "I also think that having more than N skip connections can be problematic if N is very large, for example N&gt;10^6. Then the network requires at least 1M nodes to fall in this class of networks without bad local valleys. If it is possible to remove this N-hidden-node requirement, it will be much more impressive."

We agree that the current condition on the number of skip-connections is quite strong. But on the other hand, it's not necessarily too restrictive at the level as mentioned by the reviewer. We would like to refer to Table 1 in [1] for some information on the number of neurons of the first layer of several existing networks. For instance, the first hidden layer of original VGG-Nets has already more than 3M nodes, and so if one sum up this number for all the hidden layers the total will be much than that. Moreover, in the literature it is common to find theoretical work which requires extremely larger number of neurons than the number of training samples, see e.g. <a href="https://openreview.net/forum?id=S1eK3i09YQ" target="_blank" rel="nofollow">https://openreview.net/forum?id=S1eK3i09YQ</a> which requires N^6 neurons for gradient descent to find a zero training error solution for one hidden layer networks. Nevertheless, we agree with the reviewer that it would be interesting to relax this condition in future work.

[1] Nguyen &amp; Hein. Optimization landscape and expressivity of deep cnns. 2017.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rylHw4Tx0X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to the response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJgXsjA5tQ&amp;noteId=rylHw4Tx0X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper607 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper607 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">First of all, I would like to appreciate the authors for their extensive efforts in revising and improving the paper.

I think most of my concerns were more or less addressed, except for the “assumptions” and “proof technique” parts.

First of all, I still believe it is weird that the assumptions are never used *explicitly* anywhere in the main text. The paper makes some assumptions and never uses them directly in the main text. I would suggest the authors to at least add a “proof sketch” paragraph below Lemma 3.2, and briefly outline the proof while mentioning how the assumptions come into play.

As for the proof technique part, by “as expected” I meant I would have been more surprised if the set of U with rank-deficient \Psi(U) had measure greater than zero. This was because in general, rank deficient matrices lie in a set of measure zero, and I’ve seen many results such as “if a hidden layer is wider than N and activation functions have good properties, then some matrix has full rank almost everywhere.”

Unfortunately, however, I can hardly agree that the proof is “elegant” at the moment, especially for Lemma 3.2. There are many steps that makes the proof unnecessarily longer. For example, the very first equation in step 1 is not necessary; you can just start with eq (4). Similarly, I believe that steps 2-5 can be made much more concise. In defining eq (9), why don’t you just start by “for all nodes j in layer l, define all \alpha_j to be:”? I also don’t understand a few lines above eq (10). Given that the network is not fully connected but a DAG, how can you guarantee that u_j and u_{j’} are of the same size and make them identical? For the softplus case, the choice of \beta is missing. Without this, how can you make sure that some of the data points fall into the negative side of softplus?

I agree that there are some interesting techniques used in constructing the parameter U. However, the main theoretical contribution (proof of Lemma 3.2) is hidden in the appendix, which many readers will end up skipping. My current score is based on the main text, and at least in my opinion, the main text itself doesn’t reveal anything particularly interesting.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>