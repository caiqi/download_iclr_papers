<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Diagnosing and Enhancing VAE Models | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Diagnosing and Enhancing VAE Models" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1e0X3C9tQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Diagnosing and Enhancing VAE Models" />
      <meta name="og:description" content="Although variational autoencoders (VAEs) represent a widely influential deep generative model, many aspects of the underlying energy function remain poorly understood.  In particular, it is..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1e0X3C9tQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Diagnosing and Enhancing VAE Models</a> <a class="note_content_pdf" href="/pdf?id=B1e0X3C9tQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019diagnosing,    &#10;title={Diagnosing and Enhancing VAE Models},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=B1e0X3C9tQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Although variational autoencoders (VAEs) represent a widely influential deep generative model, many aspects of the underlying energy function remain poorly understood.  In particular, it is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples.  In this regard, we rigorously analyze the VAE objective, differentiating situations where this belief is and is not actually true.  We then leverage the corresponding insights to develop a simple VAE enhancement that requires no additional hyperparameters or sensitive tuning.  Quantitatively, this proposal produces crisp samples and stable FID scores that are actually competitive with state-of-the-art GAN models, all while retaining desirable attributes of the original VAE architecture.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">variational autoencoder, generative models</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We closely analyze the VAE objective function and draw novel conclusions that lead to simple enhancements.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">16 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_H1xC7LAvaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A question regarding the analysis on Eq. (9) and the 2-stage VAE</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e0X3C9tQ&amp;noteId=H1xC7LAvaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1405 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">It is an interesting and refreshing paper. I have a question regarding the analysis on Eq. (9). When \gamma-&gt;0, the coefficient (1/\gamma) of the reconstruction term of Eq. (9) will approach infinity, which results in a loss function that is similar to that of a plain AE. To see that, we can multiply Eqs. (8) and (9) by \gamma, then the coefficient of the reconstruction term becomes 1, while that of Eq. (9) approaches 0. Note that \gamma\log(\gamma)-&gt;0 when \gamma-&gt;0. So I don't see why \hat{r} will be pushed to as small as possible. Intuitively, if we add some small (e.g., stddev=0.01) isotropic gaussian noise to x, we wouldn't expect the resulting model to be significantly different, while the analysis seems to suggest that \hat{r} will suddenly increase from r to \kappa (assuming \kappa&lt;d), since the manifold of the noisy x is d-dimensional. Moreover, it would be interesting to see if adding a second stage VAE on top of a plain AE can lead to similar performance gain.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByeNgv9KTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to the question</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e0X3C9tQ&amp;noteId=ByeNgv9KTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1405 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1405 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your interest in our work.  Regarding the situation when gamma -&gt; 0, the VAE will not actually default to a regular AE.  Note that we can multiply both reconstruction and regularization terms (eqs. (8) and (9)) by gamma and then examine the limit as gamma becomes small; however, this does not allow us to discount all of the regularization factors even though they may be converging to zero as well.  The convergence details turn out to be critical here.

To see this, consider the following simplified regularized regression problem which reflects the core underlying issue.  Assume that we would like to solve

min_w (1/gamma)||y - A w||^2 + ||w||^2,

where 1/gamma is a trade-off parameter, y is a known observation vector, A is an overcomplete matrix (full rank, with more columns than rows), and w represents unknown coefficients we would like to compute.  If gamma -&gt; 0, then any optimal solution must be in the feasible region where y = A w, meaning zero reconstruction error.  Therefore, when gamma -&gt; 0 solving this problem becomes formally equivalent to solving the constrained problem

min_w ||w||^2  subject to y = A w.

Of course we could equally well consider multiplying both sides of the original objective by gamma, producing

min_w ||y - A w||^2 + gamma ||w||^2.

This shouldn't change the optimal w since we have just multiplied by a constant independent of w.  But if gamma -&gt; 0, then technically speaking, the regularization factor gamma ||w||^2 becomes arbitrarily small; however, this does not mean that we can simply ignore it because there are an infinite number of solutions whereby the data factor ||y - A w||^2 equals zero, i.e., a fixed, minimizing constant.  The direct implication is that

limit gamma -&gt; 0 arg min_w ||y - A w||^2 + gamma ||w||^2  \neq  arg min_w ||y - A w||^2,

where the righthand side is just the objective obtained when gamma = 0, and it has an infinite number of minimizers unlike the lefthand side.  In general, the regularization factor ||w||^2 will always have an influence in choosing which solution, out of the infinite number satisfying y = A w, is optimal, and the minimizing argument will again provably be the same as from the constrained problem above.  This notion is well-established in the regularized regression literature, and generalizes to generic problems composed of data-fitting and regularization terms where the former in isolation has multiple equivalent minima.  Returning to the VAE, if extra unneeded latent dimensions are present, then there will be an infinite number of latent representations capable of producing perfect reconstructions.  The lingering KL regularization terms then determine which is optimal, per our analysis in Section 3 of the paper.

Additionally, in terms of adding small isotropic noise to observations x, the results will actually not be much different. This is because in practice, gamma need not converge to exactly zero, but only a small value near zero.  This allows the model to slightly expand around the manifold and still apply high probability to the data.  If the noise level is within such a modest expansion, then the behavior is more-or-less the same as if a low-dimensional manifold were present. Of course if added noise or other deviations from the manifold are too large, then obviously using additional dimensions to model the data may be required.

Finally, with regard to your other question, we have also considered training a second-stage VAE on top of a regular autoencoder.  This structure is discussed in footnote 5 on page 7. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1low-VDpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Relationship with the Vamp prior</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e0X3C9tQ&amp;noteId=S1low-VDpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1405 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The two-stage process you introduce seems very closely related to using a Vamp prior (<a href="https://arxiv.org/abs/1705.07120)," target="_blank" rel="nofollow">https://arxiv.org/abs/1705.07120),</a> wherein one effectively tries to replace the original prior with the aggregate posterior q(z) (though this is not achieved exactly for computational reasons).  Obviously, there are some differences, but this seems like a natural baseline that should probably be compared to and at the very least a paper that should be being cited and discussed.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkletw5FT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to the Vamp prior reference suggestion</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e0X3C9tQ&amp;noteId=Bkletw5FT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1405 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1405 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the reference to (Tomczak and Welling, 2018), which proposes a nice two-stage hierarchical prior to replace the parameter-free standardized Gaussian N(0,I) that is commonly used with VAE models.  Note that multiple stages of latent variables have been aggregated in the context of VAE-like models going back to (Rezende et al., 2014).  However, beyond the common use of two sets/stages of latent variables, our approach bares relatively little similarity to (Tomczak and Welling, 2018) or other multi-stage alternatives.   For example, the underlying theoretical design principles/analysis, aggregate energy function parameterizations, and training strategies are not at all the same.  Likewise, the empirical validation is completely different and incomparable as well; (Tomczak and Welling, 2018) focuses on demonstrating improved log-likelihood scores, while we concentrate exclusively on improving the quality of generated samples as explicitly quantified by FID scores.  And we stress that these two evaluation criteria can be almost completely unrelated to one another in many circumstances (see for example, Theis et al., "A Note on the Evaluation of Generative Models," ICLR 2016).  And as a final point of differentiation, (Tomczak and Welling, 2018) tests only on small black-and-white images and includes no comparisons against GANs, while we include testing with larger color images like CelebA and directly compare against GANs in a neutral setting.  Regardless, (Tomczak and Welling, 2018) still represents a compelling contribution, and space permitting, we can try to provide broader context in a revision.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyxF01pF6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Link is much stronger and more subtle than this</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e0X3C9tQ&amp;noteId=SyxF01pF6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1405 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your reply.  I appreciate that there are certainly differences between the two, including in their original motivations, and I certainly not trying to imply your work is just a rehashing of theirs.  I should point out that I am in no way associated with that paper so I have no ulterior motive to try and promote it or similar.

However, I think the link between the two is a lot stronger than something to do with hierarchical priors and so I disagree with your suggestion above.  The link is that both consider issues caused by the mismatch between the aggregate posterior q(z) and the prior p(z).  In your work, you learn a second network to generate samples from q(z) and thus in turn p(x|z)q(z).  In their formulation, they instead replace p(z) with q(z), therefore generating samples from exactly the same model as yours, at least in theory.  In practice, they have to make approximations because q(z) is not directly available.  Consequently, the two approaches are intimately linked to one another, the key methodological differences, in my opinion, being that in your case you only approximate q(z) after training and you use a different method to approximate q(z).  There is a bit of a trade-off here, your method for approximating q(z) is almost certainly better, but this better approximation prevents you using it during training, which is likely to lead to a worse model being learned.

Consequently, I think the link is a lot stronger than you are suggesting above, and thus this is an essential piece of related work to be considering.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryg--PcTTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e0X3C9tQ&amp;noteId=ryg--PcTTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1405 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1405 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the continued engaging dialogue, and we can try to further clarify what we believe to be critical differentiating factors.  First, you mentioned that the link between our method and (Tomczak and Welling, 2018) is that we both consider issues caused by the mismatch between the aggregate posterior q(z) and the prior p(z).  But whether explicitly stated or not, essentially all methods based on an autoencoder structure share this exact same link on some level, so this is not any particular indication of close kinship in and of itself.  And if this mismatch is ignored, then samples drawn from p(z) and passed through the decoder are highly unlikely to follow the true ground-truth distribution (see for example (Makhzani et al., 2016) mentioned in our submission).

Beyond this though, the means by which we deal with this central, shared issue are fundamentally different.  In our case, we exploit provable conditions whereby an independent second-stage VAE can effectively learn and sample from the unknown q(z) produced by a first stage VAE, and additionally, we provide direct empirical evidence supporting this theory (e.g., see Figure 1, righthand plot).  Hence it no longer matters that p(z) and q(z) are not the same since we can just sample from the latter using the second-stage VAE.  Even though this approach may seem counter-intuitive at first glance, an accurate model can in fact be learned (provably so in certain situations), and our state-of-the-art results for a VAE model relative to GANs (the very first such demonstration in the literature) provide further strong corroborating evidence.

In contrast, (Tomczak and Welling, 2018) choose to parameterize p(z) in such a way that the additional flexibility can provide simpler pathways for pushing p(z) and q(z) closer together.  This is certainly an interesting idea, but it is significantly different from ours.  But of course we agree that the ultimate purpose is the same:  to have access to a known distribution with which to initiate passing samples through the decoder, a common goal shared by all autoencoder-structured models, including ours and many others like (Makhzani et al., 2016), where an adversarial loss is used to push p(z) and q(z) together.  What ultimately distinguishes these methods is, to a large degree, the specific way in which this goal is addressed.  We have no reservations about including additional discussion of (Tomczak and Welling, 2018), and these broader points in a revised version of our paper. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Sye7L4k1AQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e0X3C9tQ&amp;noteId=Sye7L4k1AQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1405 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the reply.  I think this is a great exposition of the differences and the paper will be strengthened by making some of these points in the revision.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_Bye1QOek6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The paper establishes a new state-of-art FID scores among auto-encoder based generative models with solid theoretical insights supporting the empirical results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e0X3C9tQ&amp;noteId=Bye1QOek6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1405 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1405 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper provides a number of novel interesting theoretical results on "vanilla" Gaussian Variational Auto-Encoders (VAEs) (sections 1, 2, and 3), which are then used to build a new algorithm called "2 stage VAEs" (Section 4). The resulting algorithm is as stable as VAEs to train (it is free of any sort of adversarial training, it comes with a little overhead in terms of extra parameters), while achieving a quality of samples which is *very impressive* for an Auto-Encoder (AE) based generative modeling techniques (Section 5). In particular, the method achieves FID score 24 on the CelebA dataset which is on par with the best GAN-based models as reported in [1], thus sufficiently reducing the gap between the generative quality of the GAN-based and AE-based models reported in the literature. 

Main theoretical contributions:

1. In some cases the variational bound of Gaussian VAEs can get tight (Theorem 1).
In the context of vanilla Gaussian VAEs (Gaussian prior, encoders, and decoders) the authors show that if (a) the intrinsic data dimensionality r is equal to the data space dimensionality d and (b) the latent space dimensionality k is not smaller than r then there is a sequence of encoder-decoder pairs achieving the global minimum of the VAE objective and simultaneously (a) zeroing the variational gap and (b) precisely matching the true data distribution. In other words, in this setting the variational bound and the Gaussian model does not prevent the true data distribution from being recovered.

2. In other cases Gaussian VAEs may not recover the actual distribution, but they will recover the real manifold (Theorems 2, 3, 4 and discussions on page 5).
In case when r &lt; d, that is when the data distribution is supported on a low dimensional smooth manifold in the input space, things are quite different. The authors show that there are still sequences of encoder-decoder pairs which achieves the global minimum of the VAE objective. However, this time only *some* of these sequences converge to the model which is in a way indistinguishable from the true data distribution (and thus again Gaussian VAEs do not fundamentally prevent the true distribution from being recovered). Nevertheless, all sequences mentioned above recover the true data manifold in that (a) the optimal encoder learns to use r dimensional linear subspace in the latent space to encode the inputs in a lossless and noise-free way, while filling the remaining k - r dimensions with a white Gaussian noise and (b) the decoder learns to ignore the k - r noisy dimensions and use the r "informative" dimensions to produce the outputs perfectly landing on the true data manifold. 

Main algorithmic contributions:
(0) A simple 2 stage algorithm, where first a vanilla Gaussian VAE is trained on the input dataset and second a separate vanilla Gaussian VAE is trained to match the aggregate posterior obtained after the first stage. The authors support this algorithm with a reasonable theoretical argument based on theoretical insights listed above (see end of page 6 - beginning of page 7). The algorithm achieves state-of-art FID scores across several data sets among AE based models existing in the literature. 

Review summary: 
I would like to say that this paper was a breath of fresh air to me. I really liked how the authors make a strong point that *it is not the Gaussian assumptions that harm the performance of VAEs* in contrast to what is usually believed in the field nowadays. Also, I think *the reported FID scores alone may be considered as a significant enough contribution*, because to my knowledge this is the first paper significantly closing the gap between generative quality of GAN-based models and non-adversarial AE-based methods. 

***************
*** Couple of comments and typos:
***************
(0) Is the code / checkpoints going to be available anytime soon?
(1) I would mention [2] which in a way used a very similar approach, where the aggregate posterior of the implicit generative model was modeled with a separate implicit generative model. Of course, two approaches are very different ([2] used an adversarial training to match the aggregate posterior), however I believe the paper is worth mentioning.
(2) In light of the discussion on page 6 as well as some of the conclusions regarding commonly reported blurriness of the VAE models, results of Section 4.1 of [3] look quite relevant. 
(3) It would be nice to specify the dimensionality of the Sz matrix in definition 1.
(4) Line ater Eq. 3: I think it should be $\int p_gt(x) \log p_\theta(x) dx$ ?
(5) Eq 4: p_\theta(x|x)
(6) Page 4: "... mass to most all measurable...".
(7) Eq 34. Is it sqrt(\gamma_t) or just \gamma_t?
(8) Line after Eq 40. Why exactly D(u^*) is finite?

I only checked proofs of Theorems 1 and 2 in details and those looked correct. 

[1] Lucic et al., 2018.
[2] Zhao et al., Adversarially regularized autoencoders, 2017, <a href="http://proceedings.mlr.press/v80/zhao18b.html" target="_blank" rel="nofollow">http://proceedings.mlr.press/v80/zhao18b.html</a>
[3] Bousquet et al., From optimal transport to generative modeling: the VEGAN cookbook. 2017, https://arxiv.org/abs/1705.07642</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">9: Top 15% of accepted papers, strong accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1eNhzHVTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e0X3C9tQ&amp;noteId=B1eNhzHVTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1405 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1405 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We appreciate the detailed and positive comments, which truly reflect many of the essential contributions of our work.  Likewise to the best of our knowledge, the FID scores we report are indeed the first to close the gap between GANs and non-adversarial AE-based methods as the reviewer points out.  Regarding the small comments concluding the review, we answer as follows:


- Reviewer Comment:  Is the code / checkpoints going to be available anytime soon?

Our Response: It was our original intention to simply post the code on Github after decisions were issued and papers were de-anonymized.  However, if there is a need to make the code available earlier while preserving anonymity, we could presumably pursue that as well (but not sure if this is considered acceptable under ICLR guidelines).


- Reviewer Comment:  Reference to an alternative method for estimating the aggregate posterior, and another paper addressing causes of blurry VAE representations.

Our Response:  Thanks for the nice references.  These papers actually look very interesting; we can cite them and provide context in the revision.


- Reviewer Comment: Line after Eq. 3: I think it should be \int p_gt(x) \log p_\theta(x) dx ?

Our Response: It is true that L(\theta, \phi) &gt;= - \int p_gt(x) \log p_\theta(x) dx. However, we further have that - \int p_gt(x) \log p_\theta(x) dx  &gt;=  -\int p_gt(x) \log p_gt(x) dx, which is the expression we include below Eq. (3) in the paper. The equality holds iff KL[q_\phi(z|x) || p_\theta(z|x)] = 0 and p_\theta(x) = p_gt(x) almost everywhere.


- Reviewer Comment: Line after Eq 40. Why exactly is D(u^*) finite?

Our Response: Because \varphi(u) is a diffeomorphism, is has a differentiable inverse and \Lambda(u) = (d\varphi(u)^-1/du)^\top (d\varphi(u)^-1/du) is always finite. Furthermore, D(u^*) is the maximum of \Lambda(u) in a closed set centered at u^*, so it is finite.  We will update the proof to include these extra details.


- Reviewer Comment: Minor typos/corrections

Our Response:  Thanks for catching each of these and also checking the proofs carefully.  We have fixed each typo/suggestion in a revised version. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Bkg4rgm93Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Two-stage VAE method to generate high-quality samples and avoid blurriness</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e0X3C9tQ&amp;noteId=Bkg4rgm93Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1405 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1405 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposed a two-stage VAE method to generate high-quality samples and avoid blurriness. It is accomplished by utilizing a VAE structure on the observation and latent variable separately. The paper exploited a collection of interesting properties of VAE and point out the problem existed in the generative process of VAE.  I have several concerns about the paper:

1.	It is necessary to explain why the second-stage VAE can have its latent variable more closely resemble N(u|0,I). Even if the latent variable closely resemble N(u|0,I), How does it make sure the generated images are realistic? I admit that the VAE model can reconstruct realistic data based on its inferred latent variable, however, when given a random sample from N(u|0,I), the generated images are not good, which is true when the dimension of the latent space is high. I still can’t understand why a second-stage VAE can relief this problem.
2.	The adversarial auto-encoder is also proposed to solve the latent space problem, by comparison, what is the advantage of this paper?
3.	Why do you set the model as two separate stages? Will it enhance the performance if we train theses two-stages all together?
4.	The proofs for the theory 2 and 3 are under the assumption that the manifold dimension of the observation is r, while in reality it is difficult to obtain this r, do these theories applicable if we choose a value for the dimension of the latent space that is smaller than the real manifold dimension of the observation? How will it affect the performance of the proposed method?
5.	The value of r and k in each experiment should be specified.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJlRhQrETQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer3 -- Part 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e0X3C9tQ&amp;noteId=BJlRhQrETQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1405 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1405 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for providing feedback regarding our submission and indicating specific points of uncertainty.  We provide detailed answers to each question as follows:


1.	Reviewer Comment:  Why do the second-stage VAE latent variables more closely resemble N(0,I), and how does this ensure that the generated samples are realistic, especially if the dimension of the latent space is high?

Our Response:  These issues are addressed in Section 4 of our paper, building on foundational properties of VAE models and our theory from Sections 2 and 3, but we can provide some additional background details here.  First, it can be helpful to check reference (Makhzani et al., 2016) which defines the aggregate posterior q(z) = \int q(z|x)p_gt(x)dx, where q(z|x) serves as the encoder and p_gt(x) is the ground-truth data density.  The basic idea behind generative models framed upon an autoencoder structure (VAE or otherwise) is that two criteria are required for producing good samples: (i) small reconstruction error when passing through the encoder-decoder networks, and (ii) an aggregate posterior q(z) that is close to some known distribution like N(0,I) that is easy to sample from.  Without the latter criteria, we have no tractable way of generating random inputs to the learned decoder that will produce realistic samples resembling the training data distribution.

In the context of our paper and VAE models, we argue that the first-stage VAE provides small reconstruction errors using a minimal number of latent dimensions (if parameterized properly with a trainable decoder variance), but not necessarily an aggregate posterior q(z) that is close to N(0,I).  This is because the basic VAE cost function is heavily biased towards finding low-dimensional manifolds upon which the data resides at the expense of learning the correct distribution within this manifold, which also prevents the aggregate posterior from nearing N(0,I).  However, although the VAE may partially fail in this regard, it nonetheless provides a useful mapping to a lower-dimensional space in such a way that we can apply Theorem 1 from our work.  In this lower dimensional space we treat q(z) \neq N(0,I) as a revised ground-truth data distribution p_gt(z), and train a new VAE with latent variables u.  Based on Theorem 1, in this restricted setting there will exist at least some parameterizations of the new encoder q(u|z) and decoder p(z|u) such that perfect reconstructions are possible, p_gt(z) is fully recovered, and KL[ q(u|z) || p(u|z) ] -&gt; 0.  If this all occurs, then we have the new second-stage aggregate posterior

q(u) = \int q(u|z)p_gt(z)dz  =  \int p(u|z)p_gt(z)dz = \int p_gt(z|u)p(u)dz  = p(u) = N(0,I)

as desired.  For practical deployment, we then only need sample u from N(0,I), then z from p(z|u), and finally x from p(x|z).  Note also that if the latent dimension of z is higher than actually needed, the first-stage VAE decoder is effectively capable of blocking/pruning the extra dimensions as discussed in Section 3.  This will not guarantee high quality samples, but it is adequate for preparing the data from the aggregate posterior q(z) to satisfy Theorem 1, which can then be leveraged by the second-stage VAE as mentioned above and in our paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJ-97B4pQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer3 -- Part 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e0X3C9tQ&amp;noteId=BJ-97B4pQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1405 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1405 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">2.	Reviewer Comment:  The adversarial autoencoder is also proposed to solve the latent space problem, by comparison, what is the advantage of this paper?

Our Response:  The adversarial autoencoder (Makhzani et al., 2016) requires adversarial training, meaning that like all GAN-related models, a complex min-max problem must be optimized in search of a saddle point.  A well-recognized advantage of VAEs is that the training involves pure minimization of a fixed variational energy function, which is generally more stable and resistant to mode collapse.  We should also point out that unlike VAEs, the adversarial autoencoder has no mechanism for pruning superfluous dimensions in the latent space.  Regardless of these key differences, we are aware of no published work where the adversarial autoencoder has been shown to produce competitive results generating novel samples like other GAN-related models (rather it has been tested on auxiliary tasks like semi-supervised learning, which is not in our scope).  Indeed the exhaustive recent testing from (Lucic et al., 2018) upon which we based our experiments, does not even include the adversarial autoencoder as a benchmark.



3.	Reviewer Comment:  Why train the model as two separate stages? Will it enhance the performance if we train these two stages together?

Our Response:  We have addressed this question on the bottom of page 7, which states the following: "It should also be emphasized that concatenating the two stages and jointly training does not improve the performance. If trained jointly the few extra second-stage parameters are simply hijacked by the dominant objective from the first stage and forced to work on an incrementally better fit of the manifold. As expected then, on empirical tests (not shown) we have found that this does not improve upon standard VAE baselines."  Our theoretical results and algorithm development from Sections 2-4 also directly support this conclusion.  Regardless, we are happy to clarify further if needed.



4.	Reviewer Comment:  Do the technical proofs require knowledge of the ground-truth manifold dimensions r?  And how is the proposed algorithm affected when r is unknown?

Our Response:  None of our proofs require that the ground-truth r is known explicitly in advance.  All that is required is that we set kappa &gt;= r (please see proof statements for Theorems 1-3).  In other words, we only need to set the latent dimension kappa to be bigger than the ground-truth manifold dimension r.  The VAE then has a natural mechanism in place for discarding superfluous dimensions.  Of course obviously in practice if we set kappa to be far too large, then the training could potentially become a bit more difficult, since in addition to learning the correct ground-truth manifold, we are also burdening the model to detect a much larger number of unnecessary dimensions.  But the VAE is arguably more robust to kappa than most methods, and the basic point still holds:  we need not set kappa = r, we just need to choose kappa to be a reasonable value that is at least as big as r.  In contrast, if we set kappa &lt; r, then the theory starts to break down and practical performance will begin to degrade as expected.



5.	Reviewer Comment:  The value of r and kappa in each experiment should be specified.

Our Response:  The true latent manifold dimension r is unknown in all of our experiments since we are using real-world data.  However, for the dimension of the VAE latent code, we chose kappa = 64 for all experiments, except for the 2-Stage VAE* model results, where we used 32 for MNIST and Fashion-MNIST, 192 for CIFAR-10, and 256 for CelebA.  Note that these values were not carefully tuned and need not be exact per the arguments responding to reviewer comment 4 above.  We just tried a single smaller value for the simpler data (MNIST and FashionMNIST), and a couple larger values for the more complex ones (CIFAR-10 and CelebA).
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJxVsagYnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Careful analysis of Gaussian VAEs yields valuable insights and training procedure</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e0X3C9tQ&amp;noteId=HJxVsagYnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1405 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1405 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Overview:
I thank the authors for their interesting and detailed work in this paper. I believe it has the potential to provide strong value to the community interested in using VAEs with an explicit and simple parameterization of the approximate posterior and likelihood as Gaussian. Gaussianity can be appropriate in many cases where no sequential or discrete structure needs to be induced in the model. I find the mathematical arguments interesting and enlightening. However, the authors somewhat mischaracterize the scope of applicability of VAE models in contemporary machine learning, and don't show familiarity with the broad literature around VAEs outside of this case (that is, where a Gaussian model of the output would be manifestly inappropriate). Since the core of the paper is valuable and salvageable from a clarity standpoint, my comments below are geared towards what changes the authors may make to move this paper into the "pass" category.

Pros: 
- Mathematical insights are well reasoned and interesting. Based on the insight from the analysis in the supplementary materials, the authors propose a two-stage VAE which separate learning the a parsimonious representation of the low-dimensional (lower than the ambient dimension of the input space), and the training a second VAE to learn the unknown approximate posterior. The two-stage training procedure is both theoretically motivated and appears to enhance the output quality of VAEs w.r.t. FID score, making them rival GAN architectures on this metric.

Cons:
- The title and general tone of the paper is too broad: it is only VAE models with Gaussian approximate posteriors and likelihoods. This is hardly the norm for most applications, contrary to the claims of the authors. VAEs are commonly used for discrete random variables, for example. Many cases where VAEs are applied cannot use a Gaussian assumption for the likelihood, which is the key requirement for the proofs in the supplement to be valid (then, the true posterior is also Gaussian, and the KL divergence between that and the approximate posterior can be driven to zero during optimization--clearly a Gaussian approximate posterior will never have zero KL divergence with a non-Gaussian true posterior).
- None of the proofs consider the approximation error garnered by only having access to empirical samples through a sample of the ground truth population. (The ground-truth distribution must be defined with respect to the population rather just the dataset in hand, otherwise we lose all generalizability from a model.) Moreover, the proofs hold asymptotically. Generalization bounds and error from finite time approximations are very pertinent issues and these are ignored by the presented analyses. Such concerns have motivated many of the recent developments in approximate posterior distributions. Overall, the paper contains little evidence of familiarity with the recent advances in approximate Bayesian inference that have occurred over the past two years.
- A central claim of the paper is that the two-stage VAE obviates the need for highly adaptive approximate posteriors. However, no comparison against those models is done in the paper. How does a two-stage VAE compare against one with, e.g., a normalizing flow approximate posterior? I acknowledge that the purpose of the paper was to argue for the Gaussianity assumption as less stringent than previously believed, but all of the mathematical arguments take place in an imagined world with infinite time and unbounded access to the population distribution. This is not really the domain of interest in modern computational statistics / machine learning, where issues of generalization and computational efficiency are paramount.
- While the mathematical insights are well developed, the specifics of the algorithm used to implement the two-stage VAE are a little opaque. Ancestral sampling now takes place using latent samples from a second VAE. An algorithm box is badly needed for reproducibility.

Recommendations / Typos

I noted a few typos and omissions that need correction.

- Generally, the mathematical proofs in section 7 of the supplement are clear. At the top of page 11, though, the paragraph correctly begins by stating that the composition of invertible functions is invertible, but fails to establish that G is also invertible. Clearly it is so by construction, but the explicit reasons should be stated (as a prior sentence promises), and so I assume this is an accidental omission.
- The title of Section 8.1 has a typo: clearly is it is the negative log of p_{theta_t} (x) which approaches its infimum rather than p_{theta_t} (x) approaching negative infinity.
- Equation (4): the true posterior has an x as its argument instead of the latent z.
- Missing parenthesis under Case 2 and wrong indentation. This analysis also seems to be cut off. Is the case r &gt; d relevant here?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJes_vH4pX" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e0X3C9tQ&amp;noteId=HJes_vH4pX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper1405 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1xK2VH4T7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer1 -- Part 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e0X3C9tQ&amp;noteId=B1xK2VH4T7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1405 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1405 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for providing detailed comments regarding our manuscript, including constructive ideas on how to improve the presentation and clarify the context.  We address each main point in turn.


- Reviewer Comment:  Limitation of Gaussian assumptions for likelihoods and approximate posteriors

Our Response:  In the introduction, we state that the most commonly adopted distributional assumption is that the encoder and decoder are Gaussian.  This claim was based on an informal survey of numerous recent papers involving VAE models applied to continuous data (e.g., images, etc.).  However, we completely agree that VAEs can also be successfully applied to discrete data types like language models, where these Gaussian assumptions can be more problematic.  Although all of our theoretical developments are clearly framed in the context of continuous data on a manifold, we are happy to revise the introduction to better explain this issue up front.  And of course the whole point of our paper is rigorously showing that even with seemingly restrictive Gaussian assumptions, highly non-Gaussian continuous distributions can nonetheless be accurately modeled.

Also, just to clarify one lingering point: although the decoder p(x|z) is defined to be Gaussian, it does not follow that the associated posterior p(z|x) will necessarily be Gaussian as well.  In fact this will usually not be the case when using deep models and parameters in general position.  However, the VAE can still push the KL divergence between p(z|x) and q(z|x) to zero even when the latter is constrained to be Gaussian as long as there exists at least some specially matched encoder-decoder parameterizations capable of pushing them together everywhere except on a space of measure zero.  This was left as an open problem under general conditions in the most highly-cited VAE tutorial (Doersch, 2016), and is what we demonstrate in Section 2.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ByebcVS4TX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer1 -- Part 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e0X3C9tQ&amp;noteId=ByebcVS4TX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1405 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1405 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">- Reviewer Comment:  Approximation error arising from finite samples not addressed; missing references to advances in approximate inference

Our Response:  In an ideal world we would obviously like to have optimal finite sample approximations that closely reflect practical testing scenarios.  But such a bar is impossibly high at this point.  Overall, we believe the value of theoretical inquiry into asymptotic regimes (i.e., population data rather than finite samples) cannot be dismissed out of hand, especially when simplifying assumptions of some sort are absolutely essential in making any reasonable progress.  Even so, the true test of any theoretical contribution is the degree to which it leads to useful, empirically-testable predictions about behavior in real-world settings.  In the present context, our theory makes the seemingly counter-intuitive prediction that a simple two-stage VAE could circumvent existing problems and produce realistic samples.  We then tested this idea via the neutral DNN architecture and comprehensive experimental design from reference (Lucic et al., 2018) and it immediately worked.  It is also critical to emphasize that these experiments were designed by others to evaluate top-performing GAN models with respect to generated sample quality, they were not developed to favor our approach in any way via some carefully tuned architecture or setting.  Therefore, regardless of whether or not our theory involves asymptotic assumptions, it made testable, non-obvious predictions that were confirmed in a real-world practical environment, providing the very first VAE-based architecture that is quantitatively competitive with GANs in generating novel samples (at least with continuous data like images).  We strongly believe that this is the hallmark of a significant contribution.

The reviewer also mentions that we may be unfamiliar with certain recent advances in approximate Bayesian inference, but no references were provided.  Which papers in particular is the reviewer referring to?  We are quite open to hearing about relevant work that we may have missed; however, presently we are unaware of any overlooked references that might serve to discount the message of our paper.  Note that there is an extensive recent literature developing more sophisticated VAE inference networks using normalizing flows and related. However, to the best of our knowledge, none of these works contain quantitative evaluations of generated sample quality (our focus here), and many (possibly most) do not even contain visualizations of images generated by the model.  Please see reference (van den Berg et al., "Sylvester Normalizing Flows for Variational Inference," UAI 2018) for the latest representative example we have found.  Of course our point here is not to disparage insightful papers of this type that provide significant advances in approximate inference.  Rather we are merely arguing that they seem to be somewhat out of the scope of our present submission, especially given the limited space for broader discussions.  But we can try to squeeze in more references and background perspective of this nature if the reviewer feels it could be helpful.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJxGPNr4TX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer1 -- Part 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e0X3C9tQ&amp;noteId=BJxGPNr4TX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1405 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1405 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">- Reviewer Comment:  No comparisons against VAE models with more flexible approximate posteriors such as those produced via normalizing flows

Our Response:  We agree that more flexible, explicitly non-Gaussian approximate posteriors have recently been proposed, such as the many flavors that utilize normalizing flows.  But such models have not as of yet been objectively shown to improve sampling quality (see comments above) despite the tremendous community-wide incentive to publish such a demonstration.  Moreover, the added flexibility often comes with a significant cost (e.g., increased training difficulty, more expensive inference).  Furthermore, if we consider broader VAE modifications beyond just the encoder, then even within this wider domain, the only VAE-related enhancement we are aware of that objectively/quantitatively produces improved samples is the WAE model from ICLR this year (Tolstikhin et al., 2018), which is already explicitly addressed in Section 5 of our submission.  Consequently, unless there is some very recent reference we may have missed, our experiments represent the state-of-the-art for non-adversarial VAE/autoencoer-based structures in term of the objective evaluation of generated samples, and the first to close the gap with GANs (this is also consistent with the comments from AnonReviewer2).


- Reviewer Comment:  Some details about the proposed 2-stage process are unclear

Although there was unfortunately no space for a separate algorithm box in our submission, the three bullet points on page 6 describe the specific process we used.  Note that the ancestral sampling required is very straightforward as described in bullet point 3 on page 6.  This is exactly what we followed for generating new samples via our method, but we are happy to provide further clarification if the reviewer has a specific suggestion.


- Reviewer Comment:  Recommendations/Typos

Our Response:  We sincerely appreciate the effort in finding typos and checking the proofs.  We have corrected each of the cases the reviewer uncovered.  This will certainly be of benefit to future readers.  Additionally, r can never be greater than d, because r is the manifold dimension within the ambient space of dimension d.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>