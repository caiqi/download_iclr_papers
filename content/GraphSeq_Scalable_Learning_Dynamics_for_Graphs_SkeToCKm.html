<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Graph2Seq: Scalable Learning Dynamics for Graphs | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Graph2Seq: Scalable Learning Dynamics for Graphs" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Ske7ToC5Km" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Graph2Seq: Scalable Learning Dynamics for Graphs" />
      <meta name="og:description" content="Neural networks have been shown to be an effective tool for learning algorithms over graph-structured data. However, graph representation techniques---that convert graphs to real-valued vectors for..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Ske7ToC5Km" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Graph2Seq: Scalable Learning Dynamics for Graphs</a> <a class="note_content_pdf" href="/pdf?id=Ske7ToC5Km" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019graph2seq:,    &#10;title={Graph2Seq: Scalable Learning Dynamics for Graphs},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Ske7ToC5Km},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Neural networks have been shown to be an effective tool for learning algorithms over graph-structured data. However, graph representation techniques---that convert graphs to real-valued vectors for use with neural networks---are still in their infancy. Recent works have proposed several approaches (e.g., graph convolutional networks), but these methods have difficulty scaling and generalizing to graphs with different sizes and shapes. We present Graph2Seq, a new technique that represents vertices of graphs as infinite time-series. By not limiting the representation to a fixed dimension, Graph2Seq scales naturally to graphs of arbitrary sizes and shapes. Graph2Seq is also reversible, allowing full recovery of the graph structure from the sequence. By analyzing a formal computational model for graph representation, we show that an unbounded sequence is necessary for scalability. Our experimental results with Graph2Seq show strong generalization and new state-of-the-art performance on a variety of graph combinatorial optimization problems.
</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">graph neural networks, scalable representations, combinatorial optimization, reinforcement learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Today's neural networks for graphs do not generalize to graphs that are much bigger than the training graphs. We propose graph2seq, a method that represents vertices as time-series sequences instead of fixed-sized vectors for improved generalization.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BklljYaanQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A good paper for the conference</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Ske7ToC5Km&amp;noteId=BklljYaanQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper788 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper788 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Graph representation techniques are important as various applications require learning over graph-structured data. The authors proposed a novel method to embedding a graph as a vector. Compared to Graph Convolutions Neural Networks (GCNN), the proposed are able to handle directed graphs while GCNN can not. Overall the paper is good, the derivation and theory are solid. The authors managed to prove the proposed representation is somehow lossless, which is very nice. The experiment is also convincing.

My only concern is as follows. The authors claim that Eq. (1) is able to handle features on vertices or edges. However, in the current formulation, the evolution only depends on vertex features, thus how can it handle edge features?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rklM99wcTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Ske7ToC5Km&amp;noteId=rklM99wcTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper788 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper788 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the helpful comments. 

Including edge features: There are a few different ways to include edge features. One way would be to include a second term $\sum_{e\in\eta(v)}y_e(t)$, where $\eta(v)$ are edges incident to node v and y_e are edge features of edge e, inside the ReLU function of Equation 1. Another way is to transform the graph with edge features into a new (larger) graph where there are no edge features. This is done by converting the original graph into a new bipartite graph where one partite corresponds to vertices of the original graph, and the other partite corresponds to edges of the original graph. Each edge-node in the bipartite graph is connected to the two vertex-nodes that constitute its end points in the original graph. The edge-nodes have the edge features of the original graph, while the vertex-nodes have the vertex features. We will explain this in the revision.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BygIHT5jn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Need some clarification</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Ske7ToC5Km&amp;noteId=BygIHT5jn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper788 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper788 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a new representation learning model for graph optimization, Graph2Seq. The novelty of Graph2Seq lies in utilizing intermediate vector representation of vertices in the final representation. Theoretically, the authors show that an infinite sequence of such intermediate representations is much more powerful than existing models, which do not maintain intermediate representations. Experimentally, Graph2Seq results in greedy heuristics that generalize very well from small training graphs (e.g. 15 nodes) to large testing graphs (e.g. 3200 nodes).

Overall, the current version of the paper raises a number of crucial questions that I would like the authors to address before I make my decision.

First, some strengths of the paper:
- Theory: although I have not reviewed the proofs in details, the theorems are very interesting. If correct, the theorems provide a strong basis for Graph2Seq. In contrast, this aspect is missing from other work on ML for optimization.

- Experiments: the experiments are generally thorough and well-presented. The performance of Graph2Seq is remarkable, especially in terms of generalization to significantly larger graphs.

- Writing: the paper is very well-written and complex ideas are neatly articulated. I also liked the Appendix trying to interpret the trained model. Good job!

That being said, I have some serious concerns. Please clarify if I misunderstood anything and update the paper otherwise.

- Graph2Seq at test time: in section Testing, you explain how multiple solutions are output by G2S-RNN at intermediate "states" of the model, and the best w.r.t. the objective value is returned. If I understand all this correctly, you take the output of the T-th LSTM unit, run it through the Q-network, then select the next node (e.g. in a vertex cover solution). Then, the complexity should be O((E+V)*T_max*V), since the Graph2Seq operations are linear in the size of the graph O(E+V), a single G2S-RNN(i) takes O(V) times if you want to construct a cover of size O(V), and you repeat that process exactly T_max times, for each i between 1 and T_max. What's wrong in my understanding of G2S-RNN here? Or is your complexity incorrect?

- Local-Gather definition: in your definition of the Local-Gather model, do you assume that computations are performed for a single iteration, i.e. a single local step followed by a gather step? If so, then how is Graph2Seq infinity-local-gather? What does that even mean? I understand how some of the other GCNN-based models like Khalil et al.'s is 4-local-gather (assuming 4 embedding iterations of structure2vec), but how is Graph2Seq infinity-local-gather?

- Comparison to Structure2Vec: for fair comparison, why not apply Algorithm 2 to that method? Just run more embedding iterations up to T_max, and use the best among the solutions constructed between 1 and T_max.

Minor:
- Section 4: Vinyals et al. (2015) does not do any RL.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Skxg-2w5pQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Ske7ToC5Km&amp;noteId=Skxg-2w5pQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper788 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper788 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the helpful comments. 

1. Time complexity: Thank you for pointing out the complexity. It is as follows:

(a) The time-complexity for one forward pass of G2S-RNN (e.g., to select one vertex in minimum vertex cover) is O((E + V)T_max). This is because during each step of Graph2Seq, O(E) operations are required to update the state at each vertex based on neighbors' states, and O(V) operations are required by the GRU to aggregate the states of all vertices (Equation 19 in appendix). Since these operations have to be repeated at each step, and there are at most T_max steps, the time-complexity is O((E + V)T_max).

(b) For a fixed number of steps T, the time-complexity to compute a complete solution (e.g., to select multiple vertices such that they form a valid vertex cover) is O((E + V)T_max * V). This is because selecting one vertex has complexity O((E + V)T_max), and we may have to select O(V) vertices to obtain a valid solution to the input graph.

(c) The overall time-complexity is O((E + V)T_max * V * T_max). This is because the final solution is computed by first computing valid solutions for each T=1,2,..,T_max, and then picking the best valid solution from among them. Computing a valid solution for a fixed T takes O((E + V)T_max * V) as mentioned above, and we have to repeat the process T_max times. 

Note that aggregating states from all the vertices in the GRU is a hyperparameter choice. If only local neighborhood states are used in the GRU, the time-complexity in step (a) above becomes O(ET_max + V). 

We will clarify the complexity in Section 5. 

2. Local Gather definition: Yes, the definition of local-gather consists of one local step followed by one gather step. An algorithm is k-local-gather if in the local step, each vertex computes an embedding based on the k-hop neighborhood graph around the vertex. Structure2Vec is 4-local-gather because the four embedding iterations cause each node's embedding to depend on its 4-hop neighborhood. Graph2Seq is infinity-local-gather since the infinite number of embedding iterations cause each node's embedding to depend on the entire graph---not just on vertices a constant number of hops away from the node. Infinity is used to emphasize that the local graph neighborhoods on which the node embeddings depend are not constrained in size. For a specific graph G with diameter dia(G), it is also true that Graph2Seq is dia(G)-local-gather. We will include a remark to explain that infinity-local-gather means that a node's embedding can depend on the entire graph, regardless of the graph size.

3. Comparison to Structure2Vec: We will include an experiment that applies algorithm 2 to Structure2Vec in the paper. Note, however, that applying this procedure to Structure2Vec implicitly uses the sequence of Structure2Vec embeddings as the embedding for each vertex. Therefore, this method is a different instantiation of our idea of using sequences for node embeddings. In particular, like Graph2Seq, this method will also consider neighborhoods of different sizes around each vertex for different graphs. The only difference is that Graph2Seq additionally uses an LSTM to process the sequence. Therefore, we indeed expect that the combination of Algorithm 2 with Structure2Vec will also perform well.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SyexIerDnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea with weaknesses in the formal and empirical parts</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Ske7ToC5Km&amp;noteId=SyexIerDnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper788 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper788 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose a method for learning vector representations for graphs. The problem is relevant to the ICLR community. 

The paper has, however, three major problems:

The motivation of the paper is somewhat lacking. I agree that learning representations for graphs is a very important research theme. However, the authors miss to motivate their specific approach. They mention the importance of learning on smaller graphs and applying the learned models to larger graphs (i.e., extrapolating better). I would encourage the authors to elaborate on some use cases where this is important. I cannot think of any at the moment. I assume the authors had use cases in combinatorial optimization in mind? Perhaps it might make sense to motivate the use of GNNs to solve vertex cover etc. 

I’m not sure about the correctness of some of the theorems. For instance, Theorem 2 states 
“For any fixed k &gt; 0, there exists a function f(·) and an input graph instance G such that no k-LOCAL-GATHER algorithm can compute f(G) exactly.”  I’m not claiming that this is a false statement. What I am suspecting at the moment is that the proof might not necessarily be correct. For instance, it is known that what you call 1-LOCAL-GATHER can compute the 1-Weisfeiler-Leman partition of the nodes (sometimes also referred to as the 1-WL node coloring). Now consider the chain graph 1 - 2 - 3 - 4 - 5. Here, the partition that puts together 1-WL indistinguishable nodes are {1, 5}, {2, 4} and {3}. Hence, the 1-WL coloring is able to distinguish say nodes 2 and 3 even their 1-neighborhood looks exactly the same. A similar argument might apply to your example pairs of graphs but I haven’t checked it yet in detail. What is for sure though: what you provide in the appendix is not a proper formal proof of Theorem 2. This has to be fixed. 

The experiments are insufficient. The authors should compare to existing methods on common benchmark problems such as node or graph classification datasets. Comparing to baselines on a new set of task is not enough. Why not compare your method also on existing datasets?
If you motivate your method as one that performs well on combinatorial problems (e.g., vertex cover) you should compare to existing deterministic solvers. I assume that these are often much faster at least on smaller graphs. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkxxnJOcTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Ske7ToC5Km&amp;noteId=BkxxnJOcTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper788 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper788 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the helpful comments. 

1. Motivation: Our primary motivation is learning algorithms for combinatorial optimization on graphs. In many practical applications, it is desirable to learn an algorithm on smaller graphs that can generalize to larger graphs. For example, Mirrhosseini et al. [1] consider the problem of deciding how to optimally assign TensorFlow ops to GPUs to minimize runtime. Since directly training placement policies on large TensorFlow graphs can be extremely slow, it would be beneficial if a model can be trained on small TensorFlow graphs in a way that generalizes to large TensorFlow graphs. Another example is query optimization in databases, where the optimal order of join operations in the query plan tree has to be determined [2]. Since evaluating complex queries with large query plans can be expensive, it is again helpful if the learning algorithm can be trained on simple queries in a way that generalizes to complex queries. We will modify the introduction to emphasize these use cases. 

2. Theorem 2: The chain graph 1-2-3-4-5 results in the partitions {1, 5}, {2, 4} and {3} after the 1-hop WL algorithm if the initial node labels are chosen as their respective degrees. Since the degree label already includes one-hop information, this means, overall it is a 2-local-gather algorithm and not a 1-local-gather algorithm. If the initial node labels are chosen identically for all nodes, then the partitions are {1, 5}, {2, 3, 4}. We would appreciate clarification on what parts of the proof of Theorem 2 are unclear or informal. 

3. Baselines: As our focus is on combinatorial optimization problems, comparing on benchmark node or graph classification datasets is outside the scope of this paper and is an important future research direction. We have compared Graph2Seq to existing deterministic solvers (Gurobi), heuristics (list), approximation algorithms (matching, greedy) and a range of graph neural networks. Note that the performance plots for the Gurobi solver is implicit in the plots (e.g., Figure 2 and 3) since the approximation ratio for all other schemes have been computed relative to the Gurobi solver. The plots corresponding to the list heuristic (brown), matching algorithm (green) and greedy heuristic (yellow) are explicitly shown in Figures 2 and 3. 

[1] Device placement optimization with reinforcement learning, Mirhoseini et al, 2017
[2] Learning to optimize join queries with deep reinforcement learning, Krishnan et al, 2018
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1lebgVjp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Misunderstandings</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Ske7ToC5Km&amp;noteId=H1lebgVjp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper788 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper788 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear authors, 

Let me quickly clarify some misunderstandings. 

Theorem 2: 

- The statement "the degree label already includes one-hop information" is not correct. One-hop means considering information that is up to one hop away. The degree of the node itself is "0"-hops away.
- In your proof, you chose k=2 and then show two graphs of which you claim that there are nodes that have the same 2-hop neighborhood and, therefore, cannot be distinguished by a 2-local-gather approach. What I have tried to point out is that even if two nodes have the same neighborhood (for k = 1), they can be distinguished with 1-local-gather. The argument can be extended for larger k as well. Just increase the size of the chain: A-B-C-D-E-F-G. Now, C and D have the same 2-hop neighborhood and *still* can be distinguished with 1-WL which is a 1-local-gather approach (even if it is a 2-local-gather approach, as you claim, which is not correct because 1-WL only looks at the 1-hop neighborhood). What you are missing is that an algorithm that "only" looks at the k-hop neighborhood can still distinguish two nodes with identical k-hop neighborhood (but different l-neighborhood, l&gt;k) due to the iterative (i.e., recursive) application of the algorithm. The information is propagated throughout the graph. That's what the 1-WL algorithm is doing.
- You proof for k=2 (which is not correct as pointed out above) and write: "the example easily generalizes for larger k". You would have to at least make an argument how you want to generalize. 

Overall, there seem to be several gaps in your proof. Again, the statement "a k-local-gather approach cannot distinguish nodes in the graph that have the same k-hop neighborhood" is incorrect. 

Baselines:

Since your motivation seems to be solving combinatorial optimization problems with a graph NN and since according to your response you have used Gurobi, why not report the running times? The main problem one is faced when working with combinatorial optimization problems is the time it takes to solve them. Even if you don't stand a chance to be faster than Gurobi on smaller problems, you should be able to generate larger problems and, eventually, there should be a break-even point where your method is faster than Gurobi. It is these types of investigations that I am completely missing in the paper. There is not a single mention of the time it took to run the baselines, Gurobi, or your proposed method. (Please correct me if I'm wrong.)
Sure, it is interesting that you can approximate the solutions more tightly but at what cost? If your method takes always longer than an optimal solver, what's the point?

Concerning experiments on other benchmark datasets: This is not strictly necessary. I agree. But it would make the paper much stronger. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJlWaUdha7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author response 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Ske7ToC5Km&amp;noteId=SJlWaUdha7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper788 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper788 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the comments. 

1. Clarification on node label:
We first note that node labels greatly affect how many local aggregation steps are required to compute a function on a graph. For example, if nodes are labeled by their degrees then, as you have correctly pointed out, a 1-local algorithm is enough to distinguish vertices in the example of line graphs such as 1-2-3-4-5. In general, having node degree as a label can allow vertex embeddings in a k-local algorithm to depend on the (k+1)-hop neighborhood around the vertices. In the extreme case, node labels can encode the entire structure of the graph, e.g., if the adjacency matrix is used as a label. In this case, even a 0-local algorithm will be able to exactly compute any function over the graph because the entire graph can be inferred just by looking at a node's label. The graphs we consider in the proof of Theorem 2 are unlabeled graphs. In unlabeled graphs, attributes such as node degree are not inherently part of a node and need to be explicitly computed. 

WL being 1-local vs 2-local: 
We note that Theorem 2 makes an existential statement: for a fixed k, we are saying there *exists* some graph G and some function f(.) such that f(G) cannot be computed exactly by any k-local-gather algorithm. Therefore, to prove the theorem, all we need is one example graph G and one example function f(.) where f(G) cannot be computed exactly by any k-local-gather algorithm. We have freedom to select whatever G and f(.) we want, as long as we can prove f(G) cannot be computed exactly by any k-local-gather algorithm. 

The graph G that we choose in our proof is an *unlabeled* tree shown in Figure 5, without any node or edge attributes or features. If we run a 1-hop WL on such a graph, the first step will be to assign the respective degrees of nodes as the starting color. However, the graph we have chosen is such that there are no features intrinsically present within each node. In particular, the node degree information is absent in the nodes, and hence need to be explicitly computed. 

If we consider the 0-hop neighborhood of each node---basically just the nodes themselves---then we cannot compute the node degree since a node in isolation does not reveal how many nodes it is connected to. Therefore, we must look at the 1-hop neighborhood---the node together with all the nodes it is connected to---in order to compute the degree. Once the initial color (node degree) has been computed, we can then proceed to do one step of neighborhood color aggregation as per the WL algorithm and compute the updated node colors. Thus overall, we have performed two steps of 1-hop aggregation operations---one for computing the node degrees, and one for aggregating neighboring colors---which makes the 1-hop WL algorithm a 2-local-gather algorithm in our chosen graph. 

For some other choice of the graph, the 1-hop WL algorithm will be a 1-local-gather algorithm. For example, we can choose the graph to be such that each node intrinsically includes its degree as a label. In such a graph, the initial node degree color can be computed by looking at the 0-hop neighborhood of nodes, since the node itself (in isolation) includes this information; we do not need to look at the 1-hop neighborhood. 

Clarification of definition of local-gather: 
A k-local-gather algorithm, according to our definition, consists of two steps: a local step which computes an embedding for each vertex based on its k-hop neighborhood, and a gather step which aggregates embeddings from all vertices. In particular, the local step occurs only once, and is not computed k times. Therefore, for unlabeled feature-less graphs, such as the one we have considered in the proof, information from outside the k-hop neighborhood of a vertex cannot reach the vertex in just one step of local computation. Consequently, a k-hop local-gather algorithm cannot distinguish vertices having identical k-hop neighborhood even if the l-hop neighborhoods (for l &gt; k) are different. 

Algorithms such as the k-WL or k-step GCNN perform 1-hop local neighborhood aggregation repeatedly for k (or k+1) steps. However, these algorithms are still valid k-local-gather (or (k+1)-local-gather) algorithms. This is because: (i) the k (or k+1) steps of 1-hops aggregation causes each vertex embedding to depend on its k (or k+1) hop neighborhood, and (ii) we can think of the k (or k+1) steps of 1-hop aggregation in these algorithms together as forming the local step of a k-local-gather (or (k+1)-local-gather) algorithm. This is why a 1-hop WL algorithm which performs two steps of 1-hop aggregation operations is a valid 2-local-gather algorithm. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HkgHmDu26Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author response 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Ske7ToC5Km&amp;noteId=HkgHmDu26Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper788 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper788 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Extending proof to larger k: 
The proof of Theorem 2 can easily be extended to larger k by considering the same two trees in Figure 5, but with longer chains around each degree-3 node. Currently, around each degree-3 node there are chains of nodes, with three nodes in each chain. For a general k, we would increase the chains of three nodes to chains of l nodes, where l is any odd number greater than k. This has already been mentioned in the proof. The proof itself is identical. We will elaborate it further in the revision. 

We realize these can be confusing points and thank you for bringing this to attention. We will include explanations in the paper to clarify node labeling and the other points. We hope this answer clarifies any misunderstandings. 

2. Run times: For large graphs (size &gt; 400) we indeed observed that Gurobi is much slower and took many hours or days. We will include measurement data on how long it takes for Gurobi and other baselines to compute a solution in our revision.

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>