<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Dynamic Recurrent Language Model | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Dynamic Recurrent Language Model" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rylBZ305KQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Dynamic Recurrent Language Model" />
      <meta name="og:description" content="Language evolves over time with trends and shifts in technological, political, or cultural contexts. Capturing these variations is important to develop better language models. While recent works..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rylBZ305KQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Dynamic Recurrent Language Model</a> <a class="note_content_pdf" href="/pdf?id=rylBZ305KQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019dynamic,    &#10;title={Dynamic Recurrent Language Model},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rylBZ305KQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Language evolves over time with trends and shifts in technological, political, or cultural contexts. Capturing these variations is important to develop better language models. While recent works tackle temporal drifts by learning diachronic embeddings, we instead propose to integrate a temporal component into a recurrent language model. It takes the form of global latent variables, which are structured in time by a learned non-linear transition function. We perform experiments on three time annotated corpora. Experimental results on language modeling and classification tasks show that our model performs consistently better than temporal word embedding methods in two temporal evaluation settings: prediction and modeling. Moreover, we empirically show that the system is able to predict informative latent states in the future.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">language modeling, variational inference, dynamic model, temporal data, deep learning</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rygn9EXshm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Little awareness to previous attempts to dynamic language models</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylBZ305KQ&amp;noteId=rygn9EXshm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1167 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1167 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper aims to improve sequential prediction using LSTM by incorporating
another latent variables {z_t} that follow distinct dynamics rather than LSTM
itself. 
Experimental results on comparing with naive LSTM and dynamic word embedding
approaches confirmed that the proposed method works better than these 
baselines.

Basically the proposed method is sensible, but this paper has two clear
drawbacks: (1) it is not compared with similar previous attempts; and (2)
the proposed architecture is not guaranteed to represent "global" contextual
behavior rather than local contextual one.

(1) There have been numerous attempts in sequential modeling, especially
language models, to incorporate a global and contextual factor into prediction.
Early attempts used latent dirichlet allocation combined with n-gram models;
afterwords, LSTM became prevalent, and in fact there exists a famous baseline
of Latent LSTM allocation [1]. Therefore, it is minimally required for this
paper to compare the proposed model with latent LSTM.

(2) Latent LSTM above has a clear advantage that there is an explicit mechanism
of latent topic distribution that governs a specific sequence (global variable).
However, the proposed method in this paper, contextual behavior is determined
only locally, and might be quite prone to overfit to exceptional observations,
in this case, words. Of course, it might handle a global context by chance,
but the architecture does not guarantee it. Therefore, such a potential
disadvantage should be experimentally investigated by a comparison with a
suitable baselines noted above.

I agree with the authors that the contexual behavior should be continuous
and flexible, such as pursued in this paper. Thus a challenge is how to
suitably incorporate a global context into such a representation.
Finally, I would like to know what kind of function g is learned through this
experiment. Investigating into the strengths and weaknesses of the learned
function g is also beneficial for a more flexible language models in the 
future.

[1] Zaheer, M., Ahmed, A., &amp; Smola, A. J. (2017). Latent LSTM Allocation: Joint Clustering and Non-Linear Dynamic Modeling of Sequence Data. In International Conference on Machine Learning (pp. 3967-3976).
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1xlsTkd2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A straightforward extension of recurrent neural network language model with stochastic hidden variables. A simplified version of stochastic RNN.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylBZ305KQ&amp;noteId=H1xlsTkd2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1167 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1167 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a straightforward extension of recurrent neural network language model (RNNLM) by considering the stochastic hidden variables rather than deterministic hidden variables in traditional RNNLM. The motivation is to capture the shift of meaning in language over time based on this stochastic modeling. This RNN model is developed with twofold considerations. One is to represent the latent transition while the other is to perform the reconstruction. Two separate RNNs were run to fulfill the whole model. The key of this paper is the regularization term in Eq. (3) which constrains the transition of two neighboring hidden states.  This paper considers the global latent variables at each time step as well as the transition function over different time steps. The transition of latent variables in two neighboring steps is modeled by a Gaussian distribution with the mean driven by latent variable in former step which is similar to what dynamic topic model was doing.

Pros: A simplified version of stochastic RNN which adopts a Gaussian distribution for state transition with neural network based mean parameter. Idea is interesting. Solution is meaningful.

Cons: 
1. The stochastic or variational recurrent neural networks have been published before. Citations to these previous works are missing in this paper.
2. A variant of stochastic RNN with a number of assumptions and simplifications. Two RNNs were used and separately trained. The transition probability from z_{t-1} to z_t is independent of observation x. This is odd.
3. In variational neural network, there are an encoder and a decoder for recognition and generation. This work is missing such a structure. It is difficult to explain how latent variable is obtained and how new word is predicted.
4. Although the experimental results show improvement, the justification is still not enough. The comparison with other variational RNN (e.g. the work entitled "Sequential Neural Models with Stochastic Layers") is missing. The visualization of latent information is required.
3. Typo: The sentence in the second line of Conclusion ".. conditioned and ..." is incomplete.

Remarks:
1. Deriving a hybrid criterion and joint optimization over the parameters (for encoder and decoder) is suggested. 
2. The interpretation about how the training data are related to the stochastic transition is required.
3. Suggest to figure out the details of Eq. (3) instead of simply citing Krishnan et al. (2017).</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SkxrCV9wnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Limited novelty on the modeling side + lack of motivation on the relevance of the overall problem</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylBZ305KQ&amp;noteId=SkxrCV9wnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1167 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1167 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes to model shifts in language use over time with a recurrent neural network architecture augmented with stochastic variables. Stochastic variables are sampled at the beginning of each time segment and are supposed to capture variability introduced by the time shift. The proposed architecture is mainly compared to approaches that model evolution of “word embeddings” through time and the authors seem, throughout the paper, to contrast their approach to these streams of works. Experimental results show the proposed model architecture outperform RNNs augmented with dynamic word embedding models at test time, in language modeling settings.

The first drawback of the paper is the limited novelty of the architecture. These types of architectures are well-known and have already been used in a variety of other tasks (Fraccaro et al. <a href="https://arxiv.org/pdf/1605.07571.pdf," target="_blank" rel="nofollow">https://arxiv.org/pdf/1605.07571.pdf,</a> Serban et al. (https://arxiv.org/pdf/1612.00377.pdf) , etc...). Therefore, the contribution of this paper is to apply this architecture to model shifts in language over time.

Furthermore, it is not clear to me how the proposed model can be used to track evolution of semantic similarities between words over time as well as detecting topical changes. Estimating dynamic word embeddings has the positive aspect of enhancing interpretability by modeling the evolution of emergent “topics” across time. This seems to me one of interesting aspects in modeling time shift in language and, for this reason, it has been previously extensively studied. How would that be possible under the proposed approach ? How to extract the evolution of the semantic field of a specific term across time under the proposed model ? In this aspect, contrasting the current architecture with models based on dynamic word-embeddings harm the paper.

Following up on these last considerations, the relevance of the problem tackled in this paper seems a bit weak/unclear to me. Why would it be important to form model of language that are robust across time-spans ? Couldn’t we argue that the same models could be “retrained” on new data as it comes in which would lead to study (language) models that can continually learn in an efficient fashion ?

In summary, the main points of concern are:
- the limited novelty on the modeling side
- the lack of proper motivation on the importance of the addressed problem 

Minor remarks:

- Yao et al. (2018) “use alternate optimization that breaks the flow of gradient through time” , unclear as their method can be optimized using stochastic gradient descent, as the Yao et al. point out. Maybe one drawback is parameter efficiency.

- Learning a transition function in the latent space has also been proposed in Fraccaro et al. (https://arxiv.org/pdf/1605.07571.pdf), and should be cited.

- What would these stochastic variables capture ? Why it is a good idea for the current problem ?

- Page 4: DiffDtime -&gt; DiffTime

- How do you compute PPL in your setting ? Is it the upper bound coming from the ELBO ? It would be crucial to see evolution of the KL divergence during time, as a hint of how much information is encoded into the latent variables.

- “recursive inference”, e.g. either finetuning q for a particular x or at test time is related to Salimans et al. (https://arxiv.org/pdf/1410.6460.pdf) and therefore should be cited.

- I cannot really see that “DRLM-F significantly improves long-term performances on NYT”. From the graph, I can see a gap of ~1,1.5 perplexity point from 126 to ~124.5 which doesn’t seem striking to me.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>