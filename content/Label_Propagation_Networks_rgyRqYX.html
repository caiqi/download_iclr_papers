<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Label Propagation Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Label Propagation Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=r1g7y2RqYX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Label Propagation Networks" />
      <meta name="og:description" content="Graph networks have recently attracted considerable interest, and in particular in the context of semi-supervised learning. These methods typically work by generating node representations that are..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_r1g7y2RqYX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Label Propagation Networks</a> <a class="note_content_pdf" href="/pdf?id=r1g7y2RqYX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019label,    &#10;title={Label Propagation Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=r1g7y2RqYX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Graph networks have recently attracted considerable interest, and in particular in the context of semi-supervised learning. These methods typically work by generating node representations that are propagated throughout a given weighted graph.

Here we argue that for semi-supervised learning, it is more natural to consider propagating labels in the graph instead. Towards this end, we propose a differentiable neural version of the classic Label Propagation (LP) algorithm. This formulation can be used for learning edge weights, unlike other methods where weights are set heuristically. Starting from a layer implementing a single iteration of LP, we proceed by adding several important non-linear steps that significantly enhance the label-propagating mechanism.

Experiments in two distinct settings demonstrate the utility of our approach.
</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">semi supervised learning, graph networks, deep learning architectures</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Neural net for graph-based semi-supervised learning; revisits the classics and propagates *labels* rather than feature representations</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rJxDdu4A37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>deep learning architecture for graph semi-supervised learning</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1g7y2RqYX&amp;noteId=rJxDdu4A37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper972 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper972 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents an interesting idea for the following task: given a graph and a subset of labelled nodes, infer the labels on the remaining nodes. Here the authors will make prediction for absent labels based on local averages on the graph of the neighbouring soft labels. The main originality is that the local average is weighted and the weights are learnt. 

I had trouble understanding the details of the algorithm and the authors should be more careful in their description of the algorithm. Some points to clarify:
- section 3.1, I am not sure to understand the 'dynamic weights'. The main point here seems to be the use of an attention mechanism (which does not vary in time) applied to inputs varying in time?
- section 3.2, I do not understand equation (13). What is \theta^\tau, it does not appear in the right-hand term?

I think that using the term time is misleading. Time might refer to epochs in an optimization process, whereas time in Section 3 seems to refer to a number of layers as described in equation (6).

Please, be more explicit on the use of raw features. How are the similarities described in appendix B incorporated in the loss?

Overall, I think this paper requires a lot of clarification before being published.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJxISgAn2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Method for non-linear label propagation while learning the network weights simultaneously. Insufficient comparison to related methods, insufficient experimental evidence and explanation for the results. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1g7y2RqYX&amp;noteId=HJxISgAn2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper972 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper972 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a framework for non-linear label propagation where the weights are learned simultaneously. There are model specific and experimental setup design decisions that require justification. There also needs to be a number of ablation studies to justify the effectiveness of the different components of this framework.  Finally, there seems to be an insufficient comparison (both experimentally and theoretically) to the large amount of related literature. 
- What is the total number of parameters in the proposed network? Please clarify how this is "relatively few parameters" as compared to other methods. 
- Please compare how your method for learning weights relates to the following papers and the references therein [1,2]
[1] Online Learning of Multiple Tasks and Their Relationships. Saha et al, AISTATS, 2011. 
[2] Convex Learning of Multiple Tasks and their Structure, Cilberto et al, 2015. 
- It would be good to have an ablation study in order to discern what is the contribution of learning the weights vs propagating labels (instead of embeddings). 
- For clarity, please specify that \theta are the parameters to be learned. 
- Please explain the intuition of using entropy and KL divergence for the attention weights. Shouldn't the attention for an edge be inversely proportional to the entropy i.e. the attention should be higher if the neighboring node's label is more certain?
- Instead of the bifurcation mechanism proposed in section 3.2, isn't it possible to use a threshold to round the resulting prediction to a hard label?
- In equation 13, are the hyper-parameters a, b tuned using cross-validation? Can't we learn the \tau in the same training procedure? Please justify this design decision?
- What is the performance if the loss in equation 14 is replaced by the standard empirical loss? There needs to be an ablation study on this. 
- If the node features are available, how are they used in this framework?
- In the experimental section, why is k chosen to be equal to 1%? Please show results while varying this. 
- Please justify the line "parameterizes w using a small number (~20) of informative features based on the raw features (e.g., dimensionality reduction), the graph (e.g., edge betweenness), and the labeled set (e.g., distance from labeled nodes). " Isn't it possible to get similar performance by reducing the number of parameters so that model doesn't overfit?
- Please clearly state what is the difference in the framework from the Kipf and Welling, 2016 paper?
- Why isn't there a comparison to methods like Graph-Sage?
- Please explain this line "LPNnobif degrades with large T, and even \tau slightly above 1 makes a difference"
- Finally, please explain the trend in the results in Table 1. For example, why is the performance of the proposed method poor on the Flickr dataset, but better on the DBLP dataset?
- It would good to have uncertainty estimates for the results reported in Table 1. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SklckBGU3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea, but heuristical.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1g7y2RqYX&amp;noteId=SklckBGU3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper972 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper972 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary
This paper proposes label propagation network (LPN), a neural network to learn label prediction and similarity measure (weights) between data points simultaneously in semi-supervised setting. The proposed method simulates label propagation steps with the forward pass of LPN, enabling backpropagation through label propagation steps.

Strong points
- Learning both weights and label predictions in SSL seems to be novel (provided that the author's claim in the related work section is right).
- Good performance.
- The paper is generally well written.

Concerns
- Replacing the label propagation by forward pass of a neural network is an attractive idea, but because of that the convergence guarantee is lost.  As Figure 4 shows, LPN without bifurcation mechanism seems to suffer from convergence issue as the number of evaluation step grows. I guess that the algorithm may go wrong even with bifurcation mechanism for some data, for example if the bifurcation rate grows too fast/slow.
- The original label propagation works with weights without entropy. Does introducing entropy term (e(h_i;theta)) is always helpful? For instance, if some data points erroneously get certain during initial iterations, the whole algorithm may fail.
- The performance reported for GCN is quite different from what is presented in the GCN paper, and authors explain that this is due to the different experimental setting. For me the performance gap is quite significant to be originated from different experimental setting. Could you elaborate on this? Also, how many GCN layers were used?
- Too many hyperparameters to tune.

Minor points
- I think the line above Eq (4) should be like \tilde w_ij = w_ij / sum_k w_ik.
- Eq (10) is quite misleading. The original weight w_ij should be symmetric (w_ij = w_ji), but this is not. Also, considering the intuition behind the label propagation, I think Eq (10) should be like alpha_ij(h_i, h_j) = exp(e(h_j) + d(h_i, h_j)), not e(h_i) as written the paper.
- In the experiments setting, the authors calling their algorithm as DeepLP_alpha and DeepLP_phi. I guess these should be LPN_alpha and LPN_phi.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Bkei7b-soQ" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1g7y2RqYX&amp;noteId=Bkei7b-soQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper972 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Byguq2Sw5Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>questions about baselines / evaluation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1g7y2RqYX&amp;noteId=Byguq2Sw5Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Michael_Bronstein1" class="profile-link">Michael Bronstein</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Oct 2018</span><span class="item">ICLR 2019 Conference Paper972 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Interesting paper! I have three main issues/questions. 

First, it seems you are not using a standard split on Cora and Pubmed datasets - what is the reason? Could you please better explain in which settings your algorithm works well and in which it does not? (i.e. how sparse should the set of known nodes could be?) Using a common split makes it much easier to compare to other methods. 

Second, I am surprised you do not cite or compare to some "standard" graph CNN architectures (you can find a comprehensive review in [8]), such as spectral graph CNNs [1] (a seminal work of Bruna et al. that started the recent interest in this field), spectrum-free methods with polynomial [2] and rational [3] filters or their more recent variations with graph motifs [4]. Alongside with GAT and MoNet, you may want to look into architectures using graph shift operators[6], generalized attention mechanism on dual graphs [5], and dynamic graph update [7]. In particular, [5-6] are state-of-the-art on standard Cora split. 
    
Third, please note that [5] and [7] allow to learn the graph weights ([5] by performing convolutions on the dual graph, and [6] by updating the graph between layers). Also, in [9], in the context of geometric matrix completion, a learnable diffusion of score values (akin to "label propagation" in this context) was used. Perhaps you might want to rephrase some of the novelty statements in your introduction, or at least place them better in the context of prior works. 


1. Spectral Networks and Locally Connected Networks on Graphs, arXiv:1312.6203.

2. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering, arXiv:1606.09375

3. CayleyNets: Graph convolutional neural networks with complex rational spectral filters", arXiv:1705.07664
      
4. MotifNet: a motif-based Graph Convolutional Network for directed graphs", arXiv:1802.01572
      
5. Dual-Primal Graph Convolutional Networks, arXiv:1806.00770.

6. ON GRAPH CONVOLUTION FOR GRAPH CNNS, DSW 2018

7. Dynamic Graph CNN for learning on point clouds, arXiv:1712.00268       
  
8. Geometric deep learning: going beyond Euclidean data, IEEE Signal Processing Magazine, 34(4):18-42, 2017

9. Geometric matrix completion with recurrent multi-graph neural networks, NIPS 2017
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJgzy2ogcm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good idea and paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1g7y2RqYX&amp;noteId=SJgzy2ogcm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Mathias_Niepert1" class="profile-link">Mathias Niepert</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Oct 2018</span><span class="item">ICLR 2019 Conference Paper972 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I genuinely enjoyed reading your paper. There are two questions I'd be interested to get your answer for:

(1) Why did you chose an experiment set-up for Cora/Citeseer/Pubmed that's different to the one of previous work? I understand that you rerun all the baselines. But it would still be helpful to have the results for the standard set-up for a more straight-forward comparison. 

(2) You might want to consider "Learning Graph Representations with Embedding Propagation" (EP; self plug) as related work. It is a NIPS'17 paper that describes how label/feature embeddings can be propagated throughout the graph. It is possible with EP to propagate and learn class label embeddings. Please don't get me wrong. Your method is in several ways novel (as far as I can tell) but since you did not cite EP as related work, I thought I make you aware of it.

Thanks!</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1gZxggrcQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Comment on experimental setting + EP paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1g7y2RqYX&amp;noteId=B1gZxggrcQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper972 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Oct 2018</span><span class="item">ICLR 2019 Conference Paper972 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your constructive comments!

1) Our experimental setting follows the classic graph-SSL evaluation scheme used in numerous works (*). The setting used in Yang et al. (2016) and in several other following papers is slightly different. The main difference lies in how data points are partitioned into train, validation, and test sets. In the classic setting, the labeled set is created by sampling k% of the data uniformly at random. The labeled set can then be used for training, validation, or any other usage deemed appropriate, and all other unlabeled points are used for evaluation. In contrast, Yang et al. (2016) uses three distinct sets: (a) a small, class-balanced train set, (b) a designated (and fairly large) validation set, and (c) a test set comprised of only a subset of the unlabeled points.

As recently pointed out in Oliver et al. (2018), SSL evaluation should be done with care. Based on the above (and other subtle differences), we believe the classic setting is better suited as an evaluation scheme for our paper.

2) Thank you for pointing out the EP paper. We will add it to the related work discussed in the paper. Our approach is different from EP, since our focus is on propagating class labels (as opposed to graph labels that are the focus of EP), and as such the aggregation and non-linearities we employ are tailored for points on the simplex, and our training loss is discriminative. It is quite likely however that the two methods can be combined.

(*) Notable examples include the SSL book Chapelle et al. (2006) and, among others, papers by Zhu et al. (2003); Zhou et al. (2004); Zhang &amp; Lee (2007); Perozzi et al. (2014); Grover &amp; Leskovec (2016); Tang et al. (2015); Monti et al. (2017).
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1xSMZE_9Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Should use the fraction values k in {10%, 20%, ..., 90%}, not using  k = 1%</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1g7y2RqYX&amp;noteId=H1xSMZE_9Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Oct 2018</span><span class="item">ICLR 2019 Conference Paper972 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Using k = 1% for the classical setting is not standard. You should have a careful look in [1, 2] and use the fraction values k in {10%, 20%, ..., 90%} to show the effectiveness of your model. Otherwise, you can report the results on the standard split to compare to other models.

[1] DeepWalk: Online Learning of Social Representations. Perozzi et al. (2014).
[2] node2vec: Scalable Feature Learning for Networks. Grover &amp; Leskovec (2016)</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJgmkm6eqQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>I really like the EP-B paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1g7y2RqYX&amp;noteId=HJgmkm6eqQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Oct 2018 (modified: 03 Oct 2018)</span><span class="item">ICLR 2019 Conference Paper972 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi Niepert, I read the paper "Learning Graph Representations with Embedding Propagation" introducing EP-B last year. I really like the EP-B paper and I would like to prefer it rather than GraphSAGE also published at NIPS 2017, because the EP-B paper has a very nice idea and a well-standard experimental setup for both the transductive and inductive settings that none of graph network-related ICLR submissions (that I have read) doing experiments on Cora/Citeseer/Pubmed cites the paper and follows the same setup. At this time, in my opinion, the *unsupervised* model EP-B is the best model obtaining state-of-the-art results in the transductive and inductive settings on the three datasets, even in comparison with semi-supervised models such as GAT and GCN.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>