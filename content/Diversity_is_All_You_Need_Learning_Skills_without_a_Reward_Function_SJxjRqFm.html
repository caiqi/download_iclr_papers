<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Diversity is All You Need: Learning Skills without a Reward Function | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Diversity is All You Need: Learning Skills without a Reward Function" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SJx63jRqFm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Diversity is All You Need: Learning Skills without a Reward Function" />
      <meta name="og:description" content="Intelligent creatures can explore their environments and learn useful skills without supervision.&#10;  In this paper, we propose ``Diversity is All You Need''(DIAYN), a method for learning useful skills..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SJx63jRqFm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Diversity is All You Need: Learning Skills without a Reward Function</a> <a class="note_content_pdf" href="/pdf?id=SJx63jRqFm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019diversity,    &#10;title={Diversity is All You Need: Learning Skills without a Reward Function},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SJx63jRqFm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Intelligent creatures can explore their environments and learn useful skills without supervision.
In this paper, we propose ``Diversity is All You Need''(DIAYN), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">reinforcement learning, unsupervised learning, skill discovery</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose an algorithm for learning useful skills without a reward function, and show how these skills can be used to solve downstream tasks.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BkhzbQF1a7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clearly-written, well-demonstrated learning and application of unsupervised skills via information-theoretic / entropic methods</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJx63jRqFm&amp;noteId=BkhzbQF1a7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper752 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper752 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose a learning scheme for the unsupervised acquisition of skills. These skills are then applied to (1) accelerate reinforcement learning to maximize a reward, (2) perform hierarchical RL, and (3) imitate an expert trajectory.

The unsupervised learning of skills maximizes an information theoretic objective function. The authors condition their policy on latent variable Z, and the term skill refers to a policy conditioned on a fixed Z. The mutual information between states and skills is maximized to ensure that skills control the states, while the mutual information between actions and skills given the state is minimized, to ensure that states, not actions distinguish skills. The entropy of the mixture of policies is also maximized. Further manipulations on this objective function enable the scheme to be implemented using a soft actor-critic maximizing a pseudo reward involving a learned skill discriminator.

The authors clearly position their work in relation to others, and especially point out the differences to the most similar work, namely Gregor et al 2016. These differences while seemingly minor end up providing exceptional improvement in the number of skills learned and the domains tackled.

The question-answer style is somewhat unconventional. While the content comes across clearly, the flow / narrative is a bit broken.

Overall, I believe that applicability of the work is very wide, touching inverse RL, hierarchical RL, imitation learning, and more. The simulational comparisons are also very useful.

However, there is an issue that I'd like to see addressed:
Fig 8: In a high-dimensional task, namely 111D ant navigation, DIAYN performs slightly worse than others. Incorporating a prior on useful skills makes DIAYN perform much better. Here, apart from the comparision with other state of the art RL methods, the authors should also compare to VIC. Indeed one of the key differences to VIC was the uniform prior on skills, which the authors now break albeit in a slightly different way. Thus, it is essential to also show the performance of VIC, and comment on any similarities / differences. The relation of this prior to the VIC prior should also be made clear. Further, the performance of VIC on the half cheetah hurdle should be also be shown.

If the above issue is addressed, I strongly recommend that the work be presented at ICLR.

Minor issues / typos:
pg 1: "policy that alters that state of the environment" to "policy that alters the state of the environment"
pg 3: "mutual information between skills and states, I(A; Z)" to "mutual information between skills and states, I(S; Z)"
pg 4: "guaranteeing that is has maximum entropy" to "guaranteeing that it has maximum entropy"
pg 4: " soft actor critic" to " soft actor critic (SAC)" since SAC is used later.
pg 5: full form of VIME not introduced
Fig 5: would be good to also show the variance as a shaded area around the mean.
pg 7: "whereas DIAYN explicitly skills that effectively partition the state space" ?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJxPVKde0X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJx63jRqFm&amp;noteId=rJxPVKde0X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper752 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper752 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your comments! We are currently running the requested comparison to VIC in the hierarchical RL experiments, and will update the paper once completed. We have also corrected the typo that you mentioned.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJeAJln337" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review - Diversity is All You Need: Learning Skills without a Reward Function </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJx63jRqFm&amp;noteId=HJeAJln337"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper752 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper752 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">*Pros:*
-	Mostly clear and well-written paper 
-	Technical contribution (learning many diverse skills) is principled and well-justified (although the basic idea builds on a large body of prior work from related fields as also evidenced from the reference list)
-	Extensive emperical evaluation on multiple tasks and different scenarios (mainly toy examples) showing promising results.

*Cons:*
-	The main paper assumes detailed knowledge of the actor critic setup to fully follow and appreciate the paper (a few details provided in the appendix) 
-	p(z): it is not entirely clear to me how the dimensionality of z should be chosen in a principled manner aside from brute force evaluation (as in 4.2.2; which does not go beyond a few hundreds). What happens for many skills and would learning p(z) be preferable in this scenario?
-	Note: The work has been in the public domain for some time thereby limiting the apparent novelty. This has not influenced my decision as per ICLR policy. 

*Significance*: I think this work would be of interest to the ICLR crowd despite it having been in the public domain for a some time. It provides a simple objective for training RL models in an unsupervised manner by learning multiple diverse skills and contributes with an extensive and convincing empirical evaluation which will surely have a lasting impact in the RL subfield. 

*Further comments/questions*:
-	The authors assume that only states and not actions are observable. Intuitively, it would seem easier to obtain the desired results if the actions are also available. Could the authors perhaps clarify why it is reasonable to assume that the actions are not observable to the planner when evaluating the objective in Eq 1?  Similarly, I’d like some insight into the behaviour of the proposed method if actions are also available (and how it differs from prior art in this case)?
-	I’d suggest enforcing consistently in the way variation across random seeds is visualised in the figures (e.g. traces in fig 4, no indication in fig5, shaped background in fig 6). 
-	I’d suggest making it explicit what $\theta$ refers to in Eq. 1 (and provide some details about the SAC setup for completeness, as previously mentioned)
-	Minor typos etc: {p2, l6} missing word, “SAC” never defined, “DOF” never defined, and a few other typos/punctuation issues throughout. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1xu8tdlR7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJx63jRqFm&amp;noteId=r1xu8tdlR7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper752 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper752 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your helpful comments! 

Dimension of z: The dimension of the skill indicator, z, is a hyperparameter. If the dimension is too small, then you cannot learn many skills. If the dimension is too large, many skills are not distinguishable. While we found that a 50 dimensional categorical distribution worked well for all environments we tested, the right choice ultimately depends on the problem at hand.

Learning p(z): To answer your questions about learning large numbers of skills, we repeated the experiment in Figure 4, varying the number of skills from 10 to 300. Results are now included in Appendix E, Figure 18. We found increasing the number of skills does not solve the "rich get richer" problem. Even when learning 300 skills, the effective number of distinguishable skills quickly drops to be less than 10.

Why not condition on actions? We agree that it would be "easier" to maximize our mutual information objective if we conditioned the discriminator on both states and actions, rather than just states. In fact, the data processing inequality tells us this: the mutual information between (state, action) pairs and latents I(S, A; Z) is an upper bound for the mutual information between states and latents I(S; Z). However, the ease of taking discriminable actions is precisely the problem with this objective. In preliminary experiments, we found that conditioning the discriminator on both states and actions substantially increased the number skills that were distinguishable in action space but not state space, effectively maximizing I(A; Z). This result is problematic because, for skills to be useful for downstream tasks, they must consistently change the state.

We have fixed the typos and reworded the method section to be more accessible to readers unfamiliar with SAC.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJxA5jvqnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Strong paper with interesting contributions, but with slightly over-stated claims </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJx63jRqFm&amp;noteId=HJxA5jvqnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper752 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper752 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a method for learning skills in absence of a reward function. These skills are learned so that the diversity of the trajectories produced by each skill is maximised. This is achieved by having a discriminator attempting to tell these skills apart. The agent is rewarded for visiting states that are easy to distinguish and the discriminator is trained to better infer the skills from states visited by the agent. Furthermore, a maximum entropy policy is used to force the skills to be diverse. The proposed method is general and any RL algorithm with entropy maximisation in the objective can be used, the implementation in the paper uses the Soft Actor Critic method.

The problem that they are tackling is interesting and is of clear value for obtaining more generalisable RL algorithms. The paper is overall clear and easy to follow, the results are interesting and potentially useful, although I have some reservations regarding how they assess this usefulness in the current version of this paper.
Structure-wise, I would say that the choice of writing the paper in the form of a Q&amp;A, with very brief explanations and details was more distracting and at times unnecessary than I liked (e.g. Question 7 could move to Appendix as it is quite trivial).

I really appreciated how much care has been taken to discuss differences with the closest prior work, Variational Intrinsic Control (VIC) by Gregor et al. 
One such difference is that their prior distribution over skills is not learnt. While there are good arguments by the authors about why this is appealing (e.g. it prevents collapsing to sampling only a few skills), I feel this could be also quite a limitation of their method. This assumes that you have a good a-priori knowledge and assumptions regarding how many skills are useful or needed in the environment. This is unlikely to be the case in complex environments, where you first need to learn simple skills in order to explore the environment, and later learn to form new more complex skills. During this process, you might want to prune simplistic skills after you learnt more abstract and complex ones, for instance in the context of continual learning. I understand this could be investigated in future work, but I feel they take a rather optimistic take on this problem.

Overall, the use case for the proposed method is slightly unclear to me. While the paper claims to allow diverse set of skills to be learnt, it is highly dependent on learning varied action sequences that help you visit different part of state space, regardless of their usefulness. This means there could be learn a lot of skills that capture part of the state space that is not useful or desirable for downstream tasks. While there is a case made for DIAYN being a stepping stone for imitation learning and hierarchical RL, I don’t find the reported experiments for imitation learning and HRL convincing. In the imitation learning experiment, the distance (KL divergence) between all skills and the expert data is computed and the closest skill is then chosen as the policy imitating the expert. The results are weak and no comparisons with any LfD baselines are reported. The HRL experiments also lack comparisons to any other HRL baseline. I feel that this section is rather weak, especially compared to the rest of the paper, and I am not sure it achieves much.

As a general comment, the choice of reporting the training progress using “hours spent training” is an peculiar choice which is never discussed. I understand that for methods with varying computational costs this might be a fairer comparison but it would be perhaps good to also report progress against number of required environment interactions (including pre-training).
Another assumption made is that the method is valuable in situations where the reward function is expensive to compute and the unsupervised pre-training is free (somewhat easing the large amount of pre-training required). However, it would have been interesting to see examples of such environments in their experiments supporting these claims, as this assumption is not valid for the chosen MuJoCo environments.

Despite these comments, I still feel this is valuable work, that can clearly inspire further relevant work and deserves to be presented at ICLR.
It presents a solid contribution, given its technical novelty, proposed applications and its overall generality. 
However, the paper could use more convincing experiments to support its claims.

Additional comments and typos:
- Figure 5 lack error bars across the 5 random seeds and are crucial to assess whether this performance difference is indeed significant given the amount of pre-training required.
- Figure 7’s title and caption is missing...
- typo: page 3, last paragraph “...mutual information between skills and states, **I(S; Z )**” not I(A; Z)
- typo: page 7 paragraph next to Figure 6 “...whereas DIAYN explicitly **learns** skills that effectively partition the state space”
- typo: page 7 above Figure 8 “...make them **exceedingly** difficult for non- hierarchical RL algorithms.”
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkgRuY_lCQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJx63jRqFm&amp;noteId=SkgRuY_lCQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper752 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper752 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for all the feedback!

Learning p(z): We would like that emphasize that prior work (VIC [Gregor et al]) also requires the user to choose the number of skills. Choosing this parameter is analogous to choosing K in K-means. While we can propose various heuristics, the right choice ultimately depends on the problem at hand. Empirically, we found that a 50-dimensional categorical distribution worked well in all environments we tested. Your question hints at a deeper question: can we recursively or hierarchically compose skills to learn behaviors of increasing complexity? While this is beyond the scope of this work, we think it is an exciting direction for future research, and encourage work in this direction.

"Useless skills": We agree that without explicitly biasing our skills towards certain types of behaviors, we are likely to end up with many skills that are not useful for a particular given task. We presented one way to bias skill discovery in Section 4.2.2 (Question 7), and showed experimentally that it enabled better performance on hierarchical tasks (Figure 8 (right), purple line). We should emphasize that including a task-specific biases skill may cause poor generalization across tasks.

LfD and HRL baselines: We will run additional experiments comparing to imitation learning and hierarchical RL baselines in the final version.

For Figure 8, we plotted over time instead of iterations because TRPO and SAC have very different costs per iteration (TRPO is substantially most costly). We will include a plot over time in the final paper, but caution that TRPO will be run for many fewer iterations than SAC. We'll also fix the typos and clarifications you've suggested.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>