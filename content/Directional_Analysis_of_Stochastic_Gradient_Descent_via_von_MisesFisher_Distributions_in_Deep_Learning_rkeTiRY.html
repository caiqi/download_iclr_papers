<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Directional Analysis of Stochastic Gradient Descent via von Mises-Fisher Distributions in Deep Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Directional Analysis of Stochastic Gradient Descent via von Mises-Fisher Distributions in Deep Learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rkeT8iR9Y7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Directional Analysis of Stochastic Gradient Descent via von..." />
      <meta name="og:description" content="Although stochastic gradient descent (SGD) is a driving force behind the recent success of deep learning, our understanding of its dynamics in a high-dimensional parameter space is limited. In..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rkeT8iR9Y7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Directional Analysis of Stochastic Gradient Descent via von Mises-Fisher Distributions in Deep Learning</a> <a class="note_content_pdf" href="/pdf?id=rkeT8iR9Y7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019directional,    &#10;title={Directional Analysis of Stochastic Gradient Descent via von Mises-Fisher Distributions in Deep Learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rkeT8iR9Y7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Although stochastic gradient descent (SGD) is a driving force behind the recent success of deep learning, our understanding of its dynamics in a high-dimensional parameter space is limited. In recent years, some researchers have used the stochasticity of minibatch gradients, or the signal-to-noise ratio, to better characterize the learning dynamics of SGD. Inspired from these work, we here analyze SGD from a geometrical perspective by inspecting the stochasticity of the norms and directions of minibatch gradients. We propose a model of the directional concentration for minibatch gradients through von Mises-Fisher (VMF) distribution, and show that the directional uniformity of minibatch gradients increases over the course of SGD. We empirically verify our result using deep convolutional networks and observe a higher correlation between the gradient stochasticity and the proposed directional uniformity than that against the gradient norm stochasticity, suggesting that the directional statistics of minibatch gradients is a major factor behind SGD.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">directional statistics, deep learning, SNR, gradient stochasticity, SGD, stochastic gradient, von Mises-Fisher, angle</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">One of theoretical issues in deep learning</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_S1lAhxfZam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The theory looks good but how can it be used?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkeT8iR9Y7&amp;noteId=S1lAhxfZam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper216 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper216 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
Gradient stochasticity is used to analyse the learning dynamics of SGD. It consists of two aspects: norm stochasticity and directional stochasticity. Although the norm stochasticity is easy to compute, it vanishes when the batch size increases. Therefore, it can be hard to measure the learning dynamics of SGD. The paper is motivated by measuring the learning dynamics by the directional stochasticity. Directly measuring the directional stochasticity with the ange distribution is hard, so the paper uses vMF distribution to approximate the uniformity measurement. The paper theoretically studies the proposed directional uniformity measurement. In addition, the experiments empirically show the directional uniformity measurement is more coherent with the gradient stochasticity.

1. As I’m not a theory person, I’m not very familiar with the related work on this line. But the analysis on the directional uniformity is interesting and original. So is the vMF approximation.
2. The theoretical analysis looks comprehensive and intuitive. And the authors did a reasonably good job on the experiments.
3. This paper provides some insights that warn people to pay attention to the directions of SGD. But the paper didn’t provide an answer on how this study can inform people to improve SGD. It’s true that the directional uniformity increases over training and it is correlated to the gradient. But what could this bring us remains unstudied.
4. Can the authors provide any theoretical or empirical analysis on why the directional uniformity didn’t increase in deep models like CNN and why it increases when BN and Res are applied?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJxhAiOPTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer4</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkeT8iR9Y7&amp;noteId=SJxhAiOPTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper216 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper216 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">“The paper didn’t provide an answer on how this study can inform people to improve SGD”

We strongly agree with you and R2 that it is desirable to find a practical algorithm based on the theoretical analysis. We however believe this is out of this paper’s scope, and we leave it as future research.

“Can the authors provide any theoretical or empirical analysis on why the directional uniformity didn’t increase in deep models like CNN and why it increases when BN and Res are applied?”

Our theoretical analysis makes a few assumptions on the loss function which is induced by the choice of a network architecture, such as the well-behavedness of the loss function. We conjecture our observation that the uniformity of minibatch gradients monotonically growing with a deep convolutional network equipped with residual connections and/or batch normalization is due to the fact that the loss function induced from this kind of network confirms well with the assumptions, as were discussed for instance earlier by Li &amp; Yuan (2017) and Santurkar (2018). This is in contrast to deep networks without these latest techniques.

- Li &amp; Yuan (2017) Convergence analysis of two-layer neural networks with ReLU activation
- Santurkar et al. (2018) How does batch normalization help optimization? (No, it is not about internal covariate shift)
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ByxlCvF92m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea but implications, significance, theoretical analysis and experiments need improvements.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkeT8iR9Y7&amp;noteId=ByxlCvF92m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper216 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper216 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
Quality and clarity: good.

Originality and significance: This paper studies the stochasticity
of the norms and directions of the mini-batch gradients, to
understand SGD dynamics. The contributions of this paper can be
summarized as: a) This paper defines gradient norm stochasticity as
the ratio of the variance of the stochastic norm to the expectation
of the stochastic norm. It theoretically and empirically shows that
this value is reduced as the batch size increases b) This paper
empirically finds that the distribution of angles between
mini-batch gradient and a given uniformly sampled unit vector
converges to an asymptotic distribution with mean 90 degrees, which
implies a uniform distribution of the mini-batch gradients. c)	
This paper uses von Mises-Fisher Distribution to approximate the
distribution of the mini-batch gradients. By theoretically and
empirically observing that the estimated parameter \hat \kappa
decreases during training, they claim that the directional
uniformity of mini-batch gradients increases over SGD training.

The idea of measuring the uniformity of mini-batch gradients
through VMF distribution seems interesting. But it is unclear how
the study of this stochasticity dynamics of SGD can be related to
the convergence behavior of SGD for non-convex problems and/or the
generalization performance of SGD.

There are additional concerns/questions regarding both theoretical
part and empirical part:

[1] Section3.3: Assumption that p_i(w_0^0) =p_i(w_1^0) = p_i is not
reasonable when theoretically comparing \hat \kappa(w_1^0) and \hat
\kappa(w_0^0). The concentration parameter \hat \kappa(w) should be
estimated by the sum of the normalized mini-batch gradients "\hat
g_i(w)/||\hat g_i(w)||" . Instead of using mini-batch gradient,
this paper uses the sum of "p_i-w" by assuming that "p_i(w_0^0) -w"
is parallel to "\hat g_i(w)", which is ok. However, when comparing
\hat \kappa(w_0^0) and \hat \kappa(w_1^0), we say \hat
\kappa(w_0^0) = h(\sum p_i(w_0^0) - w_0^0) ) and \hat \kappa(w_1^0)
= h(\sum p_i(w_1^0) - w_1^0) ). It is not reasonable to use the
same p_i for p_i(w_0^0) and p_i(w_1^0) because p_i(w_0^0) -w_1^0 is
definitely not parallel to \hat g_i(w_1^0).

[2] Section 3.3: Assumption \hat g_i(w_t^{i-1}) \hat g_i(w_t^0) is
not convincing. With this assumption, the paper writes w_1^0 =
w_0^0 - \eta\sum_i \hat g_i(w_0^{i-1}) = w_0^0 - \eta\sum_i \hat
g_i(w_0^0) = w_0^0 - \eta \sum_i p_i-w_0^0. These equalities are
not persuasive. Because, \sum_i \hat g_i(w_0^0) is the full
gradient g(w_0^0) at w_0^0. In other words, these equalities imply
that from w_0^0 to w_1^0 (one epoch), SGD is doing a full gradient
descent: w_1^0 = w_0^0 -\eta g(w_0^0), which is not the case in
reality.

[3] Experiment: Batch size should be consistent with the given
assumption in the theoretical part. In theoretical part, \hat
\kappa(w_1^0) &lt; \hat \kappa(w_0^0) is based on the assumption that
|\hat g_i(wt^{i-1}| \tat for all i, with *large mini-batch size*.
But in the experiment, they prove \hat \kappa(w_1^0) &lt; \hat
\kappa(w_0^0) by using small-batch size which is 64. The authors
should either provide experiments with large batch size or try to
avoid the assumption of large batch size in theoretical part.

[4] The CNN experiment; It is better to add a discussion why the
\kappa increases in the early phase of training.

[5] The experiment results show, by the end of training, all models
FNN, DENN and CNN have very large value of \kappa which is around
10^4. This value implies that the mini-batch gradients distribution
is pretty concentrated, and it is contradictory to the statement in
the introduction which is "SGD converges or terminates when either
the norm of the minibatch gradient vanishes to zeros, or when the
angles of the mini-batch gradients are uniformly distributed and
their non-zero norms are close to each other''. It is also
contradictory to the experiment in 3.2 which implies the mini-batch
gradient are uniformly distributed after training.

[6] The notations in this paper can be improved, some notations are
using "i" for batch index, some notations are using "i" for one
data sample. Some notations in Section 3.3 and 3.1 can be moved to
Section 2 Preliminaries. It will be clearer to define all the
notations in one place.

Typos: -Section 3.1: first paragraph, E\hat g(w) -&gt; E[\hat g(w)]; -
Paragraph before Lemma2: \hat \kappa increases -&gt; \hat \kappa
decreases; - Paragraph after Theorem2: double the directions in "If
SGD iterations indeed drive the directions the directions of
minibatch gradients to be uniform".</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJlhIp_DaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkeT8iR9Y7&amp;noteId=SJlhIp_DaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper216 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper216 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
“[1] Section3.3  It is not reasonable to use the same p_i for p_i(w_0^0) and p_i(w_1^0) because p_i(w_0^0) -w_1^0 is definitely not parallel to \hat g_i(w_1^0).”

We believe the confusion may have arisen from our example in page 6 (together with Fig. 4b). Specifically, we want to clarify that we did not intend to say that \frac{\hat g_i(w_0^{i-1}}{\| \hat g_i(w_0^{i-1} \|} can be replaced with \frac{p_i-w_0^0}{\| p_i-w_0^0 \|}. Instead, our intention was to show that \sum_{i=1}^3 \frac{\hat g_i(w_0^{i-1}}{\| \hat g_i(w_0^{i-1} \|} could be replaced by \sum_{i=1}^3 \frac{p_i-w_0^0}{\| p_i-w_0^0 \|}, when all the sufficient conditions Corollary 3.1. 

“[2] Section 3.3: Assumption \hat g_i(w_t^{i-1}) [\approx] \hat g_i(w_t^0) is not convincing. “

This is not an assumption we need. In fact, what we need is for the norms to be similar, i.e., \| \hat{g}_i(w_t^{i-1})\| \approx \| \hat{g}_i(w_t^0)\| for proving Corollary 3.1 (see Appendix D for its proof.) 

We state \hat g_i(w_t^{i-1}) \approx \hat g_i(w_t^0) as one simple possible case of having similar norms of these two minibatch gradients. We will clarify this in the next revision.

“[3] Experiment: Batch size should be consistent with the given assumption in the theoretical part”

The main theoretical analysis in our paper largely depends on the assumption that the norms of minibatch gradients are similar and not that the size of minibatch is large. We discuss a large size of minibatch as one case in which those norms are similar to each other, although there may be other ways for it to happen. 

We choose a reasonable minibatch size to confirm whether and in which setup our theoretical observation can be confirmed in practice. We however thank you for your suggestion and are running experiments while varying minibatch sizes at the moment. We will update the submission with new results soon.

“[4] The CNN experiment; It is better to add a discussion why the \kappa increases in the early phase of training.”

Our theoretical analysis makes a few assumptions on the loss function which is induced by the choice of a network architecture, such as the well-behavedness of the loss function. We conjecture our observation that the uniformity of minibatch gradients monotonically growing with a deep convolutional network equipped with residual connections and/or batch normalization is due to the fact that the loss function induced from this kind of network confirms well with the assumptions, as were discussed for instance earlier by Li &amp; Yuan (2017) and Santurkar (2018). This is in contrast to deep networks without these latest techniques.

- Li &amp; Yuan (2017) Convergence analysis of two-layer neural networks with ReLU activation
- Santurkar et al. (2018) How does batch normalization help optimization? (No, it is not about internal covariate shift)

“[5] The experiment results show, by the end of training, all models FNN, DENN and CNN have very large value of \kappa which is around 10^4.”

We would like to point out that the absolute value of $\kappa$ is not a good indicator of the uniformity due to its dependence on the dimensionality, as was investigated earlier by Cutting et al. Instead, our theoretical analysis and experiments focus on the trend of \kappa over training.

- Cutting et al. [2017] Tests of concentration for low-dimensional and high-dimensional directional data

“[6] The notations in this paper can be improved”

Thanks for the suggestion! We will revise the text to make it clearer.


</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1lDUA_jaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Additional response to AnonReviewer1 re estimated kappa values</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkeT8iR9Y7&amp;noteId=H1lDUA_jaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper216 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper216 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">“The experiment results show, by the end of training, all models FNN, DENN and CNN have very large value of \kappa which is around 10^4. This value implies that the mini-batch gradients distribution is pretty concentrated, and it is contradictory to the statement”

In order to verify our earlier claim that the accuracy of the kappa estimate grows with respect to the dimensionality (the number of parameters of a neural network in our case), we have run some simulations. In these simulations, we vary the dimensionality, the number of samples and the true, underlying kappa by sampling from the vMF distribution with the designated kappa and estimating the kappa from these samples. We uploaded the plot from the simulation with 10,000 dimensions, which you can check from <a href="https://ibb.co/dNycc0" target="_blank" rel="nofollow">https://ibb.co/dNycc0</a> (both x- and y-axes are log10.) Unfortunately we could not easily go over 10,000 dimensions due to the difficulty in sampling from the vMF distribution.

As can be seen from the uploaded plot, the estimated kappa approaches the true kappa from above as the number of samples increases. When the true kappa is large, the estimation error rapidly becomes zero as the number of samples approaches 3,000. When the true kappa is low (i.e., uniform over the angles), however, the gap does not narrow completely even with 3,000 samples. 

While fixing the true kappa to 0 and the number of samples to 3,000, we vary the dimensionality to empirically investigate the estimated kappa values. We chose to use 3,000 samples to be consistent with our experiments in the paper. We ran five simulations each and report both mean and standard deviation. See below for the estimates:

d=200k, kappa=3,651.06+-5.10
d=640k, kappa=11,682.88+-3.57
d=2M,    kappa=36,526.49+-13.05

We clearly observe the trend of increasing estimated kappas w.r.t. the dimensions. This suggests that we should not compare the absolute estimated kappas across different network architectures due to the differences in the number of parameters. This agrees well with Cutting et al. [2017] which empirically showed that the threshold for rejecting the null hypothesis of the true kappa being 0 grows with respect to the dimensions.

At the end of training, DFNN+BN has approximately 2M parameters and the minimum estimated kappa we observed was 6.83 x 10^4, FNN+BN has approximately 640k parameters and the minimum estimated kappas was 2.04 x 10^4, and CNN+BN+Res has approximately 200k parameters and the minimum estimated kappas were 1.56 x 10^4. Considering these in the context of the simulation result above, we cannot say that the underlying directional distribution of minibatch gradients in all these cases at the end of training is not close to uniform.

We again emphasize that our theory focuses more on the relative decrease of kappa (i.e. the relative increase of directional uniformity) rather than the absolute value of the estimated kappa.

- Cutting et al. [2017] Tests of concentration for low-dimensional and high-dimensional directional data

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_SJeiEX3YnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Contribution not entirely clear</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkeT8iR9Y7&amp;noteId=SJeiEX3YnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper216 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper216 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: This work provides an analysis of the directional distribution of of stochastic gradients in SGD. The basic claim is that the distribution, when modeled as a von Mises-Fisher distribution, becomes more uniform as training progresses. There is experimental verification of this claim, and some results suggesting that the SNR is more correlated with their measure of uniformity than with the norm of the gradients.

Quality: The proofs appear correct to me. 

Clarity: The paper is generally easy to read.

Originality &amp; Significance: I don't know of this specific analysis existing in the literature, so in that sense it may be original. Nonetheless, I think there are serious issues with the significance. The idea that there are two phases of optimization is not particularly new (see for example Bertsekas 2015) and the paper's claim that uniformity of direction increases as SGD convergence is easy to see in a simple example. Consider f_i(x) = |x-b_i|^2  quadratics with different centers. Clearly the minimum will be the centroid. Outside of a ball of certain radius from the centroid all of the gradients grad f_i point in the same direction, closer to the minimum they will point towards their respective centers. It is pretty clear, then that uniformity goes up as convergence proceeds, depending on the arrangement of the centers.

The analysis in the paper is clearly more general and meaningful than the toy example, but I am not seeing what the take-home is other than the insight generated by the toy example. The paper would be improved by clarifying how this analysis provides additional insight, providing more analysis on the norm SNR vs uniformity experiment at the end. 

Pros:
- SGD is a central algorithm and further analysis laying out its properties is important
- Thorough experiments.

Cons:
- It is not entirely clear what the contribution is.

Specific comments:
- The comment at the top of page 4 about the convergence of the minibatch gradients is a bit strange. This could also be seen as the reason that analysis of the convergence of SGD rely on annealed step sizes. Without annealing step-sizes, it's fairly clear that SGD will converge to a kind of stochastic process.

- The paper would be stronger if the authors try to turn this insight into something actionable, either by providing a theoretical result that gives guidance or some practical algorithmic suggestions that exploit it.

Dimitri P. Bertsekas. Incremental Gradient, Subgradient, and Proximal Methods for Convex Optimization: A Survey. ArXiv 2015.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SylwdgtPpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkeT8iR9Y7&amp;noteId=SylwdgtPpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper216 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper216 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to clarify that we are not claiming to have discovered two-phase dynamics of SGD. As you have correctly pointed out, this behaviour has been known, and even more recently there have been a number of work analyzing this behaviour in deep learning. Li &amp; Yuan (2017) investigated this behaviour by considering a shallow neural network with residual connections and assuming the standard normal input distribution and showed that SGD-based learning has two phases; (1) search and (2) convergence phases. Shwartz-Ziv &amp; Tishby (2017) on the other hand investigated a deep neural network with tanh activation functions and showed that SGD-based learning has (1) drift (empirical error minimization, ERM) and (2) diffusion (representation compression) phases. Chee &amp; Toulis (2018) instead looked at the inner product between successive minibatch gradients and presented transient and stationary phases. Our work investigates the dynamics of SGD-based learning in a perspective different from all these recent works by focusing more on geometry and directional analysis of minibatch gradients. In other words, our work provides yet another perspective on understanding the dynamics of SGD-based learning which is at the core of the recent success of deep learning.

Furthermore, we would like to emphasize that our experiments, unlike most of the previous work, are conducted not only with neural networks that confirm well with theoretical assumptions but also with widely used neural networks (including latest techniques such as deep convolutional networks (Krizhevsky et al., 2015), residual networks (He et al., 2016) and batch normalization (Ioffe &amp; Szegedy, 2015). These experiments revealed that our theoretical analysis applies well when all the latest techniques are used or/and when neural networks are shallow, providing some insight into SGD-based learning of generic neural networks.

“The paper would be stronger if the authors try to turn this insight into something actionable”

We strongly agree with you that it is desirable to find a practical algorithm based on the theoretical analysis. We however believe this is out of this paper’s scope, and we leave it as future research.

- Li &amp; Yuan (2017) Convergence analysis of two-layer neural networks with ReLU activation
- Shwartz-Ziv &amp; Tishby (2017) Opening the black box of deep neural networks via information
- Chee &amp; Toulis (2018) Convergence diagnostics for stochastic gradient descent with constant learning rate.
- Krizhevsky et al., (2015) ImageNet classification with deep convolutional neural networks.
- He et al., (2016) Deep residual learning for image recognition
- Ioffe &amp; Szegedy (2016) Batch normalization: Accelerating deep network training by reducing internal covariate shift.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>