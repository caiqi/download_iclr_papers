<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Large Scale Graph Learning From Smooth Signals | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Large Scale Graph Learning From Smooth Signals" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ryGkSo0qYm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Large Scale Graph Learning From Smooth Signals" />
      <meta name="og:description" content="Graphs are a prevalent tool in data science, as they model the inherent structure of the data. Typically they are constructed either by connecting nearest samples, or by learning them from data..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ryGkSo0qYm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Large Scale Graph Learning From Smooth Signals</a> <a class="note_content_pdf" href="/pdf?id=ryGkSo0qYm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019large,    &#10;title={Large Scale Graph Learning From Smooth Signals},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=ryGkSo0qYm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Graphs are a prevalent tool in data science, as they model the inherent structure of the data. Typically they are constructed either by connecting nearest samples, or by learning them from data, solving an optimization problem. While graph learning does achieve a better quality, it also comes with a higher computational cost. In particular, the current state-of-the-art model cost is O(n^2) for n samples.
In this paper, we show how to scale it, obtaining an approximation with leading cost of O(n log(n)), with quality that approaches the exact graph learning model. Our algorithm uses known approximate nearest neighbor techniques to reduce the number of variables, and automatically selects the correct parameters of the model, requiring a single intuitive input: the desired edge density.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Graph learning, Graph signal processing, Network inference</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rJgXinpl0m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The reference of the spherical data we mentioned</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGkSo0qYm&amp;noteId=rJgXinpl0m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper51 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper51 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">This is the reference to the paper [Perraudin etal 2018] where the spherical data were used for deep learning:

Nathanaël Perraudin, Michaël Defferrard, Tomasz Kacprzak, Raphael Sgier
DeepSphere: Efficient spherical Convolutional Neural Network with HEALPix sampling for cosmological applications

Link: <a href="https://arxiv.org/abs/1810.12186" target="_blank" rel="nofollow">https://arxiv.org/abs/1810.12186</a></span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rklxB0GinX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Neat contribution</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGkSo0qYm&amp;noteId=rklxB0GinX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper51 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper51 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Learning graphs from data fine tunes standard similarity graph constructions such as k-nearest neighbor graphs.  
There has been a line of research works that focuses on learning graphs and that shows that this results in superior
results in various machine learning tasks.  The current state-of-the-art method is the method proposed by Kalofolias, 
which however is slow.   The authors suggest a method to avoid searching for the parameters that achieve a desired
level of sparsity by providing closed a formula. The parameter that determines the sparsity is theta, see proposition 1 on page 4. This was originally shown by Kalofolias. To achieve their goal, the authors first consider the degree of any given node by looking at equation (8), page 4. They prove theorem 1, that is intuitive and  provides the form of the optimal 
weights that connect this node to the rest of the nodes in the graph.  The proof is based on applying the KKT conditions on
the objective (8), with the single constraint that there are no negative weights.  Finally, since we care about the 
sparsity of the graph as a whole, the authors use the average of the parameter theta over all nodes. The authors perform 
experiments on real-world graphs, and show basic properties of their method, as well as the main source of mistakes ,i.e., disconnected nodes, figure 5.

Essentially, this paper starts from the work of Kalofolias  and improves it significantly. This by itself is 
a neat contribution, but the authors could improve their paper by showing a more complete view  of graph 
learning methods, with respect to the quality of the produced graphs and the scalability. I find this aspect of the paper narrowing its contribution, hence my evaluation. Some remarks follow.

- A different family of graph learning methods is based on the objective ||LX||_F^2 or equivalently tr(X^TLLX). 
For this objective, Daitch et al. proved certain neat properties, such as the existence of a sparse optimal graph. 
This allows Daitch et al. to solve the primal dual significantly faster than O(n^2) since by their theorem, 
O(nd) edges are required where d is the dimension of the data points. When d is large, a random projection can be applied. 
The paper should compare with this family of methods that are more scalable both with respect to the accuracy, 
and to the runtimes. 

- While the proposed method scales significantly better than Kalofolias, the datasets used are small. 

- Using LSH for k-nn graphs results in a  scalable, practical way to construct similarity graphs. The authors should cite
the following related work, and compare with such methods.
“Efficient K-Nearest Neighbor Graph Construction for Generic Similarity Measures“ by Dong, Charikar, Li. 

- An interesting experiment would be to inject outliers in the dataset, or use some dataset with outliers. 
Would this affect the tightness of the interval in equation (17)? </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HyezC8TlC7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Answer to reviewer (Part 2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGkSo0qYm&amp;noteId=HyezC8TlC7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper51 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper51 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">About the dataset sizes:
---------------------------------
When comparing graph quality on real data, we usually face the challenge of not having in hand an actual ground truth graph. To have concrete quantitative comparisons, we chose the MNIST experiment as we could use classification error of label propagation as a proxy for graph quality. Note that for 60,000 nodes, saving a full adjacency matrix would need 28.8 GB of storage (for 64-bit floats, square form of W), therefore all non scalable algorithms (including Dong etal, Kalofolias, and Daitch) would run into problems.

To add comparisons for larger data, as you suggested, we learned a large scale graph where we know the underlying node structure. We used 1000 signals from data from [Perraudin etal. 2018]. It consists of simulated cosmological mass maps, i.e. the amount of mass in the universe observed from earth in every direction. Hence  this data resides on the sphere, which can be considered as its underlying manifold. For this experiment, we learned a graph for a subpart of the sphere, i.e between 262,000 nodes (512x512 grid). For this new graph, we first see how well the theta bounds approximate the final graph sparsity we obtain. We then compute the first few eigenvectors of the Laplacian and see that the two first non trivial eigenvectors give an almost square 2-D grid embedding (like Laplacian eigenmaps), even if there was no information of x,y,z coordinates in our original data. We will add plots in the appendix (due to space constraints) showing the 2-D embedding we obtain. We note that the visual quality of the obtained embedding is significantly worse using A-NN and slightly worse using l2.

In terms of time complexity, as we mentioned in Section 5.5, our algorithm can learn in reasonable time a graph of 1 million nodes on a desktop running Matlab. We extended slightly this experiment by adding a scalability figure in the appendix. While this is a proof of scalability, for really large data one will have to consider implementation details like memory management, and using a faster programming language (like C++). 


Locality Sensitive Hashing:
------------------------------------

We are happy to cite LSH among other possibilities for finding A-NN. While a different A-NN technique might provide a better initial support and hence overall results, we believe that providing that A-NN is above some quality threshold, the final quality of the results will not be affected so much. Indeed the task of the optimization problem is to select which edges should be kept from the initial support (that is much larger than the final graph). So the optimization should compensate for at least some of the A-NN errors.
Take for example Figure 4. To obtain the L2 and Log graphs of degree k=10, we started from an A-NN graph of degree k=30 (yellow line, up right) and set to zero many erroneous edges by learning. 

While studying the effect of different A-NNs to the final quality of the graph is interesting, this contribution focuses on scaling the problem of graph learning. Doing a fair and complete comparison with more A-NN models would be a long publication itself, and could make our submission lose its focus.


Tightness of theta intervals with outliers in data:
---------------------------------------------------------------
The theta intervals of equation (17) are indeed robust to outliers. We run experiments adding outliers (10% images contaminated with large amounts of Gaussian noise) in the MNIST dataset, and plotted the theoretical versus obtained sparsity of graphs versus the choice of parameter theta. The result is almost identical with the one of Figure 2b. The explanation is that the theta intervals of eq. (17) are really dependent on the smallest distances in $Z$, rather than the largest ones: $Z^\hat$ is sorted, and B as well. Adding outliers induces additional nodes distances larger than usually, that never make it in the equation of 17, except for the few outlier nodes.

To complete the experiment, we also tried adding 10% duplicate images instead of outliers. In that case, we have 10% pairs of images that have essentially zero distances with each other. We thought this could affect the intervals much more, but we were wrong: the theta intervals are again very robust to this duplicates in the data: while the values of theta proposed are an order of magnitude larger in the case of duplicates, these values obtain graphs with degree within distance 1 from the desired one, as is the case for the outliers data, and the original MNIST data. We have added this new experiment in the appendix, but could also try to fit it in the main paper material. 



</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJxmCraxAX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Answer to reviewer (Part 1)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGkSo0qYm&amp;noteId=SJxmCraxAX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper51 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper51 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear reviewer,
We really appreciate your thorough review and the positive comments, as well as your propositions for improvements. We tried to address the points that you made in your review, and we believe that the paper is, as you suggested, stronger with these additions.

||LX||_F^2:
----------------
The models of Daitch etal are indeed very important ones in the graph learning literature. Following your comments, we tried to compare both in accuracy and scalability. Note that there are actually two models in their paper, that they call “hard” and “soft” graphs. Hard graphs have the hard constraint that each node has degree at least 1. Soft graphs allow for degrees lower than 1, but penalize them quadratically.

In their original paper, Daitch etal solve their optimization problem using SDPT3, which has a complexity significantly higher than O(v^2) for v variables. So even when v=o(n), the complexity is not better than O(n^2). Furthermore, in order to solve their problem in a more scalable way, Daitch etal restrict the support of the graph to a subset of edges. Then, by checking the KKT conditions on the dual variable, they can assess if the some edges should be added to the optimization scheme. If so, the optimization is run again. This is actually a very nice idea and it allows for solving larger problems, (particularly if SDPT3 is used). However, the search of the next relevant support costs O(n^2) again, since they have to compute the KKT conditions for all possible edges.

We started by implementing the original hard and soft Daitch algorithm and obtained similar speed as reported in the original paper. Unfortunately, we were not able to run the algorithms for more than a few thousands of nodes. Hence, in order to cope with their scalability issues and provide some comparison with our models, we have derived a “more” scalable variant of Daitch algorithm. Essentially, we did two modifications. First, we removed the support optimization using the KKT conditions and use the same support as our algorithm.  Second, we used FISTA and primal dual optimization schemes respectively for the soft and hard graph optimization instead of SDPT3. This time, our implementation scaled to the order of a 100’000 nodes for a powerful desktop computer with 64Gb of RAM. The running times of optimization are still significantly higher than our models, because the term ||LX||_F^2 =||M w||_2^2 takes p times more computation than tr(X^T L X) = ||Z w||_1, where p is the number of signals.

We can add the resulting time in Figure 1 of our paper: within 30 seconds (the new y limit of the plot), we were only able to learn a hard graph of 250 nodes using quadratic programming, and a soft graph of 2000 nodes when proximal splitting method were used. When the KKT conditions trick is not used, our version of Daitch algorithm were much faster but still significantly slower than our algorithm because of the dependencies on p. Note that we also used random projections to reduce the dimension from 300 to 20 only for the Daitch algorithms, while ours was running on the full set of 300 features

In terms of quality, in our paper we focus on scalable algorithms (A-NN, scaled L2-degrees, scaled Log-degrees). Figures 4 and 5 of our paper show this comparison, that now we also run for the scaled version of the Daitch-soft model. In our experiments, we see that the Daitch model has many wrong edges in Figure 4, while in Figure 5 it performs slightly better than the L2 scaled model for specific edge densities, but always worse than the A-NN and the scaled Log-degrees model. Also, we see that Daitch suffers the same problem as the scaled L2 model: it has many disconnected nodes (less than L2, more than Log). This is expected, as the soft constraint is a quadratic one similar to the one of the L2 model, that allows node degrees to be zero.

We believe that one major issue of the Daitch hard algorithm is that it does not provide a way to control sparsity. Hence we varied the size of the support to get different graphs. As for the soft algorithm, the regularization parameter controls the strength of the constraint, but it is very difficult to obtain arbitrary sparsity levels outside a small interval. 

[Continues in Part 2]</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HkgeCuI5nQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>marginally bellow threshold</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGkSo0qYm&amp;noteId=HkgeCuI5nQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper51 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper51 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a scalable approximate calculation of graph construction. Based on the sparse optimization formulation of a graph construction, the authors provide a way to select parameter automatically based on user desired connectivity of graph.

The problem setting, graph construction, is significant for the wide range of ML community. Overall, however, advantage/novelty of the proposed method is unclear for me.

Scalability is main advantage of the proposed method, but the authors just employed known nearest neighbor approximation methods, and thus here no technical novelty is shown.

I couldn't find connection between Section 3 and 4, these seem to be an independent topics. Main claim of the paper would be in Section 3, but the novelty would be weak as mentioned above.

Solving reverse problem is interesting, but it just provide the parameter value range which results in given sparsity level k. This doesn't provide exact value of \theta (and user still have to specify k), and selection would be possible easily without the analytical formula (e.g., by following the regularization path)

Performance verification is not convincing. Showing accuracy gain for more wide variety of datasets would be convincing.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkxn9caeRQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Answer to reviewer (Part 2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGkSo0qYm&amp;noteId=Bkxn9caeRQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper51 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper51 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Performance verification for more datasets:
---------------------------------------------------------
As we wrote in the answer to reviewer 1, it is difficult to assess the quality of graphs when the ground truth is not really known. To assess quality, we tried to use a wide variety of both datasets and proxies for graph quality. We showed the distribution quality of edges for MNIST (Figure 3), the edge accuracy for MNIST (Figure 4) and the classification error of label propagation on MNIST (Figure 5). In the latter we also compared the number of disconnected nodes. We measured the diameter of the graph between word2vec representations (Figure 6) that is best for our large scale graph, and showed qualitatively the effects of the graph in Figure 7. 

Following your comment regarding performance verification, we added a further experiment, for larger data this time (262,000 nodes). We used our learned graph for embedding all nodes on a 2-D plane. The signals were known to reside on a small part of the surface of the sphere, which is a 2-dimensional manifold. Without giving any notion of coordinates, but only smooth signals of the sphere, we were able to recover a very good 2-D embedding by using the first two non-zero eigenvectors of our learned sparse Laplacian. This is an important result, since we used no coordinate information whatsoever, only the similarity between different nodes, and the structure was very well recovered. 

In this experiment it is clear that the large scale Log model works best. The large scale L-2 graph is able to recover a meaningful 2D manifold only if we remove the disconnected nodes, and in that case, it gives erroneous results in the middle of the manifold. The A-NN has no disconnected nodes, but gives an embedding that is far from 2D, as many of the edges are erroneous. 

In their work [Perraudin etal 2018] used in a weighted 8 exact K-NN graph for their experiments (Figure B.13 in their paper). Knowing this graph, we can see we can compare different algorithms with respect to the f1 measure. Again, we observed that the learned graphs perform significantly better than A-NN.

We are adding the plots of the different embeddings, as well as the f1 scores in the body or in the appendix of our paper depending on the available space. Furthermore, we are adding a plot of expected versus obtained degrees of the graph using our theta approximations.

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1ek1FaxRX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Answer to reviewer (Part 1)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGkSo0qYm&amp;noteId=H1ek1FaxRX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper51 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper51 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear reviewer,
We would like to first of all thank you for your review. We completely agree that graph construction is a significant problem for the wide range of ML community. From your comments we understand that we have to make more obvious for the reader the novelty of our paper and its importance for the ML community.

Novelty/contribution:
--------------------------
Until now, there was no real graph learning algorithm in the literature that actually scales to problems larger than a few thousands of nodes. The previous state of the art algorithms, but also classic algorithms like the ones of Daitch etal (see comments to Reviewer 1) cannot scale due to their computational complexity that is in the order at least O(n^2). For our experiments with the MNIST dataset (60,000 nodes) none of them could be computed. In the new version of the paper, we were able to compare with Daitch only because we modified significantly their algorithms. 

Alternative to graph learning, people could resort to approximate nearest neighbor (A-NN) algorithms, that do not weight the edges, but only return a binary approximate adjacency matrix. These algorithms are more practical for large scale problems, but suffer in quality of the edges.

Our solution is the first actual graph learning solution (based on tr(X^TLX)), that thanks to A-NN scales with O(dn logn) for n nodes and d features. Due to this complexity it can scale to learn graphs of 1 million nodes in reasonable time on a laptop running Matlab. To the best of our knowledge, there is no other graph learning algorithm that could scale to this size of graphs. 

To achieve this scalability, our contribution is twofold (first, Section 3 and second, Section 4). 

First, in Section 3, we show how the optimization problem of the previous state of the art can be reduced if we know a reduced support of edges. While this is small part of our contribution, we show the details of the optimization, analyze the new computational complexity, and provide experiments to show the quality of the approximate graphs in realistic scenarios one would come across in ML. Furthermore, we will provide online our Matlab code so that the ML community can learn large scale graphs for their own data.

Secondly, in Section 4, we reduce a big drawback of the previous state of the art model. The latter suffers from the problem of how to set the two parameters $\alpha$ and $\beta$. This is a real problem when we don't even know the order of magnitude of the parameters, and the only solution seems to be trial and error, a.k.a. grid search over both parameters, in a logarithmic scale. In real ML scenarios, we want to be able to set the density of the graph, for example to 5 or 10 nearest neighbors in average. Having to try 25 or 100 different settings (5 or 10 for $\alpha$ times 5 or 10 for $\beta$ for [Kalofolias 2016]) would be a prohibitive factor for large scale problems. 

Even when these two parameters are reduced to the more intuitive ones $\theta, \delta$ (Proposition 1 in our paper), there was no way to know the order of magnitude of $\theta$ for controlling sparsity. In our experiments we had to use values of theta in the order of magnitude of $1e-6$ (Figure 2), but also of $1e2$ (Figure 10). As you propose, following the regularization path was the only way available until now, but this needs to run the algorithm many more times (like [Kalofolias, 2016] did in his paper). Hence our contribution is to propose a natural way to set theta in order to control the sparsity. We basically propose a method to link $theta$ and $k$, the number of desired edges per node.  While one may argue that $k$ still needs to be tuned, this parameter can be interpreted and is way simpler to set based on data assumptions than is impossible with $\theta$. 

Parameter value range:
-------------------------------
As you say, we give a range of parameters that shall give approximately a requested graph sparsity. In our experiments, we always use the geometric mean between the upper and lower bounds for each given k. This is equivalent to using the arithmetic mean in the log scale, that represents the order of magnitude of $\theta$, and is what we are plotting in Figure 2 and Figure 10 (logarithmic $\theta$ scale).

We are adding this information in the main text, as you pointed out it is important to make it clear, and we thank you for this comment.

[Continues in Part 2]</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BkgZllZ52X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good paper, very well written</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGkSo0qYm&amp;noteId=BkgZllZ52X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper51 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper51 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposed an approximation technique to learn the large-scale graph with the desired edge density. It was well-written and contains thorough experimental results and analysis.

A minor drawback is that while this work was motivated by the use of k-NN graph in graph convolution network (GCN), there was no evidence on how well A-NN performs in compare to k-NN with GCN.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkeZBo6g0m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Answer to reviewer</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGkSo0qYm&amp;noteId=SkeZBo6g0m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper51 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper51 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear reviewer,
Thank you for your review and your positive comments. 

Comparing the performance of GCN using different graphs (k-nn, A-NN, learned) is definitely an interesting and relevant topic. While going deep to GCNs is out of the scope of this paper, we took a step in this direction by learning a graph that could be used for deep learning. 

In the new version of the paper we are adding a new experiment, where we work with the data used by [Perraudin etal 2018] with the DeepSphere GCN. Because the graph used in DeepSphere is constructed using empirical rules and is not a real ground truth, one could ask themselves, if there exists a better way to construct it. The graph we learned using our algorithm would be a candidate. We show in our experiments that the actual graph that the authors used for deep learning (that they constructed knowing the coordinates) is close to the one we learned from smooth signals, without having any information of coordinates. Furthermore, the sphere is a 2D manifold, and our graph has properties similar to a 2D manifold graph.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>