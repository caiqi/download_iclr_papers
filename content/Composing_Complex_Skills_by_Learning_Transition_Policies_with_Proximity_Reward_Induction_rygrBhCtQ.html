<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Composing Complex Skills by Learning Transition Policies with Proximity Reward Induction | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Composing Complex Skills by Learning Transition Policies with Proximity Reward Induction" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rygrBhC5tQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Composing Complex Skills by Learning Transition Policies with..." />
      <meta name="og:description" content="Intelligent creatures acquire complex skills by exploiting previously learned skills and learning to transition between them. To empower machines with this ability, we propose transition policies..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rygrBhC5tQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Composing Complex Skills by Learning Transition Policies with Proximity Reward Induction</a> <a class="note_content_pdf" href="/pdf?id=rygrBhC5tQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019composing,    &#10;title={Composing Complex Skills by Learning Transition Policies with Proximity Reward Induction},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rygrBhC5tQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Intelligent creatures acquire complex skills by exploiting previously learned skills and learning to transition between them. To empower machines with this ability, we propose transition policies which effectively connect primitive skills to perform sequential tasks without handcrafted rewards. To effectively train our transition policies, we introduce proximity predictors which induce rewards gauging proximity to suitable initial states for the next skill. The proposed method is evaluated on a diverse set of experiments for continuous control in both bi-pedal locomotion and robotic arm manipulation tasks in MuJoCo. We demonstrate that transition policies enable us to effectively learn complex tasks and the induced proximity reward computed using the initiation predictor improves training efficiency. Videos of policies learned by our algorithm and baselines can be found at <a href="https://sites.google.com/view/transitions-iclr2019." target="_blank" rel="nofollow">https://sites.google.com/view/transitions-iclr2019.</a></span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">reinforcement learning, hierarchical reinforcement learning, continuous control, modular network</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Transition policies enable agents to execute learned skills smoothly to perform complex tasks.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rygq35VZ0X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>reviews &amp; responses are appreciated;  remaining consideration of author responses?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygrBhC5tQ&amp;noteId=rygq35VZ0X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1537 Area Chair1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1537 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The reviews and author responses are appreciated.
If there are any further comments from the reviewers with regard to the authors responses, or changes in score,
now would be the time to put these forward.

thanks again for your insights.
-- area chair</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BkghwsHAnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a> Useful  learning scheme for transitioning between options in continuous domains.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygrBhC5tQ&amp;noteId=BkghwsHAnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1537 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1537 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a scheme for transitioning to favorable starting states for executing given options in continuous domains. Two learning processes are carried out simultaneously: one learns a proximity function to favorable states from previous trajectories and executions of the option,  and the other learns the transition policies based on dense reward provided by the proximity function.
	
Both parts of the learning algorithms are pretty straightforward, but their combination turns out to be quite elegant. The experiments suggest that the scheme works,  and in particular does not get stuck in local minima. 

The experiments involve fairly realistic robotic applications with complex options,  which renders credibility to the results.    

Overall this is a nice contribution to the options literature. The scheme itself is quite simple and straightforward, but still useful. 

One point that I would like to see elaborated is the choice of exponential ("discounted") proximity function. Wouldn't a linear function of "step" be 
 more natural here? The exponent loses sensitivity as the number of steps away increases, which may lead to sparser rewards.
  
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1lb_e98Tm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygrBhC5tQ&amp;noteId=r1lb_e98Tm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1537 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018 (modified: 13 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1537 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the feedback and address the concerns in detail below.

&gt; Reviewer 3 (R3): “... the choice of exponential (“discounted”) proximity function. Wouldn’t a linear function of “step” be more natural here?”

The proximity predictor is used to reward the ending state of a transition trajectory in how close it is to the initiation set of the next primitive as well as actions that increase proximity. As R3 suggested, both linear and exponential functions are valid choices for a proximity function. 

We have experimentally compared the linear and exponential proximity functions. Our model is able to learn well with both functions and they perform similarly. We added the result to our website (Ablation study on Proximity functions: <a href="https://sites.google.com/view/transitions-iclr2019#h.p_qGO2W2Dk2q8G" target="_blank" rel="nofollow">https://sites.google.com/view/transitions-iclr2019#h.p_qGO2W2Dk2q8G</a> ) and will add to the supplementary.

Originally, we opted for the exponential proximity function with the intuition that the faster initial decay near the initiation set would help the policy discriminate successful states from failing states near the initiation set. Also, in our experiments, as we use 0.95 as a decaying factor, the proximity is still reasonably large (e.g., 0.35 for 20 time-steps and 0.07 for 50 time-steps).
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Bye6FAVCnQ" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygrBhC5tQ&amp;noteId=Bye6FAVCnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper1537 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Ske4BidpnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An elegant method with comprehensive evaluations</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygrBhC5tQ&amp;noteId=Ske4BidpnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1537 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1537 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper presents a method for learning policies for transitioning from one task to another with the goal of completing complex tasks. In the heart of the method is state proximity estimator, which measures the distance between states in the originator and destination tasks. This estimator is used in the reward for the transition policy. The method is evaluated on number of MojoCo tasks, including locomotion and manipulation.

Strengths:
+ Well motivated and relevant topic. One of the big downsides in the current state of the art is lack of understanding how to learn complex tasks. This papers tackles that problem.
+ The paper is well written and the presentation is clear.
+ The method is simple, yet original. Overall, an elegant approach that appears to be working well.
+ Comprehensive evaluations over several tasks and several baselines.

Questions:
- In the metapolicy, what ensures consistency, i.e. it selects the same policy in the consecutive steps?
- Can the authors comment on the weaknesses and the limits of the method?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">9: Top 15% of accepted papers, strong accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyxGmZcUTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygrBhC5tQ&amp;noteId=SyxGmZcUTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1537 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1537 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the feedback and address the concerns in detail below.

&gt; Reviewer 1 (R1): “In the metapolicy, what ensures consistency, … ?”

Our meta-policy executes a primitive policy and waits for a termination signal from the primitive policy before choosing the subsequent one. In other words, a termination signal (success/failure of the primitive policy) comes from the primitive policy, i.e. the walker falls down or the arm picks up a box. This call-and-return style [1-3] of execution ensures the same policy is utilized in consecutive steps until its completion. Hierarchical reinforcement methods have employed this call-and-return style when sub-policies are learned for well-defined sub-tasks that do not require a context switch during their execution.

&gt; R1: “... the weaknesses and the limits of the method?”

We discuss a few assumptions that we made and good follow-up directions below. We will also add the discussion to the revised version.

Our model-free transition policies rely on random exploration. Specifically, we made an assumption that successful transition trajectories between two consecutive policies should be achievable by random exploration (i.e. an initiation set of a primitive policy should be reachable from the ending states of the previous policies). As soon as a transition policy succeeds once, the proximity predictor will learn what good states are and subsequently the transition policy will succeed more frequently. To alleviate the exploration problem with sparse rewards, our transition policy training can incorporate exploration methods that utilize count-based exploration bonuses [4-6], curiosity-driven intrinsic rewards [7-10], etc.

Our current framework is designed to focus on acquiring transition policies that can connect a given set of primitive policies. We believe that additionally enabling an agent to adaptively augment its primitive set [11-12] based on a new environment or task is a promising future direction.

We assume our primitive policies return a signal that indicates whether the execution should be terminated or not, similar to [1-3, 13]. Without access to this termination signal, the transition policy would learn from very sparse and delayed reward. 


[1] Oh et al. “Zero-shot task generalization with multi-task deep reinforcement learning”, ICML 2017
[2] Andreas et al. “Modular multitask reinforcement learning with policy sketches”, ICML 2017
[3] Kulkarni et al. “Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation”, NIPS 2016
[4] Strehl Littman “An analysis of model-based interval estimation for markov decision processes”, Journal of Computer and System Sciences (JCSS) 2008
[5] Bellemare et al “Unifying Count-Based Exploration and Intrinsic Motivation”, NIPS  2016 
[6] Martin et al. “Count-Based Exploration in Feature Space for Reinforcement Learning”, IJCAI 2017
[7] Schmidhuber “A possibility for implementing curiosity and boredom in model-building neural controllers”, From animals to animats: Proceedings of the first international conference on simulation of adaptive behavior, 1991
[8] Pathak et al. “Curiosity-driven Exploration by Self-supervised Prediction”, ICML 2017
[9] Achiam and Sastry “Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning”, NIPS Workshop 2016
[10] Stadie et al. “Incentivizing exploration in reinforcement learning with deep predictive models”, NIPS Workshop 2015
[11] Hausman et al. “Learning an Embedding Space for Transferable Robot Skills”, ICLR 2018
[12] Gudimella et al. “Deep reinforcement learning for dexterous manipulation with concept networks”, arXiv 2017
[13] Le et al. “Hierarchical Imitation and Reinforcement Learning”, ICML 2018
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SJltXWIi3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Potentially very useful idea</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygrBhC5tQ&amp;noteId=SJltXWIi3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1537 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1537 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">** Summary **
The authors propose a new training scheme with a learned auxiliary reward function to optimise transition policies, i.e. policies that connect the ending state of a previous macro action/option with good initiation states of the following macro action/option.

** Quality &amp; Clarity **
The paper is well written and features an extensive set of experiments.

** Originality **
I am not aware of similar work and believe the idea is novel.

** Significance **
Several recent papers have proposed to approach the topic of learning hierarchical policies not by training the hierarchy end-to-end, but by first learning useful individual behavioural patterns (e.g. skills) which then later can be used and sequentially chained together by higher-level policies. I believe the here presented work can be quite helpful to do so as the individual skills are not optimised for smooth composition and are therefore likely to fail when naively used sequentially.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkgfie58TX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygrBhC5tQ&amp;noteId=Bkgfie58TX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1537 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1537 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the feedback. We are glad that the reviewer found the idea novel and useful for enabling the smooth composition of skills and that the reviewer recognized the importance of utilizing previously learned skills to compose complex skills.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>