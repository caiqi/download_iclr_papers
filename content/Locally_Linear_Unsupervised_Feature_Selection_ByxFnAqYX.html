<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Locally Linear Unsupervised Feature Selection | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Locally Linear Unsupervised Feature Selection" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ByxF-nAqYX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Locally Linear Unsupervised Feature Selection" />
      <meta name="og:description" content="The paper, interested in unsupervised feature selection, aims to retain the features best accounting for the local patterns in the data. The proposed approach, called Locally Linear Unsupervised..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ByxF-nAqYX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Locally Linear Unsupervised Feature Selection</a> <a class="note_content_pdf" href="/pdf?id=ByxF-nAqYX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019locally,    &#10;title={Locally Linear Unsupervised Feature Selection},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=ByxF-nAqYX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=ByxF-nAqYX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">The paper, interested in unsupervised feature selection, aims to retain the features best accounting for the local patterns in the data. The proposed approach, called Locally Linear Unsupervised Feature Selection, relies on a dimensionality reduction method to characterize such patterns; each feature is thereafter assessed according to its compliance w.r.t. the local patterns, taking inspiration from Locally Linear Embedding (Roweis and Saul, 2000). The experimental validation of the approach on the scikit-feature benchmark suite demonstrates its effectiveness compared to the state of the art.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Unsupervised Learning, Feature Selection, Dimension Reduction</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Unsupervised feature selection through capturing the local linear structure of the data</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Hkxcj4-lTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Locally Linear Unsupervised Feature Selection</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxF-nAqYX&amp;noteId=Hkxcj4-lTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1191 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1191 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper focuses on the problem of unsupervised feature selection, and proposes a method by exploring the locally linear embedding. Experiments are conducted to show the performance of the proposed locally linear unsupervised feature selection method. There are some concerns to be addressed.

First, the novelty and motivation of this paper is not clear. This paper seems to directly use one existing dimensionality reduction method, i.e., LLE, to explore the local structure of data. Why uses LLE rather than other methods such as LE? What are the advantages?

Second, in Section 3.3, authors state that the method might be biased due to the redundancy of the initial features. To my knowledge, there are some unsupervised feature selection to explore the redundancy of the initial features, such as the extended work of f Li et al. (2012) "Unsupervised Feature Selection via Nonnegative Spectral Analysis and Redundancy Control". 

Third, how about the computational complexity of the proposed method? It is better to analyze it theoretically and empirically.

Finally, the equation above Eq. 8 may be wrong.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkerO3_x07" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxF-nAqYX&amp;noteId=SkerO3_x07"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1191 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1191 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review and for the reference.

Q1: "This paper seems to directly use one existing dimensionality reduction method, i.e., LLE, to explore the local structure of data"

A1: Actually, LLUFS uses two dimensionality reduction approaches in complementary ways, along a 2-step process:
* An auto-encoder achieves the non-linear dimensionality reduction and constructs features, defining a compressed version Z of the initial data X;
* We now search for the subset of initial features, defining X_sub such that, if we applied LLE dimensionality reduction on Z, X_sub would be a perfect candidate (in the sense of preserving the local structure defined from W, with Z ~= WZ). 
* The gain is that the combinatorial optimization problem of finding the best subset of features of size d can be solved in a straightforward way as the score of each feature is its distorsion: ranking the initial features by increasing distorsion, the optimal set of features is the top d features.


Q2:  "Why uses LLE rather than other methods such as LE? What are the advantages?"

A2: Linear embedding can hardly be used in the first step if we want to capture non-separable patterns (e.g. XOR) in the initial representation. 
As for the second step, prior work such as [1,2,3] indicate that in order to be efficient, feature scoring must reflect data structure on a local scale. 
This observation motivates using the proposed distorsion score over global-scale methods such as PCA.

Q3: "Authors state that the method might be biased due to the redundancy of the initial features. 
To my knowledge, there are some unsupervised feature selection to explore the redundancy of the initial features, such as the extended work of Li et al. (2012) "Unsupervised Feature Selection via Nonnegative Spectral Analysis and Redundancy Control"."

A3: The authors of (Li et al., 2015) improve on NDFS [4] through an additional term on the feature importance matrix, penalizing the selection of correlated features. 
At the moment, feature redundancy is taken into account in LLUFS prior to launching the Auto-Encoder: using the feature correlation within the Auto-Encoder loss is a perspective for further work (section 3.3). 


Q4: "How about the computational complexity of the proposed method?"

A4: Theoretically: Besides the complexity of learning the Auto-Encoder, the time complexity of the prior agglomerative hierarchical clustering is O(D**2) with D the number of features (up to logarithmic terms). This complexity motivates the proposed extension (Q3). 
The time complexity of the nearest neighbor search is O(D/8 n**2) with D/8 the dimension of Z and n the number of points. 
The time complexity of computing the W matrix is  O(D/8 n k**3) with k the number of neighbors set to 6 in all problems. 
Empirically, LLUFS is slower than LAP, SPEC, MCFS and faster than NDFS. On dataset lung (203 points, 3312 features), the respective runtimes (on a single 2.67 Ghz CPU core) are :
* 0.2 seconds for LAP.
* 1.6 seconds for SPEC.
* 24.5 seconds for MCFS.
* 114.4 seconds for LLUFS (*)
* 131.0 seconds for NDFS.

(*) 24.4 seconds for the agglomerative clustering; 77.7 seconds for training the AutoEncoder; 12.3 seconds for the distorsion step.

On dataset pixraw10P (100 points, 10 000 features) :
*0.3 seconds for LAP.
*1.8 seconds for SPËC.
*258 seconds for MCFS.
*930 for LLUFS (*) 
*1646 seconds for NDFS.
(*) 614.6 seconds for the agglomerative clustering + 300 seconds for training the AutoEncoder + 15.9 seconds for the distorsion step.

Q5 "Finally, the equation above Eq. 8 may be wrong."
You are right. Thank you. We fixed the typo. 

[1] Cai et al. (2010) "Unsupervised Feature Selection for Multi-Cluster Data"
[2] Qian and Zhai (2013) "Robust Unsupervised Feature Selection"
[3] Liu et al. (2014) "Global and local structure preservation for feature selection"
[4] Li et al. (2012) "Unsupervised feature selection using non-negative spectral analysis"</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJxCdB-dnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Locally Linear Unsupervised Feature Selection</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxF-nAqYX&amp;noteId=rJxCdB-dnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1191 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1191 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this paper, the authors presented Locally Linear Unsupervised Feature Selection (LLUFS), where a dimensionality reduction is first performed to extract data patterns, which are used to evaluate compliance of features to the patterns, applying the idea of Locally Linear Embedding.

1. This work basically assumes that the dataset is (well) clustered. This might be true for most real world dataset, but I believe the degree of clustered-ness may vary by dataset. It will be nice to discuss effect of this. For example, if most data points are concentrated on a particular area not being well clustered, how much this approach get affected? If possible, it will be great to formulate it mathematically, but qualitative discussion is still useful.

2. For the dimension reduction, the authors used autoencoder neural network only. What about other techniques like PCA or SVD? Theoretical and experimental comparison should be interesting and useful.

3. This paper is well-written, clearly explaining the idea mathematically. It is also good to mention limitation and future direction of this work. It is also good to cover a corner case (XOR problem) in details.

4. Minor comments:
 - Bold face is recommended for vectors and matrices. For instance, 1 = [1, 1, ..., 1]^T, where we usually denote the left-hand 1 in bold-face.
 - It seems x_j is missing in Johnson-Lindenstrauss Lemma formula. As it is, \sum_j W_{i,j} is subject to be 1, so the formula does not make sense.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1giAnOxCQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxF-nAqYX&amp;noteId=B1giAnOxCQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1191 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1191 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review.

Q1 : "This work basically assumes that the dataset is (well) clustered. This might be true for most real world datasets, 
but I believe the degree of clustered-ness may vary by dataset. It will be nice to discuss effect of this. 
For example, if most data points are concentrated on a particular area not being well clustered, 
how much this approach get affected? If possible, it will be great to formulate it mathematically, but qualitative discussion is still useful."

A1 : Given the absence of label information, unsupervised FS algorithms rely on the assumption that there is some sort of "intrisic structure" to the data. 
Unsupervised approaches [1,2,3,4,5] assume that there are some clusters, which can be well-separated by an appropriate feature subset. 
As these clusters are defined in the initial feature space, they depend on the Euclidean distance which is arbitrarily corrupted from irrelevant features 
(except for [5] , which iteratively learns a new distance during selection).

LLUFS proposes another strategy: 
* An auto-encoder achieves the non-linear dimensionality reduction and constructs features, defining a compressed version Z of the initial data X;
* We now search for the subset of initial features, defining X_sub such that, if we applied LLE dimensionality reduction on Z, X_sub would be a perfect candidate (in the sense of preserving the local structure defined from W, with Z ~= WZ). 
* The gain is that the combinatorial optimization problem of finding the best subset of features of size d can be solved in a straightforward way as the score of each feature is its distorsion: ranking the initial features by increasing distorsion, the optimal set of features is the top d features.


Q2 : "For the dimension reduction, the authors used autoencoder neural network only. What about other techniques like PCA or SVD?"

A2 : Non separable clusters (the XOR problem) cannot be captured from a linear dimensionality reduction (PCA, SVD) method.
It is true that we could have used other non-linear dimensionality reduction methods (Isomap or MDS) to define a latent representation, instead of Auto-Encoder. However, Isomap and MDS depend on the Euclidean distance in the initial feature space, thus with same weakness as said in A1. 


"It seems x_j is missing in Johnson-Lindenstrauss Lemma formula."

You are right. Thank you. We fixed the typo. 

[1] Cai et al. (2010) "Unsupervised Feature Selection for Multi-Cluster Data",
[2] Li et al. (2012) "Unsupervised feature selection using non-negative spectral analysis"
[3] Li et al. (2014) "Clustering-guided sparse structural learning for unsupervised feature selection",
[4] Shi et al. (2014) "Robust spectral learning for unsupervised feature selection"
[5] Nie et al. (2016) "Unsupervised Feature Selection with Structured Graph Optimization"</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJgZhgKDhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The paper lacks a solid motivation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxF-nAqYX&amp;noteId=BJgZhgKDhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1191 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1191 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: The paper proposes the LLUFS method for feature selection. The idea is to first apply a dimensionality reduction method on the input data X to find a low-dimensional representation Z. Next, each point in Z is represented by a linear combination of its nearest neighbors by finding a matrix W which minimizes || Z  - WZ||. Finally, these weights are used to asses the distortion of every feature in X by considering the reconstruction loss in the original space.

Comments: There are multiple shortcomings in the motivation of the approach. First, the result of the dimensionality reduction drastically depend on the method used. It is well known that every DR method focuses on preserving certain properties of the data. For instance, PCA preserves the global structure while t-SNE works locally, maximizing the recall [1]. The choice of the DR method should justify the underlying assumption of the approach. I expect that the results of the experiments to change drastically by changing the DR method.

Second, the LLE method is based on the assumption that if the high-dimensional data is locally linear, it can be projected on a low-dimensional embedding which is also locally linear. Transitioning from a locally linear high-dimensional data to a lower dimension makes sense because there exists higher degree of freedom in the higher dimension. However, making this assumption in the opposite direction is not very intuitive. Why would the features that do not conform to the local linearity of the low-dimensional structure (which itself is obtained via a non-linear mapping) are insignificant?

Finally, there are no theoretical guarantees on the performance of the method. Is there any guarantee that, e.g. given one noisy feature in high dimension, the method will find that feature, etc.?

Minor: what is the complexity of the method compared to the competing methods? What is the runtime? Is this a practical approach on large datasets?

Overall, I do not agree with the assumptions of the paper nor convinced with the experimental study. Therefore, I vote for reject.

[1] Venna et al. "Information retrieval perspective to nonlinear dimensionality reduction for data visualization." Journal of Machine Learning Research 11, no. Feb (2010): 451-490.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HygWE6dxAm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxF-nAqYX&amp;noteId=HygWE6dxAm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1191 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1191 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review.


Q1: "First, the result of the dimensionality reduction drastically depend on the method used.
It is well known that every DR method focuses on preserving certain properties of the data.
For instance, PCA preserves the global structure while t-SNE works locally, maximizing the recall [1].
The choice of the DR method should justify the underlying assumption of the approach.
I expect that the results of the experiments to change drastically by changing the DR method."

A1: You are right, the result depends on the DR method. However:
i) a linear DR does not work (as shown on the toy XOR example)
ii) non-linear DR methods (Isomap, t-SNE, MDS, LLE) rely on the local Euclidean distance in the original space, that might be arbitrarily corrupted by random features.
iii) The non-linear DR method in LLUFS (denoising auto-encoder with D-D/2-D/4-D/8-D/4-D/2-D neural architecture) yields stable results (e.g. 98% same features are in the top 100 selected features for all datasets w.r.t. different initialisations).

Q2: "The LLE method is based on the assumption that if the high-dimensional data is locally linear,
it can be projected on a low-dimensional embedding which is also locally linear.
Transitioning from a locally linear high-dimensional data to a lower dimension makes sense because there exists higher degree of freedom
in the higher dimension. However, making this assumption in the opposite direction is not very intuitive.
Why would the features that do not conform to the local linearity of the low-dimensional structure (which itself is obtained via a non-linear mapping) are insignificant?"

A2: The approach assumes that
* X (the initial data in D dimensions) can be mapped onto Z (latent space in D/8 dimensions) with no or little loss of information;
* From Z, the idea is to find among X_sub (all datasets defined from X by selecting a subset of features) the best mapping in the LLE sense. From Z (dimension D/8) to X_sub, the decrease in dimensionality is still high (the evaluation considers the selected top-100 features, with 100 &lt;&lt; D/8 except for Madelon). 


Q3: "Finally, there are no theoretical guarantees on the performance of the method. Is there any guarantee that, e.g. given one noisy feature in high dimension, the method will find that feature, etc.?"
 
A3: You are right, there is no theoretical guarantees for LLUFS. To our best knowledge, the evaluation of all unsupervised FS methods (including ours) is based on a supervised setting.  

Q4 : "Minor: what is the complexity of the method compared to the competing methods? What is the runtime? Is this a practical approach on large datasets?"

A4 : Theoretically: Besides the complexity of learning the Auto-Encoder, the time complexity of the prior agglomerative hierarchical clustering is O(D**2) with D the number of features (up to logarithmic terms). This complexity motivates the extension proposed in the paper, to use the feature correlation within the Auto-Encoder loss to deal with redundancy (section 3.3).
The time complexity of the nearest neighbor search is O(D/8 n**2) with D/8 the dimension of Z and n the number of points. 
The time complexity of computing the W matrix is  O(D/8 n k**3) with k the number of neighbors set to 6 in all problems. 
Empirically, LLUFS is slower than LAP, SPEC, MCFS and faster than NDFS. On dataset lung (203 points, 3312 features), the respective runtimes (on a single 2.67 Ghz CPU core) are :
* 0.2 seconds for LAP.
* 1.6 seconds for SPEC.
* 24.5 seconds for MCFS.
* 114.4 seconds for LLUFS (*)
* 131.0 seconds for NDFS.
(*) 24.4 seconds for the agglomerative clustering; 77.7 seconds for training the AutoEncoder; 12.3 seconds for the distorsion step.

On dataset pixraw10P (100 points, 10 000 features) :
*0.3 seconds for LAP.
*1.8 seconds for SPËC.
*258 seconds for MCFS.
*930 for LLUFS (*) 
*1646 seconds for NDFS.
(*) 614.6 seconds for the agglomerative clustering + 300 seconds for training the AutoEncoder + 15.9 seconds for the distorsion step.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HyeU4d1UnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarifying typos</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxF-nAqYX&amp;noteId=HyeU4d1UnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1191 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1191 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We spotted three typos that could hinder the reader's comprehension :

-In the notations section , the unnormalized Laplacian should read "L = Delta - S" (instead of "L = M - S")

-At the beginning of page 3, in the formal background section, the first eigenvector should read : "Xi_0 = Delta^(1/2) 1" (instead of "Xi_0 = M^(1/2) 1 ")

-In appendix 1, the Laplacian score should read : "L_j = (1/sigma_j) * Sum_(i,k) (X[i,j] - X[k,j])S_(i,k)"

We apologize for these errors and hope those clarifications prove useful.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>