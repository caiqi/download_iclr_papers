<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Denoise while Aggregating: Collaborative Learning in Open-Domain Question Answering | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Denoise while Aggregating: Collaborative Learning in Open-Domain Question Answering" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BkVVOi0cFX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Denoise while Aggregating: Collaborative Learning in Open-Domain..." />
      <meta name="og:description" content="The open-domain question answering (OpenQA) task aims to extract answers that match specific questions from a distantly supervised corpus. Unlike supervised reading comprehension (RC) datasets..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BkVVOi0cFX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Denoise while Aggregating: Collaborative Learning in Open-Domain Question Answering</a> <a class="note_content_pdf" href="/pdf?id=BkVVOi0cFX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019denoise,    &#10;title={Denoise while Aggregating: Collaborative Learning in Open-Domain Question Answering},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BkVVOi0cFX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">The open-domain question answering (OpenQA) task aims to extract answers that match specific questions from a distantly supervised corpus. Unlike supervised reading comprehension (RC) datasets where questions are designed for particular paragraphs, background sentences in OpenQA datasets are more prone to noise. We observe that most existing OpenQA approaches are vulnerable to noise since they simply regard those sentences that contain the answer span as ground truths and ignore the plausible correlation between the sentences and the question. To address this deficiency, we introduce a unified and collaborative model that leverages alignment information from query-sentence pairs in a small-scale supervised RC dataset and aggregates relevant evidence from distantly supervised corpus to answer open-domain questions. We evaluate our model on several real-world OpenQA datasets, and experimental results show that our collaborative learning methods outperform the existing baselines significantly.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">natural language processing, open-domain question answering, semi-supervised learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose denoising strategies to leverage information from supervised RC datasets to handle the noise issue in the open-domain QA task.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BJl-Vc9Yn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Pre-training for QA helps</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkVVOi0cFX&amp;noteId=BJl-Vc9Yn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper348 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper348 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper shows that a sentence selection / evidence scoring model for QA trained on SQuAD helps for QA datasets where such explicit per-evidence annotation is not available.

Quality:
Pros: The paper is mostly well-written, and suggested models are sensible. Comparisons to the state of the art are appropriate, as is the related work description. the authors perform a sensible error analysis and ablation study. They further show that their suggested model outperforms existing models on three datasets.
Cons: The introduction and abstract over-sell the contribution of the paper. They make it sound like the authors introduce a new task and dataset for evidence scoring, but instead, they merely train on SQuAD with existing annotations. References in the method section could be added to compare how the proposed model relates to existing QA models. The multi-task "requirement" is implemented as merely a sharing of QA datasets' vocabularies, where much more involved MTL methods exist. What the authors refer to as "semi-supervised learning" is in fact transfer learning, unless I misunderstood something.

Clarity:
Apart from the slightly confusing introduction (see above), the paper is written clearly.

Originality:
Pros: The suggested model outperforms others on three QA datasets.
Cons: The way achievements are achieved is largely by using more data, making the comparison somewhat unfair. None of the suggested models are novel in themselves. The evidence scoring model is a rather straight-forward improvement that others could have come up with as well, but merely haven't tested for this particular task.

Significance:
Other researchers within the QA community might cite this paper and build on the results. The significance of this paper to a larger representation learning audience is rather small.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hyx1U1jwhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good results, but the contribution to "denoising" does not hold given the current version</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkVVOi0cFX&amp;noteId=Hyx1U1jwhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper348 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper348 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a new open-domain QA system which gives state-of-the-art performance on multiple datasets, with a large improvement on QuasarT and TriviaQA. Given the significant results, I would vote for its acceptance.

The contributions of the paper can be summarized into two parts. The first is an efficient base open-domain QA model; and the second includes several denoising methods of passage selection. I hope the authors could address the following issues during rebuttal, which I believe will make the paper stronger.

(1) The proposed base open-domain QA method (+DISTANT) itself improves a lot, which I think is the major contribution of the paper. It will be very helpful if the authors could provide ablation test to the sentence discriminator/reader modules to give better clues about why it works so well. Is it mainly because of the usage of DrQA style encoder?

(2) Although the paper has "denoising" emphasized in the title, I actually do not see this holds as a contribution. First, the proposed semantic labeler training strategies only improve significantly on TriviaQA. The improvement on Quasar-T and SearchQA is relatively marginal. I am wondering whether this is because the domain shift between SQuAD and QuasarT and SearchQA.

(3) (Cont'd from point 2) Second, the proposed SSL and CSL are not compared with any decent baselines. It will make more sense if the authors apply the Re-Ranker or S-Norm to the proposed base open-domain QA model, and compare the improvement from different methods (Re-Ranker, S-Norm, SSL, CSL). It is likely that the S-Norm could also improve on TriviaQA and the Re-Ranker could improve over Quasar-T and SearchQA. Therefore without any experimental evidence, again, the "denoising" part cannot be regarded as a contribution with positive results.

Additional question: would the authors release the code for public usage if the paper gets accepted?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1lq-2jFim" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting exploration, but weak novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkVVOi0cFX&amp;noteId=S1lq-2jFim"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper348 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">21 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper348 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper aims for open-domain question answering with distant supervision. First, the authors proposed an aggregation-based openQA model with sentence discriminator and sentence reader. Second, they use a semantic labeler to handle distant supervision problem by utilizing other span supervision tasks, and propose two different denoising methods. They run experiments on 3 open-domain QA datasets and achieve SOTA.


Strengths

1) Their semantic labeler and exploration of two different denoising methods are interesting and meaningful.
2) They conducted experiments on 3 widely-used open-domain datasets, and the performance gain is impressive.


Weakness

Although there is an impressive performance gain, the contribution of the paper seems to be marginal.
1) First of all, it is hard to say there is a contribution to the idea of sentence discriminator and sentence reader â€” people have used this framework for large-scale QA a lot. Also, the architecture of the models in this paper are almost identical to Chen et al (ACL 2017) and Lin et al (ACL 2018).
2) Thus, the contribution is more on semantic labeler and denoising method. However, this contribution is marginal as well since its role is almost the same as sentence discriminator plus pretraining methods which have widely used already.


Questions

1) What exactly is the difference between semantic labeler and sentence discriminator? For me, it seems like both of them label each sentence `yes` or `no`. My thought is sentence discriminator is only trained on the target dataset (distant supervision dataset) while semantic labeler is also trained (either jointly or separately) trained on the source dataset (span supervision dataset). (If my thought is wrong, please let me know, I would like to update my score.)
2) Chen et al (ACL 2017) have shown that pretraining QA model on span supervision dataset (SQuAD) is effective to train the model on distant supervision dataset. Similarly, Min et al (ACL 2018) have pretrained both QA model and sentence selector on SQuAD. While I think pretraining sentence selector on SQuAD is almost identical to sentence labeler with SSL method, could you give exact comparison of these different methods? For example, remove sentence labeler, and pretrain both sentence discriminator and reader on SQuAD, or jointly train them on SQuAD &amp; target dataset.


Marginal comments

1) At the beginning of Section 2.4.1, it says the semantic labeler is able to transfer knowledge from the span supervised data â€” however, the authors should be careful since people usually refers to `knowledge` as an external knowledge. This method is more like better learning of accurate sentence selection, not transferring knowledge.
2) Please mention the TriviaQA data you used is Wikipedia domain, since there are two different domains (Wikipedia and Web).
3) In References section, the conference venues in many papers are omitted.


Overall comments

The paper explored several different methods to deal with distant supervision via sentence labeling, and I really appreciate their efforts. While the result is impressive, the idea in the paper is similar to the methods that have widely used already.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>