<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Self-Aware Visual-Textual Co-Grounded Navigation Agent | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Self-Aware Visual-Textual Co-Grounded Navigation Agent" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=r1GAsjC5Fm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Self-Aware Visual-Textual Co-Grounded Navigation Agent" />
      <meta name="og:description" content="The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_r1GAsjC5Fm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Self-Aware Visual-Textual Co-Grounded Navigation Agent</a> <a class="note_content_pdf" href="/pdf?id=r1GAsjC5Fm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019self-aware,    &#10;title={Self-Aware Visual-Textual Co-Grounded Navigation Agent},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=r1GAsjC5Fm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-aware agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self- aware agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state-of-art by a significant margin (8% absolute increase in success rate on the unseen test set).</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">visual grounding, textual grounding, instruction-following, navigation agent</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a self-aware agent for the Vision-and-Language Navigation task.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">20 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SJeQkmIR3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting Approach to Route Instruction Following with Thorough Evaluation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1GAsjC5Fm&amp;noteId=SJeQkmIR3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper669 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper669 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper considers the problem of following natural language route instructions in an unknown environment given only images. Integral to the proposed ("self-aware") approach is its ability to reason over which aspects of the instruction have been completed, which are to be followed next, which direction to go in next, as well as the agents current progress. This involves two primary components of the architecture. The first is a visual-textual module that grounds to the completed instruction, the next instruction, and the next direction based upon the visual input. The second is a "progress monitor" that takes the grounded instruction as input and captures the agent's progress towards completing the instruction.


STRENGTHS

+ The paper describes an interesting approach to reasoning over which aspects of a given instruction have been correctly followed and which aspect to act on next. This takes the form of a visual-textual co-grounding model that identifies the instruction previously completed, the instruction corresponding to the next action, and the subsequent direction in which to move. The inclusion of a "progress monitor" allows the method to reason over whether the navigational progress matches the instruction.

+ The paper provides a thorough evaluation on a challenging benchmark language understanding dataset. This evaluation includes detailed comparisons to state-of-the-art baselines together with ablation studies to understand the contribution of the different components of the architecture.

+ The paper is well written and provides a thorough description of the framework with sufficient details to support replication of the results.


WEAKNESSES

- The paper would benefit from a more compelling argument for the importance of reasoning over which aspects of the instruction have been completed vs. which to act on next.

- The paper emphasizes the use of images, the visual grounding reasons over visual features.

- The paper incorrectly states that existing methods for language understanding require an explicit representation of the target. Several existing methods do not have this requirement. For example, Matuszek et al., 2012 parse free-form language into a formal logic representation for a downstream controller that interprets these instructions in unknown environments. Meanwhile, Duvallet et al., 2014 and Hemachandra et al., 2015 exploit language (together with vision and LIDAR) to learn a distribution over the unknown environment that guides grounding. Meanwhile, Mei et al., 2016 reason only over natural language text and parsed images, without knowledge of the environment or an explicit representation of the goal.

C. Matuszek, E. Herbst, L. Zettlemoyer, and D. Fox, “Learning to parse natural language commands to a robot control system,” in Proceedings of the International Symposium on Experimental Robotics (ISER), 2012.

S. Hemachandra, F. Duvallet, T. M. Howard, N. Roy, A. Stentz, and M. R. Walter, “Learning models for following natural language directions in unknown environments,” in Proc. IEEE Int’l Conf. on Robotics and Automation (ICRA), 2015

F. Duvallet, M. R. Walter, T. Howard, S. Hemachandra, J. Oh, S. Teller, N. Roy, and A. Stentz, “Inferring maps and behaviors
from natural language instructions,” in Proceedings of the International Symposium on Experimental Robotics (ISER), 2014.

- While it's not a neural approach, the work of Arkin et al., 2017 which reasons over the entire instruction history when deciding on actions (through a statistical symbol grounding formulation)⁠

J. Arkin, M. Walter, A. Boteanu, M. Napoli, H. Biggie, H. Kress-Gazit, and T. Howard. "Contextual Awareness: Understanding Monologic Natural Language Instructions for Autonomous Robots," In Proceedings of the IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), 2017

- The paper misses the large body of literature on grounded language acquisition for robotics.

QUESTIONS

* What is the effect of using positional encoding for textual grounding as opposed to standard alignment methods such as those used by Mei et al., 2016?

* Perhaps I missed it, but what happens if instructions are specified in such a way that their ordering is not consistent with the correct action ordering (e.g., with corrections interjected)?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJlcuAYc3X" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1GAsjC5Fm&amp;noteId=SJlcuAYc3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper669 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJeZ0QS9nm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good idea, unclear results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1GAsjC5Fm&amp;noteId=HJeZ0QS9nm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper669 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper669 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This submission introduces a new method for vision+language navigation which tracks progress on the instruction using a progress monitor and a visual-textual co-grounding module. The method is shown to perform well on a standard benchmark. Ablation tests indicate the importance of each component of the model. Qualitative examples show that the proposed method attends to different parts of the instruction as the agent moves. 

Here are some comments/questions:
- I like the underlying idea behind the method. The manuscript is written well for most parts.
- The qualitative examples and Figure 2 are really helpful in understanding the reasons behind the improved performance.
- There is a lot of confusion regarding the use of beam search. It's unclear from the current manuscript which results are with and without beam search. It seems like beam search was added from Ours 1 to Ours 2 in Table 2. It's not clear which rows involve beam search in Table 1. Some concerns about beam width were raised in the comments which I agree with. Please modify the submission to clearly indicate the use of beam search for each result and specify the beam width.
- The use of beam search seems unrealistic to me as I can not think of any way a navigational model using beam search can be transferred or applied to real-world. I understand that one of the baselines uses beam search, so it's fair for performance comparison purposes, but could you provide any justification of how it might be useful in real-world? If there's no reasonable justification, could you also provide all the results (along with SPL metric) without beam search, including ablation, comparing with only methods without beam search? 
- I do not understand why the OSR in the submission is 0.64 and 0.70 for Speaker-Follower and proposed method and 0.96 and 0.97 in the comments.
- It seems like the proposed method is tailored for the VLN task. In many real-world scenarios, an agent might be given an instruction which only describes the goal (such as in Chaplot et al. 2017 and Hermann et al. 2017) and not the path to the goal, could the authors provide their thoughts on whether the proposed would work well for such instructions? What would the progress monitor and textual attention distribution learn in such a scenario?

Due to confusion about results and concerns about beam search, I give a rating of 5. I am willing to increase the rating if the authors address the above concerns.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1g2c3eVn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1GAsjC5Fm&amp;noteId=S1g2c3eVn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper669 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper669 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper describes a model for vision-and-language navigation. The proposed
model adds two components to the baseline model proposed by Fried et al. (2018):

- a panoramic visual attention (referred to in this paper as "visual--textual
  co-grounding"), in which the full scene around the agent's current position is
  attended to prior to selecting a direction to follow

- an auxiliary "progress monitoring" loss which encourages the agent to to
  produce textual attentions from which the distance to the goal can be directly
  inferred

The two components combine to give state-of-the-art results on the Room2Room
dataset: small improvements over existing approaches on the "-seen" evaluation
set and larger improvements on the "-unseen" evaluation sets. These improvements
also stack with the data-augmentation approach of Fried et al.

I think this is a reasonable submission and should probably be accepted. However, I
have some concerns about presentation and a number of specific questions about
model implementation and evaluation.

PRESENTATION AND NAMING

First off: I implore the authors to find some descriptor other than "self-aware"
for the proposed model. "Self-aware" is an imprecise description of the agent in
this paper---the agent is specifically "aware" of its visual surroundings and
its distance from the goal, neither of which is meaningfully an aspect of
"self". Moreover, self-awareness means something quite different in adjacent
areas of cognitive science and philosophy; overloading the term in the specific
(and comparatively mundane) way used here creates confusion. See section 3.4 of
<a href="https://arxiv.org/abs/1807.03341" target="_blank" rel="nofollow">https://arxiv.org/abs/1807.03341</a> for broader discussion. Perhaps something
like "visual / temporal context-sensitivity" to describe what's new here? A bit
clunky, but I think it makes the contributions of this work much clearer.

As suggested in the summary above, I also think "visual--textual co-attention"
is also an unhelpfully vague description of this aspect of the contribution. The
textual attention mechanism used in this paper is the same as in all previous
work on the task. Representations of language don't even interact with the
visual attention mechanism except by way of the hidden state, and the salient
new feature of the visual attention is the fact that it considers the full
panoramic context before choosing a direction.

MODELING QUESTIONS

- p4: $y_t^{pm}$ is defined as the "normalized distance from the current
  viewpoint to the goal". Is this distance in units of length (as defined by the
  simulator) or units of time (i.e. the number of discrete "steps" needed to
  reach the goal)?

  The authors have already clarified on OpenReview that the progress monitor
  objective uses an MSE loss rather than a likelihood loss. Do I understand
  correctly that ground-truth distances are in [0, 1] but model predictions are
  in [-1, 1]? Why not use a sigmoid? Also, how does scoring beam-search
  candidates as $p_t^{pm} \times p_{k,t}$ work if $p_t^{pm}$ can flip the sign?

- The input to the progress monitor is formed by concatenating the attention
  vector $\alpha_t$ to a vector of state features, and then multiplying by a
  fixed weight matrix. How is this possible? The size of $\alpha_t$ varies
  depending on the length of the instruction sequence. Are attentions padded out
  to the length of the longest instruction in the training set? If so, how can
  the model learn when it's reached the end of a short instruction sequence?
  What would happen if the agent encountered a sequence that was too long?

EVALUATION QUESTIONS

- The progress monitor is used both as an auxiliary training objective and as a
  beam search heuristic. Is it possible to disentangle these two contributions?
  (E.g. by ignoring the scores during beam search, or by doing augmented beam
  search in a model that was trained without the auxiliary objective.)

- Not critical, but it would be nice to know if the contributions here stack
  with the pragmatic inference procedure in Fried et al.

- While, as pointed out on OpenReview, it is not required to include SPL
  evaluations, I think it would be informative to do so---the preliminary
  results with no beam search look good!

MISCELLANEOUS

p1: "without a map" If you can do beam search, you effectively have a map.

p1: "...smoothly" What does "smoothly" mean in this context?

p2: "the position of grounded instruction can follow past and future
    instructions". Is the claim here that if instructions are of the form "ACB"
    and the agent is supposed to do "ABC", that the proposed model will execute
    these instructions successfully and the baseline will not? This claim does
    not appear to be evaluated anywhere in the body of the paper.

p4: "intelligently prunes" "Intelligently" is unnecessary.

p4: "for empirical reasons" What does this mean?

p5: "Intuitively, an instruction-following agent is required..." The existence
    of non-attentive models that do reasonably well at these
    instruction-following tasks suggest that this is not actually a requirement.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hkl2ncv0jQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Confusions about Eq 6</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1GAsjC5Fm&amp;noteId=Hkl2ncv0jQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018</span><span class="item">ICLR 2019 Conference Paper669 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi interesting paper! I am trying to reproduce the result and is having problem with Eq 6. 
Specifically, the progress monitor module seems to output p_t^pm which seems to be a 1D value between -1~1 after tanh(). The target y_t^pm is also a 1D number that is between -inf ~ 1. 
In this case how do you use CrossEntropy Loss as suggested in the paper? 
Eq 6 suggested that the loss should incorporate - y_t^pm * log (p_t^pm). Well I am confused if this is still "cross entropy loss". What's more log() does not like negative values? </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJgT5ts0im" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for bringing this to our attention</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1GAsjC5Fm&amp;noteId=HJgT5ts0im"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper669 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018 (modified: 27 Oct 2018)</span><span class="item">ICLR 2019 Conference Paper669 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi,

Thank you for your interest in our paper and trying to reproduce it!

You are correct. The cross entropy loss in Eq 6 should be a Mean Squared Error loss (MSELoss). The equation should thus be changed to \sum_{t=1}^{T} (y^{pm}_{t} - p^{pm}_{t})^2.  The original cross entropy loss was a variant we experimented with predicting whether the agent is making progress or not (binary prediction). The current version with tanh() at the output of the progress monitor and trained with MSE loss gave us better performance. 

We will correct this error in the revision. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HygR_tZW2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>What about the variable length text attention weights?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1GAsjC5Fm&amp;noteId=HygR_tZW2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">27 Oct 2018 (modified: 31 Oct 2018)</span><span class="item">ICLR 2019 Conference Paper669 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks so much for the prompt reply.

One more question regarding Eq6: how do you handle the text attention weights, alpha? The supplementary material suggests that W_pm is one linear layer of shape 592 x 1. The text attention weights, however, is produced from various lengths of instructions and can have variable lengths. It seems a bit odd to simply pad zeros at the end of it to extend it to the fixed length 80. How did you handle it?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Skgq_VrU37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Zero-padding for various lengths of instructions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1GAsjC5Fm&amp;noteId=Skgq_VrU37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper669 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018</span><span class="item">ICLR 2019 Conference Paper669 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi,

Thanks for the comment. 

Zero-padding is exactly how we handled it for various lengths of instructions. We have also explored ideas like using interpolation to upsample the attention weights of short instructions to a fixed length of 80, but it produces a similar performance as without interpolation.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJee1Q403m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Training Progress Monitor</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1GAsjC5Fm&amp;noteId=BJee1Q403m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper669 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, 

Thanks a lot for your responses. My reproduction of co ground model (without progress monitor) achieved comparable performance to reported. However, training the progress monitor seems pretty hard. Mind answering a few more questions? 
- Did you include data augmentation in the reported 46% for co-ground + progress monitor?
- Any chance you could verify EQ5? 
- It seems from EQ6 that you used a training loss equal to the sum of loss on any trajectory. Did you consider using per-step loss? Did you take the average loss across batch? 
- The scale of co-ground loss and progress-monitor loss are roughly 10:1. Is this expected for training? </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_rJxkZKSCoQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Panoramic Action Space </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1GAsjC5Fm&amp;noteId=rJxkZKSCoQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018</span><span class="item">ICLR 2019 Conference Paper669 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, thanks for the good performance!

I am wondering how much the panoramic action space helps in your model. Can you report the performance without the panoramic action space? Thanks!

Best regards</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SklYDcv12X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We use the model with panoramic action space as baseline</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1GAsjC5Fm&amp;noteId=SklYDcv12X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper669 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">26 Oct 2018 (modified: 27 Oct 2018)</span><span class="item">ICLR 2019 Conference Paper669 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi,

Thank you for your interest in our work and kind words!

Our proposed method built upon the established work, and we use the panoramic action space proposed in the Speaker-Follower as the baseline (hence making the comparison to our improvements fair). We focus on highlighting the novel contributions/ideas that improve on this baseline. Removing the panoramic action space from the proposed method requires non-trivial changes to be made, including visual grounding module, action selection module, and finally the progress monitor itself. We thus encourage readers to refer to the Speaker-Follower paper for performance improvement regarding panoramic action space.  
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJx7k51tsX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Concerns on the evaluation metric and the so-called SOTA performance </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1GAsjC5Fm&amp;noteId=HJx7k51tsX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">21 Oct 2018</span><span class="item">ICLR 2019 Conference Paper669 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">This paper only reports the absolute Success Rate as the evaluation metric and hides the trajectory lengths. It is well known that the Success Rate can be generally improved by exhaustedly exploring the environment before committing to a decision. However, beam search is not appropriate for robotics, because longer trajectories have more costs (battery, wear, delays for the user, etc). 

Therefore, Success rate weighted by normalized inverse Path Length (SPL) trades-off Success Rate against Trajectory Length. SPL is defined in the paper On Evaluation of Embodied Navigation Agents (<a href="https://arxiv.org/abs/1807.06757)" target="_blank" rel="nofollow">https://arxiv.org/abs/1807.06757)</a> and introduced as one of the evaluation metrics for the VLN task.

I am not sure why the authors didn't include the trajectory lengths in the paper. But from the VLN challenge leaderboard, the SPL score of the authors' submission is only 0.02 (out of 1.00), which is severely worse than the Seq2Seq baseline (0.18). The trajectory length is 373.09 meters. It seems like the authors are gaming the Success Rate with exhaustive search. Hence, I do not think it is proper to claim that the method has achieved new SOTA performance. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Skejk716om" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Results without beam search</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1GAsjC5Fm&amp;noteId=Skejk716om"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper669 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">24 Oct 2018</span><span class="item">ICLR 2019 Conference Paper669 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi,

Thank you for raising an important discussion about metrics. 

The VLN task was recently introduced less than 1 year ago with a more than 60% success rate gap between the best-known model and human performance. Each existing work has step by step helped us to advance and reduce this gap to 33%. 

Our proposed work, when combined with beam search, is able to further close the gap to 25% measured with success rate. We focused on this metric since that was the metric used by recent state of art. Yet, there is obviously still room for improvement. Ideally, the common goal from the research community is to develop an agent that achieves a high success rate with low trajectory length. We argue that given the complexity of the VLN task which requires the agent to simultaneously achieve visual grounding, textual reasoning, temporal memorization/reasoning, and intelligently select actions to navigate, the need to relax the task along multiple directions in order to make progress is important and essential. There is no current best model for both metrics (SR and SPL), and beam search typically differentiates the two regimes.

Even from the robotics perspective, these are two important objectives that one might want to trade off (length/time and success rate) given that there is no one solution that is pareto-optimal, and beam search can be seen as an exploration mechanism which is not uncommon in robotics. Beam search, which thoroughly explores the environment, eases the burden for the agent in intelligently selecting actions given the progress made towards the goal so that the agent can solely focus on identifying the implicit target represented by navigational instruction. However, the best performing model with beam search is still more than 20% lower than the human performance. Further, note that our state of art success rate has only about 29% of the trajectory length of the Speaker-Follower model which also uses beam search.

Nonetheless, we agree that the newly introduced SPL metric is also important, though it emphasizes a different aspect of the navigation task. For future comparison, we thus submitted our proposed model without beam search to the test server. Our result achieves state of art SPL results compared to existing approaches (note that we exclude submissions after the ICLR deadline) and is shown in the table below (the leaderboard only allows one result to be shown from a team). For each metric, with or without beam search, our proposed method outperforms existing approaches by a large margin.

--------------------------------------------------------------------------------------------------------------------------------------
	  	      						               Test-Unseen
   							length↓    NE↓     SR↑     OSR↑  	SPL↑ 
--------------------------------------------------------------------------------------------------------------------------------------
									without beam search
--------------------------------------------------------------------------------------------------------------------------------------
Seq2Seq Baseline		   	8.13           7.85     0.20    0.27       0.18
Look Before You Leap	   	        9.15	          7.53     0.25    0.32       0.23    
Ours		  	 			18.04         5.67     0.48    0.59       0.35
--------------------------------------------------------------------------------------------------------------------------------------
									with beam search
--------------------------------------------------------------------------------------------------------------------------------------
Speaker-Follower		   	         1257.38    4.87     0.53    0.96   	0.01 
Ours			   			 373.09      4.48     0.61    0.97   	0.02 
--------------------------------------------------------------------------------------------------------------------------------------</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hye-BY21nX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Consider adding ablations to paper / supplementary?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1GAsjC5Fm&amp;noteId=Hye-BY21nX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">26 Oct 2018</span><span class="item">ICLR 2019 Conference Paper669 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I think the ablation with or without beam search is very valuable, please add it to paper or at least supplementary? 
Plus, ablation with (only visual grounding) (only textual grounding) (only cogrounding without progress monitor) would be very illuminating as well. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1lxWit7nX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Additional results for ablation study</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1GAsjC5Fm&amp;noteId=r1lxWit7nX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper669 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018</span><span class="item">ICLR 2019 Conference Paper669 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, 

Thank you for the suggestions on ablation study. Below are the results as requested. We replaced the soft attention on visual or textual inputs with a simple mean pooling, e.g., only visual grounding means we simply use mean-pooling on textual input, and vice versa. 


------------------------------------------------------------------------------------------------------------------------------------------------
			   Co-Grounding	  Progress    Beam 	  Validataion-Seen 	        Validation-Unseen
		    #     Visual	Textual     Monitor     Search 	  NE↓        SR↑        OSR↑ 	NE↓	         SR↑        OSR↑
------------------------------------------------------------------------------------------------------------------------------------------------
Baseline 										  4.36       0.54 	0.68	        7.22         0.27        0.39
------------------------------------------------------------------------------------------------------------------------------------------------
		    1         ✔ 								  3.94 	0.62 	0.73 	6.34 	0.40 	0.53
		    2    	             ✔						  3.60 	0.65 	0.75 	6.27 	0.43 	0.54
Ours	    3         ✔ 	     ✔						  3.65 	0.65 	0.75 	6.07 	0.42 	0.57
		    4         ✔ 	     ✔		✔ 				  3.56 	0.65 	0.75	        5.89 	0.46 	0.60
		    5         ✔  	     ✔		✔ 		  ✔ 		  3.23 	0.70 	0.78 	5.04 	0.57 	0.70
------------------------------------------------------------------------------------------------------------------------------------------------</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkgCnbU-6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Result is not clear</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1GAsjC5Fm&amp;noteId=rkgCnbU-6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper669 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1. It seems that the performance with "Textual only" is 1% beyond the Co-Grounding model. So why not use the "textual only" in the further experiments?

2. Could you give more details on the method that you submitted to the test server for the "Test-Unseen" result? Is it "Co-Grouding" or "Co-Grounding + Progress Monitor"?
If the progress monitor is included, is it still non-beam search result? Because the progress monitor is only used in beam-search according to your paper.

Please let me know if I have any misunderstanding. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HyeQF5h_2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Contradictions of statistics reported above</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1GAsjC5Fm&amp;noteId=HyeQF5h_2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018</span><span class="item">ICLR 2019 Conference Paper669 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The 5th row reported above clearly corresponds to 2nd row of Table 2 in your paper: 
1  3.65 0.65 0.75 6.07 0.42 0.57
2  3.23 0.70 0.78 5.04 0.57 0.70
3  3.04 0.71 0.78 4.62 0.58 0.68

So all the reported numbers in Table 2 use beam search. However, 1st row in Table 2 matches entirely with 3rd row reported above (has no beam search),  which is contradicting and confusing. 

In the table below, you claimed the proposed approach is better than Speaker-Follower with or without beam search. However, the following comparison was also misleading, as you used beam size 15. Why can't you adopt the same beam size and compare? On the other hand, the 61% SR is 4% boost compared to no data-augmentation counterpart in Table 1 of paper, while on validation-unseen, the gap is merely 1%, any idea why?

									with beam search
--------------------------------------------------------------------------------------------------------------------------------------
Speaker-Follower		   	         1257.38    4.87     0.53    0.96   	0.01 
Ours			   			 373.09      4.48     0.61    0.97   	0.02 </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SygyyCBKh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Only progress monitor use beam search during inference</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1GAsjC5Fm&amp;noteId=SygyyCBKh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper669 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018</span><span class="item">ICLR 2019 Conference Paper669 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, 

We are sorry if there is any confusion regarding table 2 in the paper. As stated in the Sec. 2.3 of “Progress Monitor” in the paper, during inference we use beam search with the progress monitor. Therefore, the 1st row in our proposed method with only co-grounding does not use beam search, which outperformed baseline with panoramic action space by 15% on validation-unseen SR. We hope that the ablation study table above clarifies this. 

In the comment below, we have shown that, with beam size 5, our proposed method already achieved the state-of-the-art performance. By further increasing beam size, the performance gradually increases until it saturates.

With panoramic action space, many of the beams are actually empty when using a very large beam size due to limited navigable directions per viewpoints. Thus, increasing the beam size to a larger number will not necessarily help if the competing between beams already provides good selections (we achieved this by leveraging the progress monitor). Nonetheless, we provide the result with a beam size 40 as per requested. 

----------------------------------------------------------------------------------------------------------------------------------------------------------------------
			Co-Grounding	  Progress    Data			        Beam 	  		Validation-Seen 	    Validation-Unseen
		    #  Visual	Textual     Monitor     Augmentation	Search (size) 	NE↓    SR↑    OSR↑      NE↓     SR↑     OSR↑
----------------------------------------------------------------------------------------------------------------------------------------------------------------------
Ours		   ✔  	     ✔		✔ 		  ✔ 	  		        40		  		3.13    0.70    0.77 	    4.51    0.58     0.68
----------------------------------------------------------------------------------------------------------------------------------------------------------------------
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1l-VpoQ37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Beam size</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1GAsjC5Fm&amp;noteId=B1l-VpoQ37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018</span><span class="item">ICLR 2019 Conference Paper669 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">thanks for the ablation study.

I notice that in your paper the beam size is 15, while the beam size used in the speaker-follower model is 40. Playing with the beam size can influence the performance and the trajectory lengths (shorter lengths as you said). But it might not be appropriate to claim this as the contribution of your work. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJelNhWS2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Difference in beam size and their performance</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1GAsjC5Fm&amp;noteId=BJelNhWS2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper669 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018</span><span class="item">ICLR 2019 Conference Paper669 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, 

Thanks for the opportunity to further clarify our usage of a smaller beam size (15 as opposed to 40).

We do not claim that using a smaller number of beams is one of the major contributions. It was a nice side effect that resulted from our progress monitor, where we can evaluate the partial and unfinished candidate routes during beam search. As a result, we are able to maintain a lower number of beams but still achieve state-of-the-art success rate. 

Below are the results with different beam size for reference. 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
			Co-Grounding	  Progress    Data			        Beam 	  		Validation-Seen 	                        Validation-Unseen
		    #  Visual	Textual     Monitor     Augmentation	Search (size) 	length↓   NE↓    SR↑    OSR↑ 	length↓     NE↓     SR↑     OSR↑
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
		    1         ✔  	     ✔		✔ 		  ✔ 	  		        5		  		159.19	3.03    0.71    0.79 	        168.13	   4.77     0.55     0.68
Ours	    2         ✔  	     ✔		✔ 		  ✔ 	  		        10		  		271.51	3.11    0.71    0.78 	        277.13	   4.64     0.57     0.68
		    3         ✔  	     ✔		✔ 		  ✔ 	  		        15		  		355.13	3.04    0.71    0.78 	        360.46	   4.62     0.58     0.68
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_H1eXo56ji7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A note from the challenge organizers on the SPL metric</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1GAsjC5Fm&amp;noteId=H1eXo56ji7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Peter_Anderson1" class="profile-link">Peter Anderson</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">23 Oct 2018</span><span class="item">ICLR 2019 Conference Paper669 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">ICLR reviewer guidelines state that "no paper will be considered prior work if it appeared on arxiv, or another online venue, less than 30 days prior to the ICLR deadline." The paper defining the SPL metric appeared on arXiv on 18 July. However, as an organizer of the VLN challenge and a co-author of the arXiv paper mentioned above, I would like to state for the benefit of reviewers that the SPL metric was not added to the public VLN leaderboard until September 8th (19 days before the ICLR deadline). In fairness to authors with work in progress, reviewers may wish to exclude this metric from the definition of prior work for ICLR 2019 since it was not implemented on the leaderboard 30 days prior to the deadline. Existing work on the dataset has been primarily evaluated in terms of 'Success Rate', as reported in this submission. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>