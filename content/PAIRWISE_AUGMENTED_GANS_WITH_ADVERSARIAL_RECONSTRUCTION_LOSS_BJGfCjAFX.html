<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>PAIRWISE AUGMENTED GANS WITH ADVERSARIAL RECONSTRUCTION LOSS | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="PAIRWISE AUGMENTED GANS WITH ADVERSARIAL RECONSTRUCTION LOSS" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJGfCjA5FX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="PAIRWISE AUGMENTED GANS WITH ADVERSARIAL RECONSTRUCTION LOSS" />
      <meta name="og:description" content="We propose a novel autoencoding model called Pairwise Augmented GANs. We train a generator and an encoder jointly and in an adversarial manner. The generator network learns to sample realistic..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJGfCjA5FX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>PAIRWISE AUGMENTED GANS WITH ADVERSARIAL RECONSTRUCTION LOSS</a> <a class="note_content_pdf" href="/pdf?id=BJGfCjA5FX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019pairwise,    &#10;title={PAIRWISE AUGMENTED GANS WITH ADVERSARIAL RECONSTRUCTION LOSS},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BJGfCjA5FX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We propose a novel autoencoding model called Pairwise Augmented GANs. We train a generator and an encoder jointly and in an adversarial manner. The generator network learns to sample realistic objects. In turn the encoder network at the same time in turn is trained to map the true data distribution to the prior in a latent space. To ensure good reconstructions we introduce an augmented adversarial reconstruction loss. Here we train a discriminator to distinguish two types of pairs: the object with its augmentation and the one with its reconstruction. We show that such adversarial loss compares objects based on the content rather than on the exact match. We experimentally demonstrate that our model generates samples and reconstructions of quality competitive with state-of-the-art on datasets MNIST, CIFAR10, CelebA and achieves good quantitative results on CIFAR10. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Computer vision, Deep learning, Unsupervised Learning, Generative Adversarial Networks</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a novel autoencoding model with augmented adversarial reconstruction loss. We intoduce new metric for content-based assessment of reconstructions. </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BklqJdQs3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Adding and encoder for the GANs is studied. But the distinctions from existing models are not obvious.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJGfCjA5FX&amp;noteId=BklqJdQs3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper876 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper876 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper propose a adversary method to train a bidirectional GAN with both an encoder and decoder. Comparing to the existing works, the main contribution is the introducing of an augmented reconstruction loss by training a discriminator to distinguish the augmentation data from the reconstructed data. Experimental results are demonstrated to show the generating and reconstruction performance.

The problem studied in this paper is very important, and has drawn a lot of researchers' attentions in recent years.  However, the novelties of this paper is very limited. The techniques used to train a bidirectional GAN are very standard. The only new stuff may be is the proposed reconstruction loss defined on augmented samples and reconstructed ones. But this is also not a big contribution, seems just using a slightly different way to guarantee reconstruction. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ryeeGRn_hX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Adversarial reconstruction loss is an interesting idea, but the paper need more polishing</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJGfCjA5FX&amp;noteId=ryeeGRn_hX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper876 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper876 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper is easy to follow. Here are some questions:

1. The argument about ALI and ALICE in the second paragraph of the introduction, “… by introducing a reconstruction loss in the form of a discriminator which classifies pairs (x, x) and (x, G(E(x)))”, however, in ALI and ALICE, they use one discriminator to classify pairs (z, x) and (z, G(z)). Therefore, “… the discriminator tends to detect the fake pair (x, G(E(x))) just by checking the identity of x and G(E(x)) which leads to vanishing gradients” is problematic. Therefore, the motivation in the introduction may be some modification.

2. The authors failed to compare their model with SVAE [1] and MINE [2], which are improved versions of ALICE. And we also have other ways to match the distribution such as Triple-GAN [3] and Triangle-GAN [4], I think the authors need to run some comparison experiments.

3. The authors should discuss more about the augment mapping a(x), i.e., how to choose a(x). I think this is quite important for this paper. At least some empirical results and analysis, for example, how inception score / FID score changes when using different choices of a(x).

4. This paper claims that the proposed method can make the training more robust, but there is no such experiment results to support the argument.

[1] chen et al. Symmetric variational autoencoder and connections to adversarial learning, AISTATS 2018.
[2] Belghazi et al, Mutual Information Neural Estimation, ICML 2018.
[3] Li et al. Triple Generative Adversarial Nets, NIPS 2017.
[4] Gan et al. Triangle generative adversarial networks, NIPS 2017.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJgC-o2xnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Quite a lot of experiments, but the choice of r(y|x) is not well justified, and some theoretical issues</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJGfCjA5FX&amp;noteId=BJgC-o2xnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper876 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">27 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper876 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Thank you for an interesting read.

The paper proposes adding an adversarial loss to improve the reconstruction quality of an auto-encoder. To do so, the authors define an auxiliary variable y, and then derive a GAN loss to discriminate between (x, y) and (x, decoder(encoder(x))). The algorithm is completed by combining this adversarial "reconstruction" loss with adversarial loss functions that encourages the matching of marginal distributions for both the observed variable x and the latent variable z. 

Experiments present quite a lot of comparisons to existing methods as well as an ablation study on the proposed "reconstruction" loss. Improvements has been shown on reconstructing input images with significant numbers.

Overall I think the idea is new and useful, but is quite straight-forward and has some theoretical issues (see below). The propositions presented in the paper are quite standard results derived from the original GAN paper, so for that part the contribution is incremental and less interesting. The paper is overall well written, although the description of the augmented distribution r(y|x) is very rush and unclear to me.

There is one theoretical issue for the defined "reconstruction" loss (for JS and f-divergences). Because decoder(encoder(x)) is a deterministic function of x, this means p(y|x) is a delta function. With r(y|x) another delta function (even that is not delta(y=x)), with probability 1 we will have mismatched supports between p(y|x) and r(y|x). 

This is also the problem of the original GAN, although in practice the original GAN with very careful tuning seem to be OK... Also it can be addressed by say instance noise or convolving the two distributions with a Gaussian, see [1][2].

I think another big issue for the paper is the lack of discussion on how to choose r(y|x), or equivalently, a(x). 

1. Indeed matching p_{\theta}(x) to p^*(x) and q(z) to p(z) does not necessarily returns a good auto-encoder that makes x \approx decoder(encoder(x)). Therefore the augmented distribution r(y|x) also guides the learning of p(y|x) and with appropriately chosen r(y|x) the auto-encoder can be further improved.

2. The authors mentioned that picking r(y|x) = \delta(y = x) will result in unstable training. But there's no discussion on how to choose r(y|x), apart from a short sentence in experimental section "...we used a combination of reflecting 10% pad and the random crop to the same image size...". Why this specific choice? Since I would imagine the distribution r(y|x) has significant impact on the results of PAGAN, I would actually prefer to see an in-depth study of the choice of this distribution, either theoretically or empirically. 

In summary, the proposed idea is new but straight-forward. The experimental section contains lots of results, but the ablation study by just removing the augmentation cannot fully justify the optimality of the chosen a(x). I would encourage the authors to consider the questions I raised and conduct extra study on them. I believe it will be a significant contribution to the community (e.g. in the sense of connecting GAN literature and denoising methods literature).

[1] Sonderby et al. Amortised MAP Inference for Image Super-resolution. ICLR 2017.
[2] Roth et al. Stabilizing Training of Generative Adversarial Networks through Regularization. NIPS 2017.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>