<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Augment your batch: better training with larger batches | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Augment your batch: better training with larger batches" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1V4QhAqYQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Augment your batch: better training with larger batches" />
      <meta name="og:description" content="Recently, there is regained interest in large batch training of neural networks, both of theory and practice. New insights and methods allowed certain models to be trained using large batches with..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1V4QhAqYQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Augment your batch: better training with larger batches</a> <a class="note_content_pdf" href="/pdf?id=H1V4QhAqYQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019augment,    &#10;title={Augment your batch: better training with larger batches},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=H1V4QhAqYQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Recently, there is regained interest in large batch training of neural networks, both of theory and practice. New insights and methods allowed certain models to be trained using large batches with no adverse impact on performance. Most works focused on accelerating wall clock training time by modifying the learning rate schedule, without introducing accuracy degradation. 
We propose to use large batch training to boost accuracy and accelerate convergence by combining it with data augmentation. Our method, "batch augmentation", suggests using multiple instances of each sample at the same large batch. We show empirically that this simple yet effective method improves convergence and final generalization accuracy. We further suggest possible reasons for its success.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Large Batch Training, Augmentation, Deep Learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Improve accuracy by large batches composed of multiple instances of each sample at the same batch</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">13 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HyxXFQr93Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea with insufficient support</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1V4QhAqYQ&amp;noteId=HyxXFQr93Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1352 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1352 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper tested a very simple idea: when we do large batch training, instead of sampling more training data for each minibatch, we use data augmentation techniques to generate training data from a small minibatch. The authors claim the proposed method has better generalization performance. 

I think it is an interesting idea, but the current draft does not provide sufficient support.

1. The proposed method is very simple. In this case, I would expect the authors provide more intuitive explanations. It looks to me the better generalization comes from more complicated data augmentation, not from the proposed large batch training.  

2. It is unclear to me what is the benefit of the proposed method. Even provided more computing resources, the proposed method is not faster than small batch training. The improvement on test errors does not look significant. If given more computing resources, and under same timing constraint, we have many other methods to improve performance. For example, a simple thing to do is t0 separately train networks with standard setting and then ensemble trained networks. Or apply distributed knowledge distillation like in (Anil 2018 
Large scale distributed neural network training through online distillation)

3. The experiments are not strong. The largest batch considered is 64*32, which is relatively small. In figure 1 (b), the results of M=4,8,16,32 are very similar, and it looks unstable. It is unclear what is the default batchsize for Imagenet. In Table 1, the proposed method tuned M as a hyperparameter. The baselines are fairly weak, the authors did not compare with any other method. I would expect at least the following baselines:
i)  use normal large batch training and complicated data augmentation, train the model for same number of epochs
ii) use normal large batch training and complicated data augmentation, train the model for same number of iterations
ii) use normal large batch training and complicated data augmentation, scale the learning rate up as in Goyal et al. 2017

4. For theorem 1, it is hard to say how much the theoretical analysis based on linear approximation near global minimizer would help understand the behavior of SGD. I fail to understand the the authors’ augmentation. Following the author’s logic, normal large batch training decrease the variability of &lt;H&gt;_k and \lambda_max, which converges to ‘’flat’’ minima. It contradicts with the authors’ other explanation. 

5. In section 4.2, I fail to understand why the proposed method can affect the norm of gradient. 


6. Related works:
Smith et al. 2018 Don't Decay the Learning Rate, Increase the Batch Size. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJxBD9g52X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>simple idea that works along with some theory to support it</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1V4QhAqYQ&amp;noteId=HJxBD9g52X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1352 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1352 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper describes a new method for data augmentation which is called batch augmentation. The idea is very simple -- include in your batch M augmentations of the each training sample, effectively this will increase the size of the batch by M. I have not seen a similar idea to this proposed before. As the authors show this simple technique has the potential to increase training convergence and final accuracy. Several experiments support the paper's claims illustrating the effectiveness of the technique on a variety of datasets (e.g. CIFAR, ImageNet, PTB) and architectures (ResNet, Wide-ResNet, DenseNet, MobileNets). Following that there's a more theoretical section which provides some analysis on why the method works, and seems also reasonable. Overall simple idea, well written-paper with clear practical application and of potential great interest to many researchers</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Bkxz6RFD27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1V4QhAqYQ&amp;noteId=Bkxz6RFD27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1352 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1352 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper shows that training with large batch size (e.g., with MxB samples) serves as an effective regularization method for deep networks, thus improving the convergence and generalization accuracy of the models. The enlarged batch of MxB consists of multiple (i.e., B) transforms of each of the M samples from the given batch; the transform is executed by a data augmentation method such as Cutout or Dropout. The authors also provide a theoretical explanation for the working of the method, suggesting that the enlarged batch training decreases the gradient variance during the training of the networks.

The paper is well written and easy to follow. Also, some interesting results are experimentally obtained such as the figures presented in Figure4. Nevertheless, the experimental studies are not very satisfactory in its current form.
 
Major remarks:

1.	In terms of regularization with transformed data in a given batch, the proposed method is related to MixUp (Zhang et al., mixup: Beyond empirical risk minimization), AdaMixUp (Guo et al., MixUp as Locally Linear Out-Of-Manifold Regularization), Manifold Mixup (Verma et al., Manifold Mixup: Learning Better Representations by Interpolating Hidden States), and AgrLearn (Guo et al. Aggregated Learning: A Vector Quantization Approach to Learning with Neural Networks). It would be useful for the authors to discuss how the proposed strategy differs from them or empirically show how the proposed regularization method compares to them in terms of regularization effect.  For example, in MixUp, AdaMixup and Manifold Mixup, the samples in a given batch will be linearly interpolated with randomly reshuffled samples of the same batch. In these sense, using them as baselines would make the contribution of the proposed method much significant. 
2.	In the experiments, it seems the authors use different data augmentation methods for different datasets  (except for Cifar10 and Cifar100), it would be useful to stick with a particular data augmentation method for all the datasets, for example, it would be interesting to see the performance of also using Cutout for the MobileNet and ResNet50 on the ImageNet data set. 
3.	Regarding the experimental study, I wonder if it would be beneficial to include three variations of the proposed method. First, use baseline with the same batch size, namely BxM, but with sampling with replacement. That is, using the same batchsize as that in Batch Augmentation but with repeated samples. In this way, the contribution of the data augmentation in the proposed method would be much clearer. Second, as suggested from the results in the PTB data in Table1, using only Dropout obtains very minor improvement over the baseline method. In this sense, using other data augmentation methods instead of Cutout for the image tasks would make the contribution of the paper much clear. Third, training the networks with the batchsize of BxM, but excluding the original data samples in the given batch would be another interesting experiment. That is, all samples of the batch in the batch augmentation are synthetic samples. 

Minor remarks:

1.	Is the regularized model robust to adversarial attacks as suggested in Mixup and Manifold Mixup?
2.	Would it be beneficial to include various data augmentation methods for the same batch? That is, each transformed sample may come from a different data augmentation strategy.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SygfJScCh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Note on Manifold Mixup and Adv. Robustness</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1V4QhAqYQ&amp;noteId=SygfJScCh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Alex_Matthew_Lamb1" class="profile-link">Alex Matthew Lamb</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1352 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">"1.	Is the regularized model robust to adversarial attacks as suggested in Mixup and Manifold Mixup?"

Both mixup and manifold mixup have improved robustness to the single-step FGSM attack.  Neither leads to robustness on multi-step attacks like PGD.  </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HygOo-VbqQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>clarifications</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1V4QhAqYQ&amp;noteId=HygOo-VbqQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1352 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1352 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to clarify the main points of our paper:
1. In many cases, it is possible to increase the batch size without affecting the wall clock time, due to surplus compute power (e.g., see Table 2). In those cases, our BA method can be used to take advantage of this large batch size, and improve the final model accuracy, as we demonstrated.
2. We suggested in section 4 that the reason that BA works well is that it enables the model to observe more augmentations, with only a small effect on the variance (since most of the samples in the mini-batch are highly correlated). This is in contrast to standard large-batch + augmentation (i.e., Regime Adaptation from [1]) which works less well since it has a larger effect on the variance.

The comments here suggested that instead of using BA with M=10, we simply increase the number of iterations x10 (keeping a small batch size of B=64). This results with the same accuracy gain as doing BA with the same M. This is not surprising since we observe more augmentations, while not changing the mini-batch variance. However, we would not gain from the computational benefits of a larger batch as the training time can be roughly x10 times longer.

We would also take the opportunity to report a result obtained after the paper was submitted:
Using the AlexNet model, with [B=512, M=8], we obtained a top-1 accuracy of 62.308% (up from baseline 57% and from 60% obtained by [2]). This echoes our messages clearly given table 2 in the paper -- using the same wall-clock time you can increase your model accuracy significantly by using BA.

[1] "Train longer generalize better" (2017) - Hoffer, Hubara, Soudry
[2] "Scaling SGD Batch Size to 32K for ImageNet Training" - You, Gitman, Ginsburg</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ByxmMenAFX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Mixup?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1V4QhAqYQ&amp;noteId=ByxmMenAFX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1352 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Haven't had a chance to read the whole paper yet but was surprised to not see any mention of mixup. Seems to me like it would make sense to send 1-2 augmented copies of each sample to the device then expand the size of the batch by using mixup on multiple combinations of those samples. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1xfC5BZ57" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Mixup</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1V4QhAqYQ&amp;noteId=r1xfC5BZ57"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1352 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1352 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your interest. Notice that mixup requires a mixed input from two separate labels as well as a mixed target by same amount. It does not deal with data augmentations as BA (multiple instances of same sample). Therefore, Mixup approach is orthogonal to ours and both can be combined. We welcome our readers to try and incorporate our ideas in this setting.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Syg9ycvsY7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Are the comparisons done with the same amount of image instances?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1V4QhAqYQ&amp;noteId=Syg9ycvsY7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Eddie_Smolyansky1" class="profile-link">Eddie Smolyansky</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018</span><span class="item">ICLR 2019 Conference Paper1352 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, it seems to me that due to the augmentation, there are now M times more image instances per epoch of training, right?
Perhaps I missed it in the paper, but have your comparisons (in Fig1, Fig2 for example)  been done with the same amount of image instances per epoch? A clarification could be to show Fig1, Fig2 with x axis being "image instances" and not "epoches".

My question boils down to: how have you demonstrated that the improved accuracy is due to the augmentations, as opposed to simply having a larger batch size? </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BylUSs_3Y7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>answer - comparing same number of instances</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1V4QhAqYQ&amp;noteId=BylUSs_3Y7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1352 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Sep 2018 (modified: 02 Oct 2018)</span><span class="item">ICLR 2019 Conference Paper1352 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank you for the interest and question. 
Training with large batch was noted in previous works to cause degradation in validation accuracy. While previous works focused on reducing the wall clock time without suffering from this degradation ("generalization gap"), our method is first to suggest significant improvement with large batches. 
If we understand you correctly, you are interested to see if our improvements can also be gained with training using larger batches for the same number of iterations (called "regime adaptation" in [1]). 
That way, the same number of image instances is seen by the model as in our method (but with a larger number of epochs). This kind of comparison is described in the last paragraph of section 3.1.
The full training results are available at <a href="https://drive.google.com/file/d/1mcHSnIx_dxjwTeYuUIrJmmcaLKQ-jDU5/view?usp=drivesdk" target="_blank" rel="nofollow">https://drive.google.com/file/d/1mcHSnIx_dxjwTeYuUIrJmmcaLKQ-jDU5/view?usp=drivesdk</a>
here you can see a comparison between 
(1) baseline B=64 training
(2) our batch augmentation (BA) method with M=10
(3) regime adaptation (RA) with B=640 and 10x more epochs

In the validation accuracy graph, you can observe that although the same number of sample instances were seen for both (2) and (3), our BA method still achieved a considerable improvement.
We hope we answered your concerns.

[1] "Train longer generalize better" (2017) - Hoffer, Hubara, Soudry</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyxdsQZAKm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Followup</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1V4QhAqYQ&amp;noteId=SyxdsQZAKm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Eddie_Smolyansky1" class="profile-link">Eddie Smolyansky</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Sep 2018</span><span class="item">ICLR 2019 Conference Paper1352 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the response, what a great platform to have a discussion. I believe you have understood my question correctly and addressed it.

Small comments: 
1. There might be a typo in this paper. In the final paragraph of section 3.1 you report the results of [1] are 93.04 but in the paper itself I find 93.07. Is it a coincidence that it's the same number as the baseline for "ResNet44 (He et al., 2016)" in this paper (shouldn't it be higher)?
2. I'm assuming in (3) above you mean B=64. Also, I can't access the gdrive link but I believe your report. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hyx56MEb9X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>answer</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1V4QhAqYQ&amp;noteId=Hyx56MEb9X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1352 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1352 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks. 
1. Indeed, that is a typo. It should be 93.07%, we will fix it in the next revision.
2. In the graph we posted,  RA was measured with B=640, see the clarification we posted for the case B=64.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Bkgdfp03FX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Comparison</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1V4QhAqYQ&amp;noteId=Bkgdfp03FX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Sep 2018</span><span class="item">ICLR 2019 Conference Paper1352 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Would you please show the results of "RA (B=64, M=1, Epochs=1000)"?
It may be a more proper comparison with BA than "RA (B=640, M=1, Epochs=1000)".</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1gG-GNW5m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>clarification</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1V4QhAqYQ&amp;noteId=B1gG-GNW5m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1352 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1352 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your interest. Please see the clarification we added to address your question.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>