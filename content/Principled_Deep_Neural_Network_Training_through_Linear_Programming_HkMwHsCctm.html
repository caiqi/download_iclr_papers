<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Principled Deep Neural Network Training through Linear Programming | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Principled Deep Neural Network Training through Linear Programming" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HkMwHsCctm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Principled Deep Neural Network Training through Linear Programming" />
      <meta name="og:description" content="Deep Learning has received significant attention due to its impressive performance in many state-of-the-art learning tasks. Unfortunately, while very powerful, Deep Learning is not well understood..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HkMwHsCctm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Principled Deep Neural Network Training through Linear Programming</a> <a class="note_content_pdf" href="/pdf?id=HkMwHsCctm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019principled,    &#10;title={Principled Deep Neural Network Training through Linear Programming},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HkMwHsCctm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Deep Learning has received significant attention due to its impressive performance in many state-of-the-art learning tasks. Unfortunately, while very powerful, Deep Learning is not well understood theoretically and in particular only recently results for the complexity of training deep neural networks have been obtained. In this work we show that large classes of deep neural networks with various architectures (e.g., DNNs, CNNs, Binary Neural Networks, and ResNets), activation functions (e.g., ReLUs and leaky ReLUs), and loss functions (e.g., Hinge loss, Euclidean loss, etc) can be trained to near optimality with desired target accuracy using linear programming in time that is exponential in the size of the architecture and polynomial in the size of the data set; this is the best one can hope for due to the NP-Hardness of the problem and in line with previous work. In particular, we obtain polynomial time algorithms for training for a given fixed network architecture. Our work applies more broadly to empirical risk minimization problems which allows us to generalize various previous results and obtain new complexity results for previously unstudied architectures in the proper learning setting.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">deep learning theory, neural network training, empirical risk minimization, non-convex optimization, treewidth</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Using linear programming we show that the computational complexity of approximate Deep Neural Network training depends polynomially on the data size for several architectures</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SJeK-G2dTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>On the practical relevance of our approach</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkMwHsCctm&amp;noteId=SJeK-G2dTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper96 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper96 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">First of all, we would like to thank the three reviewers for their careful assessment of our paper. We are currently working on a new version of our article which includes several improvements addressing the reviewers' concerns. One issue that was raised by the three reviewers is related to the practicality of our approach and the interpretation of the overall complexity bound. We provide a justification and clarification here.

We would like to stress that one of the key points is that the LP size depends *linearly* on the sample size. For a given architecture the bounds are *polynomial* in 1/epsilon. Indeed, it is the linear dependence on the sample size in such a generic setting that is surprising as this provides strong justification why training scales nicely with the size of the data, as observed in practice. We expect that this insight will have applications beyond our LP approach. In addition, to the best of our knowledge, before our paper the best training complexity bounds become polynomial in the sample size only *after* fixing the architecture parameters (input dimension, depth, width, etc.), whereas ours is polynomial in the sample size irrespective of the architecture; we should have been more clear in the presentation of our results.

The reviewers are correct in that the LPs we are constructing are large and likely to be unsolvable by writing down our formulation directly and relying on off-the-shelf solvers. This fact would make our contribution seem, at this stage, impractical. However, the history of LPs shows that in many interesting cases, very large LPs can be solved to proved optimality or near-optimality while only generating a vanishingly small part of the LP. We might say that such algorithmic approaches follow an incremental strategy. The classical example is given by Edmonds' weighted matching algorithm. From a purely practical perspective, we have the LPs arising in the airline or mining industry, among others,  where (again) the LP is never actually written down. In all these cases one has an LP with a rich underlying structure, and the algorithms exploit that structure to solve the problem. In these examples and others, the theoretical understanding of the LP structure is used to drive the development of incremental solution strategies. In this paper, we have focused on laying a theoretical foundation.

To further expand on this point, note that the development of practical counterparts to theoretical algorithms can be a challenging task, and much effort is needed. In the case of Mixed-Integer Programming, for example, exact algorithms heavily rely on heuristics that do not have any guarantee, but that work well in practice and allow the algorithms to run faster without compromising overall exactness. Our hope is that our novel "polyhedral" interpretation of training can work together with state-of-art and practically efficient training algorithms (like stochastic gradient descent) in order to produce more complete training algorithms (perhaps with near optimality guarantees).  An idea that comes to mind would be a partially enumerative version of SGD based on our LP/IP.

Besides laying the foundations to what we think will become practical, we strongly believe that a theoretical understanding of the training problems is an important and necessary contribution to the ML community. For example, the way our approach works effectively implicitly "decomposes" the problem for each data point, and the LP merges them back together without losing any information nor optimality guarantee, even in this non-convex setting. This bears close resemblance to SGD where single data points (or batches of those) are used in a given step, and as such our results might provide a new perspective, through low treewidth, on *why* the current practical approaches work so well, as our LPs are working similarly.

Lastly, regarding the choice of outlet, we strongly consider ICLR to be a great fit for our paper as our work can be viewed as an improvement of the work in ICLR by Arora et al. (2018) in the bounded case, which posed several questions open that we answer here. Fundamental understanding of the training problem has recently gained increasing traction within the machine learning community. See, for example, the recent paper by Manurangsi and Reichman (arXiv:1810.04207; posted after ours). This paper analyzes the training problem in the special case of ReLUs, obtaining an algorithm with a worse dependency on epsilon than ours.

Best regards.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ByeXDEy3hm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>exponential complexity; practical relevance unclear</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkMwHsCctm&amp;noteId=ByeXDEy3hm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper96 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper96 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This work reformulates the neural network training as an LP with size that is exponential in the size of the architecture and data dimension, and polynomial in the size of the data set. They further analyze generalization properties. It extends previous works on 1-hidden-layer neural-nets (say, In Arora et al. (2018)). 

Pros: Establish new time complexity (to my knowledge) of general neural-nets. 

Cons: It seems far from having a practical implication. Exponential complexity is huge (though common in TCS and IP communities). No simulation was presented. Not sure which part of the approach is useful for practitioners.
    My feeling is that the paper is a bit too theoretical and less relevant to ICLR audience. More theoretical venues may be a better fit. 

Other questions:
--The authors mentioned “that is exponential in the size of the architecture (and data dimension)  and polynomial in the size of the data set;” and "this is the best one can hope for due to the NP-Hardness of the problem ". 
a)	The time complexity is exponential in both the size of neural-net and the data dimension (the latter seems to be ignored in abstract). Is there a reference that presents results on NP-hardness in terms of both parameters, or just one parameter? 
b)	The NP-hardness reduction may give an exp. time algorithm. Is there a simple exponential time algorithm? If so, I expect the dependence on the size of the data set is exponential, and the contribution of this paper is to improve to polynomial. The authors mentioned one discretization method, but are there others? More explanation of the importance of the proved time complexity will be helpful. 

-- Novelty in technical parts: The idea of tree-width graph was introduced in Bienstock and Muñoz (2018). The main theorem 3.1 is based on explicit construction for Theorem 2.5, and Theorem 2.5 is an immediate generalization of a theorem in Bienstock and Muñoz (2018) as mentioned in the paper. Thus, this paper looks like an easy extension of Bienstock and Muñoz (2018) --intuitively, minimizing polynomials by LP seems to be closely related to solving neural-nets problems by LP. Could the authors explain more on the technical novelty? 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyeUXih_TX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reviewer response (2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkMwHsCctm&amp;noteId=SyeUXih_TX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper96 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper96 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">- "b) The NP-hardness reduction may give an exp. time algorithm. Is there a simple exponential time algorithm? If so, I expect the dependence on the size of the data set is exponential, and the contribution of this paper is to improve to polynomial."

Recall that our result constructs a *uniform* LP, meaning, an LP that encodes (approximately) all input data-sets and that can be used for training with any input data. Constructing such LP is also NP-hard since it includes the NP-hard setting (as a face). 

As the reviewer points out, there is a simple exponential time algorithm that can do the same: simply consider an epsilon-grid to enumerate over all possible (approximate) inputs, and for each input solve the training problem. This would yield an algorithm with exponential dependency on the sample size.

In the non-uniform case, however, one can achieve a polynomial dependency on the sample size with a simple algorithm. We refer the Reviewer to Appendix B of the current version, where we discuss the differences between a "uniform" and a "non-uniform" LP.

The contribution of this paper, we believe, is not only that we provide a linear dependency on the data, but more importantly that we show that one can give a uniform LP (of size linear in D) that would work for all samples of a given size, without compromising on the linear dependence on the sampling size.

- "The authors mentioned one discretization method, but are there others? More explanation of the importance of the proved time complexity will be helpful."

Regarding the importance of the proved time complexity, we address this comment in the common response to all reviewers in this forum.

Regarding the discretization method, the reviewer touches on an important question in non-convex optimization and algebraic geometry, and an active research field. For example, the work of Piazzon et al. [1] or Vianello [2], study how one can discretize a potentially non-convex region while satisfying certain approximation guarantees. In general, this is a complex task when little structure is present, as we are assuming in order to tackle the general ERM problem. If there is a stronger geometrical structure, however, one can discretize in a more efficient way. A classical example is the Ben-Tal and Nemirovski [3] approximation of the second-order cone, where a custom discretization is performed which uses the geometry of such convex cone. Nonetheless, the non-convexities present in the DNN setting made us stick to the "inverse powers of 2" discretization, as it is clean and provides an efficient approximation.

[1] Piazzon, Federico, and Marco Vianello. "Jacobi norming meshes." Math. Inequal. Appl 19 (2016): 1089-1095.
[2] Vianello, Marco. "Norming meshes by Bernstein-like inequalities." Math. Inequal. Appl 17.3 (2014): 929-936.
[3] Ben-Tal, Aharon, and Arkadi Nemirovski. "On polyhedral approximations of the second-order cone." Mathematics of Operations Research 26.2 (2001): 193-205.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJgHZon_aQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review response (1)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkMwHsCctm&amp;noteId=BJgHZon_aQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper96 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper96 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank the reviewer for the comments and suggestions. The reviewer raised some important points that we hope the next version will amend.  We are currently working on an improved version of the paper that will include these. In the meantime, we would like to comment on all points raised by this review.

- "It seems far from having a practical implication. Exponential complexity is huge (though common in TCS and IP communities). No simulation was presented. Not sure which part of the approach is useful for practitioners. My feeling is that the paper is a bit too theoretical and less relevant to ICLR audience. More theoretical venues may be a better fit."
    
We have provided a justification and clarification in an above statement in this forum. We hope you find it satisfactory.

- "The authors mentioned “that is exponential in the size of the architecture (and data dimension)  and polynomial in the size of the data set;” and "this is the best one can hope for due to the NP-Hardness of the problem ".  a) The time complexity is exponential in both the size of neural-net and the data dimension (the latter seems to be ignored in abstract). Is there a reference that presents results on NP-hardness in terms of both parameters, or just one parameter?"

The referee raises an interesting point regarding whether these two parameters can be decoupled. The input dimension and the parameter space dimension are typically related to each other in the NP-hardness results. For example, in the paper by Blum and Rivest (1992), NP-Hardness of the training problem is proved with respect to a parameter "n" which is the input dimension *and* the parameter space dimension (roughly). If we plug in that architecture size into our result, we obtain an exponential dependency on n. In a sense, "the best one can hope for".

A recent paper submitted to this conference, entitled "Complexity of Training ReLU Neural Network", works on this subject as well. They prove polynomial time solvability of the training problem for fixed input dimension, however, they consider a *fixed* architecture (the polynomial dependency is with respect to the sample size).

If these parameters are decoupled, it is not known if the exponential dependence in the parameter space dimension can be alleviated. Quoting the paper by Arora et al. (2018):
 
"we are not aware of any complexity results which would rule out the possibility of an algorithm which trains to global optimality in time that is polynomial in the data size and/or the number of hidden nodes, assuming that the input dimension is a fixed constant"

As the referee noted, our phrasing is a bit confusing. In the phrase quoted by the reviewer, we were considering all the term "n+m+N" to be the "size of the architecture", since the input dimension can be also considered as part of the architecture. We now realize this is not standard, and somewhat confusing, so we will clarify this in the revised version.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJeC4Fx52Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkMwHsCctm&amp;noteId=HJeC4Fx52Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper96 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper96 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies the problem of proper learning of deep neural network. In particular, the focus is on doing
approximate empirical risk minimization over the class of neural networks of a fixed architecture. The main 
result of the paper is that approximate ERM can be formulated as an LP problem that is of size exponential in the
network parameters and the input dimensionality. The paper uses a framework of Bienstock and Munoz that shows how to 
write a binary optimization problem as a linear problem with size dependent on the treewidth of an appropriate graph
associated with the optimization problem. In order to apply the framework, the authors first discretize the parameter
space appropriately and then apply analyze the treewidth of the discretized space. The authors also provide treewidth
analysis of specific architectures including fully connected networks, and CNNs with various activations.

Most of the technical work in the paper involves analyzing the treewidth of the resulting discretized problem. The nice 
feature of the result is that it holds for worst case data sets, and hence, the exponential dependence on various
parameters is unavoidable. On the other hand, it is unclear to me as to how these ideas might eventually lead to 
practical algorithms or shed light on current training practices in the deep learning community. For instance, it would
be very interesting to investigate if under certain assumptions on the data generation process, one can get small LPs
that depend exponentially only in the depth, as opposed to the input dimensionality.  

I also feel that section 5 does not add much to the main results of the paper and can be skipped or moved entirely to the appendix. On a technical note, I don't see where the dependence on the input dimensionality appears in Theorem 5.1. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJxYsSnOpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkMwHsCctm&amp;noteId=BJxYsSnOpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper96 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper96 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">First of all, thank you very much for the thorough review. We appreciate the feedback provided and we are happy that you consider the "worst-case" feature of our approach an important one. We are currently working on an improved version of the paper, which will include your suggestions and clarify your concerns. In the meantime, we would like to comment on the points raised in this review.

- "It is unclear to me as to how these ideas might eventually lead to practical algorithms or shed light on current training practices in the deep learning community."

Given that the three reviewers touched on this aspect we provide a justification and clarification in a separate message in this forum. We hope you find it satisfactory.

- "For instance, it would be very interesting to investigate if under certain assumptions on the data generation process, one can get small LPs that depend exponentially only in the depth, as opposed to the input dimensionality."
    
The reviewer is correct. In order to provide a cleaner analysis, we are assuming that all data in "[-1,1]^{n+m}" is a possible input. The "n+m" term in the exponent of the LP sizes is a consequence of this assumption as per our approach. However, as the reviewer noted, one can make use of additional structure in the data generation process in order to alleviate the LP size from the input-dimensionality.

If each data point belongs to a set "U", for example, and our grid over "U" has "M" points then our LP size will depend on "M" instead of "(2L/\epsilon)^{n+m}". Only the parameter space dimension will remain in the exponent, as the reviewers suggests. 

While interesting in its own right, this is beyond the scope of the current paper and left for future work. However, we will add a remark on this in the new manuscript for the curious reader. 
  
- "I also feel that section 5 does not add much to the main results of the paper and can be skipped or moved entirely to the appendix. On a technical note, I don't see where the dependence on the input dimensionality appears in Theorem 5.1."
    
We believe Section 5 is necessary, as Generalization of ERM estimators is an important feature to have. Nonetheless, we agree with the reviewer in that it is not at the core of the main results. The new version will have the entire Generalization discussion in the Appendix.

Regarding the dependence on the input dimensionality, it is correct that Theorem 5.1 does not need any. We are working with Lipschitz constants that depend on the infinity norm, which only looks at entries individually. Moreover, this Theorem is only making a statement on the minimum number of data points needed to achieve a certain approximation guarantee, which depends on the distribution of the data (through  "\sigma^2") but not necessarily on their dimension.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SkxC5Jptnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Solid work that could discuss the implications for the ICLR community better</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkMwHsCctm&amp;noteId=SkxC5Jptnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper96 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper96 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This is very solid work and the framework allows one to plug-in existing complexity measures to provide complexity upper bounds for (some) DNNs. The main idea is to rephrase an empirical risk minimization problem in terms of a binary optimization problem 
using a discretization of the continuous variables. Then this formulation is used to provide a as a moderate-sized linear program of its convex hull. 

In my opinion, every paper that provides insights into the complexity and generalization of deep learning is an important contribution. Moreover, the present paper is based on 
a recent insight of the authors, i.e., it is based on solid grounds. However, it would have been nice to also show some practical insights. The main take-aways message is that we need exponential time. Is this practical for networks with with millions of parameters? Or does this imply that deep learning is hopeless (in theory)? To be fair, the authors touch upon this in the conclusions, but only 1-2 sentences. This discussion should be extended. Nevertheless, I agree that the bridge built is important and may indeed trigger some very important future contributions. 

The authors should, however, also review other work on linear programming for deep networks coming from the machine learning community such as 

Brandon Amos, Lei Xu, J. Zico Kolter:
Input Convex Neural Networks. 
ICML 2017: 146-155

Given the background of the average ICLR reader, the authors should also introduce (at least the intuitions) improper and proper learning setups in the introduction before using them.   This also holds for other terminology from complexity theory. Indeed, the authors cannot introduce/review all complexity theory. However, they should try their best and fill the rest by a reference to an introductionary book or directly to the appendic. Without, while important for the ICLR community, the authors run the risk that the paper would better be suited by a learning theory venue. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HygOw7hdpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkMwHsCctm&amp;noteId=HygOw7hdpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper96 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper96 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">First of all, thank you very much for the thorough review. We are glad you find our work a solid contribution. We are currently working on an improved version of the paper, which will include your suggestions and clarify your concerns. In the meantime, we would like to comment on all points raised in your review.

- "It would have been nice to also show some practical insights. The main take-aways message is that we need exponential time. Is this practical for networks with with millions of parameters? Or does this imply that deep learning is hopeless (in theory)? To be fair, the authors touch upon this in the conclusions, but only 1-2 sentences. This discussion should be extended. Nevertheless, I agree that the bridge built is important and may indeed trigger some very important future contributions."
    
Thank you very much for viewing our work as providing a bridge between communities; this was indeed our intention. We will extend the discussion on the exponential dependency of our approach. Regarding the practicality of our approach, given that the three reviewers touched on this aspect we provide a justification and clarification in a separate message in this forum for the three referees. We hope you find it satisfactory.

- "The authors should, however, also review other work on linear programming for deep networks coming from the machine learning community such as Brandon Amos, Lei Xu, J. Zico Kolter: Input Convex Neural Networks. ICML 2017: 146-155"
    
Thank you very much for the pointer to this work. We will review this work and add the corresponding citation in the revised manuscript.

- "Given the background of the average ICLR reader, the authors should also introduce (at least the intuitions) improper and proper learning setups in the introduction before using them.   This also holds for other terminology from complexity theory. Indeed, the authors cannot introduce/review all complexity theory. However, they should try their best and fill the rest by a reference to an introductionary book or directly to the appendic. Without, while important for the ICLR community, the authors run the risk that the paper would better be suited by a learning theory venue."
    
Thank you for this suggestion. We will provide a more careful introduction for technical terminology to expand the reach of our paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>