<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learning and Planning with a Semantic Model | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learning and Planning with a Semantic Model" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SJgs1n05YQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learning and Planning with a Semantic Model" />
      <meta name="og:description" content="Building deep reinforcement learning agents that can generalize and adapt to unseen environments remains a fundamental challenge for AI. This paper describes progresses on this challenge in the..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SJgs1n05YQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning and Planning with a Semantic Model</a> <a class="note_content_pdf" href="/pdf?id=SJgs1n05YQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 10 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019learning,    &#10;title={Learning and Planning with a Semantic Model},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SJgs1n05YQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=SJgs1n05YQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Building deep reinforcement learning agents that can generalize and adapt to unseen environments remains a fundamental challenge for AI. This paper describes progresses on this challenge in the context of man-made environments, which are visually diverse but contain intrinsic semantic regularities. We propose a hybrid model-based and model-free approach, LEArning and Planning with Semantics (LEAPS), consisting of a multi-target sub-policy that acts on visual inputs, and a Bayesian model over semantic structures. When placed in an unseen environment, the agent plans with the semantic model to make high-level decisions, proposes the next sub-target for the sub-policy to execute, and updates the semantic model based on new observations. We perform experiments in visual navigation tasks using House3D, a 3D environment that contains diverse human-designed indoor scenes with real-world objects. LEAPS outperforms strong baselines that do not explicitly plan using the semantic content.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">deep reinforcement learning, generalization, semantic structure, model-based</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a hybrid model-based &amp; model-free approach using semantic information to improve DRL generalization in man-made environments.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SygmrKz4T7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We have revised our paper with more experiments and clarifications</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJgs1n05YQ&amp;noteId=SygmrKz4T7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1015 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1015 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have updated the paper to accommodate reviewers’ concerns, including many changes in experiment section and additional evaluations with a new **metric SPL (Sec 6.4, Figure 5)**.

Here are details of what we changed.
(1) Improved introduction section (Sec.1) and algorithm section (Sec. 4) incorporating reviewer’s comments
(2) Reviewers are complaining about we use ground truth semantic signals in the experiments. In the current version, all the results of LEAPS in experiment section (Sec.6) are **using CNN ** to extract semantic signals. The case of ground truth signals are deferred to appendix D. We emphasize that our LEAPS agent is robust to semantic signals and has comparable performances with different semantic signals. All our previous claims remain true.
(3) Reviewer3 suggests us to use a more informative metric, **SPL**, which considers episode length information, in addition to success rate. We evaluate LEAPS under SPL in Sec 6.4 (Fig.5). Surprisingly, under SPL metric, LEAPS outperforms all the baselines much **more significantly**. Please check Sec 6.4 for details. 
(4) To better illustrate the effectiveness of LEAPS, we show the success rate and SPL in numbers in Figure.5. LEAPS agents have overall better performances under both metrics.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HyeBO63p3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning and Planning with a Semantic Model </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJgs1n05YQ&amp;noteId=HyeBO63p3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1015 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1015 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This work proposes a hybrid model for robot visual navigation in synthetic indoor environments, specifically a combination of a  high-level planning scheme (model-based) with a low-level behavior based approach (model-free) . The main contribution is on the high-level based planning that is based on semantic cues from the environment, specifically the construction of a semantic prior about rooms connectivity. By using this prior the system is able to generalize to new environments simplifying an initial robot exploration phase. 

The semantic prior is implemented by the construction of a graph representation that encodes room connectivity. Links between rooms (nodes) are given by Bernoulli variables which are inferred by previous experiences and an exploration phase in the current environment.  

Results are one of the weaker parts of the paper, success rates are very low, even for short planning horizons (figures 3,4,5).  Furthermore, it is not clear the real relevance of the semantic prior because relative performance with respect to baselines is not significant. In general, while a room connectivity prior can be of help, I believe is not so critical for indoor robot navigation. There are prior works on Robotics that has shown more impact using structural priors, such as, presence of corridors, doors, etc, or "object-room" spatial relations. The low success rate is even more critical if one considers that the validation is based on synthetic environments.

In general the paper is easy to follow, although, there are some details missing, specially in terms of model description. My main concern is that the paper is limited in technical novelty and it suffers from a lack of practical significance.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1xmEqG4p7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Please check Fig.5 (new), experiments show that LEAPS is effective</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJgs1n05YQ&amp;noteId=r1xmEqG4p7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1015 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1015 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt;&gt;&gt;&gt; For low success rate:
It is because this semantic navigation task itself is extremely challenging for RL. Therefore we proposed an HRL framework, i.e., a semantic model, to improve performance of general RL methods.
We emphasize that the low success rate is not training performance. Instead, it is generalization performance on unseen environments. Current state-of-the-art results on semantic navigation tasks still suffer from this low success rate (e.g., fig7 in <a href="https://arxiv.org/pdf/1609.05143.pdf," target="_blank" rel="nofollow">https://arxiv.org/pdf/1609.05143.pdf,</a> row 3 in table 1 in https://arxiv.org/pdf/1810.06543.pdf). 
Another misunderstood point is that the “planning horizons”  here are steps in the “semantic model”, i.e., the number of rooms the agent needs to go through to reach the goal. It is NOT the actual episode length for the agent. We show the stats of averaged length of ground truth shortest path in Appendix F, where the avg length is 46.86.  Note the agent has 9 available actions and the environment has strong partial observability (visual signal of different rooms are typically blocked by walls). The task is indeed challenging. Moreover, our goal is to improve the performance of model-free RL approaches. So we believe the relative improvement instead of the success rate itself should be focused more on.

&gt;&gt;&gt;&gt; Improvement is not significant:
To better illustrate that our method is effective, we illustrate all the success rate in numbers in Figure.5. In all the horizons, LEAPS agents have the best success rates overall and for all targets requiring planning computations (i.e., plan-steps &gt; 1). Particularly, the relative improvements are higher for targets requiring more planning (e.g., plan-steps = 3)
Thanks to the suggestion by Reviewer3, we utilize a better metric, Success weighted by Path Length (SPL), which considers episode length in the evaluation metric. In SPL, success episodes with faraway targets will be assigned more credits. We introduce SPL in Sec 6.4 and show the results in Figure 5. In SPL, our approach overall outperforms all the baselines with significant margins. More importantly, as more planning computations (longer horizons), the margin becomes increasingly larger. This again indicates that LEAPS is indeed effective for those faraway targets.

&gt;&gt;&gt;&gt;  For synthetic dataset:
Sorry for the confusion. We have updated the introduction section accordingly. We here are refering to environments that resemble the real-world semantic properties. The houses in House3D are designed by humans and share the same semantic regularities as the real world.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1xSyCP53m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper proposes a hybrid model-based and model-free approach called LEAPS, consisting of a multi-target sub-policy that acts on visual inputs, and a Bayesian model over semantic structures. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJgs1n05YQ&amp;noteId=B1xSyCP53m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1015 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1015 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The contributions of this paper are in the area of semantic modelling, where the authors propose an approach called LEAPS consisting of a multi-target sub-policy that acts on visual inputs, and a Bayesian model over semantic structures. The fundamental premise of the proposed approach is that when placed in an unseen environment the agent plans with the semantic model based on new observations. Particularly, the authors propose to learn a Bayesian model over the semantic level and infer the posterior structure via the Bayes rule. The proposed approach is validated with experiments in visual navigation tasks using a 3D environment that contains diverse human-designed indoor scenes with real world objects. Finally, the authors show the key role of using semantic context compared to the baselines that do not consider semantic context.

The parer is interesting, well structured and and clearly written. Also, the addressed topic of incorporating semantic model in the context of learning and planning is very interesting.

The related work is extensively presented with pertinent and up-to-date literature. Furthermore, the background section presents well the DRL notations.

In section 5, how the values for e.g between dinning room and garage 0.05, dinning room and kitchen 0.7 are learned, and how generalisable is this approach to other applications - because the way those priors are determined do not seem very explicit?

Furthermore in the experiments it does not seem explicit how the semantic model is updated in light of new information, I think this deserves further explanation or to be clearly pinpointed?

Also, what are the key requirements that make  the semantic model interpretable. Because, the way the validation is conducted in this paper, it seems that ithe nterpretability is quite specific to House3D - is it generalisable to other applications and under which conditions?

Otherwise, I believe that the questions asked in the experiments section are well answered with the experimental results
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJxKpsMETm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for the valuable comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJgs1n05YQ&amp;noteId=HJxKpsMETm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1015 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1015 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The prior distribution is learned from our training environments by random exploration. This semantic prior can be generally applied to tasks that require reasoning over the relations between rooms.
Our approach can be applied to other tasks and applications involving semantic properties and relations as well. The Bayesian framework as well as the learning algorithm is general. When a new domain, we just define the Bayesian model over concepts in the new domain and the overall framework still works.
As stated in Sec 6.2, we update our semantic model every 30 time steps. A show case is in Figure 2 where we show the updated graph with posterior probabilities on each edge. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BkxW6t-4jm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>non-effective method (works well only with groundtruth information), convoluted writing, improper evaluation metric</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJgs1n05YQ&amp;noteId=BkxW6t-4jm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1015 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1015 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a hybrid model-free and model-based RL agent for the task of navigation. Reaching the target is decomposed into a set of sub-goals, and the plan is updated as the agent explores the environment. The method has been tested in the House3D environment for the task of RoomNav, where the goal is to navigate towards a certain room. 

The idea of integrating RL agents with semantic knowledge is interesting. However, the paper has several major issues that should be addressed in the rebuttal:

(1) The experiment results in Figure 3 and Figure 4 are based on groundtruth room information. The only experiment that is fully automatic is the one in Figure 5. However, there is no difference between the proposed method and the baselines in that case. So the proposed method is not effective without groundtruth information.

(2) The only evaluation metric that is used is "Success Rate". That metric is not sufficient for evaluation of navigation agents since it does not include episode length information. All of the results should be based on the protocol mentioned in "On Evaluation of Embodied Navigation Agents", arXiv 2018. 

(3) There is no termination action according to Appendix B. So the agent does not know if it is at the target or not. It seems the agent will stop if it issues "stay still" three times. That is different from termination action. Also, it is confusing what 450 pixels means for a scene classifier that works on the image.

(4) The paper is written in a convoluted way:
   (a) It is not clear if the semantic model is trained along with the RL model end-to-end or not.
   (b) Regarding multi-target sub-policies, is there a separate policy for each pair of intermediate targets? 
   (c) Regarding inference and planning on M, what is \tau exactly? How is the length of the plan determined? 
   (d) Why is the model updated only after a fixed number of steps? That increases the episode length. 

(5) The number of T_i's is manually set to 8. That causes serious generalization issues. How do we know how many T_i's exist in a new environment?


Minor comments:
- The paper mentions "An example of such environments is House3D which contains 45k real-world 3D scenes". House3D includes only synthetic scenes. They should not be called real-world scenes.
- How is the reward shaping done?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1gCIsfV67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We have added results with SPL (Fig.5), LEAPS is much more effective under SPL metric.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJgs1n05YQ&amp;noteId=B1gCIsfV67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1015 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018 (modified: 11 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1015 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
&gt;&gt;&gt; groundtruth information:
We have replaced all the plots in the updated paper with CNN-LEAPS, namely in the current version, **all the signals are extracted by CNN**. For a clear comparison, we illustrate the success rate in numbers in *Figure 5*. LEAPS has the highest success rate overall and for all targets requiring planning computations (i.e., plan-steps &gt; 1). For targets with plan-steps = 1, they  require little planning so it is as expected that improvements by LEAPS is tiny.

We also show in Appendix D that CNN-LEAPS has comparable performances to LEAPS using ground truth signals.

&gt;&gt;&gt; evaluation metric
Thanks for pointing out this metric. We agree that this is a better metric and we have evaluated the performance of all approaches under SPL in Sec 6.4. All the results are shown in Figure 5. 
** Surprisingly, under SPL, LEAPS has very significant overall improvements over all the baseline methods in all the horizons (rightmost column in Figure 5). ** Particularly, as the horizon becomes longer, namely with more allowed planning steps, the margin between LEAPS and other baselines are increasingly larger. Which again indicates that LEAPS is indeed able to solve faraway targets. 
Although we appreciate the reviewer for pointing out the paper “On Evaluation of Embodied Navigation Agents”, we think it is unfair to say “All of the results *should* be based on the protocol mentioned in” this paper. It is a very recent paper (on arxiv in Jul) while our project started much earlier. Also, we think as a research work, it is still debatable to say which metric is better or worse (e.g., the paper itself uses the term "recommend"). We are not required to follow this unpublished work (though it is indeed insightful). Nevertheless, we have already added results under SPL metrics and hope that Reviewer 3 is happy with these results.


&gt;&gt;&gt; termination action
The design of action space is an orthogonal issue to our focus. Here we propose a HRL framework improve generalization ability of DRL agents on environments with semantic regularities. For this purpose, we choose the RoomNav task on House3D environment. Even without a termination action in the action space, the task itself is already very challenging due to two factors: (1) long horizon and large action space (see Appendix F for details), i.e., avg shortest path is 46.86 steps away and the agent has 9 actions; (2) strong partial observability, i.e., many existing navigation works are within the same room while our task needs to navigate through rooms --- visual signals of other rooms are blocked by walls and doors.
We agree that ultimately for building a navigation agent, we should eventually include termination action in the action space, as suggested by the paper “On Evaluation of Embodied Navigation Agents”. However, we again emphasize that this is orthogonal to our focus (we are proposing an learning framework), and since this paper is very recent and unpublished, we think it is unfair to complain us on the design of action space according to it.

&gt;&gt;&gt; writing
Thanks for the comments and we updated the paper accordingly.
(a) We have added Sec 4.4 in the main paper to further clarify that LEAPS is trained in a two-step fashion. First sub-policies and then semantic models. 
(b) it is a conditional LSTM policy conditioning on memory, current visual input and the semantic target.
(c) We updated the texts in Sec 4.2. tau denotes a sequence of concepts that leads to the goal concept with the highest probability. The length varies according to the planning result on the semantic graph.
(d) The semantic model should be updated periodically. Updating it after a fixed amount of steps is the easiest way. We found this simple approach generally works well. Yes, it may indeed increase the episode length. This is why in some cases under short horizon LEAPS may not have the highest SPL. We carefully discussed this in Sec 6.4.

&gt;&gt;&gt; size of concepts
We can never expect a human who never know the concept of apple before to solve a semantic task “find an apple”. It is analog to natural language where each word in the agent’s dictionary can be considered as a concept here. The T_i’s can be understood as the word tokens the agent understands. In environments with man-made semantic structures, these concepts (T_i’s) generalize. If at test time we cannot categorize some scenes into our known concepts, it is completely fine. Practically, we do have a concept called “other” in our implementation (see footnote 1) representing all those scenes not belonging to any of predefined concepts in our dictionary.

Minor comments:
-- we have updated the introduction section accordingly
-- we added description to reward shaping in Appendix G.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HyghQ49gCm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>SPL results seem incorrect</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJgs1n05YQ&amp;noteId=HyghQ49gCm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1015 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1015 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for addressing my comments. However, the SPL results do not seem correct. Success rate should be the upperbound for SPL so SPL cannot be higher than success rate, but in Figure 5 there are several cases that SPL is higher than success rate. Please fix the results so we can better evaluate the paper. We just need to see the episode length. Any metric that considers both episode length and success rate together would work. SPL is one option. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkgUxkoe07" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>SPL results are correct. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJgs1n05YQ&amp;noteId=SkgUxkoe07"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1015 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1015 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the concern. Our SPL results are correct. Note that in Table. 5 (or Fig. 5) we report success rate as percentage (in the unit of 0.01), and report SPL as per-mille (in the unit of 0.001). For example, 53.4 / 58.4 in the table means that the success rate is 53.4% while SPL is 0.0584. So SPL is indeed upper-bounded by success rate, as suggested by the reviewer (and by the definition of SPL). 

Overall, SPL is low for all the methods, due to the difficulty of our navigation task (it is multi-room, and unknown environment to the agent).  

For episode length, please check Table. 7 (or Fig. 7).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>