<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Coverage and Quality Driven Training of Generative Image Models | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Coverage and Quality Driven Training of Generative Image Models" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rylxxhRctX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Coverage and Quality Driven Training of Generative Image Models" />
      <meta name="og:description" content="Generative modeling of natural images has been extensively studied in recent years, yielding remarkable progress. Current state-of-the-art methods are either based on maximum likelihood estimation..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rylxxhRctX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Coverage and Quality Driven Training of Generative Image Models</a> <a class="note_content_pdf" href="/pdf?id=rylxxhRctX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019coverage,    &#10;title={Coverage and Quality Driven Training of Generative Image Models},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rylxxhRctX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=rylxxhRctX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Generative modeling of natural images has been extensively studied in recent years, yielding remarkable progress. Current state-of-the-art methods are either based on maximum likelihood estimation or adversarial training. Both methods have their own drawbacks, which are complementary in nature. The first leads to over-generalization as the maximum likelihood criterion encourages models to cover the support of the training data by heavily penalizing small masses assigned to training data. Simplifying assumptions in such models limits their capacity and makes them spill mass on unrealistic samples. The second leads to mode-dropping since adversarial training encourages high quality samples from the model, but only indirectly enforces diversity among the samples. To overcome these drawbacks we make two contributions. First, we propose a model that extends variational autoencoders by using deterministic invertible transformation layers to map samples from the decoder to the image space. This induces correlations among the pixels given the latent variables, improving over factorial decoders commonly used in variational autoencoders. Second, we propose a unified training approach that leverages coverage and quality based criteria. Our models obtain likelihood scores competitive with state-of-the-art likelihood-based models, while achieving sample quality typical of adversarially trained networks. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">deep learning, generative modeling, unsupervised learning, maximum likelihood, adversarial learning, gan, vae</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Generative models that yield Gan-like samples and achieve competitive likelihood on held-out data. </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_S1eKyTOnpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Updated version </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylxxhRctX&amp;noteId=S1eKyTOnpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1045 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1045 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have updated the pdf to take into considerations some remarks made in the review. Specifically,
the ablation study in Table 1 has been updated as suggested, the novelty of the architecture used is stated more clearly, visualisations of reconstructions are available in appendix C, and approximate likelihood performance for the GAN baseline, obtained by training an inference network with a frozen gan generator as decoder, is now provided.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Sye3lkqxTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting, but there are some unclear issues</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylxxhRctX&amp;noteId=Sye3lkqxTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1045 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1045 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=Sye3lkqxTX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">It is well known that optimizing divergences of different directions leads to different learning behaviors - the mode covering behavior of maximum likelihood training and the mode missing behavior of GAN training. This paper makes a good presentation in explaining coverage-driven training and quality-driven training. 
Techinically, this paper make two contributions. First, extend VAEs by using deterministic invertible transformation layers to map samples from the decoder to the image space. Second, use the loss Eq. (8) to train the generator.

However, there are some unclear issues.

First, the differences between losses may not fully explain the behavior of GANs [I. J. Goodfellow. NIPS 2016 tutorial: Generative adversarial networks. arXiv:1701.00160, 2017], as also seen from some recent studies. For example, using the sum of two divergences [Chen et al., 2018] does not make a significant improvement. 
Also shown in Table 1, CQ performs better than VAE, but is inferior to GAN.
Using the two-term loss Eq. (8) may not be the key for improvement.

Only after adding the additional flow-based layers, CQF outperforms GAN. Therefore, in Table 1, it would be better to include the result of VAE using the additional flow-based layes for ablation study. 

Second, it is also not clear from the paper that such VAE using the additional flow-based layes is new or not.

Third, the results are not as good as the state-of-the-art. In Table 4, SN-GANs perform the best in three out of four cases.

Fourth, the model consists of three networks - encoder, decoder and discriminator. Evaluation about the model's inference capability is necessary in addition to showing its generation capability, since it is equipped with a decoder.

Some typos:
P5: L_Q(p*)+L_Q(p*) ?
Table 4: QSF ?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1eUt8Qma7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author response to reviewer 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylxxhRctX&amp;noteId=S1eUt8Qma7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1045 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1045 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your review. We address the main points of this review below and are happy to discuss further if some points are unclear. We will shortly post a revised version of our paper taking into account this feedback. 

Our main contribution is a model that can be evaluated in terms of likelihood while also producing compelling samples. Our approach addresses the main drawbacks of existing generative models: For GANs: mode-dropping and the lack of quantitative measures to assess the model fit to held out data. For likelihood-based models: the inferior sample quality as compared to GANs.


[Also shown in Table 1, CQ performs better than VAE, but is inferior to GAN.
Using the two-term loss Eq. (8) may not be the key for improvement.]

 In Table 1, the VAE, GAN and CQ generators have the same architecture and capacity. The use of Eq(8) forces CQ to cover the full support of the dataset. In contrast, a GAN can focus on a subset of the support, and offer better quality on this subset alone. Thus, for a fixed model capacity, the use of Eq (8) is expected to make CQ inferior to GAN in terms of quality (IS, FID), and superior in terms of coverage (BPD). This is indeed what our results show in practice. In this setting, Eq (8) is the only key to improvement in terms of quality over VAE, and in terms of coverage over GAN.


[Only after adding the additional flow-based layers, CQF outperforms GAN. Therefore, in Table 1, it would be better to include the result of VAE using the additional flow-based layers for ablation study.]

The performance of a VAE with NVP flow layers trained by MLE is already in the paper, in Table 3b (where we evaluated different configurations of the NVP layers). We agree with your suggestion that it is useful to already present one setting in Table 1, and will do so in our  revision. The results show that with the additional NVP layers, the model is better in all three measures as compared to the baseline VAE. As compared to CQF,  the model trained with MLE is better in BPD, and worse in IS and FID, as expected. As compared to the standard VAE, IS grows by 1.0 point from 2.0 to 3.0 IS, and FID decreases from 171.0 to 112.0. These improved values are still far from typical GAN performance. Samples in appendix B also show over-generalization typical of MLE. This shows, qualitatively this time, that flow alone is not enough, and the adversarial loss in  Eq(8) is essential. CQF then experimentally demonstrates that the key to achieving full performance is to use the two together. 
We will clarify this in the main text, by adding more qualitative examples. 


[Second, it is also not clear from the paper that such VAE using the additional flow-based layes is new or not.]

To the best of our knowledge, we are the first to propose and report experimental results of a VAE model with NVP layers in the decoder. The NVP decoder layers remove the naive factorized Gaussian assumption typical in VAE decoders, and significantly improves data log-likelihood (BPD) and sample quality. We will underline this in the introduction of the paper. 

The third and fourth questions are adressed in the next response.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1eXzImmp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>End of author response to reviewer 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylxxhRctX&amp;noteId=B1eXzImmp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1045 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018 (modified: 10 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1045 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Third, the results are not as good as the state-of-the-art. In Table 4, SN-GANs perform the best in three out of four cases.]

We would like to point out that the fourth measure, BPD, is simply unavailable for GAN methods, since they have a degenerate low-dimensional support in the image space, and lack a mechanism to evaluate the density on the support. In particular held-out train data is likely to be off the low dimensional support, and have an infinite negative log-likelihood. This drawback is fixed in likelihood based models (including ours) that have a full support over the image space, and offer exact or approximate likelihood evaluation. 

Although you are right that our approach does not beat the state of the art IS and FID numbers on the CIFAR-10 and STL datasets, it does give results that are fairly close (and better in one case). This is the contribution of our paper: showing that we can have (near) state of the art sample quality and a full support and likelihood evaluation on held-out train data.
For our model  CQF [+Residual, +flow, +large D] (Ours) the comparison is as follows: 
IS CIFAR-10: ours 8.1, state-of-the-art 8.2 (higher is better)
IS STL : ours 8.6, state-of-the-art 9.1 (higher is better)
FID CIFAR-10: ours 18.6, state-of-the-art 21.7 (lower is better)
FID STL : ours 52.7, state-of-the-art 40.1 (lower is better)


[Fourth, the model consists of three networks - encoder, decoder and discriminator. Evaluation about the model's inference capability is necessary in addition to showing its generation capability, since it is equipped with a decoder.]

To evaluate our modelâ€™s inference capability, we will include reconstructions of encoded images in the revision of our paper. As is typical for flexible VAEs (IAF for instance) they are indistinguishable from the corresponding ground truth images to the naked eye, and testify outstanding inference performance. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJgraPppn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>see below</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylxxhRctX&amp;noteId=HJgraPppn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1045 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1045 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper presents the use of an invertible transformtion layer in addition to the conventional variational autoencoder to map samples from decoder to image space, and shows it improves over both synthesis quality and diversity. 
The paper is well motivated, and the main motivation is nicely presented in Fig.1, and the main idea clearly shown in Fig.2 in an easy-to-understand manner. Existing works are properly discussed in the context before and after the main method. Convincing results are presented in the experimental section, with ablation tests in Tables 1-3, quantitative comparison in Table 4, and qualitative visual images in Figs.4-5. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1gksEQmpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author response to reviewer 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylxxhRctX&amp;noteId=r1gksEQmpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1045 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1045 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review and appreciation of our work. We are happy to discuss any questions that you may have during the discussion period.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>