<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rkl42iA5t7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES" />
      <meta name="og:description" content="Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rkl42iA5t7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES</a> <a class="note_content_pdf" href="/pdf?id=rkl42iA5t7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019network,    &#10;title={NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rkl42iA5t7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=rkl42iA5t7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint. We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a heuristic that leads to a parameter-free approach that automatically selects the compression used at each layer. Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively. In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio. Finally, we show how PFA is an effective tool for simultaneous compression and domain adaptation.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Artificial Intelligence, Deep learning, Machine learning, Compression</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprints.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BkgKD7doaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>New version of the manuscript available</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkl42iA5t7&amp;noteId=BkgKD7doaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper700 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper700 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have uploaded a new version of our paper to address reviewers' comments. Here are the highlights of the changes:

- We have added the references mentioned by the reviewers.
- We have clarified a few sentences to avoid misunderstandings.
- We have included a section regarding the complexity analysis of PFA, as well as the actual time required by PFA to run on each layer of VGG-16 using the ImageNet dataset.
- We have included compression results on the ImageNet dataset.
- We have corrected the numbers in Table 1 regarding the FGA algorithm.

Thanks to the changes recommended by the reviewers, the claims in the paper are strengthened.  Independently of the architecture or dataset, PFA consistently provides better compression or accuracy than the state of the art.

We would like to thank the reviewers once more for their valuable feedback. We hope they will find the changes satisfactory or we will wait for new feedback.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJlRitO53Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting approach, second time reviewing</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkl42iA5t7&amp;noteId=HJlRitO53Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper700 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper700 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a compression method based on spectral analysis. The basic idea is to analyse correlation between responses of difference layers and select those that are more relevant discarding the others. That, in principle (as mentioned in the paper) differs from other compression methods based on compressing the weights independently of the data being used. Therefore, in theory (nothing shown), different task would provide different outputs while similar works would compress in the same manner. 

Then, the paper proposes a greedy algorithm to select those filters to be kept rather than transforming the layer (as it has been usually done in the past [Jaderberg et al]). This is interesting (from a practical point of view) as would lead to direct benefits at inference time. 

This is the second time i review this paper. I appreciate the improvements from the first submission adding some interesting results. 

I still miss results in larger systems including imagenet. How all this approach actually scales with the complexity of the network and the task?

There have been recent approaches incorporating low-rank approximations that would be interesting to couple with this approach. I am surprissed these are not even cited ('Compression aware training' at NIPS 2017 or Coordinating filters at ICCV2017 both with a similar approach (based on weights tho). Pairing with those approaches seems a strong way to improve your results. 


</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Skeeao8ma7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Evidence of our claim, and preview of the results on ImageNet (including complexity analysis and actual times)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkl42iA5t7&amp;noteId=Skeeao8ma7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper700 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper700 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We appreciate your review and would like to thank you for your feedback and time. Please find our answers below.

&gt; Therefore, in theory (nothing shown), different task would provide different outputs while similar works would compress in the same manner.

The experiments presented in section 4.5 and Figure 3 provide evidence that with PFA different complexities in the tasks lead to different architectures; similar tasks lead to similar architectures. We will clarify the connection between the claim and Section 4.5. In those experiments, we show how the architecture produced by PFA (starting from the same trained model) differs depending on the complexity of the task. For example, VGG-16 trained on CIFAR-100 and then compressed using 10 labels (R1 line in Figure 3.b) has 150 filters in the last layer (and overall it has 52% of the original filters), whereas a simpler task with 2 labels (S4 line in Figure 3.b) has only 45 filters in the last layer (and overall it has 36% of the original filters). The compression obtained in the two cases is clearly different and reflects the complexity of the tasks with respect to what the trained model has already learnt. On the other hand, the two tasks with 10 labels (R1 and R2 lines) lead to similar compression.

&gt; I still miss results in larger systems including imagenet. How all this approach actually scales with the complexity of the network and the task?

We have just completed the experiments on ImageNet. Here is a summary. In terms of complexity, we have a dedicated section in Appendix C. However, we will consider move the main conclusions to the main paper. In summary, the complexity of the PFA algorithm per layer is O(mn^2 + n^3), where n is the number of filters, and m is the number of samples. The task does not affect the complexity because the labels are not used in PFA. For the experiment on ImageNet (m=1.2M, ILSVRC2012) PFA took the longest on the last two layers (n = 4096) which were computed in roughly 120 seconds. Here is the full table of times when computing PFA sequentially (non-parallel CPU implementation):

block 0 (64 filters)
conv0: 1.19s
conv1: 1.28s

block 1 (128 filters)
conv0: 2.74s
conv1: 2.83s

block 2 (256 filters)
conv0: 4.26s
conv1: 4.83s
conv2: 4.78s

block 3 (512 filters)
conv0: 8.92s
conv1: 9.43s
conv2: 9.92s

block 4 (512 filters)
conv0: 9.18s
conv1: 9.22s
conv2: 9.22s

fully connected block (4096 filters)
fc1: 142.17s
fc2: 112.74s

As mentioned in Appendix C, PFA has to run once at the end of the training step and as shown above the time consumed by PFA is a negligible compared to the whole training time. In exchange for this marginal extra-time PFA provides the long-term benefit of a smaller footprint and faster inference, which in the lifetime of a deployed network will quickly surpass the time initially required by PFA. In addition, when working in a setting where the network is periodically re-trained with new incoming data, having a smaller network will add to the saved time.

In terms of performance, these are the results of PFA-KL on ImageNet
                                                        Footprint;  Top1 change; Top5 change
PFA-KL from scratch                   69.30%	    -1.89%             -0.97%
PFA-KL with filter selection        69.30%	    +2.39%            +1.41%

&gt; There have been recent approaches incorporating low-rank approximations that would be interesting to couple with this approach. I am surprissed these are not even cited ('Compression aware training' at NIPS 2017 or Coordinating filters at ICCV2017 both with a similar approach (based on weights tho). Pairing with those approaches seems a strong way to improve your results.

Thank you for the references. We will add them to our state-of-the-art review. Both techniques are interesting and could further improve the compression rate of PFA. The approach of modifying the original loss in order to induce a specific property in the full model is smart but goes against the philosophy that we have adopted for PFA. We envision PFA to be easy to use: no-parameter strategy (PFA-KL), no need to modify the original loss function (which would require additional hyper-parameters tuning), and the ability to start from pre-trained models or to use known training hyper-parameters. Nevertheless we are eager to consider how these other techniques could be paired with PFA in future work and see what level of improvement can be achieved.

We hope we have answered all your concerns. Please let us know if you think there are more opportunities for improvement.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJlCzy7q37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A decent pruning strategy.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkl42iA5t7&amp;noteId=BJlCzy7q37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper700 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper700 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes to prune convolutional networks by analyzing the observed correlation between the filters of a same layer as expressed by the eigenvalue spectrum of their covariance matrix. The authors propose two strategies to decide of a compression level, one based on an eigenvalue threshold, the other one based on a heuristic based on the KL divergence between the observed eigenvalue distribution and the uniform one. This is a bit bizarre but does not require searching a parameter. Once one has decided the number of filters to keep, one can either retrain the network from scratch, or iteratively remove the most correlated filters, which, unsurprisingly, work better.

The authors perform credible experiments on CIFAR-10 and CIFAR-100 that show the results one would expect. They should probably have run ImageNet experiments because many earlier papers on this topic use it as a benchmark and because the ImageNet size often reveals different behaviors.

In conclusion I believe this is a very decent paper, but not a very exciting one.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJxNw2I7Tm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Preview of the results on ImageNet</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkl42iA5t7&amp;noteId=HJxNw2I7Tm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper700 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper700 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your positive comments and for your feedback and time. Please find a preview of the results on ImageNet below.

&gt; They should probably have run ImageNet experiments because many earlier papers on this topic use it as a benchmark and because the ImageNet size often reveals different behaviors. 

We agree with the reviewer. We have just completed the experiments on ImageNet. Here is a summary of the results.

                                                        Footprint;  Top1 change; Top5 change
PFA-KL from scratch                   69.30%	    -1.89%             -0.97%
PFA-KL with filter selection        69.30%	    +2.39%            +1.41%

We will report these results. In addition to the experiments on ImageNet, are there other enhancements that would elevate the paper?
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ryl3i9nH37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting hyper-parameter free compression, but missing experiments and clarification/corrections needed</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkl42iA5t7&amp;noteId=ryl3i9nH37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper700 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper700 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper introduces an approach to compressing a trained neural network by looking at the correlation of the filter responses in each layer. Two strategies are proposed: one based on trying to preserve the energy of the original activations and one based on looking at the KL divergence between the normalized eigenvalues of the activation covariance matrix and the uniform distribution.

Strengths:
- The KL-divergence-based method is novel and has the advantage of not requiring to define any hyper-parameter.
- The results show the good behavior of the approach.

Weaknesses:

Method:
- One thing that bothers me is the spatial max pooling of the activations of convolutional layers. This means that is two filters have high responses on different regions of the input image, they will be treated as correlated. I do not understand the intuition behind this.
- In Section 2, the authors mention that other methods have also proposed to take the activation into account for pruning, but that they aim to minimize the reconstruction error of these activations. In fact, this is also what PFA-En does; for a given dimension, PCA gives the representation that minimizes the reconstruction error. Therefore, the connection between this method and previous works is stronger than claimed by the authors.
- While it is good that the KL-divergence-based method does not rely on any hyper-parameter, the function \psi used in Eq. 3 seems quite ad hoc. As such, there has also been some manual tuning of the method.

Experiments:
- In Table 1, there seems to be a confusion regarding how the results of FGA are reported. First, in (Peng et al., 2018), the %FLOPS is reported the other way around, i.e., the higher the better, whereas here the lower the better. Similarly, in (Peng et al., 2018), a negative \Delta in accuracy means an improved performance (as stated in the caption of their Table 2, where the numbers reported here were taken). As such, the numbers reported here, and directly taken from this work, are misinterpreted. 
- Furthermore, Peng et al., 2018 report much better compression results, with %FLOP compression going up to 88.58%. Why are these results not reported here? (To avoid any misunderstanding, I would like to mention that I am NOT an author of (Peng et al., 2018)).
- Many of the entries in Table 1 are empty due to the baselines not reporting results on these datasets or with the same network. This makes an actual comparison more difficult.
- Many compression methods report results on ImageNet. This would make this paper more convincing.
- While I appreciate the domain adaptation experiments, it would be nice to see a comparison with Masana et al., 2017, which also considers the problem of domain adaptation with network compression and, as mentioned in Section 2, also makes use of the activations to achieve compression.

Related work:
- It is not entirely clear to me why tensor factorization methods are considered being so different from the proposed approach. In essence, they also perform structured network pruning.
- The authors argue that performing compression after having trained the model is beneficial. This is in contrast with what was shown by Alvarez &amp; Salzmann, NIPS 2017, where incorporating a low-rank prior during training led to higher compression rates.
- The authors list (Dai et al., 2018) as one of the methods that aim to minimize the reconstruction error of the activations. Dai et al., 2018 rely on the mutual information between the activations in different layers to perform compression. It is not entirely clear to me how this relates to reconstruction error.

Summary:
I do appreciate the idea of aiming for a hyper-parameter-free compression method. However, I feel that there are too many points to be corrected or clarified and too many missing experiments for this paper to be accepted to ICLR.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJxU4AU76X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Answers to comments on weaknesses</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkl42iA5t7&amp;noteId=rJxU4AU76X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper700 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper700 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comments. We appreciate your review.

&gt; ... spatial max pooling... I do not understand the intuition behind this.

Pooling is a relaxation to ease the next step in the process. Jordao et al. 2018 compares different forms of pooling for compression: global max pooling (as in PFA), avg pooling and a spatial preserving max pooling. They observe that global max pooling performs the best.

The intuition is that if two filters are correlated they might be redundant for the end task, even if they learn different features. For example, in order to decide if an image contains a face there is no need to detect nose, mouth,  eyes, etc... one (or more) of these features might be sufficient. That said, we agree that exploring alternatives to max pooling is a potential for future research.

&gt; ... other methods have also proposed to take the activation into account for pruning, ... but they aim to minimize the reconstruction error.... In fact, this is also what PFA-En does;

PFA-En uses the spectral energy of the filters' responses only to decide how many filters should be preserved. Our filter selection does not account for the reconstruction error.

&gt; While it is good that the KL-divergence-based method does not rely on any hyper-parameter, the function \psi used in Eq. 3 seems quite ad hoc. As such, there has also been some manual tuning of the method.

\psi could be tuned to a given task, though we do not. The proposed \psi is the function that empirically worked the best in an initial evaluation, and has not been tuned in any of our experiments.

&gt; In Table 1, there seems to be a confusion regarding how the results of FGA are reported. ...

Thank you for spotting this mistake.

&gt; Peng et al., 2018 report much better compression results, with %FLOP compression going up to 88.58%. Why are these results not reported here?

Table 1 is meant to provide a numerical comparison for similar compression rates. Fig. 2b provides all compression rates for FGA (Peng et al.). However, we made the same mistake in reporting the numbers for FGA. Here is the correct comparison:
     Footprint; Accuracy change
FGA  11.31%;   -1.93%
PFA   9.64%;    -2.37%

FGA  13.20%;    -0.57%
PFA  14.37%%;  -0.27%

FGA  18.54%;    -0.02%
PFA  19.27%;    +0.50%

FGA  39.67%;    +0.39%
PFA  43.11%;    +1.70%

&gt; Many of the entries in Table 1 are empty ... This makes an actual comparison more difficult.

We agree. Sadly there is no agreed benchmark. We hope, that the amount of comparison provided is enough for the reader to form an opinion. Such opinion should also be influenced by other factors (beyond numbers): easy of implementation, practicality, ease of parameters tuning, etc...

&gt; Many compression methods report results on ImageNet. This would make this paper more convincing.

We have just completed the experiments on ImageNet. Here is a summary:
                                                        Footprint;  Top1 change; Top5 change
PFA-KL from scratch                   69.30%	    -1.89%             -0.97%
PFA-KL with filter selection        69.30%	    +2.39%            +1.41%

&gt; While I appreciate the domain adaptation experiments, it would be nice to see a comparison with Masana et al., 2017.

Our results on domain adaptation are meant to support our claim that with PFA different complexities in the tasks lead to different compressions.

&gt; It is not entirely clear to me why tensor factorization methods are considered being so different from the proposed approach.

In the paper, by “tensor factorization” we refer to those algorithms that split weight tensor into a sequence of smaller tensors. By “structured pruning” we refer to those algorithms (like PFA) that remove full filters from the current layer, without attempting to replace or approximate them. This terminology follows other discussions, such as Peng et al. 2018, Liu et al. 2017, and Li et al. 2017.

&gt; The authors argue that performing compression after having trained the model is beneficial. This is in contrast with what was shown by Alvarez&amp;Salzmann, NIPS 2017.

Alvarez&amp;Salzmann (A&amp;S) claim that is beneficial to modify the original loss in order to induce some properties in the full model in order to ease compression. This is not in contrast with our findings. The difference between A&amp;S and PFA is that PFA does not need to modify the loss. From here on, the workflow is the same: we both compress after training and fine-tune the compressed model.

&gt; I do appreciate the idea of aiming for a hyper-parameter-free compression method. However, I feel that there are too many points to be corrected or clarified and too many missing experiments for this paper to be accepted to ICLR.

Thank you for your time and consideration; we hope you will find our answers and new experiments satisfactory. We believe our work is stronger after addressing your concerns. Please let us know if you have further questions and consider updating your rating.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>