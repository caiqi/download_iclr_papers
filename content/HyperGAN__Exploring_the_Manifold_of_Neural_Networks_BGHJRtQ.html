<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>HyperGAN:  Exploring the Manifold of Neural Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="HyperGAN:  Exploring the Manifold of Neural Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1GHJ3R9tQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="HyperGAN:  Exploring the Manifold of Neural Networks" />
      <meta name="og:description" content="We introduce HyperGAN, a generative adversarial network that learns to generate all the parameters of a deep  neural network. HyperGAN first transforms low dimensional noise into a latent space..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1GHJ3R9tQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>HyperGAN:  Exploring the Manifold of Neural Networks</a> <a class="note_content_pdf" href="/pdf?id=B1GHJ3R9tQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019hypergan:,    &#10;title={HyperGAN:  Exploring the Manifold of Neural Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=B1GHJ3R9tQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We introduce HyperGAN, a generative adversarial network that learns to generate all the parameters of a deep  neural network. HyperGAN first transforms low dimensional noise into a latent space, which can be sampled from to obtain diverse, performant sets of parameters for a target architecture. We utilize an architecture that bears resemblance to adversarial autoencoders, but with the data term substituted to be classification loss, which is equivalent to minimizing the KL-divergence between the generated network parameter distribution with a unknown true parameter distribution. We apply HyperGAN to classification, showing that HyperGAN can learn to generate parameters which solve the MNIST and CIFAR-10 datasets with competitive performance to fully supervised learning, while learning a rich distribution of effective parameters. We also show that HyperGAN can also provide better uncertainty than standard ensembles. We show this by evaluating the robustness of HyperGAN-generated ensembles to domain-shift, testing with out of distribution data as well as adversarial examples. We see that in addition to being highly accurate on inlier data, HyperGAN can provide reasonable uncertainty estimates.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">hypernetworks, generative adversarial networks, anomaly detection</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We use a GAN to generate parameters of a neural network in one forward pass.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">5 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BkgFAhVU3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1GHJ3R9tQ&amp;noteId=BkgFAhVU3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper984 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper984 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a technique for learning a distribution over parameters of a neural network such that samples from the distribution correspond to performant networks. The approach effectively encourages sampled parameters to have low loss on the training set, and also uses an adversarial loss to encourage the distribution of parameters to be Gaussian distributed. This approach can improve performance slightly by using ensembling and can be useful for uncertainty estimates for out-of-distribution examples. The approach is tested on a few simple problems and is shown to work well.

I am definitely in favor of exploring adversarial divergences (using a critic as a differentiable loss to compare two distributions) in unusual settings, and this paper certainly does this. The idea of transforming samples from a prior such that the transformed sample corresponds to useful network parameters is interesting. The results also seem promising. However, currently the mathematical description of this method is completely unclear and ridden with many errors. I can understand at a reasonable level what the approach is doing from Figure 1, but the definitions and equations given in Equation 3 are at times nearly incomprehensible "mathiness". I'm giving the paper a borderline accept because the idea is interesting and the results are OK; I will raise my score if Section 3 is dramatically improved. I give some specific examples of issues with Section 3 in my specific comments below. I'd also note that the paper does a somewhat poor job comparing to existing work - only section 4.2 includes a comparison to existing "uncertainty" methods. This should also be improved - the authors should implement the existing methods and use them as a point of comparison in all of their experiments. As a final high-level note, the approach is described at various points as an "autoencoder" particularly in reference to the adversarial autoencoder. However, the approach does not "autoencode" anything - there is no reconstruction term, or input apart from the noise samples. The only thing it has in common with the adversarial autoencoder is the use of a critic to enforce a distributional constraint. Calling it, or comparing it to, an autoencoder is confusing and misleading.

Specific comments:

- You mention fast weights in related work. I believe Hinton and Plaut were the first to propose fast weights in "Using Fast Weights to Deblur Old Memories", and I'd also suggest mentioning "Using Fast Weights to Attend to the Recent Past" which is a more recent demonstration that fast weights can be useful on modern problems.
- The are some issues with your description of Equation 1: First, I don't believe you define G(z) (I assume it is the "decoder" network; please define it). Second, in practice I don't believe you actually use JSD or MMD for D_z; you use a critic architecture which in some limit approximates some statistical divergence but in practice they typically don't (see e.g. Arjovsky and Bottou 2017; Fedus et al. 2017; Rosca et al. 2018). Third, writing Q_z \sim Q(z | x) seems strange to me - Q_z is a distribution, and I don't believe that Q(z | x) is a distribution over distributions, so how are you sampling a distribution (Q_z) from Q(z | x) as suggested by the use of the \sim notation? I think you simply mean that Q_z is Q(z | x) approximately marginalized over x.
- Equation 2 is also not clear. First, the sentence before starts "Suppose the real parameters \theta^* \sim \Theta..." The equation itself does not include \theta^* or \Theta so I don't see what this is referring to. Second, the expression for an m-dimensional is written \mathcal{N}(0, \sigma^2, I_m). It's not clear why there is a comma before I_m, and I_m is not defined (though I assume it is the m \times m identity matrix) - did you mean to multiply I_m by \sigma^2? Third, it looks like you actually define P_z twice, once as "an m-dimensional isotropic Gaussian" and again as "a Kd-dimensional isotropic Gaussian"; am I to infer that m = Kd? Why use both? Fourth, you mention the joint P(x, y) but the expectation is taken over P_x and P(y | x). Why call it P_x and not P(x)? And why not compute the expectation over P(x, y)? Fifth, you write "Here the encoder..." -- you never define that Q(z) or G is "the encoder", I assume Q(z). It is strange to take the expectation over Q(z) (I assume sampling z \sim Q(z)) but then have the term Q(z) appear in (2). How are Q(z) and Q_z related? On that note, I don't see how (2) is an autoencoder, since there is no Q(z | x) term. It appears instead that you are sampling z from Q(z) which doesn't condition on x. So what is being autoencoded? Related, you write "all the q_k (that will generate different layers) will be correlated, unlike dimensions of z which are drawn to be independent from each other." But if Q(z) = [q_1, ..., q_K] then doesn't the secont term in (2) suggest that they are being enforced to be similar to the prior P_z, and therefore uncorrelated? Note that you also say later on "The job of the regularizer D_z(P_z, Q_z) is to force each embedding q_n to approximate P_z." Frankly at this point I will stop pointing out issues with this equation and discussion since they are so widespread.
- In your definition of your ensemble scoring rule, you are taking the sum over N + 1 elements (n = 0 to N) but dividing by N.
- In 4.2, do you use the same model architecture/training/regularization etc. as in previous studies? If not I think comparing the different methods will be conflated by differences in training procedures. Since you do not report results in many experimental settings, I assume you don't.
- In Figure 3, why not plot the true standard deviation around the true function? It appears you are only plotting +/- 3 stndard deviations for the learned function.
- Why not include 100 models L2 on Figure 3?
- It's not clear to me why you define your "disagreement d" when it appears the same as the entropy score you used in 4.4.
- A stronger and more convincing attack would be to attack the ensemble of models, instead of attacking a single model and testing on the ensemble.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJee6VaBhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good experimental results but lacking rigour</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1GHJ3R9tQ&amp;noteId=SJee6VaBhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper984 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper984 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This works propose a new approach to learn to sample (or generate) the parameters of a deep neural networks to solve a task. They propose a new architecture inspired by hyper networks and adversarial auto-encoders, where the parameters of the networks are generated from a low dimensional latent space. By using an ensemble of networks sampled with their approach they're able to get state of the art results on uncertainty estimation.

The notations are confusing and the paper contains several mistakes. In particular:
- P_z is used to represent different distributions. It sometimes refers to the distribution of the latent variables and sometimes to the prior over the weight embeddings. Different notation should be used to represent different quantity.
- D_z sometimes refers to the regularization term or to the discriminator.
- Eq 2. I believe there is a bug in the equation, the expectation is over Q(z) but it should be P_z (distribution of the latent variable z), otherwise it doesn't make much sense.
- The equation for the cross entropy is wrong. If y_i are the true labels and F(x_i, theta) is the prediction then it should be y_i*log(F(x_i, theta)).
- It's not clear if the loss of the discriminator should be maximized for the parameters of the discriminator and minimized with respect to the parameters of the encoder. Furthermore it would be interesting to study what is the impact of this particular choice of loss for the discriminator. In particular I invite the author to compare the loss proposed to the loss in [1].
Fixing these, would make the paper much easier to understand.

The authors motivates their approach by drawing a link with wasserstein (WAE) and adversarial auto-encoders. While this could be interesting I think this link should be made more formal. 
Indeed, the WAE is derived from the wasserstein distance between the true data distribution and the distribution of the model. However it's not clear if the approach proposed can still be derived from such a principle. I would invite the author to make the link between wasserstein distance minimization and their approach more explicit.

To my knowledge the method proposed is novel, however using implicit posterior to learn the weights is not novel and several other works have looked at it. In particular I think [1,2] should be discussed in the related work. 
The difference with traditional bayesian approach such as variational inference should also be discussed, since the approach is really close to approximating the posterior with an implicit distribution and computing the KL term using a GAN (like in [3,4]).

I think one interesting novelty that needs to be emphasized is that the model has both: parameters that are point estimates (the parameters of the generators) and parameters that are sampled from a posterior distribution (the weight embeddings). 

Pros:
- Good and promising experimental results.

Cons:
- The paper combines several tricks and ideas but it's not really clear what is important and why such an approach works. For example how important is the latent space and the encoder ? Could we just sample directly the weight embeddings from a gaussian and remove the regularization ?
- The other points mentioned above about the clarity of the paper.

Others:
- The title is misleading, the manifold is not really explored... If the author really want to explore the manifold some interesting questions are:  what happens if we try to interpolate between two latent variables ? What do the latent variables represent ? what's the influence of the dimension of the latent space ?
- In the experiments: what is the number of networks used for the other methods ?
- It would be nice to have a plot showing the accuracy as a function of the perturbation in section 4.5.

Conclusion:
The experimental results seem promising however the motivation for the approach is not clear. I think fixing some of the points mentioned above could greatly improve the clarity of the paper and make it a stronger submission. In the current state I don't believe the paper is rigorous enough to be accepted.

References:
[1] Pawlowski, N., Rajchl, M., &amp; Glocker, B. (2017). Implicit weight uncertainty in neural networks. arXiv:1711.01297.
[2] Wang, K. C., Vicol, P., Lucas, J., Gu, L., Grosse, R., &amp; Zemel, R. (2018, July). Adversarial Distillation of Bayesian Neural Network Posteriors. ICML
[3] Mescheder, L., Nowozin, S., &amp; Geiger, A. (2017, July). Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks. ICML
[4] Huszár, F. (2017). Variational inference using implicit distributions. arXiv:1702.08235.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkgVi_vVhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice idea</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1GHJ3R9tQ&amp;noteId=rkgVi_vVhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper984 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper984 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">TL;DR. I find the manuscript to contain interesting ideas, yet I believe there is room for improvement.

* Summary

For any given specific network architecture, the manuscript aims at learning a distribution over the weights (rather than point-wise estimates of the weights). This is achieved through using a two-steps procedure, in which a "hypernetwork" is trained to output weights for the network of interest, and a GAN is then used to (adversarially) generate samples from a distribution $Q$ which is assumed not too far (in a KL sense) from a Gaussian prior $P$.

* Major issues

I find the central idea to be of interest to the ICLR community. However I have found a number of shortcomings to be addressed before I could recommend acceptance. The following list is in no particular order.

- References: 20 out of 22 (!) references are preprints, about half of which are 3+ years old. Most of them are now published in proceedings and I strongly feel proper credit should be given to authors by replacing outdated preprints with correct citations.
- Links with Bayesian deep learning: I feel this should be more carefully discussed in the manuscript. The sentence "We have proposed a generative, non-Bayesian solution [...]" should be explained, as from what I gather HyperGAN samples (GAN-like) weights (i.e., networks) from a distribution $Q$ which is deemed not too far (in the KL sense) from a prior distribution $P$. How is that not Bayesian?
- Numerical experiments. Table 4: what are the numbers reported? If single evaluation, I do not believe any conclusion may be drawn. If averages over multiple repetitions, no conclusion can be drawn without reporting standard deviations. In addition, I do not quite grasp the purpose of the 1D toy example.
- Overall, I think the authors should try and make their contributions and method clearer. For example, a pseudo-code of the whole procedure might help readers understand the gist. 
- Architecture specific: I find the claim that HyperGAN explores the manifold of neural nets too strong. As the whole procedure is architecture-specific, I would find more appropriate to change that claim to "exploring the weights distribution for a specific architecture".
- Code availability: the scope of the paper is diminished by the fact that no code is available by the time of review. A toolbox (not disclosing the authors' identities) should be made available to support the manuscript claims. Last sentence (page 8) is likely to be outdated and should be removed. 

* Minor issues

- some typos: architecture (caption figure 1), $G(Q(z))$ (missing parenthesis, page 3), sum index $n$ not used in the last equation (page 7).
- "Perhaps the first proposed method..." (page 2). Such imprecise statements must be avoided.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1gR6GG7cm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A closely related paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1GHJ3R9tQ&amp;noteId=r1gR6GG7cm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~James_Lucas1" class="profile-link">James Lucas</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Oct 2018</span><span class="item">ICLR 2019 Conference Paper984 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Previous work has addressed parameter generation using GANs. Please check out this paper: <a href="http://proceedings.mlr.press/v80/wang18i.html" target="_blank" rel="nofollow">http://proceedings.mlr.press/v80/wang18i.html</a>

In my opinion, this work still has novelty but a discussion/comparison seems due.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HklazP-Oc7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for the input about related work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1GHJ3R9tQ&amp;noteId=HklazP-Oc7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper984 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Oct 2018 (modified: 12 Oct 2018)</span><span class="item">ICLR 2019 Conference Paper984 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for alerting us of this recent work we missed. We will be sure to cite and compare against it in the final paper. 
Our paper is, although, quite different from the aforementioned reference [1]. The offline setting in [1] is the approach we considered and didn't take (page 3) - to first train many networks and train a GAN on them. In the online setting in [1], a GAN is trained from samples taken from a single training procedure, with noise added from MCMC. The MCMC samples and GAN updates are interleaved. With this approach, the parameters that are used to train the GAN are highly correlated with each other and the diversity of the networks that the trained GANs can generate is questionable.

In contrast, HyperGAN generates networks purely from random noise samples. Our approach doesn't use correlated examples in training and hence we believe it can generate more diverse networks (Table 3). We also show that HyperGAN can achieve higher classification accuracy on testing data and ensembles of models help significantly, showcasing the diversity of generated networks. 

[1] Wang, K.-C., Vicol, P., Lucas, J., Gu, L., Grosse, R., and Zemel, R. Adversarial Distillation of Bayesian Neural
Network Posteriors. In Proceedings of the 35th International Conference on Machine Learning, 2018</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>