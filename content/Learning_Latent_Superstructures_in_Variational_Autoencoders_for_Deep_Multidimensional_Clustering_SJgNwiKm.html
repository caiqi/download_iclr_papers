<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learning Latent Superstructures in Variational Autoencoders for Deep Multidimensional Clustering | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learning Latent Superstructures in Variational Autoencoders for Deep Multidimensional Clustering" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SJgNwi09Km" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learning Latent Superstructures in Variational Autoencoders for..." />
      <meta name="og:description" content="We investigate a variant of variational autoencoders where there is a superstructure of discrete latent variables on top of the latent features. In general, our superstructure is a tree structure..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SJgNwi09Km" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning Latent Superstructures in Variational Autoencoders for Deep Multidimensional Clustering</a> <a class="note_content_pdf" href="/pdf?id=SJgNwi09Km" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019learning,    &#10;title={Learning Latent Superstructures in Variational Autoencoders for Deep Multidimensional Clustering},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SJgNwi09Km},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We investigate a variant of variational autoencoders where there is a superstructure of discrete latent variables on top of the latent features. In general, our superstructure is a tree structure of multiple super latent variables and it is automatically learned from data. When there is only one latent variable in the superstructure, our model reduces to one that assumes the latent features to be generated from a Gaussian mixture model. We call our model the latent tree variational autoencoder (LTVAE). Whereas previous deep learning methods for clustering produce only one partition of data, LTVAE produces multiple partitions of data, each being given by one super latent variable. This is desirable because high dimensional data usually have many different natural facets and can be meaningfully partitioned in multiple ways.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">latent tree model, variational autoencoder, deep learning, latent variable model, bayesian network, structure learning, stepwise em, message passing, graphical model, multidimensional clustering, unsupervised learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We investigate a variant of variational autoencoders where there is a superstructure of discrete latent variables on top of the latent features.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Bke3mO223X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Well written, carefully thought through, and very interesting paper with impressive empirical results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJgNwi09Km&amp;noteId=Bke3mO223X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper253 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper253 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper introduces a new VAE model, the latent tree VAE (LTVAE), which aims to learn models with multifaceted clustering, that is separate clusterings are enforced on different subsets of the latent features.  This is achieved using a tree-structured prior on a set of discrete "super latent variables" (Y_1,...,Y_L) that identify which cluster the datapoint falls into for each separate facet (i.e. there is a separate clustering associated with each Y_n).  The subset of the standard latent variables z then form a Gaussian mixture model (GMM) for each Y_n.   Both the structure of this setup (i.e. the associated graphical model) and the parameters (i.e. means and variances of the clusters) are learned during training.  This introduces a number of computational challenges not usually seen in for VAE training, for which, seemingly well thought through, novel schemes are introduced, most notably a message passing scheme for calculating gradients of the log marginal p(z).

Overall I think this is a very good paper.  The exposition of the work is, for the most part, very good - the paper was a pleasure to read.  I think that the key idea is novel and adds something unique and useful to the literature, I thus think it is work which will be of substantial interest to the ICLR community.  The quality of the paper is also very good: algorithmic details seem to have been well thought through and the experimental evaluation is above average, both in terms of apparent performance and in the breadth of experiments considered.  I would very much like to see this work accepted to ICLR and I think that the extra use of space over 8 pages in the submission is justified.  However, I do have some questions and concerns that I would like to see addressed in the rebuttal period and I may lower my score if they are not.

The key issues I would like to see addressed further discussion on are:
a) There is no discussion about what is done for the encoder in the paper.  This is surely a very important consideration here as if the encoder is not expressive enough, this will impact the learned models.  For example, the dependency structures of the latent space induce particular dependencies in the posterior that must be carefully handled to avoid harming the learning (see e.g. <a href="https://arxiv.org/abs/1712.00287)." target="_blank" rel="nofollow">https://arxiv.org/abs/1712.00287).</a>
b) I would like to see some numerical results for the similarity between the different clusterings that are learned.  A lot of the novelty of the work rests on being able to pick up different clusterings with the different facets.  However, the results suggest that the clusterings may actually have very significant overlap and so this should be quantified.
c) The approach is presuming substantially slower than a setup where the structure is pre-fixed.  I think it is fine even if there is a big slow down, but I would like to see timing information so that the reader can assess how much higher the time cost is.
d) As far as I can tell (sorry if I have made a mistake), the presented results are from single runs.  I would like to see information about the variability across different runs so that the fragility of the approach can be assessed.
e) I would like to see more justification for having a dependency structure between the Y's, ideally both in motivating this choice and in experimental evaluation to check it (more generally ablation studies for different components of the algorithm would improve the paper).  Might it be possible to use this in a way the encourages the different clusterings to be distinct from one another?

Other comments:
1) Though the writing is generally very good, there are a few exceptions:
- The second paragraph in the intro becomes a list of related work from the point where DEC is introduced.  This should be moved to the related work to improve the flow (just cite those papers at the end of the first sentence in the third paragraph) and it would be good for it to be less of a list of separate things and more something that puts the current work in the context of other approaches.
- The paragraph after Eq 3 needs some rewriting
- The explanations around and including equations 5 and 6 were quite poor: \pi is referred to but not used, it is not made clear that that g is the gradient of log p(z) instead of p(z), use brackets for the log in Eq 6 to avoid ambiguity
2) The reference formatting is wrong (i.e. cite is used everywhere instead of citep)
3) I thought the motivation for the approach in the intro was very good
4) As the seemingly most related work, it would be good to elaborate more on the Goyal et al paper and the differences of your approach to theirs.  Is there a reason this is not used as a baseline in the experiments?
5) I could not understand the step from the gradient to the gradient of the log in Eq 6.  Is this because p(y_b|z) = f(y_b) Norm(..)?
6) The text in figures 2 and 3 is too small and difficult to make out.
7) I think it is misleading to talk about p(z) as being a marginal likelihood and would use the term marginal prior, or just marginal, instead.
8) I thought Figure 4b provided a nice demonstration.
9) Is there a reason that log likelihood / ELBO scores are only provided on MNIST and only for the LTVAE / VAE?  I might be wrong, but I thought at least some of the other baselines provide this and those results presumably already exist as a side effect from calculating the clustering scores?  Relatedly, I'm aware that a previous version of this work included estimates of the normalized mutual information -- is there any reason these are no longer included?
10) Did the larger dimensional latent spaced used for the qualitative results improve or worsen the performance of previous metrics?

Minor points / typos
- mehod -&gt; method
- of generation network -&gt; of the generation network
- brackets in eq 7
- MoG not defined in section 4.5</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1l3m3kUhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting approach, but experiments could have a more in-depth analysis</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJgNwi09Km&amp;noteId=H1l3m3kUhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper253 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper253 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose to augment the Variational AutoEncoder [1] with a latent prior modeled by a Gaussian Latent Tree Model [2], allowing to introduce a hierarchical structure of clusters in the learned representation. The LT-VAE not only learns the location of each cluster to best represent the data, but also their number and the hierarchical structure of the underlying tree. This is achieved by a three-step learning algorithm. Step 1 is a traditional training of the encoder and decoder neural networks to improve their fitting of the data. Step 2 is an EM-like optimization to better fit the parameters of latent prior to the learned posterior. And step 3 adapts the structure of the latent prior to improve its BIC score [3], which balances a good fit of the latent posterior with the number of parameter (and thus complexity) of the latent prior.

Experiments on synthetic data confirms the ability of the model to discover latent multifaceted clustering, and tests on 4 datasets shows it to be competitive with other unsupervised clustering models. Qualitative interpretation of samples from the learned model shows that the model learns a clustering that is clearly relevant to the data, while maybe not obvious to interpret.

The paper is well written and easy to follow (I however found a few typos and small mistakes that I'll list at the end of this review). The idea of using a structure on the latent prior of a VAE to learn a clustering of the data is not new, but the authors propose here an interesting approach to it, with a clearly described algorithm.

However, I would have liked to see a more in-depth analysis of the behavior of the model on the various datasets, and my reading of this paper raised several questions that found no answer:

1. What gains does the hierarchical structure on the Y variables provide? The paper does not analyze whether the models they trained actually learned conditional dependencies on the Y_i variables. How would this compare to the same model, with the only difference that the Y_i are fixed to be independent of each other (but still learning the number of Y_i and how the z_j are distributed between them) ?

2. This is linked to the previous one. On the tests of the dataset, how do the different facets interact with each other? How are the samples from the different clusters of facet 2 when facet 1 is fixed to a particular cluster? Assuming the learned dependency is that Y_1 is the parent of Y_2, does the interpretation of each value of Y_2 change depending on the value of Y_1?

3. The VAE with diagonal gaussian latent has a natural tendency to achieve sparcity in its latent space [4], making it robust to having too many latent neurons. Does this property hold with LT-VAE? If so, are the "unused" neurons organized in a particular way among the different learned facets?

I'd be reluctant to accept this paper without answers to points 1 and 2, which in my opinion are needed to justify the "tree" part of the "latent tree model" choice for the latent space. I'd also be very interested in an answer to point 3, which would give good insights regarding the design choices for applying this model to new problems (how important is the choice of the size of the latent space?), but I'm not considering it blocking acceptance.

[1] <a href="https://arxiv.org/abs/1312.6114" target="_blank" rel="nofollow">https://arxiv.org/abs/1312.6114</a>
[2] http://jmlr.org/papers/volume5/zhang04a/zhang04a.pdf
[3] https://projecteuclid.org/euclid.aos/1176344136
[4] https://arxiv.org/abs/1706.05148

--------------------------------

Notes and typos:

- In the introduction, "Deep clustering network network (DCN)", the word "network" is repeated 
- After equation 5, "... where \pi( . ) denotes the parent node ...", the "pi" symbol does not appear in the equation at all, neither in the following equation, so I guess this part of the sentence should be removed
- In section 3.3, you write that you define 5 operators, but follow by listing 7 (NI, ND, SI, SD, NR, PO and UP)
- In section 4.1, I believe W lives in R^(10x4) not R^(10x2)
- In section 4.5 the acronym "MoG" ("Mixture of Gaussian" I guess) is used without being introduced previously</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1xAvgXQs7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJgNwi09Km&amp;noteId=B1xAvgXQs7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper253 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper253 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a latent tree superstructure for the latent space of VAE’s. The idea itself is novel and interesting, and could have major impact in learning structured manifolds.

The overall presentation of the method is direct but slightly confusing. It seems that the zb grouping corresponds to different dimensions of the full z_i-vector of a single data point x_i. This should be made more explicit. 

The method itself has three levels of groupings: the zb’s, the conditioned variables Yb, and the connections between the Y’s. The method is also called a  Bayesian Network, but the paper seems to avoid defining it as a BN. I wonder if the method could be presented in a simpler form, if all the structure is necessary, and if the method could be defined directly as a BN. For instance, why do the Y’s have to have a hierarchical tree structure, wouldn’t a “flat” grouping into zb's be sufficient? 

In eq 2 the p(z) is defined as a mixture of Y-conditioned Gaussians, while in eq 4 its defined in the conventional encoder form N(z ; mu_x, sigma_x). These forms don’t seem to be compatible with each other. The term H seems to be entropy, but its not explained. It can’t be computed if we use the eq 2 definition of p(z). The interplay between these two structures is unclear. Furthermore, in fig 1 the tree is showed as a network (no arrows), while in fig 2 its a tree. I can’t find the definition for the dependencies P(Y | Y’), are these simply conditional density tables, or are they implicit? I also can’t see how are the \Sigma_{yb} defined. Are they of full rank? What is their dimension?

The inference sections are well motivated and efficient techniques are used. 

The synthetic experiment has 4 dimensional “z”, but the “W” matrix is 10x2, these do not match. What is the connection between Y_1 and Y_2 (in fig4 there is a dependency between)? Why is the dependency undirected if the model is a tree? The fig4b does not show ground truth to assess how well the model fits. The experiment should also include comparisons to the mentioned earlier works, and show how they perform. Why is there an arrow from the green scatter to the z3/z4? The main problem of the synthetic example is that it does not demonstrate why the tree structure learning is useful. The experiment should highlight a case where there is a natural latent tree structure corresponding to some realistic phenomena in real datasets.

The section 4.3. shows that the proposed method does find better representations of the MNIST than VAE, but does not mention that there are numerous extended VAE methods (and others) that would perform better than the LTVAE here. Those should be at least acknowledged, and preferably compared to.

The main results of the paper are very good with great performance in clustering, and the facets and clusters look great. The system has clearly learnt meaningful latent structures.

There are no learning curves or running time analyses. One would expect the proposed method to be slow with multiple levels of inference (tree structure, tree parameters, AE networks), and this should be discussed. How large datasets can it handle?

Overall the paper proposes a BN-style structure on VAE latent space with great performance, but somewhat incomplete experimental section, and some presentation issues.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>