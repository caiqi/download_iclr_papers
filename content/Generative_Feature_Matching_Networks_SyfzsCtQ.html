<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Generative Feature Matching Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Generative Feature Matching Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Syfz6sC9tQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Generative Feature Matching Networks" />
      <meta name="og:description" content="We propose a non-adversarial feature matching-based approach to train generative models. Our approach, Generative Feature Matching Networks (GFMN), leverages pretrained neural networks such as..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Syfz6sC9tQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Generative Feature Matching Networks</a> <a class="note_content_pdf" href="/pdf?id=Syfz6sC9tQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 14 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019generative,    &#10;title={Generative Feature Matching Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Syfz6sC9tQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=Syfz6sC9tQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We propose a non-adversarial feature matching-based approach to train generative models. Our approach, Generative Feature Matching Networks (GFMN), leverages pretrained neural networks such as autoencoders and ConvNet classifiers to perform feature extraction. We perform an extensive number of experiments with different challenging datasets, including ImageNet. Our experimental results demonstrate that, due to the expressiveness of the features from pretrained ImageNet classifiers, even by just matching first order statistics, our approach can achieve state-of-the-art results for challenging benchmarks such as CIFAR10 and STL10.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Generative Deep Neural Networks, Feature Matching, Maximum Mean Discrepancy, Generative Adversarial Networks</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A new non-adversarial feature matching-based approach to train generative models that achieves state-of-the-art results.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Hkln8-idpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We have submitted a revised version of the paper.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syfz6sC9tQ&amp;noteId=Hkln8-idpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper783 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper783 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
In order to better address the questions and suggestions from the reviewers, we have uploaded a revised version of the paper. We have done a careful proof reading on the paper, corrected all the typos pointed and greatly improved/expanded some sections and added new ones, as follows: 

(1) We have improved and expanded “Sec. 2.4. Matching Features with ADAM Moving Average”, which now includes a more detailed description of our proposed Adam Moving Average (AMA); and also brings more information regarding the motivation and intuition behind AMA;

(2) We have improved and expanded “Sec 4.2.3. Adam Moving Average and Training Stability”, which now contains more detailed experiments to further demonstrate the advantage of AMA over the simple Moving Average (MA). This section also contains more discussions about AMA vs MA experimental results;

(3) We have added a new section “4.3 Discussion”, which contains a more thorough discussion about the experimental results and comparison with state-of-the-art adversarial methods and MMD-based methods.

We have added two new appendices:
(1) “A.9. Impact of Adam Moving Average for VGG19 feature extractor.”, which presents experimental results that also indicate the advantage of AMA over MA when VGG19 feature extractor is employed.

(2) “A.10. Visual Comparison between GFMN and GMMN Generated Images.”, which shows a visual comparison between images generated by GFMN and GMMN (Li et al., 2015).
 
We hope we addressed here the main concerns of the reviewers and that the new revised paper will help them in further appreciating the technical contributions of the paper and in  improving their overall assessment. We think our paper brings an exciting result to the deep learning community that conveys a simple  and yet exciting message:  feature matching in the space of pre-trained deep CNN allows an efficient training of generative models that circumvents the cumbersome min/max game in GANs. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1gDNyd0hm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review for "Generative Feature Matching Networks"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syfz6sC9tQ&amp;noteId=S1gDNyd0hm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper783 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper783 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a non-adversarial feature matching generative model (GFMN). In feature matching GANs, the discriminator extract features that are employed by the generator to match the real data distribution. Through the experiments, the paper shows that the loss function is correlated with the generated image quality, and the same pretrained feature extractor (pre-trained on imagenet) can be employed across a variety of datasets. The paper also discusses the choice of pretrained network or autoencoder as the feature extractor. The paper also introduces an ADAM-based moving average. The paper compares the results with on CIFAR10 and STL10 with a variety of recent State-of-the-art approaches in terms on IS and FID. 

+ The paper is well written and easy to follow. - However, there are some typos that should be addressed. Such as:
“The decoder part of an AE consists exactly in an image generator ”
“Our proposed approach consists in training G by minimizing”
“Different past work have shown” -&gt; has
“in Equation equation 1 by”
“have also used” better to use the present tense.

+ It suggests a non-adversarial approach to generate images using pre-trained networks. So the training is easier and the quality of the generated images, as well as the FID and IS, are still comparable to the state-of-the-art approaches.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hyx75Mo_pX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Answer to AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syfz6sC9tQ&amp;noteId=Hyx75Mo_pX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper783 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper783 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
We would like to thank the reviewer for the positive feedback.  We have done a careful proof reading on the paper and addressed the typos pointed by the reviewer. Additionally, we have greatly improved and expanded sections 2.4 and 4.2.3 and added the new section 4.3 as well as two new appendices (A.9 and A.10). In the post destined to all the reviewers, we give more details about the main changes in the new version of the paper.
 
We would like to reinforce that our paper presents a solid work backed by an extensive number of experiments and discussions. Moreover, as we present a method that evidence the power of pretrained DCNN representations for training generative models, we believe that our work is a perfect fit for ICLR and of big interest for its community.

Please let us know if you need any additional clarification which would help you to better evaluate the work and increase the overall rating.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HylSrUITnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syfz6sC9tQ&amp;noteId=HylSrUITnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper783 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper783 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper consists of two contributions: (1) using a fixed pre-trained network as a discriminator in feature matching loss ((Salimans et al., 2016). Since it's fixed there is no GAN-like training procedure. (2) Using "ADAM"-moving average to improve the convergency for the feature matching loss.

The paper is well written and easy to follow but it lack of some intuition for the proposed approach. There also some typo, e.g. "quiet" -&gt; quite. Overall, it's a combination of several published method so I would expect a strong performance/analysis on the experimental session.

Detailed Comment:

For contribution (1):

The proposed method is very similar to (Li et al. (2015)) as the author pointed out in related work besides this work map to data space directly. Is there any intuition why this is better? 

The proposed loss (the same as (Salimans et al., 2016)) only try to matching first-order momentum. So I assume it is insensitive to higher-order statistics. Does it less successful at producing samples with high visual fidelity?

For contribution (2):

"one would need big mini-batches which would result in slowing down the training." why larger batch slowing down the training? Is there any qualitative results? Based recent paper e.g. big gan, it seem the model can benefit a lot from larger batch. In the meanwhile, even larger batch make it slower to converge, it can improve throughput. 

Again, can the author provide some intuition for these modification? It's also unclear to me what is ADAM(). Better to link some equation to the original paper or simply write down the formulation and give some explanation on it.

For experiments:

I'm not an expert to interpret experimental results for image generation. But overall, the results seems not very impressive. Given the best results is using ImageNet as a classifier, I think it should compare with some semi-supervised image generation paper.

For example, for CIFAR results, it seems worse than (Warde-Farley &amp; Bengio, 2017), Table 1, semi-supervised case. If we compare unsupervised case (autoencoder), it also seems a lot worse. 

Appendix A.8 is very interesting / important to apply pre-trained network in GAN framework. However, it only say failed to train without any explanation.

I think even it just comparable with GAN, it is interesting if there is no mode collapsing and easy to train. However, it has no proper imagenet results (it has a subset, but only some generated image shows here). 

In summary, this paper provide some interesting perspectives. However, the main algorithms are very similar to some existing methods, more discussion could be used to compare with the existing literature and clarify the novelty of the current paper. The empirical results could also be made more stronger by including more relevant baseline methods and more systematic study of the effectiveness of the proposed approach. I tend to give a weak reject or reject for this paper.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByxhmrsOaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Answer to AnonReviewer1 (Part 1/3)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syfz6sC9tQ&amp;noteId=ByxhmrsOaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper783 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper783 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank the reviewer for the detailed questions, comments and suggestions. We believe they have helped us improve the quality of the paper. We have done substantial changes in the text in order to address your questions/comments/suggestions. 
In the post destined to all reviewers, we give a detailed description of the main changes in the new version of the paper. We kindly ask the reviewer to take a look at the post as well as the new version of the paper.
Please see below our answers for your questions/comments.
 
Regarding contribution (1): 

Rev: “the main algorithms are very similar to some existing methods”
 
Please note that all MMD methods are similar in the sense that they all perform moment matching. Using the same line of argumentation, you would also argue that all papers presenting new GANs are very similar to each other because all of them use the same adversarial strategy. One of the key points that makes our work unique is the demonstration that we can use pretrained neural networks as a powerful kernel function that allows robust and effective moment matching, a result that has never been demonstrated before. Throughout an extensive number of experiments with multiple datasets and different feature extractor architectures we demonstrate the robustness and effectiveness of the method. Moreover, to the best of our knowledge, this is the first work to present state-of-the-art results for CIFAR-10 and STL10 using a non-adversarial method that moves away completely from the problematic min/max game of GANs, which is an impressive result.
 

Rev: “The proposed method is very similar to (Li et al. (2015)) as the author pointed out in related work besides this work map to data space directly. Is there any intuition why this is better?” “… more discussion could be used to compare with the existing literature and clarify the novelty of the current paper.”
 
We have included a new section “4.3 Discussion” which better details the differences and advantages of GFMN over GANs and other MMD approaches. In short:
 
(1) Compared to GANs, we achieve better results than the state-of-the-art SN-GAN while completely avoiding the problematic min/max game. Our method has stable training and no mode-collapsing.
 
Compared to other MMD approaches: 
(2) we present far better quantitative results than GMMN (Li et al., 2015). Additionally, the new Appendix A.10 shows a visual comparison between GMMN and GFMN results. The main reason why GFMN results are significantly better than GMMN is because GFMN uses a strong, robust kernel function (a pretrained DCNN), which, together with our AMA trick, allows a stable and effective training with small minibatches. On the other hand, the Gaussian kernel used in GMMN requires a very large minibatch size in order to work well, which is impractical due to memory limitations and computational cost;
 
(3) Compared to recent adversarial MMD methods (Li et al., 2017; Bikowski et al., 2018) GFMN presents significantly better results while avoiding the problematic min/max game;
 
(4) GFMN achieves similar results to the Method of Learned Moments (MoLM) (Ravuri et al., 2018), while using 50x less moments/features to perform matching. In other words, while MoLM can be used in large-scale environments only, GFMN can be used in single GPU environments achieving the same or better performance.
 

Rev: “The proposed loss (the same as (Salimans et al., 2016)) only try to matching first-order momentum. So I assume it is insensitive to higher-order statistics. Does it less successful at producing samples with high visual fidelity?”
 
It is not quite correct to say that our proposed approach is insensitive to higher-order statistics. Note that using a pretrained DCNN to extract features is equivalent to use a highly non-linear kernel function to map the data into a very high dimensional space (hundreds of thousands of dimensions in our case). What we show in our experiments is that, in this very high dimensional space, matching first order statistics is already enough to achieve state-of-the-art results. We demonstrate empirically that our strategy is more efficient and effective than methods such as GMMN, which use a Gaussian kernel to match all the moments. The problem with using a Gaussian kernel is that it requires a very large minibatch size in order to produce good estimates, which is not feasible in practice (Li et al., 2017). 
 
Regarding visual fidelity, our method produces images that are better or on par with other systems that use similar generator architectures, similar computational resources and do not use conditional generation. For instance, if you compare the quality of the (unconditional) generated images using ImageNet Dogs, you will see that our results are much better than (Salimans et. al, 2016) and (Zhao et al., 2017). It is not fair to compare our results with large-scale experiments or with systems that explicitly model conditional generation.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HygWkrouTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Answer to AnonReviewer1 (Part 2/3)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syfz6sC9tQ&amp;noteId=HygWkrouTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper783 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper783 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Regarding contribution (2): 
 
Rev: ""one would need big mini-batches which would result in slowing down the training." why larger batch slowing down the training? Is there any qualitative results? Based recent paper e.g. big gan, it seem the model can benefit a lot from larger batch. In the meanwhile, even larger batch make it slower to converge, it can improve throughput. "
Rev: “Again, can the author provide some intuition for these modification? It's also unclear to me what is ADAM(). Better to link some equation to the original paper or simply write down the formulation and give some explanation on it.”
 
We have significantly extended and improved Sec. 2.4 which describes our proposed Adam Moving Average (AMA). We have included more details about the motivation and intuition behind the proposed method. We have also added ADAM equations so that the description is self-contained. 


In summary, not training time, but memory usage is the main motivation for using moving averages. When using images larger than 32x32 and DCNNs that produce millions of features, this can easily result in memory issues. Moving average is a strategy to alleviate this problem.
We propose the ADAM Moving Average (AMA) to further promote stable training when using small minibatches. The main advantage of AMA over simple moving average (MA) is its adaptive first order and second order moments that ensure a stable estimation of the moving averages. In fact, this is a non-stationary estimation since the mean of the generated data changes in the training, and it is well known that ADAM optimizer works well for such online and non-stationary losses (Kingma &amp; Ba, 2015).
 
We have improved and expanded “Sec 4.2.3.”, which now contains more detailed experiments to further demonstrate the advantage of AMA over MA. Sec 4.2.3 now brings more discussions about AMA vs MA experimental results. Additionally, we have added a new Appendix “A.9”, which presents experimental results that also indicate the advantage of AMA over AM when VGG19 feature extractor is employed.
 
 
Regarding experiments:
 
Rev: “Given the best results is using ImageNet as a classifier, I think it should compare with some semi-supervised image generation paper. For example, for CIFAR results, it seems worse than (Warde-Farley &amp; Bengio, 2017), Table 1, semi-supervised case. If we compare unsupervised case (autoencoder), it also seems a lot worse.”
 
Comparing our results with the ones from semi-supervised approaches is not fair because we do not use labels from the target dataset, CIFAR10. Nevertheless, the difference between the result of our best system for CIFAR10 and the semi-supervised system reported in Table 1 of (Warde-Farley &amp; Bengio, 2017) is not statistically significant: Inception Score (IS) of 7.99 (ours) vs 8.06 (theirs). This is actually an impressive result in itself, because our system does not use labels from CIFAR10 and our critic (feature extractor) is not updated during the training of the generator.
 
Although you might argue that the best results from GFMN are obtained with feature extractors that were trained in a supervised manner (classifiers), note that:
(1) We have also tried to initialize the discriminator of GANs with a pretrained ImageNet classifier, but it failed to train (more on this in next answer). 
(2) The accuracy of the classifier does not seem to be a very important factor for generating good features (VGG19 classifier produces better features although it is less accurate than Resnet18, see Appendix A.3); we are very confident that GFMN will also achieve state-of-the-art results when trained with features from classifiers pretrained using unsupervised methods such as the one recently proposed by Caron et al. (2018). This is something that can be explored in future works.
 
Rev: “Appendix A.8 is very interesting / important to apply pre-trained network in GAN framework. However, it only say failed to train without any explanation.”
 
We have expanded Appendix A.8 with additional details/explanations on the reasons why WGAN-GP fails when we use a pretrained VGG19/Resnet18 to initialize the discriminator. In short, the discriminator, being pretrained on ImageNet, can quickly learn to distinguish between real and fake images. This limits the reliability of the gradient information from the discriminator, which in turn renders the training of a proper generator extremely challenging or even impossible. This is a well-known issue with GAN training (Goodfellow et al., 2014) where the training of the generator and discriminator must strike a balance. This phenomenon is covered in (Arjovsky et al., 2017) Section 3 (illustrated in their Figure 2) as one motivation for works on Wasserstein GANs.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HkeAiVjdpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Answer to AnonReviewer1 (Part 3/3)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syfz6sC9tQ&amp;noteId=HkeAiVjdpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper783 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper783 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Regarding experiments:
 
Rev: “I think even it just comparable with GAN, it is interesting if there is no mode collapsing and easy to train. However, it has no proper imagenet results (it has a subset, but only some generated image shows here).” “The empirical results could also be made more stronger by including more relevant baseline methods and more systematic study of the effectiveness of the proposed approach.”
 
We compare our results with Miyato (ICLR 2018), Bikowski (ICLR 2018) and Ravuri et al. (ICML 2018) which are very strong up-to-date baselines (all published in 2018) and were the state-of-the-art results by the time we submitted the paper.
 
Regarding ImageNet results, note that in this paper we do not propose to perform conditional generation. All (very recent) papers reporting IS/FID for ImageNet perform conditional generation. Our ImageNet results should be compared with (Ravuri et al., 2018) for the Daisy portion, and (Salimans et. al, 2016; Zhao et al., 2017) for the ImageNet dogs portion. Again, it is not fair to compare our results with the ones from large-scale experiments or with systems that explicitly model conditional generation.
 
Note that the focus of this paper is to demonstrate that we can train effective generative models without adversarial training by employing frozen pretrained neural networks. We selected benchmarks that have been used for the last three/four years by the community on gen. models: CIFAR10, CelebA, MNIST, STL10. We performed an extensive number of experiments, reported quantitative results using two metrics (IS and FID)  and systematically assessed multiple aspects of the proposed approach: 
(1)  We checked the advantage of AMA vs MA;
(2)  demonstrated the correlation of loss vs. image quality;
(3)  evaluated different methods for pretraining the feature extractor (autoencoding, classification); 
(4)  checked different architectures for the feat. extractor (DCGAN, VGG19, Resnet18);
(5)  assessed the impact of the number of features/layers used;
(6)  evaluated in-domain and cross domain feature extractors; 
(7)  tested the benefit of initializing the generator; 
(8)  evaluated the joint use of multiple feature extractors (VGG19 + Resnet18) for training the same generator;
(9)   performed experiments with WGAN-GP initialized with VGG19/Resnet18;
(10) presented results for different portions of ImageNet;
(11) presented visual comparison of images generated between GFMN and GMMN;
(12) compared our results with state-of-the-art methods.
Moreover, the improved Sec. 4.2.3 now contains even more experiments and discussions regarding the advantages of using AMA.
 
Finally, we would like to reinforce that our paper presents a solid work backed by an extensive number of experiments and discussions. Moreover, as we present a method that provides evidence for the power of pretrained DCNN representations for learning generative models, we believe our work is a perfect fit for ICLR and is of great interest for its community.

Please let us know if you need any additional clarification which would help you to better evaluate the work and increase the overall rating.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Sylur4oupQ" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syfz6sC9tQ&amp;noteId=Sylur4oupQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper783 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJepAmsuTm" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syfz6sC9tQ&amp;noteId=SJepAmsuTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper783 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkx9_6p42Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting results supported by experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syfz6sC9tQ&amp;noteId=rkx9_6p42Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper783 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper783 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper introduces Generative Feature Matching Networks (GFMNs) which is a non-adversarial approach to train generative models based on feature matching. GFMN uses pretrained neural networks such as Autoencoders (AE) and Deep Convolutional Neural Networks (DCNN) to extract features. Equation (1) is the proposed loss function for the generator network. In order to avoid big mini-batches, the GFMN performs feature matching with ADAM moving average.The paper validates its proposed approach with several experiments applied on benchmark datasets such as CIFAR10 and ImageNet.

The paper is well-written and straight-forward to follow. The problem is well-motivated by fully discussing the literature and the proposed method is clearly introduced. The method is then validated using several different experiments.

Typos:
** Page 1 -- Paragraph 3 -- Line 8: "(2) mode collapsing in not an issue"</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1lk0MjOaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Answer to AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syfz6sC9tQ&amp;noteId=H1lk0MjOaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper783 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper783 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
We would like to thank the reviewer for the positive feedback.  We have done a careful proof reading on the paper and addressed the typos pointed by the reviewer. Additionally, we have greatly improved and expanded sections 2.4 and 4.2.3 and added the new section 4.3 as well as two new appendices (A.9 and A.10). In the post destined to all the reviewers, we give more details about the main changes in the new version of the paper.
 
We would like to reinforce that our paper presents a solid work backed by an extensive number of experiments and discussions. Moreover, as we present a method that evidence the power of pretrained DCNN representations for training generative models, we believe that our work is a perfect fit for ICLR and of big interest for its community.

Please let us know if you need any additional clarification which would help you to better evaluate the work and increase the overall rating.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>