<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Music Transformer | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Music Transformer" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rJe4ShAcF7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Music Transformer" />
      <meta name="og:description" content="Music relies heavily on repetition to build structure and meaning.  Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rJe4ShAcF7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Music Transformer</a> <a class="note_content_pdf" href="/pdf?id=rJe4ShAcF7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019music,    &#10;title={Music Transformer},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rJe4ShAcF7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Music relies heavily on repetition to build structure and meaning.  Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure.  The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important.  Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018).  This is impractical for long sequences such as musical compositions since their memory complexity is quadratic in the sequence length.  We propose an algorithm that reduces the intermediate memory requirements to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minute-long (thousands of steps) compositions with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies.   We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-competition, and obtain state-of-the-art results on the latter.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">music generation</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We show the first successful use of Transformer in generating music that exhibits long-term structure. </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BJxpLWBLp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Implementation trick to reduce memory footprint of transformer / experiments on music generation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJe4ShAcF7&amp;noteId=BJxpLWBLp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1531 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1531 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents an implementation trick to reduce the memory footprint of relative attention within a transformer network. Specifically, the paper points out redudant computation and storage in the traditional implementation and re-orders matrix operations and indexing schemes to optimize. As an appllication, the paper applies the new implementation to music modeling and generation. By reducing the memory footprint, the paper is able to train the transformer with relative attention on longer musical sequences and larger corpora. The experimental results are compelling -- the transformer with relative attention outperforms baselines in terms of perplexity on development data (though test performance is not reported) and by manual evaluation in a user study.

Overall, I'm uncomfortable accepting this paper in its current form because I'm not sure it constitutes a large enough unit of novel work. The novelty here, as far as I can tell, is essentially an implementation trick rather than an algorithm or model. Transformer networks have been applied to music in past work -- the only difference here is that because of the superior implementation the model can be trained from larger musical sequences. All that said, I do think the proposed implementation is useful and that the experimental results are compelling. Clearly, when trained from sufficient data, transformer networks have something to offer that is different from </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkeg8XwNaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Cool idea, memory usage could be analysed deeper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJe4ShAcF7&amp;noteId=rkeg8XwNaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1531 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1531 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors address the problem raised by applying a fully attentional network (FAN) to model music. 
They argue clearly for the need of relational positional embedding in that problem (instead of absolute positional as in vanilla FAN), and highlight the quadratic memory footprint of the current solution (Shaw et al. 2018).

The main contribution of the paper is a solution to this, consisting in a smart idea (sect 3.4.1 and 3.4.2) which allows them to compute relative embeddings without quadratic overhead.
The model performs indeed better than Shaw et al.'s on the single data-set they compared both. On the other one, the argument is that Shaw et al. 2018 cannot be applied because the sequences are too long.

I have two concerns with the paper:
	1/ it is very hard to read at times. In particular, the main contribution took me several passes the understand. I list below a few recommendations for improvement 
	2/ the main argument is that the model requires less memory and is faster. However, the only empirical evidence in that direction is given in the introduction (Sect 1.1., second paragraph).
		The following points remain unclear to me:
			a) why can't the Relative Transformer be applied to Piano-e composition. What is the maximal length that is possible?
			b) how much faster / less memory is the relative music transformers? The only data-point is in Sect 1.1., which seems indeed impressive (but then one wonders why this is not exploited further). A deeper analysis of the comparative memory footprint would greatly strengthen the paper in my opinion.
			
Why "music" relative transformers? Nothing in the model restrict it to that use case. The use of FAN over audio has been explored with limited success, one of the reasons being that - similarly to this use-case here - audio sequences tend to be longer than text.
		
minor comments:
	- abstract, ln9: there seems to be a verb missing
	- p1,ln-2: "dramatic" improvements seems to be exaggerated
	- p2,ln11: "too long". too long for what?
	- p4,ln15: (Table 1). is one sentence by itself. Also, a clear explanation of that table is missing
	- p5,item 2: an explanation in formula would be helpful for those not familiar with reshaping
	- Fig3: it seems very anecdotical. Similar green bloxes might be placed on the left plot
	- sect4.1.1,ln3. that sentence does not parse
	- Table 2: what is cpsi?
	- $l$ is nicer formatted as $\ell$
	- care should be taken to render the Figures more readable (notably the quality of Fig 4, and labels of Fig 7)
	- footnotes in Figures are not displayed (Table 2 and 4)
	- the description of the human evaluation leaves some open questions. I could not come up with 180 ratings (shouldn't it be 180 * 3 ratings?). Also, at least the values of Relative Transformer vs other 3 models should be shown (or all 6 comparisons). Here you call "relative transformer" your model, previously you used that term to refer to (Shaw et al. 2018).
		when reporting statistical significance, there are some omissions which should be clarified.
	- (Shaw et al. 2018) has been published at NAACL. For such an important citation, you should update the reference from the arxiv version.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rylqizmohQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An application of transformer to music generation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJe4ShAcF7&amp;noteId=rylqizmohQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1531 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1531 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this paper the authors propose an algorithm to reduce the memory
requirements for calculating relative position vectors in a
self-attention (transformer) network, based on the work of [Vaswani et
al., 2017; Shaw et al. 2018]. The authors applied their model to a music
generation task, and evaluated it on two datasets (J.S. Bach Chorales
and Piano-e-Competition). Their model obtained improvements over the
state-of-the-art in the Piano-e-Competition set in terms of
log-likelihoods. Additionally, they performed human evaluation on the
Piano-e-Competition set showing preference of the participants for their
method over the state-of-the-art.

The application of the transformer network seems suitable for the task,
and the authors fairly justify their motivations and choices. They show
improvements over the-state-of-the-art for one data-set and explained
their results. They also show an interesting application of
sequence-to-sequence models for generating complete pieces of music
based on a given melody.

My main concern is the novelty of the paper. The authors use the model
proposed by [Shaw et al. 2018] with an additional modification to manage
very long sequences proposed by [Liu et al., 2018; Parmar et al., 2018],
(chunking the input sequences in non-overlaping blocks and calculating
attention only on the current and the previous blocks). Their main
contribution is to reduce the memory requirement for matrix operations
for calculating the relative position vectors of the self-attention
function, which was sub-optimal in [Shaw et al. 2018]. The memory
reduction is from O(L^2D+L^2) to O(LD+L^2). I would qualify this as an
optimization in the implementation of the existing method rather than a
new approach.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJlnOWEc2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Improved efficiency of transformer on long sequences, but a bit difficult to follow</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJe4ShAcF7&amp;noteId=rJlnOWEc2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1531 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1531 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper describes a method for improving the (sequence-length) scalability of the Transformer architecture, with applications to modeling long-range interactions in musical sequences. The proposed improvement is applied to both global and local relative attention formulations of self-attention, and consists of a clever re-use (and re-shaping) of intermediate calculations. The result shaves a factor of L (sequence length) from the (relative) memory consumption, facilitating efficient training of long sequences. The method is evaluated on MIDI(-like) data of Bach chorales and piano performances, and compares favorably to prior work in terms of perplexity and a human listener evaluation.

The results in this paper seem promising, though difficult to interpret.  The quantitative evaluation consists of perplexity
scores (Tables 2 and 3), and the qualitative listening study is analyzed by pairwise comparisons between methods. While the proposed method achieves the highest win-rate in the listening study, other results in the study (LSTM vs Transformer) run contrary to the ranking given by the perplexity scores in Table 3. This immediately raises the question of how perceptually relevant the (small) differences in perplexity might be, which in turn clouds the overall interpretation of the results. Of course, perplexity is not the whole story here: the focus of the paper seems to be on efficiency, not necessarily accuracy, but one might expect improved efficiency to afford higher model capacity and improve on accuracy.


The core contributions of this work are described in sections 3.4 and 3.5, and while I get the general flavor of the idea, I find the exposition here both terse and difficult to follow. Figures 1 and 2 should illustrate the core concept, but they lack axis labels (and generally sufficient detail to decode properly), and seem to use the opposite color schemes from each-other to convey the same ideas.  Concrete image maps using real data (internal feature activations) may have been easier to read here, along with an equation that describes how the array indices map after skewing.

The description in 3.4 of the improved memory enhancement is also somewhat difficult to follow.  The claim is a reduction from O(DL^2) to O(DL), but table 1 lists this as O(DL^2) to O(DL + L^2).  In general, I would expect L to dominate D, which still leaves the memory usage in quadratic space, so it's not clear how or why this constitutes an improvement. The improvement due to moving from global to local attention is clear, but this does not appear to be a contribution of this work.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ByxtatmFcQ" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJe4ShAcF7&amp;noteId=ByxtatmFcQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper1531 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>