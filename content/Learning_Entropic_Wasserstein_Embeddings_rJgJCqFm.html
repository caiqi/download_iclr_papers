<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learning Entropic Wasserstein Embeddings | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learning Entropic Wasserstein Embeddings" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rJg4J3CqFm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learning Entropic Wasserstein Embeddings" />
      <meta name="og:description" content="Despite their prevalence, Euclidean embeddings of data are fundamentally limited in their ability to capture latent semantic structures, which need not conform to Euclidean spatial assumptions...." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rJg4J3CqFm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning Entropic Wasserstein Embeddings</a> <a class="note_content_pdf" href="/pdf?id=rJg4J3CqFm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 14 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019learning,    &#10;title={Learning Discrete Wasserstein Embeddings},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rJg4J3CqFm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=rJg4J3CqFm" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Despite their prevalence, Euclidean embeddings of data are fundamentally limited in their ability to capture latent semantic structures, which need not conform to Euclidean spatial assumptions. Here we consider an alternative, which embeds data as discrete probability distributions in a Wasserstein space, endowed with an optimal transport metric. Wasserstein spaces are much larger and more flexible than Euclidean spaces, in that they can successfully embed a wider variety of metric structures. We propose to exploit this flexibility by learning an embedding that captures the semantic information in the Wasserstein distance between embedded distributions. We examine empirically the representational capacity of such learned Wasserstein embeddings, showing that they can embed a wide variety of complex metric structures with smaller distortion than an equivalent Euclidean embedding. We also investigate an application to word embedding, demonstrating a unique advantage of Wasserstein embeddings: we can directly visualize the high-dimensional embedding, as it is a probability distribution on a low-dimensional space. This obviates the need for dimensionality reduction techniques such as t-SNE for visualization.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Embedding, Wasserstein, Sinkhorn, Optimal Transport</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We show that Wasserstein spaces are good targets for embedding data with complex semantic structure.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rJg9pF6g0m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Questions regarding word2cloud</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg4J3CqFm&amp;noteId=rJg9pF6g0m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Abhinav_Maurya1" class="profile-link">Abhinav Maurya</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper978 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1. In eq. 9, should (1-r_{x_i,x_j}) and r_{x_i,x_j} be interchanged? When r_{x_i,x_j}=1, we should want to minimize the Wasserstein distance between the related embeddings. And when r_{x_i,x_j}=0, we would want to have the unrelated embeddings at a margin of at least m. For context, in the paper by Hadsell et al which is cited as the source of the contrastive loss function, the binary relationship variable is defined in the opposite manner: "Y = 0 if X1 and X2 are deemed similar, and Y = 1 if they are deemed dissimilar."

2. What does the point cloud correspond to? I couldn't find a clear statement of it. My best guess was that it is the set of all point embeddings associated with words in a sentence. This discrete point cloud is then made continuous by KDE. Is my understanding correct?

Thanks for your time.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SkgboCbq37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A simple and interesting idea on how to map data in a discrete (Wasserstein) space</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg4J3CqFm&amp;noteId=SkgboCbq37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper978 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper978 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper learns embeddings in a discrete space of probability distributions, endowed with a (regularized) Wasserstein distance.

pros:

- interesting idea, nice results, mostly readable presentation.
- the paper is mostly experimental but the message delivers clearly the paper’s objective
- the direct visualisation is interesting
- the paper suggests interesting problems related to the technique

cons:

- to be fair with the technique, the title should mention the fact that the paper minimises a regularised version of Wasserstein distances (Wasserstein -&gt; Sinkhorn ? put “regularised" ?)
- and to be fair, the paper should put some warnings related to regularisation -- this is not a distance anymore, sparsity is affected by regularisation (which may affect visualisation). Put some reminders in the conclusion, reword at least the third paragraph in the introduction.
- the paper could have been a little bit more detailed on Section 2.3, in particular for its third paragraph. Even when it is an experimental paper.
- the direct visualisation is interesting in the general case but has in fact a problem when distributions are highly multimodal, which can be the case in NLP. This blurs the interpretation.
- the paper delivers a superficial message on the representation: I do not consider that nice having modes near physical locations (Paris, France) is wrong. It is also a city. However, it would have been interesting to see the modes of “city” (or similar) to check whether the system indeed did something semantically wrong.

Questions:

- beyond that last remark comes the problem as to whether one can ensure that semantic hierarchies appear in the plot: for example if Nice was only a city, would we observe a minimal intersection with the support of word “city” ? (intersection to be understood at minimal level set, not necessarily 0).

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByxV7nuupQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your feedback!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg4J3CqFm&amp;noteId=ByxV7nuupQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper978 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper978 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your helpful comments! We made several small revisions, according to your suggestions.

To make it more clear that we are using regularized transport, we have made four changes:
1.  The title is now “Learning Entropic Wasserstein Embeddings.”
2.  In the introduction, we expanded the existing discussion of the Sinkhorn divergence and moved it to a separate paragraph (now the fifth paragraph).
3.  In Section 2.3, we note that we are using the Sinkhorn divergence, and briefly discuss its theoretical properties.
4.  In the conclusion, we state that our empirical results are for the Sinkhorn divergence.

We have expanded the third paragraph of Section 2.3, to provide more detail on embedding capacity results for Wasserstein spaces.

Our intent with including “nice” in the visualization section was to show first an error (Figure 3c), which is that “nice” is visibly distant from “kind” (a synonym), then to show the explanation for the error (Figure 3d), which is that the network learned that “nice” is a city in France, while ignoring its second meaning. To make this clearer, we have changed the caption for Figure 3d to “Explaining a failed association.” Please let us know if this addresses your comment.

The idea to look at semantic hierarchies is very interesting, and we did investigate this briefly. We observed that the minimal level sets of the parent and child in the hierarchy were often partially overlapping -- for instance the embedding of “city” has two major modes, one of which overlaps with a number of cities, including “nice.” As you point out, however, it is not immediately clear how hierarchies should manifest in this type of embedding, particularly as the parent can have semantics not shared by the children (e.g. “state,” which has multiple non-geographic meanings). Perhaps the parent (or one of its modes) should be near to the barycenter of the children? This would be interesting to investigate further.

Thank you again for your feedback!</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_r1gPBWZ8hm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A very nice and original work.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg4J3CqFm&amp;noteId=r1gPBWZ8hm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper978 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper978 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper ‘Learning Discrete Wasserstein Embeddings' describes a new embedding method that,
contrary to usual embedding approaches, does not try to embed (complex, structured) data into an 
Hilbertian space where Euclidean distance is used, but rather to the space of probability measures
endowed with the Wasserstein distance. As such, data are embed on an empirical 
distribution supported by Diracs, which locations can be determined by a map that is learnt from data.
Interestingly, authors note a 'potential universality' for W_p(R^3) (from a result of Andoni et al., 2015), 
suggesting that having Diracs in R^3 could embed potentially any kind of metric on symbolic data. 

Experimental validations are presented on graph and word embedding, and a discussion on visualization of 
the embedding is also proposed (since the Diracs are located in a low dimensional space).   

All in all the paper is very clear and interesting. The idea of embedding in a Wasserstein space is 
original (up to my knowledge) and well described. I definitely believe that this work should be presented
at ICLR. I have a couple of questions and remarks for the authors:
 - It is noted in section 3.2 that both Diracs location and associated weights could be optimized. Yet the authors 
   chose to only optimize locations. Why not only optimizing the weights (as in an Eulerian view of probability
   distributions) ? The sentence involving works of Brancolini and Claici 2018 is not clear to me. Why weighting 
   does not improve asymptotically the approximation quality ? 
 - Introducing the entropic regularization is mainly done for being able to differentiate the Wasserstein 
   distance. However, few is said on the embeddability of metrics in W^\lambda_p(R). Is using an entropic 
   version of W moderating the capacity of embedding ? At least experimentally, a discussion could be made  
   on the choice of the regularization parameter, at least in section 4.1. In eq. (9), it seems that it is not 
   the regularized version of W. ? 
 - I assume that the mapping is hard to invert, but did the authors tried to experiment reconstructing an object 
   of interest by following a geodesic in the Wasserstein space ?  
 - It seems to me that authors never give generalization results. What is the performance of the metric approximation 
   when tested on unseen graphs or words ? This point should be clarified in the experiment.        

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1eUJa_d6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your comments!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg4J3CqFm&amp;noteId=r1eUJa_d6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper978 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper978 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your thoughtful comments! We have made several small edits, according to your suggestions.

(Brancolini 2009) and (Kloeckner 2012) show that, when using a weighted point cloud to approximate an absolutely continuous, compactly-supported measure, the order of convergence in p-Wasserstein distance when allowing non-uniform weights is O(n^(-1/d)), which is the same rate as when restricted to uniform weights (Dudley 1969). Non-uniform weights might buy you an improved constant term, but the rate is the same. We have expanded the description of this fact in Section 3.2, for clarity.

The embedding capacity of W^\lambda_p(R^d) is unknown, so far as we are aware, except in the weak sense that the approximation error with respect to the p-Wasserstein distance vanishes as the regularizer is taken zero (Carlier 2017; Genevay 2018). We have added a discussion of this distinction between W_p and W^\lambda_p to Section 2.3.

Inverting the mapping and following geodesics in Wasserstein space would definitely be interesting. We have added this to the suggested future work in the conclusion. An approach such as (Seguy 2015) might be useful here.

We have updated the notation in eq. (9) to highlight the fact that entropic regularization is used for learning word embeddings.

We have added a comment on generalization performance to Section 4.1, paragraph 3.

Thank you again for your feedback!

(Brancolini 2009) Alessio Brancolini, Giuseppe Buttazzo, Filippo Santambrogio, Eugene Stepanov. Long-term planning versus short-term planning in the asymptotical location problem. ESAIM: Control, Optimisation, and Calculus of Variations 15, no. 3 (2009).
(Kloeckner 2012) Benoit Kloeckner. Approximation by Finitely Supported Measures. ESAIM: Control, Optimisation, and Calculus of Variations 18, no. 2 (2012).
(Dudley 1969) Richard Dudley. The Speed of Mean Glivenko-Cantelli Convergence. Annals of Mathematical Statistics 40, no. 1 (1969).
(Carlier 2017) Guillaume Carlier, Vincent Duval, Gabriel Peyré, Bernhard Schmitzer. Convergence of Entropic Schemes for Optimal Transport and Gradient Flows. SIAM Journal on Mathematical Analysis 49, no. 2 (2017).
(Genevay 2018) Aude Genevay, Lénaic Chizat, Francis Bach, Marco Cuturi, Gabriel Peyré. Sample Complexity of Sinkhorn Divergences. arXiv:1810.02733 (2018).
(Seguy 2015) Vivien Seguy and Marco Cuturi. Principal Geodesic Analysis for Probability Measures under the Optimal Transport Metric. NIPS 2015.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1gutHkNnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Very Limited Contribution</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg4J3CqFm&amp;noteId=B1gutHkNnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper978 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper978 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes embedding the data into low-dimensional Wasserstein spaces. These spaces are larger and more flexible than Euclidean spaces and thus, can capture the underlying structure of the data more accurately. However, the paper simply uses the automatic differentiation to calculate the gradients. Thus, it offers almost no theoretical contribution (for instance, how to calculate these embeddings more efficiently (e.g. faster or more efficient calculation of the Sinkhorn divergence), how to motivate loss functions than can benefit the structure of the Wasserstein spaces, what the interpretation of phi(x) is for each problem, e. g. word embedding, etc.). Additionally, the experiments are unclear and need further improvement. For instance, which method is used to find the Euclidean embedding of the datasets? Have you tried any alternative loss functions for the discussed problems? Are these embeddings useful (classification accuracy, other distortion measure)? How does the 2-D visualizations compare to DR methods such as t-SNE? What is complexity of the method? How does the runtime compare to similar methods?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkxbM0_d6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your feedback!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg4J3CqFm&amp;noteId=BkxbM0_d6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper978 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper978 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comments! We are confident that the contributions of our work are both novel and interesting to the ICLR community.  We are eager to address your concerns, which can be addressed in a minor revision to our text or experiments.  Some responses are provided here, and we are happy to continue the conversation and/or provide additional information at your request. 

Most importantly, although you mention automatic differentiation as a negative, its use for approximating gradients of optimal transport-based divergences is non-obvious and was very recently a primary contribution of the AISTATS paper (Genevay 2018), which provides a way to differentiate transport for use in a neural network that is both stable and efficient. In a sense, our work is among the first to apply these new developments to a representation learning/embedding problem. 

More generally, efficient evaluation of optimal transport distances and their derivatives is a well-known and long-standing challenge in optimization. We leverage the current state of the art, in terms of efficiency, by using the Sinkhorn approximation to the Wasserstein distance (Section 2.2).

While we appreciate the suggestion to show t-SNE plots and can add them to the paper if acceptance to ICLR is contingent on this change, it is worth noting that they will communicate different, more limited information in comparison to our visualizations. As is well-known in the data science community, it is easy to ascribe signal to noise when interpreting the locations of the t-SNE points, as they are not intended to capture locations or distances in the original embedding space. Of course, we are happy to generate the figures and see if they are useful, at your request.

We respectfully highlight a few instances in which your questions are partially addressed in the existing text. We will gladly revise and/or augment the text for clarity, with guidance on the most effective ways to communicate the ideas below.

1. The interpretation of phi(x) for word embeddings is discussed in Section 4.2.1, where we demonstrate direct visualization of the embedding. 
2. The Euclidean and hyperbolic embeddings are computed nearly identically to the Wasserstein embedding -- same loss function, same optimizer (Adam), different learning rate. This was mentioned in Section 4.1, fifth paragraph; we have also edited this to clarify that the optimizer is the same, and would be happy to add additional detail to this description.
3. The loss function for the first problem (complex networks) is dictated by the problem statement itself: We are establishing that one can learn Wasserstein embeddings that achieve low mean distortion, so our loss is the mean distortion. This is stated in Section 4.1, paragraph 2. Note that distortion is the standard criterion for metric embeddings.
4. The utility of the word embeddings for scoring semantic similarity of words is described in Section 4.2, paragraph 4, and in Table 2, where we evaluate the word embeddings on several benchmarks. Note also that we compare to five alternative methods.

Thank you again for your feedback!  We appreciate your time and look forward to the possibility of sharing our work in ICLR soon.

(Genevay 2018) Aude Genevay, Gabriel Peyré, Marco Cuturi. Learning Generative Models with Sinkhorn Divergences. AISTATS 2018.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>