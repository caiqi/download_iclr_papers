<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Optimal margin Distribution Network | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Optimal margin Distribution Network" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HygcvsAcFX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Optimal margin Distribution Network" />
      <meta name="og:description" content="Recent research about margin theory has proved that maximizing the minimum margin like support vector machines does not necessarily lead to better performance, and instead, it is crucial to..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HygcvsAcFX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Optimal margin Distribution Network</a> <a class="note_content_pdf" href="/pdf?id=HygcvsAcFX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019optimal,    &#10;title={Optimal margin Distribution Network},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HygcvsAcFX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Recent research about margin theory has proved that maximizing the minimum margin like support vector machines does not necessarily lead to better performance, and instead, it is crucial to optimize the margin distribution. In the meantime, margin theory has been used to explain the empirical success of deep network in recent studies. In this paper, we present ODN (the Optimal margin Distribution Network), a network which embeds a loss function in regard to the optimal margin distribution. We give a theoretical analysis for our method using the PAC-Bayesian framework, which confirms the significance of the margin distribution for classification within the framework of deep networks. In addition, empirical results show that the ODN model always outperforms the baseline cross-entropy loss model consistently across different regularization situations. And our ODN model also outperforms the other three loss models in generalization task through limited training data.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Optimal margin distribution, Deep neural network, Generalization bound</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">This paper presents a deep neural network embedding a loss function in regard to the optimal margin distribution, which alleviates the overfitting problem theoretically and empirically.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_ryerWMf93m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Promising work, but an in-depth study of the handcrafted margin loss function is lacking</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygcvsAcFX&amp;noteId=ryerWMf93m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper289 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper289 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">I consider that improving the generalization capability of neural networks on small dataset is an important line of research, and the method proposed here empirically provides great results.

The proposed margin loss (Equation 1) is said to be "specially adapted for accelerating the convergence velocity of networks by [the authors]". I would like this statement to be explained better, or at least backed by empirical evidence. In the current state, I consider that the paper lacks an in-depth study of the properties of this handcrafted loss. Few is said on the benefits of having both a linear behavior for points inside the margin and a quadratic loss for far points. The impact of loss hyperparameters (r, \gamma,\mu) should be discussed thoughtfully; at some points in the paper, r and \gamma are referred as margin mean and margin variance parameters, but this interpretation is not explained. Moreover, almost nothing is said about \mu.
By considering a simplified loss function, the provided PAC-Bayes generalization bound (Theorem 1) consider solely the flat loss region [r-\gamma, r+\gamma], but shed no light on the benefit of the hinge and quadratic parts. I conceive that this might be hard to study theoretically, but the authors should at least provide a empirical study of these. 

The empirical experiments show great evidence that the proposed method successfully improve generalization capability of neural networks on small datasets compared to classical methods. I appreciate the Inter/intra class variance study of Tables 2 and 3. I would like the mathematical expression of the "hinge loss" and the "soft hinge loss" models to be explicitly written (it is not clear in the text if the soft hinge uses an hyperparameter). In the same spirit of my above comments, I would like to see how each loss hyperparameters impacts the results, instead of having access solely to the parameter values selected by the validation process.

Typos and minor comments:
- Abstract: "And our ODN model also outperforms the other three loss models..." Which three loss models?
- Section 3: "Specially, define L_0 as r=\theta..." I think it should be r=0
- Section 4.1: model-s =&gt; models
- Page 7 (and elsewhere): Table. 2 =&gt; Table 2
- Please specify that "Xent" stands for cross-entropy
- Figure 3: Please use larger font sizes
- Proof of Lemma 2: Equation. 4 =&gt; Equation 4 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1xq2nxq2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>PAC-Bayesian analysis for DNNs</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygcvsAcFX&amp;noteId=S1xq2nxq2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper289 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper289 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a PAC-Bayesian bound for a margin loss.

Theorem 1 seems specific to ReLU activations. I wonder whether this theorem holds for other activations since most deep neural networks can use different activations at different layers instead of only the ReLU activation for all the layers. In Section 3, only Definition 3 is related to the activation. Can an activation satisfying Definition 3 have a similar bound to Theorem 1? Moreover, since the convolutional layer is a simplified case of the fully connected layer discussed in Section 3, does the convolutional layer simplify the bound in Theorem 1?

There are some typos in this paper.
“To derive a expected risk bound”: a -&gt; an
“used to formalize error-resilience in Arora et al. (2018) as following:”: following: -&gt; follows.
“the deep network from layer i to layer j”, “injected before level i”: i,j should be in the math mode.
“dependent on the network structure .” there is an additional blank space after ‘structure’.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1lXsaCBnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>just seem interesting</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygcvsAcFX&amp;noteId=S1lXsaCBnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper289 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper289 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper presents an improvement on the previous work by [Neyshabur et el, ICLR 2018].
More precisely, an emprical generalization bound is provided by using PAC-Bayesian empirical 
bounds. To obtain the claimed improvement over the works [Barlett et al, NIPS 2017] and 
[Neyshabur et el, ICLR 2018], the authors have paid attention carefully (by putting some 
conditions) on the change of the layers (layer and interlayer cushion) as well as the activation 
contraction. It is also worth noting that the paper is using a differrent loss function comparing 
to [Neyshabur et el, ICLR 2018], which the author called Optimal Margin Distribution Loss.
Although the results seem interesting, the analysis is not convincible for me.
A plus point is that the paper presents interesting numerical experiments showing the promising of the approach.

Major comments:
1) The statement of the Theorem 1 is not clear: 
is it just under the assumptions of the lemmas
or is it under all definitions and lemmas?
2) The proof of Theorem 1 is not clear:
 how do you get the inequality (5)?
how do you get an upper bound on the KL divergence?
 This is not trivial for me!
3) What is \rho in Theorem 1 and in Definition 2?
4) Your remark after Theorem 1 is not clear for me.
  you claim that the product is (3) is large, what if we restrict all the spectral norms equal to 1?
 a simple counter example would fit better the explanation here, I guest.

Minor comments:
1) The Lemma 1 and 2 are almost the same to Lemma 1 and 2 in [Neyshabur et el, ICLR 2018]
without precisely citations. I wonder how do you obtain your Lemma 1?
2) page3, after formula (1), your loss will first DECREASING, not "increasing".
Check the sentence "Fig. 1 shows, equation 1 will produce a linear loss increasing progressively with the margin distance...."
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1eRD8ag3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Extreme similarity to a paper but without mentioning the source.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygcvsAcFX&amp;noteId=H1eRD8ag3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">27 Oct 2018 (modified: 31 Oct 2018)</span><span class="item">ICLR 2019 Conference Paper289 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Parts of this submission shows great similarity (and sometimes identical sentences) to the following NIPS paper:
“Large Margin Deep Networks for Classification” by Elsayed et al. (arXiv March 2018).

While the authors seem to have read the above reference and even adapted portions without much modification, they surprisingly do not mention the source. Below are some examples.

## Section 2 of this submission vs Section 3 of Elsayed’s ##

1. Near the beginning of Section 2 the submission:
“Define the decision boundary for each class pair {i, j} as: D{i,j} , {x | fi(x) = fj (x)}”.
The English text, the equation and even variable naming is *identical* to Eq(1) in Elsayed’s.
2. *Immediately* after this equation, the submission continues the same as Elsayed’s, with minor wording change:
Submission: “Constructed on this definition, the margin distance of a sample point x to the decision boundary Di,j is defined by the smallest translation of the sample point to establish the equation as:”
Elsayed’s: “Under this definition, the distance of a point x to the decision boundary D{i,j} is defined as the smallest displacement of the point that results in a score tie:”
Elsayed’s: “
3. The above sentence is followed by an equation that is the same as Eq 2 of Elsayed’s, including the naming convention (small delta, x, f_i, f_j).
4. Immediately after this:
Submission: “We present an approximation to γ by linearizing f_i”.
Elsayed’s: Above their Eq 6: “We present an approximation to d by linearizing f_i”, and then the same exact formula in Eq 7 of Elsayed’s appears in the submission (even similar notation conventions, like nabla_x).

## Section 4.1 ##

The middle of the first paragraph read is:
“there is a gradient term in the loss itself, which can make the computation expensive. To reduce computational cost, in the backpropagation step we considered the gradient term EQN2 as a constant, so that we recomputed the value of EQN1 at every forward propagation step.”
Elsayed’s Section 4.1, also middle of the first paragraph:
“....presence of gradients in the loss itself…. The backpropagation step for parameter updates requires the computation of second-order gradients. To further reduce computation cost to a manageable level…. treating the denominator EQN1 in (15) as a constant w.r.t. w for backpropagation. The value of EQN2 is recomputed at every forward propagation step.”

Also the end of this paragraph in the submission reads as:
“Furthermore, since the denominator item could be too small, which would cause numerical problem, we added an \epsilon with small value to the denominator so that clip the loss at some threshold.”
Elsayed’s closing sentence for their Section 4.1: “Finally, to improve stability when the  denominator is small, we found it beneficial to clip the loss at some threshold [denoted by \epsilon].”


</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJlzk7uV3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Sorry for the missing of this reference, but the contributions of these two works are totally different.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygcvsAcFX&amp;noteId=SJlzk7uV3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper289 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 31 Oct 2018)</span><span class="item">ICLR 2019 Conference Paper289 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the comments. Our responses are proved below.

Q1. Regarding the margin definition
To our knowledge, the margin definition in section 2 is widely used in multi-class learning setting, just to name a few, the classic book [Mohri M. 2012] (cf. chapter 8.2), the top journal / conference papers [Crammer K. JMLR 2001], [Zhang T. ICML 2017], [Peter L. B. NIPS 2017]. Therefore, the margin definition is not proposed by Elsayed et al., and we treat it as a common knowledge. Note that Elsayed et al. also use this definition without citation.

Q2. Regarding the linear approximation
Thanks for pointing out the missing reference and we will cite Elsayed's paper in the revised version. However, we want to clarify that the novelty of our paper is irrelevant to this linear approximation. To be specific, we summarize the difference between our paper and Elsayed's paper in the next question.

Q3. The contributions of our paper and Elsayed's paper are totally different
The main contribution of Elsayed's work is to introduce the hinge loss into deep models across all layers to improve the empirical performance. In contrast, our contribution is to theoretically prove that, the deep learning overfitting problem can be alleviated by a well-designed loss function inspired by the optimal margin distribution principle. Therefore, our paper and Elsayed's work have totally different motivations, as we do theoretical analysis for the margin distribution of deep learning frameworks, while they mainly focus on the empirical improvement.

[Mohri M. 2012] Mohri M, Rostamizadeh A, Talwalkar A. Foundations of machine learning[M]. MIT press, 2012.
[Crammer K. JMLR 2001] Crammer K, Singer Y. On the algorithmic implementation of multiclass kernel-based vector machines[J]. Journal of machine learning research, 2001, 2(Dec): 265-292.
[Zhang T. ICML 2017] Teng Zhang and Zhi-Hua Zhou. Multi-class optimal margin distribution machine. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pp. 4063–4071, 2017.
[Peter L. B. NIPS 2017] Peter L. B, Dylan J. F, and Matus J. T. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pp. 6241–6250, 2017.

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Bkes3De1s7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Minor comments：</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygcvsAcFX&amp;noteId=Bkes3De1s7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~XINYI_LIN1" class="profile-link">XINYI LIN</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Oct 2018</span><span class="item">ICLR 2019 Conference Paper289 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">This is an interesting work. Considering margin distribution is more important than the minimum margin to the performance of model in the theoretical research in AdaBoost, applying this margin distribution principle to Deep Neural Network is a novel insight. Moreover, this paper provides a theoretical foundation of generalization bound, which can be used to search the value of hyper-parameters. In the experiment result, consistent with theoretical result , the DNN improves the performance on small sample learning problem, through introducing the margin variance to loss function. I read a similar thought in another paper submitted in this ICLR( R( <a href="https://openreview.net/forum?id=HJlQfnCqKX" target="_blank" rel="nofollow">https://openreview.net/forum?id=HJlQfnCqKX</a> ),  ), it introduces a statistic $\hat{R}^2$ through a lot of empirical tests, to vertify the importance of the margin distribution. I think $\hat{R}^2$ is similar to the conception of margin variance in your paper. And the margin bound in your paper implement the future work in their paper, as they write: "We believe that the method developed here can be used in complementary with existing generalization bound".
Here is some suggestion to your paper:
1. Read the another paper I mentioned, and expand the related works;
2. In the Section 2, the hyper-parameter $r$ and $\theta$ representing margin mean and margin variance did not be stated, this trivial mistake makes me confused about this loss function;
3. Since the margin distribution is so important to get a better "representation", did the author consider the application of this ODN to adversary attack problems in the future work?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1gyjLXJiQ" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygcvsAcFX&amp;noteId=S1gyjLXJiQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper289 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>