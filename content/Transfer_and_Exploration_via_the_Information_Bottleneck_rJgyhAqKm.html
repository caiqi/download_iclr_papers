<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Transfer and Exploration via the Information Bottleneck | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Transfer and Exploration via the Information Bottleneck" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rJg8yhAqKm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Transfer and Exploration via the Information Bottleneck" />
      <meta name="og:description" content="A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rJg8yhAqKm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Transfer and Exploration via the Information Bottleneck</a> <a class="note_content_pdf" href="/pdf?id=rJg8yhAqKm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019transfer,    &#10;title={Transfer and Exploration via the Information Bottleneck},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rJg8yhAqKm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=rJg8yhAqKm" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned model with an information bottleneck, we can identify decision states by examining where the model accesses the goal state through the bottleneck. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Information bottleneck, policy transfer, policy generalization, exploration</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Training agents with goal-policy information bottlenecks promotes transfer and yields a powerful exploration bonus</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">19 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SkgQ1PcgCm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Paper Updated to address reviewer feedback.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg8yhAqKm&amp;noteId=SkgQ1PcgCm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper989 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper989 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have updated the paper with the following changes to address reviewer comments:
- Remove references to "useful habits"  (Reviewer 1)
- Added mathematical description of the proposed method in Appendix (Section A) to answer (Reviewer 1, and Reviewer 3)
- Added comparisons to exploration methods (ICM curiosity driven learning) and hierarchical methods (Feudal RL) on a more complicated navigation tasks, which includes branching as well as dead ends. (All reviewers)
- Added comparisons to the state of the art Off policy methods (SAC).  (All reviewers)
- Added more experiments showing that the proposed method is more general by using for following scenarios. (All Reviewers) 
        (1) Using the proposed information regularizer for multi-agent communication, and improving against the strong baseline. 
        (2) Using the proposed method for instruction following where the goal is given by language instruction. 
        (3) Preliminary results showing that the method can be used to provide middle ground b/w model free RL and model based RL. 

Thank you for your time! The authors appreciate the time reviewers have taken for providing feedback. which resulted in improving the presentation of our paper. Hence,  we would appreciate it if the reviewers could take a look at our changes and additional results, and let us know if they would like to either revise their rating of the paper, or request additional changes that would alleviate their concerns. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hyg3Bds96Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>More challenging navigation Environment - To ALL reviewers</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg8yhAqKm&amp;noteId=Hyg3Bds96Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper989 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper989 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank all the reviewers for their valuable time and feedback. 

In order to answer the  questions raised by all the reviewers, we setup another challenging task (Fig 7 main paper). Here, the goal of the agent is to navigate to a goal position. We use a partially observed formulation of the task, where the agent only observes a small number of squares ahead of it and the agent only gets a reward of “1” when it reaches the goal.  For standard RL algorithms, these tasks are difficult to solve due to the partial observability of the environment, sparse reward (as the agent receives a reward only after reaching the goal), and low probability of reaching the goal via random walks (precisely because these  junction states are crucial states where the right action must be taken and several junctions need to be crossed). This task also has dead ends as well as more complex branching factor which was not present in the task which we tried for the submitted paper.  We demonstrate the results on this env where we compare generalization performance to model free methods (a2c, ppo), and goal oriented baselines (UVFA) setup, as well as strong exploration baselines (ICM, count based exploration) and goal based hierarchical baselines (like Option Critic and Feudal Networks). We would be happy to add comparisons to other baselines which reviewers have in mind. 

We first show direct policy transfer in which we first train a particular algorithm on 6 x 6 maze, and evaluate the direct policy transfer on 11 x 11 maze. In order to be exhaustive, we also compare to hierarchical baselines like Option Critic Architecture and Feudal Networks. We tested on 11 X 11 mazes by running a policy trained with different algorithms. The success rate is the number of times the agent solves a larger task (11 x 11) while it is being trained on the smaller task (6 x 6).  All these were run for 100M steps, and averaged over 3 random seeds.

Algorithm (Train on 6 x 6)                                                    (Evaluate on 11 x 11) (variance)
Actor Critic                                                                                               5% (1%)
PPO (Proximal Policy Optimization)                                                    8% (1%)
Actor Critic + CT based exploration                                                     7% (1%)
Goal Based (UVFA)                                                                                15% (4%)
Curiosity Driven Learning using inverse models.                            47% (5%)
Option Critic                                                                                            17% (4%)
Feudal RL                                                                                                37% (4%)
Proposed Method                                                                                 64% (2%)

When generalizing to larger mazes, the agent learns to solve the task  64\% of the times, whereas other hierarchical agents solve &lt;50\% of mazes (Table 3).
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HyeBMFj9p7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>InfoBot Comparison to State of the art off policy methods (Soft Actor Critic) - To ALL reviewers</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg8yhAqKm&amp;noteId=HyeBMFj9p7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper989 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper989 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We also compared the  proposed method  to the state of the art off-policy methods like SAC. For this domain, we again use high value states as an approximation to the goal state following [1] . In order to implement this, we maintain a buffer of 20000 states, and choose the state with the highest value under the current value function, as a proxy to the goal.  

We compare the proposed method with SAC in  sparse reward scenarios. We evaluate the proposed algorithm  on 4 MuJoCo tasks in OpenAI Gym. 

The result in Figure. 11 (Section F appendix) shows the performance of the proposed method showing improvement over the baseline on the sparse reward HalfCheetah-v2, Walker2d-v2, Swimmer-v2, Hopper-v2 tasks. We did not observe much benefit over SAC in dense reward scenario. Unlike Minigrid, and maze goal based navigation task, the reward structure in this benchmark is  dense in that the agent always receives a reasonable amount of reward according to its continuous progress. Hence, in the dense reward scenario, the learning problem for the policy regularized with goal bottleneck is not simpler as compared to that of the regular policy, and hence the  agent may not be forced to generalize across different contexts (by learning ``useful'' habits).

To verify our conjecture, we conducted experiments by making the reward which agent gets from the environment sparse. More specifically, the modified tasks give an reward once in 50 steps. We see that in this scenario, the agent trained with goal bottleneck regularization performs much better as compared to the SAC baseline. 


[1]  Recall Traces, <a href="https://arxiv.org/abs/1804.00379" target="_blank" rel="nofollow">https://arxiv.org/abs/1804.00379</a>
[2]  SAC, Soft Actor Critic https://arxiv.org/abs/1801.01290
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkgF37cxCQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>InfoBot for Instruction Following - To ALL REVIEWERS</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg8yhAqKm&amp;noteId=rkgF37cxCQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper989 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper989 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Here we want to show that the proposed method can also be in the context of interactive worlds for spatial reasoning where the goal is given by language instruction, and hence the proposed method is very general. The agent is placed in an interactive world, and agent can take actions to reach the goal specified by language instruction. For ex. Reach the north-most house, the problem could be challenging because the language instruction is is highly context dependent. Therefore, for better generalization to unseen worlds, the model must jointly reason over the instruction text and environment configuration. Here, the text instructions can have both local and global references to objects. Local references require an understanding of spatial prepositional phrases such as ‘above’, ‘next to’ in order to reach the goal. This is invariant to the global position of the object references, on the other hand, global references contains superlatives such as ‘easternmost’ and ‘topmost’, which require reasoning over the entire map. Here we compare the proposed method to UVFA. We use the exact same experimental setup as in [1].

Method                                          Local (Policy Quality)                         Global (Policy Quality)
UVFA                                                         0.57                                                     0.59
InfoBot                                                     0.89                                                     0.81

[1] "Representation Learning for Grounded Spatial Reasoning" <a href="https://arxiv.org/abs/1707.03938" target="_blank" rel="nofollow">https://arxiv.org/abs/1707.03938</a>

The details for all these experiments have also been mentioned in Appendix (Section G). 

We would appreciate it if the reviewer(s) could take another look at our changes and additional results, and let us know if the reviewer would like to request additional changes that would alleviate reviewers concerns. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_SJxWVtZ5pX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review Number 3 (So sorry for the delay!)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg8yhAqKm&amp;noteId=SJxWVtZ5pX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper989 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper989 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose a new regularizer for policy search in a multi-goal RL setting. The objective promotes a more efficient exploration strategy by encouraging the agent to learn policies that depend as little as possible on the target goal. This is achieved by regularizing standard RL losses with the negative conditional mutual information I(A;G|S). Although this regularizer cannot be optimize, the authors propose a tractable bound. The net effect of this regularizer is to promote more effective exploration by encouraging the agent to visit decision states, in which goal-depend decisions play a more important role. The idea of using this particular regularizer is inspired by an existing line of work on the information bottleneck.

I find the idea proposed by the authors to be interesting. However, I have the following concerns, and overall I think this paper is borderline.

1. The quality of the experimental validation provided by the authors is in my opinion borderline acceptable. Although the method performs better on toy settings, it seems barely better on more challenging ones. Experiments in section 4.5 lack detail and context.
2. The clarity of the presentation is also not great.
    2.1. The two-stage nature of the method was confusing to me. I didn’t understand the role of the second stage. Most focus is on the first stage, and only very little on the second stage. For example, I was confused about why the sign of the regularizer was flipped.
    2.2. I was confused by how exactly the bounds (3) and (4) we applied and in what order.
    2.3. I think the intuition of the method could be better explained and better validated by experiments.

I also have the following additional comments:
* How is the regularizer applied with other policy search algorithms besides Reinforce? Was it done in the paper? I can’t say for sure. Specifically, when comparing to PPO, was the algorithm compared to a version of PPO augmented with this regularizer? Why yes or why no?
* More generally, experiments where more modern policy search algorithms are combined with the regularizer would be helpful. In particular, does it matter which policy search algorithm we use with this method?
* Experimental plots in section 4.4 are missing error bars, and I can’t tell if the results are significant without them.
* I thought the motivation for choosing this regularizer was lacking. The authors cite the information bottleneck literature, but we shouldn’t need to read all these papers, the main ideas should be summarized here.
* The argument for how the regularizer improves exploration seemed to me very hand-wavy and not well substantiated by experiments.
* I would love to see a better discussion of how the method is useful when he RL setting is not truly multi-goal.
* The second part of the algorithm needs to be explained much more clearly.
* What is the effect of the approximation on Q?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1e5rrncT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for your feedback!  (1/4)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg8yhAqKm&amp;noteId=r1e5rrncT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper989 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper989 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the positive and constructive feedback. We appreciate that the reviewer finds that our method interesting. 

"The quality of the experimental validation provided by the authors is in my opinion borderline acceptable. Although the method performs better on toy settings, it seems barely better on more challenging ones. Experiments in section 4.5 lack detail and context." 

In order to make our experiments more rigorous, we conducted more experiments to answer reviewers concern. 

1) More challenging Navigation setup.  - We ask the reviewer to refer to heading "More challenging navigation Environment". Also for more details refer to Section 4.6 in the main paper.  We compared to several strong baselines like hierarchical RL methods, strong exploration methods as well as goal based methods. We would be happy to add more comparisons which reviewer has in mind.

2) Comparison to State of the art Off policy Methods (SAC) in sparse  rewards. We ask the reviewer to refer to heading "InfoBot Comparison to State of the art off policy methods (Soft Actor Critic)". Also, for more details refer to  Section F in the appendix.
 
3) Application of the proposed method in multi-agent communication, such that the goal in the proposed method corresponds to the information obtained because of communication with another agent in multi-agent communication channel. Here,  we  want  to  show  that  by  training  agents  to  develop  “default  behaviours”  as  well  as  the knowledge of when to break those behaviours, using an information bottleneck can also help in other
scenarios like multi-agent communication. Consider multiagent communication, where in order to
solve a task, agents require communicating with another agents. Ideally, an agent would would like
to communicate with other agent, only when its essential to communicate, i.e the agents would like
to minimize the communication with another agents. Here we show that selectively deciding when to
communicate with another agent can result in faster learning. We follow the same  experimental setup as in the paper (Mordatch and Abbeel, 2018). 

Method                                                 Train Reward                   TestReward
No Communication                               -0.919                                  -0.920
Communication                                      -0.36                                   -0.472
Communication (with KL cost)            -0.293                                  -0.38 

(Lower is better). More details about this experimental setup can be found in Section D (Appendix). 

I. Mordatch and P. Abbeel.  Emergence of grounded compositional language in multi-agent pop-
ulations.

We acknowledge that the paper was certainly lacking polish and accept that this may have made the paper difficult to read in places. We have uploaded a revised version in which we have revised the problem statement and writing as per the reviewer's suggestions. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Skevj_35pX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>More Intuition (2/4)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg8yhAqKm&amp;noteId=Skevj_35pX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper989 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper989 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">"I thought the motivation for choosing this regularizer was lacking. The authors cite the information bottleneck literature, but we shouldn’t need to read all these papers, the main ideas should be summarized here."

We focus on multi-goal environments and goal-conditioned policies. The problem statement is quite simple: we aim to propose an algorithm whereby we incentive agents to learn task structure by training policies that perform well under a variety of goals, while not overfitting to any individual goal. We achieve this by training agents that, in addition to maximizing reward, minimize the policy dependence on the individual goal, quantified by the conditional mutual information  I(A; G | S). In order to minimize this quantity, we formulate it using ideas from variational information bottleneck. To make the paper self-explanatory, we added the mathematical description of the proposed method in the appendix (Section A).

"The argument for how the regularizer improves exploration seemed to me very hand-wavy and not well substantiated by experiments." 


We evaluate in Table 2 how the regularizer improves exploration, the basic idea is how can we transfer the knowledge in form of decision states. Basically the intuition is, what we would like from any unsupervised/supervised transferrable exploration technique is to build a policy that is somehow good for adapting to or solving new problems. Here we try to generalize  to new mazes the knowledge acquired by the encoder in the form of the KL estimator. Basically, the intuition is that high KL = interesting state, even before the agent has discovered a single path to the goal. So if we can use egocentric observations and generalize effectively, we can predict which points have high KL before we have even learned to traverse the maze, and then we can use these high-KL regions as rewards without the need to have solved that particular maze in advance, using knowledge transferred from other mazes.

To do this, we first train agents with a goal bottleneck on one set of environments (MultiRoomN2S6) where they learn the sensory cues that correspond to decision states. Then, we use this  knowledge to guide exploration on another set of environments (MultiRoomN3S4, MultiRoomN4S4, and MultiRoomN5S4). And hence in this new environments, we are training another policy from scratch,  And using the KL from D_KL(p(z|s, g) | N(0,1)) as an exploration bonus to guide exploration. As shown in Table 2, We outperform strong exploration methods like Curiosity Driven Learning (ICM) and count based exploration. We would be happy to add other comparisons which reviewer has in mind.

We did another experiment in which we  show how the bonus reward of visiting decision states can help the exploration on the training tasks as compared to the policy learned from equation (1), and the policies learned without information of other tasks. The intuition is that  once the agent has been trained using goal bottleneck, then in the transfer state, the proposed  model can identify novel subgoals for further exploration using the learned knowledge of decision states in part 1, thus guiding the agent through a sequence of potential decision  states and through new regions of the state space.  We show that the agent trained with InfoBot visits more diverse states as compared to the baseline agent. We ask the reviewer to refer to Section E in appendix.

"How is the regularizer applied with other policy search algorithms besides Reinforce? Was it done in the paper? I can’t say for sure. Specifically, when comparing to PPO, was the algorithm compared to a version of PPO augmented with this regularizer? Why yes or why no?"

Yes, the proposed method can be combined with any policy based method (such that we have a parameterized form for the policy). In our experiments, we combine this with a2c for Minigrid Env as well as Atari setup, PPO for mujoco based envs, as well as state of the art off policy algorithms like soft actor critic. (SAC). Since, our method relies on goals, in order to minimize the I(A;G|S), in Atari as well as mujoco domains, we use high value states as a proxy to the goal state following [1] . In order to implement this, we maintain a buffer of 20000 high value states, and choose the state with the highest value under the current value function, as a proxy to the goal.  

[1]  Recall Traces, <a href="https://arxiv.org/abs/1804.00379" target="_blank" rel="nofollow">https://arxiv.org/abs/1804.00379</a>
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bke-kjhq6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Proposed Method is general and can be combined with a2c/ppo/TD3/SAC. (3/4)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg8yhAqKm&amp;noteId=Bke-kjhq6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper989 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper989 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">As shown in the experiments, our method outperforms the baseline in the Atari case, as well as compared to PPO for continuous control tasks (Section 4.4), as well as Soft Actor Critic on Sparse reward Continuous control problems.

"More generally, experiments where more modern policy search algorithms are combined with the regularizer would be helpful. In particular, does it matter which policy search algorithm we use with this method?"

We combined the proposed method with state of the art off policy algorithm (Soft actor Critic), PPO and A2C. In general, the proposed method is agnostic to the chosen policy search algorithm. We would be happy to add more comparisons, which the reviewer has in mind.

"Experimental plots in section 4.4 are missing error bars, and I can’t tell if the results are significant without them."

Experimental results in section 4.4 are averaged over 5 random seeds. We found the baseline (PPO) as well as proposed method to have less variance across different runs with different random seeds. Since, we were plotting many baselines (InfoBot with Low value states, InfoBot with High Value States, InfoBot with zero KL cost), and hence for clarity, we did not include the error bars. But if the reviewer wants, we would be happy to include them. 

We would appreciate it if the reviewer could take another look at our changes and additional results, and let us know if the reviewer would like to request additional changes that would alleviate reviewers concerns. We hope that our updates to the manuscript address the reviewer's concerns about clarity, and we hope that the discussion above addresses the reviewer's concerns about empirical significance. We once again thank the reviewer for the thorough feedback of our work.

</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryxhXE9eAQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>InfoBot for Instruction Following - New Result (4/4)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg8yhAqKm&amp;noteId=ryxhXE9eAQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper989 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper989 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have added another result where we use our method for instruction following i.e the agent has to navigate to a particular goal where the goal is given by the language instruction. We ask the reviewer to refer to heading "InfoBot for Instruction Following" or refer to appendix (section G). 

We would appreciate it if the reviewer could take another look at our changes and additional results, and let us know if the reviewer would like to request additional changes that would alleviate reviewers concerns. We hope that our updates to the manuscript address the reviewer's concerns about clarity, and we hope that the discussion above addresses the reviewer's concerns about empirical significance. We once again thank the reviewer for the thorough feedback of our work.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_SkgYQUf_6m" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg8yhAqKm&amp;noteId=SkgYQUf_6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper989 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJlWokadnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea. Not sure the significance of its experimental results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg8yhAqKm&amp;noteId=SJlWokadnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper989 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper989 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=SJlWokadnQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes the concept of decision state, which is the state where decision is made “more” dependent to a particular goal. The authors propose a KL divergence regularization to learn the structure of the tasks, and then use this information to encourage the policy to visit the decision states. The method is tested on several different experiment setups.

In general the paper is well-written and easy to follow. Learning a more general policy is not new (as also discussed in the paper), but using the learned structure to further guide the exploration of the policy is novel and interesting.

I have a couple questions about the experimental part though, mostly about the baselines.
1. What is the reasoning behind the selection of the baselines, e.g. A2C as the baseline for the miniGrid experiments? 
2. What are the performances of the methods in Table 2, in direct policy generalization? Or is there any reason not reporting them here?
3.  What is the reasoning of picking “Count-base baseline” for Figure 4, rather than the method of curiosity-based exploration?
4. For the Mujoco tasks, there are couple ones outperforming PPO, e.g. TD3, SAC etc.. [1,2] The authors should include their results too. 
5. As an ablation study, it would be interesting to see how the bonus reward of visiting decision states can help the exploration on the training tasks, compared to the policy learned from equation (1), and the policies learned without information of other tasks.
6. Lastly, the idea of decision states can also be used in other RL algorithms. It would be also interesting to see if this idea can further improve their performances.

Other comments:
1. Equation (3) should be \le.
2. Why would Equation (5) hold? 
3. Right before section 2.2, incomplete sentence.


Disclaimer: The reviewer is not familiar with multitask reinforcement learning, and the miniGrid environment in the paper. Other reviewers should have better judgement on the significance of the experimental results.

[1] Haarnoja, Tuomas, et al. "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor." arXiv preprint arXiv:1801.01290 (2018).
[2] Fujimoto, Scott, Herke van Hoof, and Dave Meger. "Addressing Function Approximation Error in Actor-Critic Methods." arXiv preprint arXiv:1802.09477 (2018).</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkluo4qgCQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>InfoBot for Instruction Following - New Result</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg8yhAqKm&amp;noteId=rkluo4qgCQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper989 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper989 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have added another result where we use our method for instruction following i.e the agent has to navigate to a particular goal where the goal is given by the language instruction. We ask the reviewer to refer to heading "InfoBot for Instruction Following" or refer to appendix (section G). 

We would appreciate it if the reviewer could take another look at our changes and additional results, and let us know if the reviewer would like to request additional changes that would alleviate reviewers concerns. We hope that our updates to the manuscript address the reviewer's concerns about clarity, and we hope that the discussion above addresses the reviewer's concerns about empirical significance. We once again thank the reviewer for the thorough feedback of our work.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1eLE0i5am" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for your feedback!  (1/3)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg8yhAqKm&amp;noteId=S1eLE0i5am"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper989 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper989 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for their time and feedback. We  hope to address concerns the reviewer has here.

"What is the reasoning behind the selection of the baselines, e.g. A2C as the baseline for the miniGrid experiments? "

We build from A2C with using goal-conditioned policies and the KL regularization. Hence, The A2C with no kl-regularization is the immediate baseline to consider.  Since for maze experiments, the env is of the  nature of mini-grid POMDP environments with sparse rewards, as well as discrete action, a2c worked out of the box and hence was the most straightforward baseline for comparison.  We would be happy to add other comparisons which reviewer has in mind. 


&gt;&gt; What are the performances of the methods in Table 2, in direct policy generalization? Or is there any reason not reporting them here?

The setup in direct policy generalization is different as to what we evaluate in Table 2. Direct policy generalization refers to first training an agent with a goal bottleneck on one set of environments (MultiRoomN2S6), and then evaluate the trained agent (without fine tuning on new set of environment ((MultiRoomN3S4, MultiRoomN4S4, and MultiRoomN5S4)).

What we evaluate in Table 2, is how can we transfer the knowledge in form of decision states. 
Basically the intuition is, what we would like from any unsupervised/supervised transferrable exploration technique is to build a policy that is somehow good for adapting to or solving new problems. Here we try to generalize  to new mazes the knowledge acquired by the encoder in the form of the KL estimator. Basically, the intuition is that high KL = interesting state, even before the agent has discovered a single path to the goal. So if we can use egocentric observations and generalize effectively, we can predict which points have high KL before we have even learned to traverse the maze, and then we can use these high-KL regions as rewards without the need to have solved that particular maze in advance, using knowledge transferred from other mazes.

To do this, we first train agents with a goal bottleneck on one set of environments (MultiRoomN2S6) where they learn the sensory cues that correspond to decision states. Then, we use this  knowledge to guide exploration on another set of environments (MultiRoomN3S4, MultiRoomN4S4, and MultiRoomN5S4). And hence in this new environments, we are training another policy from scratch,  And using the KL from D_KL(p(z|s, g) | N(0,1)) as an exploration bonus to guide exploration. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkgXc1n5pQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Visitation Count  (2/3)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg8yhAqKm&amp;noteId=SkgXc1n5pQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper989 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper989 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">"What is the reasoning of picking “Count-base baseline” for Figure 4, rather than the method of curiosity-based exploration?"

The variance across different runs for curiosity based baseline was high, and thats why we reported only count based exploration. Though in table 2, we report the “MAX” performance we got using curiosity driven exploration (over 5 runs), while for InfoBot we average over 5 random seeds. 

"would be interesting to see how the bonus reward of visiting decision states can help the exploration on the training tasks, compared to the policy learned from equation (1), and the policies learned without information of other tasks."

We thank the reviewer for the suggestion. This is indeed an insightful experiment which can help understand the exploration mechanism can effectively identifies decision states, and thus the  model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision  states and through new regions of the state space. Section E in appendix shows that the agent trained with InfoBot visits more diverse states as compared to the baseline agent. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkeROX29am" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Decision States (Middle ground b/w Model based and Model free RL) / Comparison to Soft Actor Critic/ Multiagent communication (3/3)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg8yhAqKm&amp;noteId=SkeROX29am"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper989 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper989 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The idea of decision states can also be used in other RL algorithms. It would be also interesting to see if this idea can further improve their performances.

The reviewer is right. The notion of decision points can be used at other places too like planning, combination of model based and model free RL.  Identifying useful decision states can provide a
comfortable middle ground between model-free reasoning and model-based planning. For example,
imagine planning over individual decision states, while using model-free knowledge to navigate
between bottlenecks: aspects of the environment that are physically complex but vary little between
problem instances are handled in a model-free way (the navigation between decision points), while
the particular decision points that are relevant to the task can be handled by explicitly reasoning
about causality, in a more abstract representation. We demonstrate this using a similar setup as in
imagination augmented agents  (Weber et al., 2017). In imagination augmented agents, model free
agents are augmented with imagination, such that the imagination model can be queried to make
predictions about the future. We use the dynamics models to simulate imagined trajectories, which
are then summarized by a neural network and this summary is provided as additional context to
a policy network.  Here, we use the output of the imagination module as a “goal” and we want to
show that only near the decision points (i.e potential subgoals) the agent wants to make use of the
information which is a result of running imagination module).  Here, we want to see, at which points in the state space the policy wants to access the information provided by running the imagination module.  Ideally, only at the decision states (i.e potential sub-goals) policy should access the output of the imagination moduleFor more details, we ask the reviewer to refer to section 4.7 in the main paper.

(Weber et. al, 2017) - Imagination Augmented Agents  <a href="https://arxiv.org/abs/1707.06203" target="_blank" rel="nofollow">https://arxiv.org/abs/1707.06203</a>

======================================================================

2) Application of the proposed method in multi-agent communication, such that the goal in the proposed method corresponds to the information obtained because of communication with another agent in multi-agent communication channel. Here,  we  want  to  show  that  by  training  agents  to  develop  “default  behaviours”  as  well  as  the knowledge of when to break those behaviours, using an information bottleneck can also help in other scenarios like multi-agent communication. Consider multiagent communication, where in order to solve a task, agents require communicating with another agents. Ideally, an agent would would like to communicate with other agent, only when its essential to communicate, i.e the agents would like to minimize the communication with another agents. Here we show that selectively deciding when to
communicate with another agent can result in faster learning. We follow the same  experimental setup as in the paper (Mordatch and Abbeel, 2018). 

Method                                                 Train Reward                   TestReward
No Communication                               -0.919                                  -0.920
Communication                                      -0.36                                   -0.472
Communication (with KL cost)            -0.293                                  -0.38 

(Lower is better). More details about this experimental setup can be found in Section D (Appendix). 

I. Mordatch and P. Abbeel.  Emergence of grounded compositional language in multi-agent pop-
ulations.

===================================================================================

3. Comparison to Soft Actor Critic - We compared the  proposed method  to the state of the art off-policy methods like SAC (Soft Actor Critic). For this domain, we again use high value states as an approximation to the goal state following [1] . In order to implement this, we maintain a buffer of 20000 states, and choose the state with the highest value under the current value function, as a proxy to the goal. 

[1]  Recall Traces, https://arxiv.org/abs/1804.00379
[2]  SAC, Soft Actor Critic https://arxiv.org/abs/1801.01290

We ask the reviewer to refer to heading "InfoBot Comparison to State of the art off policy methods (Soft Actor Critic)". Also, for more details refer to  Section F in the appendix. 

Closing:
Thank you for your time. We hope you find that our revision addresses your concerns.
 We hope that our updates to the manuscript address the reviewer's concerns about clarity, and we hope that the discussion above addresses the reviewer's concerns about empirical significance. We once again thank the reviewer for the thorough feedback of our manuscript. Please let us know if anything is unclear here, if you’re uncertain about part of the argument, or if there is any other comparison that would be helpful in clarifying things more.

</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJxR90hl0Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for the rebuttal</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg8yhAqKm&amp;noteId=rJxR90hl0Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper989 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper989 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I have read other reviews and I think the rebuttal has addressed my concerns on the significance of the experimental results, thus I increase the score from 6 to 7. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_BJxrL1eLoQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Potentially useful but poorly motivated and evaluated</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg8yhAqKm&amp;noteId=BJxrL1eLoQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper989 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper989 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a method of regularising goal-conditioned policies with a mutual information term. While this is potentially useful, I found the motivation for the approach and the experimental results insufficient. On top of that the presentation could also use some improvements. I do not recommend acceptance at this time.

The introduction is vague and involves undefined terms such as "useful habits". It is not clear what problems the authors have in mind and why exactly they propose their specific method. The presentation of the method itself is not self-contained and often relies on references to other papers to the point where it is difficult to understand just by reading the paper. Some symbols are not defined, for example what is Z and why is it discrete?

The experiments are rather weak, they are missing comparison to strong exploration baselines and goal-oriented baselines.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Skg4AV5l0X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>InfoBot for Instruction Following - New Result </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg8yhAqKm&amp;noteId=Skg4AV5l0X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper989 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper989 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">In order to be empirically thorough, we have added another result where we use our method for instruction following i.e the agent has to navigate to a particular goal where the goal is given by the language instruction. We ask the reviewer to refer to heading "InfoBot for Instruction Following" or refer to appendix (section G). 

We would appreciate it if the reviewer could take another look at our changes and additional results, and let us know if the reviewer would like to request additional changes that would alleviate reviewers concerns. We hope that our updates to the manuscript address the reviewer's concerns about clarity, and we hope that the discussion above addresses the reviewer's concerns about empirical significance. We once again thank the reviewer for the thorough feedback of our work.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJgBWjs9am" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for your feedback!  (1/2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg8yhAqKm&amp;noteId=HJgBWjs9am"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper989 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper989 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the feedback. We have conducted additional experiments to address the concerns raised about the evaluation, and we clarify specific points below. We believe that these additions address all of your concerns about the work, though we would appreciate any additional comments or feedback that you might have.

"While this is potentially useful, I found the motivation for the approach  It is not clear what problems the authors have in mind and why exactly they propose their specific method."

We acknowledge that the paper was certainly lacking polish and accept that this may have made the paper difficult to read in places. We have uploaded a revised version in which we have revised the problem statement and writing as per the reviewer's suggestions. We focus on multi-goal environments and goal-conditioned policies. The problem statement is quite simple: we aim to propose an algorithm whereby we incentive agents to learn task structure by training policies that perform well under a variety of goals, while not overfitting to any individual goal. We achieve this by training agents that, in addition to maximizing reward, minimize the policy dependence on the individual goal, quantified by the conditional mutual information  I(A; G | S). In order to minimize this quantity, we formulate it using ideas from variational information bottleneck. To make the paper self-explanatory, we added the mathematical description of the proposed method in the appendix (Section A).

"The presentation of the method itself is not self-contained and often relies on references to other papers to the point where it is difficult to understand just by reading the paper. "

We again acknowledge that the paper was missing certain parts which made the paper difficult to read. We have added another section in the appendix which gives a more mathematical description of the proposed approach. We realized because of the way we have explained things there could be some fundamental misunderstanding about the proposed method. Thus, we would like to clarify this misunderstanding, not only with the intent of convincing you of the idea behind the proposed method but also with the intent of making amends to  the method description where necessary so that readers may not arrive at the same conclusions as you. We added the mathematical description of the proposed framework in the appendix (Section A). 
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1xMDas56m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>More experimental results (Navigation/Comparison to Soft Actor Critic / Multiagent communication)  (2/2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJg8yhAqKm&amp;noteId=H1xMDas56m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper989 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper989 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt;&gt; The experiments are rather weak, they are missing comparison to strong exploration baselines and goal-oriented baselines.

In order to address reviewer’s concern, we did several  more experiments.

We added these experiments to the main paper

1) More challenging Navigation setup.  - We ask the reviewer to refer to heading "More challenging navigation Environment". Also for more details refer to Section 4.6 in the main paper. 

2) Comparison to State of the art Off policy Methods (SAC) in sparse  rewards. We ask the reviewer to refer to heading "InfoBot Comparison to State of the art off policy methods (Soft Actor Critic)". Also, for more details refer to  Section F in the appendix.
 
3) Application of the proposed method in multi-agent communication, such that the goal in the proposed method corresponds to the information obtained because of communication with another agent in multi-agent communication channel. Here,  we  want  to  show  that  by  training  agents  to  develop  “default  behaviours”  as  well  as  the knowledge of when to break those behaviours, using an information bottleneck can also help in other
scenarios like multi-agent communication. Consider multiagent communication, where in order to
solve a task, agents require communicating with another agents. Ideally, an agent would would like
to communicate with other agent, only when its essential to communicate, i.e the agents would like
to minimize the communication with another agents. Here we show that selectively deciding when to
communicate with another agent can result in faster learning. We follow the same  experimental setup as in the paper (Mordatch and Abbeel, 2018). 

Method                                                 Train Reward                   TestReward
No Communication                               -0.919                                  -0.920
Communication                                      -0.36                                   -0.472
Communication (with KL cost)            -0.293                                  -0.38 

(Lower is better). More details about this experimental setup can be found in Section D (Appendix). 

I. Mordatch and P. Abbeel.  Emergence of grounded compositional language in multi-agent pop-
ulations.


We would appreciate it if the reviewer could take another look at our changes and additional results, and let us know if the reviewer has request for additional changes that would alleviate the reviewer's concerns.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>