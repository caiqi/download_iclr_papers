<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Complement Objective Training | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Complement Objective Training" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HyM7AiA5YX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Complement Objective Training" />
      <meta name="og:description" content="Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years. Although being a..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HyM7AiA5YX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Complement Objective Training</a> <a class="note_content_pdf" href="/pdf?id=HyM7AiA5YX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 14 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019complement,    &#10;title={Complement Objective Training},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HyM7AiA5YX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=HyM7AiA5YX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Learning with a primary objective, such as softmax cross entropy for classification and sequence generation, has been the norm for training deep neural networks for years. Although being a widely-adopted approach, using cross entropy as the primary objective exploits mostly the information from the ground-truth class for maximizing data likelihood, and largely ignores information from the complement (incorrect) classes. We argue that, in addition to the primary objective, training also using a complement objective that leverages information from the complement classes can be effective in improving model performance. This motivates us to study a new training paradigm that maximizes the likelihood of the ground-truth class while neutralizing the probabilities of the complement classes. We conduct extensive experiments on multiple tasks ranging from computer vision to natural language understanding. The experimental results confirm that, compared to the conventional training with just one primary objective, training also with the complement objective further improves the performance of the state-of-the-art models across all tasks. In addition to the accuracy improvement, we also show that models trained with both primary and complement objectives are more robust to adversarial attacks.
</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">optimization, entropy, image recognition, natural language understanding, adversarial attacks, deep learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose Complement Objective Training (COT), a new training paradigm that optimizes both the primary and complement objectives for effectively learning the parameters of neural networks.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">13 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rJeeh8J5pm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Adversarial robustness claim is highly misleading</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyM7AiA5YX&amp;noteId=rJeeh8J5pm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper880 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">This paper argues in the abstract that "we also show that models trained with both primary and complement objectives are more robust to adversarial attacks."

However, in the evaluation section, the authors only attempt a very simple transferability attack: generate adversarial examples on one model, and transfer them to another. This does not imply adversarial robustness, neither in the white-box nor black-box setting.

To argue black-box robustness, the authors should evaluate against more recent black-box attacks such as the Boundary Attack (ICLR'18) or SPSA (ICML'18). Both of these attacks have effectively broken many black-box defenses in the past.

If the authors wish wish to argue full white-box adversarial robustness, they should further try optimization based attacks (Madry et al. 2018, Carlini &amp; Wagner 2017).

As is, this paper should not claim robustness to adversarial examples: at best, it can claim a 10% improvement in accuracy to transfer attacks.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1ebZ7sRam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for the comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyM7AiA5YX&amp;noteId=S1ebZ7sRam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper880 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper880 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comments. We understand that the adversarial attack techniques used here may not be state-of-the-art methods; however, we want to emphasize that the primary goal of this paper is to improve model's accuracy, although experimental results do show that robustness is also one of the benefits of the models trained by COT. 

We agree with the reviewer that transfer adversarial attack is different from the classic settings of adversarial attacks. To verify our method under standard adversarial attacks, we have conducted additional experiments on white-box attack, and provided the results below; the experimental results confirmed that COT is indeed more robust to this type of attacks, and therefore we believe the main conclusion that COT is more robust (compared to baselines) to adversarial attack still holds. We will add these results of the white-box attack into the final version of the paper. Additionally, we will rename the current experiments to “transfer attacks” to avoid confusions. The definition of the transfer attacks can be found in several recent publications [1, 2, 3].

For the white-box attacks, we conducted the experiments as also suggested by AnonReviewer3. The update is to set adversarial perturbations to be Epsilon * Sign (Primary gradient + Complement gradient). Results indicate that COT is more robust to this type of white-box attacks under standard settings.

Test errors on Cifar10 under FGSM white-box adversarial attacks
===========================================================
				              Baseline	                        COT
ResNet-110 			      62.23%                 		48.09%
PreAct ResNet-18               65.60%                 		50.58%
ResNeXt-29 (2×64d)           70.24%                 		57.06%
WideResNet-28-10	       59.39%                 		49.70%
DenseNet-BC-121               65.97%                 		50.80%
===========================================================
 
The reviewer also suggested to try out several recent methods on white-box and black-box attacks. We do agree with the reviewer that it's a great idea. However, since the main focus of the current paper is to improve accuracy, and the manuscript is already close to the page limit, we feel it's better to study this problem in a separate paper. As a matter of fact, we are planning on a follow-up work with the focus on the robustness of the models trained with COT. 
 
[1] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow. “Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples.” Arxiv, 2016

[2] Yanpei Liu, Xinyun Chen, Chang Liu, Dawn Song. “Delving into Transferable Adversarial Examples and Black-box Attacks.” In International Conference on Learning Representation, 2017.

[3] Wieland Brendel, Jonas Rauber, Matthias Bethge. “Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models.” In International Conference on Learning Representation, 2018.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkeB3kugCQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>FGSM results are not strong attacks</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyM7AiA5YX&amp;noteId=BkeB3kugCQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper880 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I understand this is not the main purpose of your paper, but again, you claim "we also show that models trained with both primary and complement objectives are more robust to adversarial attacks." At present, you simply have not shown that fact.

Thank you for running some white-box numbers, but FGSM is unfortunately not sufficient. I hate to appeal to authority, to argue this, but see ( <a href="https://openreview.net/forum?id=SkgVRiC9Km&amp;noteId=rkxYnt8JpQ&amp;noteId=rkxYnt8JpQ" target="_blank" rel="nofollow">https://openreview.net/forum?id=SkgVRiC9Km&amp;noteId=rkxYnt8JpQ&amp;noteId=rkxYnt8JpQ</a> ).

Prior work, and papers under submission this year, make very careful claims with respect to adversarial examples.  See for example the Manifold Mixup paper under submission this year that instead writes the correct and honest statement "Manifold Mixup achieves ... robustness to single-step adversarial attacks". You should claim only what you can demonstrate.

It is perfectly fine that you want to only show adversarial robustness as a side-effect of your main work, but you should be accurate in how you phrase what you have shown. There is a big difference between being robust to single-step attacks and transfer attacks, and actually being robust. Hundreds of papers claim the former, very few claim the latter.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJlWte4WR7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Follow-up</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyM7AiA5YX&amp;noteId=HJlWte4WR7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper880 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper880 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the clarifications on the recent research trending in adversarial attacks as well as your great suggestions on making the claim precise. We will adopt your suggestion and make it clear in the paper that the proposed training objectives make the models more robust to single-step adversarial attacks instead of claiming general robustness. We will use this new statement consistently across our updated version of the paper. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SyeTbV6eA7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>How to talk about FGSM Results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyM7AiA5YX&amp;noteId=SyeTbV6eA7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper880 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I agree with the Nov/19 anonymous comment, and one thing that I'll add is that I think it's worth discussing robustness to the FGSM attack, because it means that the decision boundary is being moved away from the data points, in a certain subset of directions.  I think this is different from adversarial robustness in general, which considers perturbations which give maximum error.  

It would be interesting to think about something like "the volume of the subset of the epsilon-ball around the data points which increases error by k%" - and then we could claim that some methods reduce that volume without claiming that every single point in the epsilon-ball has low error.  </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_B1lv2Bdph7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice idea but leaves several questions not answered</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyM7AiA5YX&amp;noteId=B1lv2Bdph7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper880 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper880 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this manuscript, the authors propose a secondary objective for softmax minimization. This complementary objective is based on evaluating the information gathered from the incorrect classes. Considering these two objectives leads to a new training approach. The manuscript ends with a collection of tests on a variety of problems.

This is an interesting point of view but the manuscript lacks discussion on several important questions:

1) How is this idea related to regularization? If we increase the regularization parameter, we can attain sparse parameter vectors. 
2) Would this method also complement from overfitting?
3) In the numerical experiments, the comparison is carried out against a "baseline" method. Do the authors use regularization with these baseline methods? I believe the comparison will be fair  if the regularization option is turned on for the baseline methods.
4) Why combining the two objectives in a single optimization problem and then solving the resulting problem is not an option instead of the alternating method given in Algorithm 1?
5) How does alternating between two objectives change the training time? Do the authors use backpropagation?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1ejg15YaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer1 [1/2] </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyM7AiA5YX&amp;noteId=r1ejg15YaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper880 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper880 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank the reviewer for all the insightful feedbacks. Below we provide the explanations for each question or comment raised by the reviewer:


(Q1) How is this idea related to regularization? If we increase the regularization parameter, we can attain sparse parameter vectors. 

(A1) Conventionally, regularization techniques (e.g., Ridge or Lasso) are applied on the parameter space. We want to point out that all the results reported in the manuscript, for both baselines and models trained by COT, have already used L2-norm regularization on the parameter space, exactly as specified in the original papers  (e.g., ResNet [1], WideResNet [2], and DenseNet [3]). In other words, COT is applied on top of the existent of those regularization techniques.

If your questions haven’t been addressed satisfactorily, please kindly let us know and we will be happy to discuss further.


(Q2) Would this method also complement from overfitting?

(A2) Thank you for the comment. We would like to further clarify what you meant by saying “complement from overfitting.” Our interpretation of the question is: whether COT could be used to fight against overfitting. Overfitting means a model fails to generalize, and in our paper we have reported the generalized performance of models trained by COT on the test data, which confirms models trained by COT generalize better. In addition, we also calculate the loss gap "(testing loss - training loss)" and report the results in the following table, where a smaller gap indicates that a model generalizes better. Experimental results confirm that models trained by COT seem to generalize better due to the smaller gap between training and testing loss.

 "(Testing loss - training loss)” from the state-of-the-art architectures on Cifar10 
==================================================
				                Baseline	        COT
ResNet-110 			        0.36                 0.33
PreAct ResNet-18                 0.28                 0.26
ResNeXt-29 (2×64d)             0.20                 0.19
WideResNet-28-10		0.23                 0.21
DenseNet-BC-121           	0.22                 0.22
=================================================


(Q3) In the numerical experiments, the comparison is carried out against a "baseline" method. Do the authors use regularization with these baseline methods? I believe the comparison will be fair if the regularization option is turned on for the baseline methods.

(A3) Yes, the regularization (e.g., L2 Norm) techniques are used in all of the baseline methods, as specified in their original papers (e.g., ResNet [1], WideResNet [2], and DenseNet [3]). We agree with the reviewer that “the comparison will be fair if the regularization option is turned on for the baseline methods,” and that is exactly we did in our paper: all the hyper-parameters, regularization and other training techniques are configured in the same way as in the original papers. For the details of experimental setup, please refer to the Section 3.2 in our manuscript.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJlOtAKta7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer1 [2/2]</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyM7AiA5YX&amp;noteId=HJlOtAKta7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper880 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper880 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
(Q4) Why combining the two objectives in a single optimization problem and then solving the resulting problem is not an option instead of the alternating method given in Algorithm 1?

(A4) We are very grateful for this novel idea, and we have conducted several preliminary experiments to explore this idea. Below are the comparisons between (a) the original COT method, and (b) the approach of combining the two objectives into one single objective. The experimental results show that the original COT method works better in almost all cases, and we conjecture that these two methods converge to different local minima. This idea is worth exploring, and we leave it as a straight-line future work. 

Test error of the state-of-the-art architectures on Cifar10 
===========================================================
				                Combining into one objective	  COT
ResNet-110 			        7.42%                 		                  6.84%
PreAct ResNet-18                 4.92%                 		                  4.86%
ResNeXt-29 (2×64d)             4.79%                 		                  4.55%
WideResNet-28-10		4.00%                 		                  4.30%
DenseNet-BC-121           	4.64%                 		                  4.62%
===========================================================

Test error of the state-of-the-art architectures on Cifar100
===========================================================
				                 Combining into one objective	  COT
ResNet-110 			         28.80%                 		                  27.90%
PreAct ResNet-18                  25.30%                 		                  24.73%
ResNeXt-29 (2×64d)              23.20%                 		                  21.90%
WideResNet-28-10		 21.96%                 		                  20.99%
DenseNet-BC-121           	 22.17%                 		                  20.54%
===========================================================


(Q5) How does alternating between two objectives change the training time? Do the authors use backpropagation?

(A5) Yes, we do use backpropagation. One additional backpropagation is required in each iteration when applying COT, and therefore the overall training time is about 1.6 times longer according to our experiments.


[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. “Deep Residual Learning for Image Recognition.” In IEEE Conference on Computer Vision and Pattern Recognition, 2016.
[2] Sergey Zagoruyko, Nikos Komodakis. “Wide Residual Networks
.” In British Machine Vision Conference, 2016.
[3] Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger, David Lopez-Paz. “Densely Connected Convolutional Networks
.” In IEEE Conference on Computer Vision and Pattern Recognition, 2017.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkgvjRFtpm" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyM7AiA5YX&amp;noteId=rkgvjRFtpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper880 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_Syx8_6U63X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Simple and sensible heuristic with impressive improvement</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyM7AiA5YX&amp;noteId=Syx8_6U63X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper880 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper880 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper considers augmenting the cross-entropy objective with "complement" objective maximization, which aims at neutralizing the predicted probabilities of classes other than the ground truth one. The main idea is to help the ground truth label stands out more easily by smoothing out potential peaks in non-ground-truth labels. The wide application of the cross-entropy objective makes this approach applicable to many different machine/deep learning applications varying from computer vision to NLP. 

The paper is well-written, with a clear explanation for the motivation of introducing the complement entropy objective and several good visualization of its empirical effects (e.g., Figures 1 and 2). The numerical experiments also incorporate a wide spectrum of applications and network structures as well as dataset sizes, and the performance improvement is quite impressive and consistent. In particular, the adversarial attacks example looks very interesting.

One small suggestion is that the authors can also make some comments on the connection between the two-step update algorithm (Algorithm 1) with multi-objective optimization. In particular, I would suggest the authors also try some multi-objective optimization techniques apart from the simple but effective heuristics, and see if some Pareto-optimality can be guaranteed and better practical improvement can be achieved.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJg8OW9FT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyM7AiA5YX&amp;noteId=rJg8OW9FT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper880 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper880 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
(Q1) One small suggestion is that the authors can also make some comments on the connection between the two-step update algorithm (Algorithm 1) with multi-objective optimization. In particular, I would suggest the authors also try some multi-objective optimization techniques apart from the simple but effective heuristics, and see if some Pareto-optimality can be guaranteed and better practical improvement can be achieved.

(A1) We sincerely thank the reviewer for the helpful and constructive suggestion about associating COT with multi-objective optimization. This is really a brilliant idea. As a straight-line future work, we will survey multi-objective optimization techniques, and explore the direction of formulating COT into a multi-objective optimization problem.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_r1gc8uIw27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting new idea, good experimental results, some points to clarify.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyM7AiA5YX&amp;noteId=r1gc8uIw27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper880 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper880 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
========
Summary
========

The paper deals with the training of neural networks for classification or sequence generation tasks, using a cross-entropy loss. Minimizing the cross-entropy means maximizing the predicted probabilities of the ground-truth classes (averaged over the samples). The authors introduce a "complementary entropy" loss with the goal of minimizing the predicted probabilities of the complementary (incorrect) classes. To do that, they use the average of sample-wise entropy over the complement classes. By maximizing this entropy, the predicted complementary probabilities are encouraged to be equal and therefore, the authors claim that it neutralizes them as the number of classes grows large. The proposed training procedure, named COT, consists of alternating between the optimization of the two losses.

The procedure is tested on image classification tasks with different datasets (CIFAR-10, CIFAR-100, Street View House Numbers, Tiny ImageNet and ImageNet), machine translation (training using IWSLT dataset, validation and test using TED tst2012/2013 datasets), and speech recognition (Gooogle Commands dataset). In the experiments, COT outperforms state-of-the-art models for each task/dataset.

Adversarial attacks are also considered for the classification of images of CIFAR-10: using the Fast Gradient Sign and Basic Iterative Fast Gradient Sign methods on different models, adversarial examples specifically designed for each model, are generated. Then results of these models are compared to COT on these examples. The authors admit
that the results are biased since the adversarial attacks only target part of the COT objective, hence more accurate comparisons should be done in future work.

===========================
 Main comments and questions
===========================

End of page 1: "the model behavior for classes other than the ground  truth stays unharnessed and not well-defined". The probabilities  should still sum up to 1, so if the ground truth one is maximized,  the others are actually implicitly minimized. No?

Page 3, sec 2.1: "optimizing on the complement entropy drives ŷ_ij to 1/(K − 1)". I believe that it drives each term ŷ_ij /(1 − ŷ_ig ) to be equal to 1/(K-1). Therefore, it drives ŷ_ij to (1 − ŷ_ig)/(K-1) for j!=g.

This indeed flattens the ŷ_ij for j!=g, but the effect on ŷ_ig is not controlled. In particular this latter can decrease. Then in the next step of the algorithm, ŷ_ig will be maximized, but with no explicit control over the complementary probabilities. There are two objectives that are optimized over the same variable theta. So the question is, are we sure that this procedure will converge? What prevents situations where the probabilities will alternate between two values? 

For example, with 4 classes, we look at the predicted probabilities of a given sample of class 1:
Suppose after step 1 of Algo 1, the predicted probabilities are:  0.5 0.3 0.1 0.1 
After step 2:  0.1 0.3 0.3 0.3
Then step 1: 0.5 0.3 0.1 0.1
Then step 2: 0.1 0.3 0.3 0.3
And so on... Can this happen? Or why not? Did the algorithm have trouble converging in any of the experiments?

Sec 3.1:
"additional efforts for tuning hyper-parameters might be required for optimizers to achieve the best performance": Which hyper-parameters are considered here? If it is the learning rate, why not use a different one, tuned for each objective?

Sec 3.2:
The additional optimization makes each training iteration more costly. How much more? How do the total running times of COT compare to the ones of the baselines? I think this should be mentioned in the paper.

Sec 3.4:
As the authors mention, the results are biased and so the comparison is not fair here. Therefore I wonder about the  relevance of this section. Isn't there an easy way to adapt the attacks to the two objectives to be able to illustrate the conjectured robustness of COT? For example, naively having a two steps perturbation of the input: one based on the gradient of the primary objective and then perturb the result using the gradient of the complementary objective?

===========================
Secondary comments and typos
===========================

Page 3, sec 2.1: "...the proposed COT also optimizes the complement objective for neutralizing the predicted probabilities...", using maximizes instead of optimizes would be clearer.

In the definition of the complement entropy, equation (2), C takes as parameter only y^hat_Cbar but then in the formula, ŷ_ig appears. Shouldn't C take all \hat_y as an argument in this case?

Algorithm 1 page 4: I find it confusing that the (artificial) variable that appears in the argmin (resp. argmax) is theta_{t-1}
(resp. theta'_t) which is the previous parameter. Is there a reason for this choice?

Sec 3:
"We perform extensive experiments to evaluate COT on the tasks" --&gt; COT on tasks

"compare it with the baseline algorithms that achieve state-of-the-art in the respective domain." --&gt; domainS

"to evaluate the model’s robustness trained by COT when attacked" needs reformulation.

"we select a state- of-the-art model that has the open-source implementation" --&gt; an open-source implementation

Sec 3.2:
Figure 4: why is the median reported and not the mean (as in Figure 3, Tables 2 and 3)?

Table 3 and 4: why is it the validation error that is reported and not the test error?

Sec 3.3:
"Neural machine translation (NMT) has populated the use of neural sequence models": populated has not the intended meaning.

"We apply the same pre-processing steps as shown in the model" --&gt; in the paper?

Sec 3.4:
"We believe that the models trained using COT are generalized better" --&gt; "..using COT generalize better"

"using both FGSM and I-FGSM method" --&gt; methodS

"The baseline models are the same as Section 3.2." --&gt; as in Section 3.2.

"the number of iteration is set at 10." --&gt; to 10

"using complement objective may help defend adversarial attacks." --&gt; defend against

"Studying on COT and adversarial attacks.." --&gt; could be better formulated

References: there are some inconsistencies (e.g.: initials versus first name)


Pros
====
- Paper is clear and well-written
- It seems to me that it is a new original idea
- Wide applicability
- Extensive convincing experimental results

Cons
====
- No theoretical guarantee that the procedure should converge
- The training time may be twice longer (to clarify)
- The adversarial section, as it is,  does not seem relevant for me

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1eR7XqFaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer3 [1/2]</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyM7AiA5YX&amp;noteId=S1eR7XqFaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper880 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper880 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
We sincerely thank the reviewer for the useful and detailed comments. Below we provide explanations for each of your comments or questions.  


(Q1) End of page 1: "the model behavior for classes other than the ground truth stays unharnessed and not well-defined". The probabilities should still sum up to 1, so if the ground truth one is maximized,  the others are actually implicitly minimized. No?

(A1) Your understanding is totally correct. We have changed the original text to a more clear statement:

“Therefore, for classes other than the ground truth, the model behavior is not explicitly optimized --- their predicted probabilities are indirectly minimized when ŷ_ig is maximized since the probabilities sum up to 1.”

We want to thank the reviewer again for crystalizing the manuscript.


(Q2) Page 3, sec 2.1: "optimizing on the complement entropy drives ŷ_ij to 1/(K − 1)". I believe that it drives each term ŷ_ij /(1 − ŷ_ig ) to be equal to 1/(K-1). Therefore, it drives ŷ_ij to (1 − ŷ_ig)/(K-1) for j!=g.

This indeed flattens the ŷ_ij for j!=g, but the effect on ŷ_ig is not controlled. In particular this latter can decrease. Then in the next step of the algorithm, ŷ_ig will be maximized, but with no explicit control over the complementary probabilities. There are two objectives that are optimized over the same variable theta. So the question is, are we sure that this procedure will converge? What prevents situations where the probabilities will alternate between two values? 

For example, with 4 classes, we look at the predicted probabilities of a given sample of class 1:
Suppose after step 1 of Algo 1, the predicted probabilities are:  0.5 0.3 0.1 0.1 
After step 2:  0.1 0.3 0.3 0.3
Then step 1: 0.5 0.3 0.1 0.1
Then step 2: 0.1 0.3 0.3 0.3
And so on... Can this happen? Or why not? Did the algorithm have trouble converging in any of the experiments?

(A2) Thanks for the detailed comment. As the reviewer pointed out, “drives ŷ_ij to 1/(K − 1)” was indeed a typo and should be corrected to “drive ŷ_ij /(1 − ŷ_ig) to 1/(K-1)”. We have modified the manuscript correspondingly. Indeed, maximizing complement entropy in Eq(2) only drives “ŷ_ij /(1 − ŷ_ig) to 1/(K-1)”, and therefore in the example provided above, the predicted probabilities after step 2 can be “0.1 0.3 0.3 0.3” or “0.5, (1 - 0.5)/3, (1 - 0.5)/3, (1 - 0.5)/3”, or other values so long as the incorrect classes (ŷ_ij's) receive similar predicted probabilities. According to our observations from the experiments, the probabilities tend to converge to “0.5, (1 - 0.5)/3, (1 - 0.5)/3, (1 - 0.5)/3”. Experiments show that the algorithm does not have trouble converging; the algorithm converges smoothly in all the experiments we have conducted. Again, we thank the reviewer for the insightful comment; studying the theory of COT convergence is an intriguing topic and we leave it as a future work.


(Q3) Sec 3.1: "additional efforts for tuning hyper-parameters might be required for optimizers to achieve the best performance": Which hyper-parameters are considered here? If it is the learning rate, why not use a different one, tuned for each objective?

(A3) Hyper-parameters in this statement indeed refer to the learning rate, and we have modified the statement in the manuscript to avoid confusion; the modified statement is provided below:

“therefore, additional efforts for tuning learning rates might be required for optimizers to achieve the best performance.”

Regarding the second question about tuning learning rates, we have conducted several experiments with different learning rates specifically tuned for each objective. The experimental results show that using the same learning rate for both primary and complement objectives leads to the best performance when Eq(3) is used as the complement objective.


(Q4) Sec 3.2: The additional optimization makes each training iteration more costly. How much more? How do the total running times of COT compare to the ones of the baselines? I think this should be mentioned in the paper.

(A4) Yes, one additional backpropagation is required in each iteration when applying COT. On average, the total training time is about 1.6 times longer compared to the baselines. Thanks for the suggestion, and we have included this in the latest manuscript (section 2.2).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1eWWmcKaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer3 [2/2]</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyM7AiA5YX&amp;noteId=S1eWWmcKaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper880 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper880 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
(Q5) Sec 3.4: As the authors mention, the results are biased and so the comparison is not fair here. Therefore I wonder about the  relevance of this section. Isn't there an easy way to adapt the attacks to the two objectives to be able to illustrate the conjectured robustness of COT? For example, naively having a two steps perturbation of the input: one based on the gradient of the primary objective and then perturb the result using the gradient of the complementary objective?

(A5) Thanks for the comment. We should have made clear that “black box” [1] (rather than “white box”) adversarial attacks are considered in the manuscript. Specifically, we follow the common practice of generating adversarial examples using both FGSM and I-FGSM methods with the gradients from a baseline model; this way, the model trained by COT is actually a “black box” to these attacks. We have modified the manuscript to clarify this part. Also, thanks for the great suggestion of forming adversarial attacks using “both” gradients (from both primary &amp; complement objectives). We are designing and conducting experiments at the moment and will share results when ready.


For the part of secondary comments and typos, we appreciate your thorough reading again and have corrected all these typos according to your suggestions. Meanwhile, in the following, we also provided explanations to your secondary comments.


(Q1) Page 3, sec 2.1: "...the proposed COT also optimizes the complement objective for neutralizing the predicted probabilities...", using maximizes instead of optimizes would be clearer.

(A1) Thanks for the suggestion. We have reworded the manuscript to “maximizes.”


(Q2) In the definition of the complement entropy, equation (2), C takes as parameter only y^hat_Cbar but then in the formula, ŷ_ig appears. Shouldn't C take all \hat_y as an argument in this case?

(A2) Since the probabilities sum up to one, ŷ_ig can be inferred from y^hat_Cbar. Also, for us, it seems more direct and clear to show that complement entropy is calculated from y^hat_Cbar when C takes y^hat_Cbar as the only argument. Therefore, we incline to keep the orignal formulation. If the reviewer has strong preference, please kindly let us know and we are happy to make changes accordingly.


(Q3) Algorithm 1 page 4: I find it confusing that the (artificial) variable that appears in the argmin (resp. argmax) is theta_{t-1}
(resp. theta'_t) which is the previous parameter. Is there a reason for this choice?

(A3) Thanks for the comment. Originally, we want to notify readers that there are two backprops within one iteration. We agree that those symbols are confusing and therefore we have modified the manuscript with those symbols removed.


(Q4) Sec 3.2  Figure 4: why is the median reported and not the mean (as in Figure 3, Tables 2 and 3)?

(A4) Thanks for pointing this out. This is a typo and we have already corrected it in the manuscript: median -&gt; mean.


(Q5) Sec 3.2, Table 3 and 4: why is it the validation error that is reported and not the test error?

(A5) Thanks for the detailed comment. For a fair comparison, we report the error in the exact same way as the open-sourced repo from the ResNet authors:
<a href="https://github.com/KaimingHe/deep-residual-networks." target="_blank" rel="nofollow">https://github.com/KaimingHe/deep-residual-networks.</a>


(Q6) Sec 3.3: "Neural machine translation (NMT) has populated the use of neural sequence models": populated has not the intended meaning.

(A6) We thank the reviewer for pointing out this typo. We have already corrected it in our manuscript: populated -&gt; popularized


(Q7) "Studying on COT and adversarial attacks.." --&gt; could be better formulated

(A7) Thanks for the comment again. We have modified the manuscript as follows: "Studying on the relationship between COT and adversarial attacks…”


[1] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz. “Mixup: Beyond Empirical Risk Minimization.” In International Conference on Learning Representation, 2018.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>