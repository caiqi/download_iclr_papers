<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Excessive Invariance Causes Adversarial Vulnerability | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Excessive Invariance Causes Adversarial Vulnerability" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BkfbpsAcF7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Excessive Invariance Causes Adversarial Vulnerability" />
      <meta name="og:description" content="Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BkfbpsAcF7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Excessive Invariance Causes Adversarial Vulnerability</a> <a class="note_content_pdf" href="/pdf?id=BkfbpsAcF7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 10 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019excessive,    &#10;title={Excessive Invariance Causes Adversarial Vulnerability},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BkfbpsAcF7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=BkfbpsAcF7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shift. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from epsilon-adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks.
We show that such excessive invariance occurs across various tasks and architecture types. We propose the usage of bijective deep networks to enable access to all variations. We introduce metameric sampling as an analytic attack for these networks, requiring no optimization, and show that it uncovers large subspaces of misclassified inputs. Then we apply these networks to MNIST and ImageNet and show that one can manipulate the class-specific content of almost any image without changing the hidden activations. Further, we identify an insufficiency of the cross-entropy loss as a reason for these failures. We extend the standard cross-entropy loss to strengthen the model against such manipulations via an information-theoretic analysis, providing the first approach tailored explicitly to overcome invariance-based vulnerability. We conclude by empirically illustrating its ability to control undesirable class-specific invariance in multiple experiments, showing promise to overcome one major cause for adversarial examples.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Generalization, Adversarial Examples, Invariance, Information Theory, Invertible Networks</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We show deep networks are not only too sensitive to task-irrelevant changes of their input, but also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Byx23FoQaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Revision uploaded</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkfbpsAcF7&amp;noteId=Byx23FoQaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper778 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper778 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Reviewers, we thank you very much for helping us to substantially improve the manuscript.

We have addressed all raised concerns either with additional experiments and results, with additional discussions in the manuscript or through other aspects of our revision.

We were delighted to see the positive reaction by all reviewers to our developed ideas and your suggestions and concerns greatly improved the paper. The new distribution shift experiments, as well as new results and discussion of non-bijective networks and their relationship to bijective ones, significantly increase the practical relevance of the work. 

Given the tension between the positive comments to most of our contributions, the ratings and the fact that the main concerns are related to our proposed solution, we would like to point out that the developed training objective is only one out of four major contributions of the paper.

We list our updated contributions here again for clarity:

1 - We introduce an alternative viewpoint on adversarial examples, one of the major failures in modern machine learning algorithms, give a formal definition of it and show its practical relevance for commonly used architectures in the updated experiments and discussion.

2 - We build a competitive bijective ImageNet/MNIST classifier to tractably compute such adversarial examples exactly. Based on this, we provide what may be the first analytic adversarial attack method in the literature.

3 - We prove that a major reason for invariance-based vulnerability is the commonly used cross-entropy objective and show from an information-theoretic viewpoint what may be done to overcome this.

4 - We put our theoretical results into practice: based on bijective networks we introduce a practically useful loss and illustrate as a proof-of-concept that it largely overcomes the problem of excessive invariance, making it a promising way forward. Additionally, we have now included more quantitative experiments showing robustness to adversarial distribution shifts on a newly introduced benchmark.

In the revision we have:

-- Thoroughly revised and updated the whole manuscript to make all of our contributions more clear and incorporate all raised concerns.
-- Updated figures and descriptions and moved large parts of section 2 to the appendix to improve clarity. 
-- Added an adversarial distribution shift benchmark to stress test our proposed objective and show its effectiveness in challenging settings.
-- Added new results on non-bijective networks for the metameric samples and the distribution shift experiments to show non-bijective networks have the same issues as the bijective networks we use. 
-- Added a discussion on the relationship between ResNets and RevNet-type networks, providing evidence that they are closely related. 
-- Added additional references from the literature providing evidence of false excessive invariance in non-bijective architectures.
-- Added a random batch of metameric samples to the appendix, to showcase the consistency of our results.

Please let us know if you have any more questions or if there is anything else we can do to make you reconsider your rating.

Thank you once again for your effort.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SyeLf8916Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Very interesting ideas, could use a few additional experiments to be more convincing</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkfbpsAcF7&amp;noteId=SyeLf8916Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper778 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper778 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=SyeLf8916Q" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper explores adversarial examples by investigating an invertible neural network. They begin by first correctly pointing out limitations with the commonly adopted "l_p adversarial example" definition in literature. The main idea involves looking at the preimage of different embeddings in the final layer of an invertible neural network. By training a classifier on top of the final embedding of the invertible network the authors are able to partition the final embedding into a set of "semantic variables", which are the components used for classification of the classifier, and a set of "nuisance variables" which are the complement of the logit variables. This partition allows the authors to define entire subspaces of adversarial images by holding the logit variables fixed and varying the nuisance variables, and applying the inverse to these modified embeddings. The authors are able to find many incorrectly classified images with this inversion technique. The authors then define a new loss which minimizes the mutual information between the nuisance variables and the predicted label. 

I found the ideas in this paper quite interesting and novel. Starting with the toy problem of adversarial spheres is great, and it's convincing that the inversion technique can be used to find errors on this dataset even when the classification accuracy is (empirically) 100%. The resulting adversarial images generated by applying their technique are also quite interesting, and this is a cool interesting way to study the robustness of networks in non-iid settings.

The main weakness is on the evaluation of their proposed new training objective, and I have a few suggestions as to how to strengthen this evaluation. It would be very convincing to me if the authors could show that their new training objective increases robustness to distributional shift. A potential benchmark for distributional shift could be <a href="https://arxiv.org/abs/1807.01697" target="_blank" rel="nofollow">https://arxiv.org/abs/1807.01697</a> (or just picking a subset of these image corruptions). If the proposed objective shows improvement on this benchmark (or a related one) then this would be a solid contribution.

One question I have for the authors is how typical the behavior in Figure 4 is? For any fixing of the logits, are all/most metameric samples classifiable by a human oracle? That is do you ever get garbage images from this sampling process. Adding a collection of random samples to the Appendix to demonstrate typical behavior could help demonstrate this.

Edit: After paper additions I am changing my score to a 7. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByeqLhs7TX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Added new experiments on distribution shift and large batch of metameric samples</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkfbpsAcF7&amp;noteId=ByeqLhs7TX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper778 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper778 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
--------------------------------------------------------

We thank you very much for acknowledging our work as interesting and novel, as well as for the appreciation of our developed methodologies.

We answer your questions below.

--------------------------------------------------------

Q: Does the new training objective increase robustness to distributional shift?
--
Thank you for raising this point. To shed light on the effect of our loss under adversarial distribution shifts we have added new experiments on a dataset we introduce to precisely test our claims. We term the dataset shiftMNIST and designed it such that it follows distribution shifts D_Adv of the form we assumed for Theorem 6.

Our results reveal, that our proposed loss does indeed reduce the errors under challenging distribution shifts up to 38% as compared to cross-entropy trained ResNets and RevNets, highlighting the efficacy of our proposed objective. 

Further, the results also show once again how badly standard networks can fail, even though in one task only one single pixel is removed, leaving the image semantics almost entirely unchanged. The results are one more piece of evidence for the insufficiency of cross-entropy based information maximization and the excessive invariance it may lead to in practice. 

We sincerely thank you for bringing this up.

--------------------------------------------------------

Q: What is the typical behavior of samples shown in Figure 4?
--
The metameric samples shown are representative and we have observed similar quality throughout the whole validation set, sometimes with slight colored artifacts though. We have added a large batch of metameric samples to the appendix to give the reader a better idea about their typical behavior. 

--------------------------------------------------------

We believe your review have substantially improved the manuscript, thank you.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HklfNANAh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting problem with an unconvincing solution</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkfbpsAcF7&amp;noteId=HklfNANAh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper778 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper778 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies a new perspective on why adversarial examples exist in machine learning -- instead of seeing adversarial examples as the result of a classifier being sensitive to changes in irrelevant information (aka nuisance), the authors see them as the result of a classifier being invariant to changes in relevant (aka semantic) information. They show how to efficiently find such adversarial examples in bijective networks. Moreover, they propose to modify the training objective so that the bijective networks could be more robust to such attacks.

Pros:
 -- clarity is good (except for a few places, e.g. no definition of F(x)_i in Definition 1; Page 6 "three ways forward" item 3: I(y;z_n|z_s) = I(y;z_s) should be I(y;z_n|z_s) = I(y;z_n).)
 -- the idea is original to the best of my knowledge
 -- the mathematical motivation is sound
 -- Figure 6 seems to show that the proposed defense works on MNIST (However, would you provide more details on how you interpolated z_n? Moreover, what do the images generated with z_s from one input and z_n from another input look like (in your method)?)

Cons:
 -- scope: as all the presented problems and solutions assume bijective mapping, I wonder how is it relevant to the traditional perspective of adversarial attack and defense? It seems to me that the contribution of this paper is identifying a problem of bijective networks and then proposing a solution, thus its significance is restricted.
 -- method: while the mathematical motivation is sound, I'm not sure if the proposed training objective can achieve that goal. To elaborate, I see problems with both terms added in the proposed loss function:
 (a.) for the objective of maximizing the cross entropy of the nuisance classifier, it is possible that I(y;z_n) is not reduced, but rather the information about y is encoded in a way that the nuisance classifier is not able to decode, similar to what happens in a one-way function (for example, see <a href="https://en.wikipedia.org/wiki/Cryptographic_hash_function" target="_blank" rel="nofollow">https://en.wikipedia.org/wiki/Cryptographic_hash_function</a> ). In the MNIST experiments, the nuisance classifier is a three-layer MLP, which may be too weak and susceptible to information concealing.
 (b.) for the objective of maximizing the likelihood of a factorized model of p(z_s, z_n), I don't see how optimizing it would reduce I(z_s; z_n). In general, even if z_s and z_n are strongly correlated, one can still fit such a factorized model. This only ensures that I(Z_s; Z_n) = 0 for Z_s, Z_n *sampled from the model*, but does not necessarily reduce I(z_s; z_n) for z_s, z_n *used to train the model*. The discrepancy between p(Z_s, Z_n) and p(z_s, z_n) could be huge, in which case one has the model misspecification problem which is another topic.
 (c.) a side question: why is the MLE objective using likelihood rather than log likelihood? Since the two cross entropy losses are similar to log likelihood, I feel there is a mismatch here.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJeo0yhma7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Added new experiments on non-bijective networks and proposed objective alongside thorough revision of manuscript</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkfbpsAcF7&amp;noteId=HJeo0yhma7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper778 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper778 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
--------------------------------------------------------

We are glad that you find most of our major contributions original, interesting, clear and mathematically sound.
We also thank you for your thoughtful questions and comments, we address them below.

--------------------------------------------------------

Q: How are findings related to non-bijective networks?
--
Thank you for bringing this up, we have revised the manuscript to answer this important question very clearly to show our identified problems, analysis and conclusion are not limited to bijective networks.
We summarize below.

----------
-- Our identified problem of excessive invariance occurs in many other networks as well.
----------

We have added results on the gradient-based equivalent of our analytic metameric sampling attack to the paper. We match the logit vector of one image with the logits of another image via gradient-based optimization and no norm-based restriction on the input. We do so on an ImageNet-trained state of the art ResNet-154 and see that the problem we have identified in bijective nets is the same here, if not worse as the metameric samples look even cleaner. Qualitative results are added to figure 5.

Besides that, multiple papers have observed excessive invariance. On the adversarial spheres problem [1], for instance, the authors show their quadratic network does almost perfectly well while ignoring up to 60% of *semantically meaningful* input dimensions. Another line of work has also shown that similar behavior can appear in ReLU networks as well [2].

We have also added an additional set of experiments to the revised manuscript that shows how cross-entropy trained ResNets fail badly under distribution shifts that exploit their excessive invariance, giving another piece of evidence that our findings are not limited to bijective networks, but applicable to the most successful deep network architecture around as well.

----------
-- There is a close relationship between bijective nets and SOTA architectures.
----------

Bijective networks are closely related to ResNets, they are in fact provably bijective under mild assumptions, as shown by a recent publication [3]. Further, it has been shown that ResNets and RevNet-type networks differ only in their dimension splitting scheme from one another [4]. And finally, bijective iRevNets have been shown to have many equivalent progressive properties to ResNets throughout the layers of their learned representation [5].

In summary, there is ample evidence, that bijective RevNet-type networks are not the reason for the problems we observe, but rather extremely similar to ResNets, the de-facto state-of-the-art architecture, while providing a powerful framework to study and combat problems like excessive invariance.

[1] Gilmer, Justin, et al. "Adversarial spheres." 
[2] Behrmann, Jens, et al. "Analysis of Invariance and Robustness via Invertibility of ReLU-Networks."
[3] Behrmann, Jens, David Duvenaud, and JÃ¶rn-Henrik Jacobsen. "Invertible Residual Networks."
[4] Grathwohl, Will, et al. "FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models."
[5] Jacobsen, JÃ¶rn-Henrik, Arnold Smeulders, and Edouard Oyallon. "i-RevNet: Deep Invertible Networks."</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1xjee27aX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Part II</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkfbpsAcF7&amp;noteId=B1xjee27aX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper778 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper778 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
---------------------------------------------------------

Q: Can the training objective achieve its goal?

----------
-- (a) The nuisance classifier is not powerful enough to decode y from z_n.
----------

This is indeed a common problem when formulating a bound this way and it is the same problem GANs face. However, in practice, GANs often work and we also find that the nuisance classifier does indeed do its job, one could even validate this post-hoc by training a more powerful nuisance classifier to confirm it.

Additionally, we also have metameric sampling as a validation method. If the information about the class is only hidden in z_n, but not removed, then metameric sampling would reveal this. Replacing z_n of one category with a z_n from another category would then change the category of the reconstruction, but we see that this is not happening when applying our loss. Thus, we conclude that the objective is successful, albeit it having its challenges.

----------
-- (b) The factorial maximum likelihood objective does not lead to independence.
----------

We agree, that there is no guarantee that the loss will lead to full independence, but it does encourage it at least.
On the other hand, our evaluation method (metameric sampling) is not based on samples from the model but is based on the activations of real data points. Thus, according to your argumentation, this sampling method would reveal strong dependencies between the subspaces. In practice, we see this is not the case, as shown in figure 7 on the right, where combinations of z_n from one class and z_s from another do indeed lead to a change of nuisance/style in the original image, but not to a change of category. Empirically this means our objective was successful and most of the label information has been removed from z_n.

To further analyze the objective, we have added another experiment to assess if it can successfully defend against targeted distribution shifts as considered in Theorem 6.
We introduce a new dataset termed shiftMNIST, it augments MNIST with additional highly predictive features at train time and removes or randomizes those features at test time, while leaving the digits themselves as the stable predictive variable.

Our experiments reveal, that the baseline cross-entropy trained ResNet and fiRevNet fail badly on these problems, while our proposed loss reduces the error under such distribution shift up to 38%. This provides more evidence that our proposed objective does achieve its goal in practice.

----------

In summary, we do agree that the lower bound and the maximum likelihood objectives have their respective issues and we added some discussion on this to the manuscript. However, in practice, the metameric samples and our additional distribution shift experiments show that the loss does, in fact, work as intended, making it a promising way forward.

---------------------------------------------------------

Q: What do the images generated with z_s from one input and z_n from another input look like (in your method)?
--
Those images (the metameric samples) are already shown in the last row in the top block of figure 7, we have adapted the figure and added some more description to it, to make everything more clear.
In the baseline the metameric samples are adversarial examples, meaning one can turn any image into any class without changing the logits at all. With our objective (shown on the right side), this is not possible anymore as keeping z_s fixed and exchanging z_n only affects the style of the image, not its class-specific content. The objective has achieved its goal and successfully defended against the metameric sampling attack.

---------------------------------------------------------

Minor:
We have fixed the typos and added the log to the MLE objective, thank you.

---------------------------------------------------------

Thank you once again for the detailed review, we were able to significantly improve the manuscript based on it.
We have revised multiple parts, added new experiments and added discussions to answer your concerns.

We hope we were able to answer everything to your satisfaction, please let us know if there are any more open points.

Thank you once again!</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_r1e1SFU_2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The ideas are appealing and should enventually lead to fine contributions but the paper is disbalanced with wrong detail distribution </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkfbpsAcF7&amp;noteId=r1e1SFU_2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper778 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper778 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=r1e1SFU_2m" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper focuses on adversarial vulnerability of neural networks, and more specifically on perturbation-based versus invariance-based adversarial examples and how using bijective networks (with so-called metameric sampling) may help overcoming issues related to invariance. The approach is used to get around insufficiencies of cross-entropy-based information-maximization, as illustrated on experiments where the proposed variation on CE outperforms CE. 

While I am not a neural network expert, I felt that the ideas developed in the paper are worthwhile and should eventally lead to useful contributions and be published. This being said, I did not find the paper in its present form to be fit for publication in a high-tier conference or journal. The main reason for this is the disbalance between the somehow heavy and overly commented first four pages (especially in Section 2) contrasting with the surprisingly moderate level of detail when it comes to bijective networks, supposedly the heart of the actual original contribution. To me this is severely affecting the overall quality of the paper. The contents of sections 3 and 4 seem relevant, but I struggled find out what precisely is the main contribution in the end, probably because of the lack of detail on bijective networks mentioned before. Again, I am not an expert, and I will indicate that in the system of course, but while I cannot completely judge all aspects of the technical relevance and the originality of the approach, I am fairly convinced that the paper deserves to be substantially revised before it can be accepted for publication.   

Edit: After paper additions I am changing my score to a 6. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BklM_OiQ6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thoroughly revised manuscript uploaded</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkfbpsAcF7&amp;noteId=BklM_OiQ6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper778 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper778 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
--------------------------------------------------------

We thank you very much for acknowledging our work being appealing and our contributions being publication-worthy.
We also thank you for your thoughts and comments on the structure of the manuscript.

--------------------------------------------------------

Q: Overly commented first pages, imbalanced with section 3 and 4.

We have done our best to fix this and have substantially revised the paper. We removed large portions of section 2 and added it to the appendix, we added additional details about bijective networks, re-structured section 3 and 4 and added another experiment to emphasise our main contributions more. Finally, we have adjusted the abstract and contributions in the introduction accordingly.

--------------------------------------------------------

Q: Lacking detail on bijective network.

The main components we are using are based on Real-NVP[1]/Glow[2] and iRevNet[3] networks, which are widely known and cited in the paper, so we decided not to put too much focus on their details.
However, in the revision we have added some additional details, for instance, we have added figure 3 that explains the architecture we are using.

[1] Dinh, Laurent, Jascha Sohl-Dickstein, and Samy Bengio. "Density estimation using Real NVP." 
[2] Kingma, Diederik P., and Prafulla Dhariwal. "Glow: Generative flow with invertible 1x1 convolutions."
[3] Jacobsen, JÃ¶rn-Henrik, Arnold Smeulders, and Edouard Oyallon. "i-RevNet: Deep Invertible Networks."
--------------------------------------------------------

Please let us know if you have any more comments or concerns!

Thank you once again.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>