<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJeY6sR9KX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Aligning Artificial Neural Networks to the Brain yields Shallow..." />
      <meta name="og:description" content="Deep artificial neural networks with spatially repeated processing (a.k.a., deep convolutional ANNs) have been established as the best class of candidate models of visual processing in the primate..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJeY6sR9KX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures</a> <a class="note_content_pdf" href="/pdf?id=BJeY6sR9KX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019aligning,    &#10;title={Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BJeY6sR9KX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Deep artificial neural networks with spatially repeated processing (a.k.a., deep convolutional ANNs) have been established as the best class of candidate models of visual processing in the primate ventral visual processing stream. Over the past five years, these ANNs have evolved from a simple feedforward eight-layer architecture in AlexNet to extremely deep and branching NASNet architectures, demonstrating increasingly better object categorization performance. Here we ask, as ANNs have continued to evolve in performance, are they also strong candidate models for the brain? To answer this question, we developed Brain-Score, a composite of neural and behavioral benchmarks that score any ANN on how brain-like it is, together with an online platform where ANNs can be submitted to receive a Brain-Score and their rank relative to other models. Deploying our framework on dozens of state-of-the-art ANNs, we found that ResNet and DenseNet families of models are the closest models from the Machine Learning community to primate ventral visual stream. Curiously, best current ImageNet models, such as PNASNet, were not the top-performing models on Brain-Score. Despite high scores, these deep models are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. To further map onto anatomy and validate our approach, we built CORnet-S: a neural network developed by using Brain-Score as a guide with the anatomical constraints of compactness and recurrence. Although a shallow model with four anatomically mapped areas with recurrent connectivity, CORnet-S is a top model on Brain-Score and outperforms similarly compact models on ImageNet.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Computational Neuroscience, Brain-Inspired, Neural Networks, Simplified Models, Recurrent Neural Networks, Computer Vision</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BkgUZyJi27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Not enough analysis</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJeY6sR9KX&amp;noteId=BkgUZyJi27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper824 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper824 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Please consider this rubric when writing your review:
1. Briefly establish your personal expertise in the field of the paper.
2. Concisely summarize the contributions of the paper.
3. Evaluate the quality and composition of the work.
4. Place the work in context of prior work, and evaluate this work's novelty.
5. Provide critique of each theorem or experiment that is relevant to your judgment of the paper's novelty and quality.
6. Provide a summary judgment if the work is significant and of interest to the community.

1. I am a researcher working at the intersection of machine learning and
biological vision.  I have experience with neural network models and
visual neurophysiology.

2. This paper makes two contributions: 1) It develops Brain-Score - a
dataset and error metric for animal visual single-cell recordings.  2)
It develops (and brain-scores) a new shallow(ish) recurrent network
that performs well on ImageNet and scores highly on brain-score. 

3. The development of Brain-Score is a useful invention for the field.  A nice
aspect of Brain-Score is that responses in both V4 and IT as well as behavioral
responses are provided.   I think it could be more useful if the temporal dynamics (instead of the
mean number of spikes) was included.    This would allow to compare temporal
responses in order to compare "brain-like" matches.

4. This general idea is somewhat similar to a June 2018 Arxiv paper
(Task-Driven Convolutional Recurrent Models of the Visual System)
<a href="https://arxiv.org/abs/1807.00053" target="_blank" rel="nofollow">https://arxiv.org/abs/1807.00053</a>
but this is a novel contribution as it is uses the Brain-Score dataset.

One limitation of this approach relative to the June 2018 ArXiv paper
is that the Brain-Score method is just representing the mean neural
response to each image - The Arxiv paper shows that different models
can have different temporal responses that can also be used to decide
which is a closer match to the brain.

5. More analysis of why CORNET-S is best among compact models would greatly
strengthen this paper.  What do the receptive fields look like?  How do they compare
to the other models.  What about the other high performing networks (e.g. DenseNet-169)?
How sensitive are the results to each type of weight in the network?   What about feedback connections
(instead of local recurrent connections)? 

6. This paper makes a significant contribution, in part due to the
development and open-sourcing of Brain-Score.  The significance of the
contribution of the CORnet-S architecture is limited by the
lack of analysis into what aspects make it better than other models.



</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1goI8dgTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>can you please clarify "type of weight"?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJeY6sR9KX&amp;noteId=r1goI8dgTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper824 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper824 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review. Could you please clarify what you mean by the "type of weight in the network"?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1ewCw7HTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>clarifying "type of weight"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJeY6sR9KX&amp;noteId=B1ewCw7HTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper824 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper824 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The question is generally asking about what specifically about the CORNET-S architecture makes it perform well 
on Brain-Score (and which for ImageNet) compared to other similar networks.    The types of weights refers to feedback vs recurrent vs convolutional filters vs pooling (and connections between which layers are most critical).
I was just asking for some form of ablation study or analysis similar to Figure 4 in <a href="https://arxiv.org/pdf/1807.00053v2.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1807.00053v2.pdf</a>  to show what parts of your architecture you feel are critical for a good 
model of visual processing in the brain.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJgSTjDd6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJeY6sR9KX&amp;noteId=SJgSTjDd6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper824 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper824 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Great, thank you for the quick clarification! All of these are great suggestions, and we are currently running analyses to quantify what parts are critical to the model.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_Hyx7wwpchX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting attempt to bridge the gap between ML and neuroscience</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJeY6sR9KX&amp;noteId=Hyx7wwpchX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper824 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper824 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this interesting study, the authors propose a score (BrainScore) to (1) compare neural representations of an ANN trained on imagenet with primate neural activity in V4 and IT, and (2) test whether ANN and primate make the same mistakes on image classification.  They also create a shallow recurrent neural network (Cornet) that performs well according to their score and also reasonably well on imagenet classification task given its shallow architecture.

The analyses are rigorous and the idea of such a score as a tool for guiding neuroscientists building models of the visual system is novel and interesting.

Major drawbacks:

1. Uncertain contribution to ML: it remains unclear whether architectures guided by the brain score will indeed generalize better to other tasks, as the authors suggest.

2. Uncertain contribution to neuroscience: it remains unclear whether finding the ANN resembling the real visual system most among a collection of models will inform us about the inner working of the brain.


The article would also benefit from the following clarifications:

3. Are the recurrent connections helping performance of Cornet on imagenet and/or on BrainScore?

4. Did you find a correlation between the neural predictivity score and behavioral predictivity score across networks tested? If yes, it would be interesting to mention.

5. When comparing neural predictivity score across models, is a model with more neurons artificially advantaged by the simple fact that there is more likely a linear combination of neurons that map to primate neural activations? Is cross-validation enough to control for this potential bias?

6. Fig1: what are the gray dots?

7. “but it also does not make any assumptions about significant differences in the scores, which would be present in ranking. “
What does this mean?

8. How does Cornet compare to this other recent work: <a href="https://arxiv.org/abs/1807.00053" target="_blank" rel="nofollow">https://arxiv.org/abs/1807.00053</a> (June 20 2018) ?

Conclusion:
This study presents an interesting attempt at bridging the gap between machine learning and neuroscience. Although the impact that this score will have in both ML and Neuroscience fields remains uncertain, the work is sufficiently novel and interesting to be published at ICLR. I am fairly confident in my evaluation as I work at the intersection of deep learning and neuroscience.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BklwxmF537" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>promising work, further tests needed</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJeY6sR9KX&amp;noteId=BklwxmF537"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper824 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper824 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This is an interesting paper in which the authors propose a shallow neural network, the architecture of which was optimized to maximize brain score, and show that it outperforms other shallow networks at imagenet classification. The goal of this paper — allowing brain data to improve our neural nets — is great. The paper is well written (except for a comment on clarity right below), and presents an interesting take on solving this problem. 

It is a little unclear how the authors made CORnet optimize brain score: “However, note that CORnet-S was developed using Brain-Score as a guiding benchmark and although it was never directly used in model search or optimization, testing CORnet-S on Brain-Score is not a completely independent test.” Making these steps clearer is crucial for evaluating better what the model means. In the discussion “We have tested hundreds of architectures before finding CORnet-S circuitry and thus it is possible that the proposed circuits could have a strong relation to biological implementations.” implies that the authors trained models with different architectures until the brain score was maximized after training. A hundred(s) times of training on 2760 + 2400 datapoints are probably plenty to overfit the brainscore datasets. The brain score is probably compromised after this, and it would be hard to make claims about the results on the brain modeling side. The author acknowledge this limitation, but perhaps a better thing is to add an additional dataset, perhaps one from different animal recordings, or from human fMRI? 

Arguably, the goal of the paper is to obtain a model that overpowers other simpler models, and not necessarily to make claims about the brain. The interesting part of the paper is that the shallow model does work better than other shallow models. The authors mention that brain score helps CORnet be better at generalizing to other datasets. Including these results would definitely strengthen the claim since both brain score and imagenet have been trained on hundreds of times so far. 

Another way to show that brain score helps is to show it generalizes above or differently from optimizing other models. What would have happened if the authors stuck to a simple, shallow model and instead of optimizing brain score optimized performance (hundreds of times) on some selected image dataset (this selection dataset is separate from imagenet, but the actual training is done on imagenet) and then tested performance on imagenet? Is the effect due to the brain or to the independent testing on another dataset?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>