<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Finite Automata Can be Linearly Decoded from Language-Recognizing RNNs | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Finite Automata Can be Linearly Decoded from Language-Recognizing RNNs" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1zeHnA9KX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Finite Automata Can be Linearly Decoded from Language-Recognizing RNNs" />
      <meta name="og:description" content="We study the internal representations that a recurrent neural network (RNN) uses while learning to recognize a regular formal language. Specifically, we train an RNN on positive and negative..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1zeHnA9KX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Finite Automata Can be Linearly Decoded from Language-Recognizing RNNs</a> <a class="note_content_pdf" href="/pdf?id=H1zeHnA9KX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019finite,    &#10;title={Finite Automata Can be Linearly Decoded from Language-Recognizing RNNs},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=H1zeHnA9KX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We study the internal representations that a recurrent neural network (RNN) uses while learning to recognize a regular formal language. Specifically, we train an RNN on positive and negative examples from a regular language, and ask if there is a simple decoding function that maps states of this RNN to states of the minimal deterministic finite automaton (MDFA) for the language. Our experiments show that such a decoding function exists, that it is in fact linear, but that it maps states of the RNN not to MDFA states, but to states of an abstraction obtained by clustering small sets of MDFA states into "superstates". A qualitative analysis reveals that the abstraction often has a simple interpretation. Overall, the results suggest a strong structural relationship between internal representations used by RNNs and finite automata, and explain the well-known ability of RNNs to recognize formal grammatical structure. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Language recognition, Recurrent Neural Networks, Representation Learning, deterministic finite automaton</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Finite Automata Can be Linearly decoded from Language-Recognizing RNNs</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_B1xVWwh3hm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting exploratory research, some more examples are desired</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1zeHnA9KX&amp;noteId=B1xVWwh3hm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1511 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1511 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper investigates internal working of RNN, by mapping its hidden states
to the nodes of minimal DFAs that generated the training inputs and its 
abstractions. Authors found that in fact such a mapping exists, and a linear
decoder suffices for the purpose. 
Inspecting some of the minimal DFAs that correspond to regular expressions, 
induced state abstractions are intuitive and interpretable from a viewpoint of
training RNNs by training sequences.

This paper is interesting, and the central idea of using formal languages to
generate feeding inputs is good (in fact, I am also doing a different research
that also leverages a formal grammar with RNN).

Most of the paper is clear, so I have only a few minor comments:

- In Figures 4 and 5, the most complex MDFA of 14 nodes does not have the
  lowest testing accuracies. In other words, testing accuracies is not
  generally proportional to the complexity of MDFA. Why does this happen?

- As noted in the footnote in page 5, state abstraction is driven by the idea
  of hierarchical grammars. Then, as briefly noted in the conclusion, why not
  using a simple CFG or PCFG to generate training sequences? 
  In this case, state abstractions are clear by definition, and it is curious
  to see if RNN actually learns abstract states (such as NP and VP in natural
  language) through mapping from hidden states to abstracted states.

- Because this paper is exploratory, I would like to see more examples
  beyond only the two in Figure 6. Is it possible to generate a regular 
  expression itself randomly to feed into RNN?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJllpRIqn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea, serious clarity problems</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1zeHnA9KX&amp;noteId=HJllpRIqn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1511 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1511 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper aims to show that an RNN trained to recognize regular languages effectively focuses on a more abstract representation of the FSA of the corresponding language. 

Understanding the type of information encoded in the hidden states of RNNs is an important research question. Recent results have shown connections between existing RNN architectures and both weighted (e.g., Chen et al., NAACL 2018, Peng et al., EMNLP 2018) and unweighted (Weiss et al., ACL 2018) FSAs. This paper asks a simple question: when trained to recognize regular languages, do RNNs converge on the same states as the corresponding FSA? While exploring solutions to this question is potentially interesting, there are significant clarity issues in this paper which make it hard to understand it. Also, the main claim of the paper â€” that the RNN is focusing on a low level abstraction of thew FSA â€” is not backed-up by the results.

Comments:

â€” The authors claim that the RNN states map to FSA states with *low* coarseness, but Figure 3b (which is never referred to in textâ€¦) shows that in most cases the ratio of coarseness is at least 1/3, and in some cases &gt; 1/2. 

â€” Clarity:
While the introduction is relatively clear starting from the middle of section 3 there are multiple clarity issues in this paper. In the current state of affairs it is hard for me to evaluate the full contribution of the paper.

- The definitions in section 3 were somewhat confusing. What is the conceptual difference between the two accuracy definitions? 

- When combining two states, does the new FSA accept most of the strings in the original FSAs? some of them? can you quantify that? Also, figure 6 (which kind of addresses this question) would be much more helpful if it used simple expressions, and demonstrated how the new FSA looks like after the merge.

- section 4 leaves many important questions unanswered:
1. Which RNN was used? which model? which parameters? which training regime? etc.
2. How were the expressions sampled? the authors mention that they were randomly sampled, so how come they talk about DATE and EMAIL expressions?
3. What is the basic accuracy of the RNN classifier (before decoding)? is it able to learn to recognize the language? to what accuracy? 

- Many of the tables and figures are never referred to in text (Figure 3b, Figure 5)

- In Figure 6, there is a mismatch between the regular expression (e.g., [0-9]{3}â€¦.) and the transitions on the FSA (a-d, @).

- How come Figure 3a goes up to 1.1? isnâ€™t it bounded by 1? (100%?)

- The negative sampling procedure should be described in the main text, not the appendix. Also, it is not clear how come shuffling the characters is considered an independent distribution.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Bkl2Tb__nX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Well written paper -- One major concern</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1zeHnA9KX&amp;noteId=Bkl2Tb__nX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1511 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1511 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Paper Summary -
The authors trained RNNs to recognize formal languages defined by random regular expressions, then measured the accuracy of decoders that predict states of the minimal deterministic finite automata (MDFA) from the RNN hidden states. They then perform a greedy search over partitions of the set of MDFA states to find the groups of states which, when merged into a single decoder target, maximize prediction accuracy. For both the MDFA and the merged classes prediction problems, linear decoders perform as well as non-linear decoders.
Clarity - The paper is very clear, both in its prose and maths.
Originality - I don't know of any prior work that approaches the relationship between RNNs and automata in quite this way.
Quality/Significance - I have one major concern about the interpretation of the experiments in this paper.

The paper seems to express the following logic:
1 - linear (and non-linear) decoders aren't so good at predicting MDFA states from RNN hidden states
2 - if we make an "abstract" finite automata (FA) by merging states of the MDFA to optimize decoder performance, the linear (and non-linear) decoders are much better at predicting this new, smaller FA's states.
3 - thus, trained RNNs implement something like an abstract FA to recognize formal languages.

However, a more appropriate interpretation of these experiments seems to be:
1 - (same)
2 - if we find the output classes the decoder is most often confused between, then merge them into one class, the decoder's performance increases -- trivially. in other words, you just removed the hardest parts of the classification problem, so performance increased. note: performance also increases because there are fewer classes in the merged-state FA prediction problem (e.g., chance accuracy is higher).
3 - thus, from these experiments it's hard to say much about the relationship between trained RNNs and finite automata.

I see that the "accuracy" measurement for the merged-state FA prediction problem, \rho, is somewhat more complicated than I would have expected; e.g., it takes into account \delta and f(h_t) as well as f(h_{t+1}). Ultimately, this formulation still asks whether any state in the merged state-set that contains f(h) transitions under the MDFA to the any state in the merged state-set that contains f(h_{t+1}). As a result, as far as I can tell the basic logic of the interpretation I laid out still applies.

Perhaps I've missed something -- I'll look forward to the author response which may alleviate my concern.

Pros - very clearly written, understanding trained RNNs is an important topic
Cons - the basic logic of the conclusion may be flawed (will await author response)

Minor -
The regular expression in Figure 6 (Top) is for phone numbers instead of emails.
"Average linear decoding accuracy as a function of M in the MDFA" -- I don't think "M" was ever defined. From contexts it looks like it's the number of nodes in the MDFA.
"Average ratio of coarseness" -- It would be nice to be explicit about what the "ratio of coarseness" is. I'm guessing it's (number of nodes in MDFA)/(number of nodes in abstracted DFA).
What are the integers and percentages inside the circles in Figure 6?
Figures 4 and 5 are difficult to interpret because the same (or at least very similar) colors are used multiple times.
I don't see "a" (as in a_t in the equations on page 3) defined anywhere. I think it's meant to indicate a symbol in the alphabet \Sigma. Maybe I missed it.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1eW-odEn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1zeHnA9KX&amp;noteId=S1eW-odEn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1511 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">This is a nice piece of work, well-written, on a hot topic, providing an interesting novel approach and some important insights.

I would like to point out 2 recent works on the matter that could be interesting to discuss in the paper if accepted:

- In [1], the authors prove the equivalence between linear 2-order RNN and weighted automata. The linearity restriction clearly echoes the one of this paper.

- In [2], the authors show that non-linear RNN can be efficiently approximated by weighted automata, suggesting as strong link between the states of the automata and the inner representation of RNN, as in this paper.

[1] Connecting Weighted Automata and Recurrent Neural Networks through Spectral Learning, Guillaume Rabusseau, Tianyu Li, Doina Precup, <a href="https://arxiv.org/abs/1807.01406" target="_blank" rel="nofollow">https://arxiv.org/abs/1807.01406</a>

[2] Explaining Black Boxes on Sequential Data using Weighted Automata, Stephane Ayache, Remi Eyraud, Noe Goudian, https://arxiv.org/abs/1810.05741</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>