<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learning Programmatically Structured Representations with Perceptor Gradients | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learning Programmatically Structured Representations with Perceptor Gradients" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SJggZnRcFQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learning Programmatically Structured Representations with Perceptor..." />
      <meta name="og:description" content="We present the perceptor gradients algorithm -- a novel approach to learning symbolic representations based on the idea of decomposing an agent's policy into i) a perceptor network extracting..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SJggZnRcFQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning Programmatically Structured Representations with Perceptor Gradients</a> <a class="note_content_pdf" href="/pdf?id=SJggZnRcFQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019learning,    &#10;title={Learning Programmatically Structured Representations with Perceptor Gradients},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SJggZnRcFQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We present the perceptor gradients algorithm -- a novel approach to learning symbolic representations based on the idea of decomposing an agent's policy into i) a perceptor network extracting symbols from raw observation data and ii) a task encoding program which maps the input symbols to output actions. We show that the proposed algorithm is able to learn representations that can be directly fed into a Linear-Quadratic Regulator (LQR) or a general purpose A* planner. Our experimental results confirm that the perceptor gradients algorithm is able to efficiently learn transferable symbolic representations as well as generate new observations according to a semantically meaningful specification.
</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">representation learning, structured representations, symbols, programs</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SJl6pksG6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting perspective, but the paper could be stronger with experiments that reflect its original motivations</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJggZnRcFQ&amp;noteId=SJl6pksG6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1141 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1141 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The high-level problem this paper tackles is that of learning symbolic representations from raw noisy data, based on the hypothesis that symbolic representations that are grounded in the semantic content of the environment are less susceptible to overfitting.

The authors propose the perceptor gradients algorithm, which decouples the policy into 1) a perceptor network that maps raw observations to domain-specific representations, which are inputs to 2) a pre-specified domain-specific control or planning program. The authors claim that such a decomposition is general enough to accommodate any task encoding program.

The proposed method is evaluated on three experiments: a simple control task (cartpole-balancing), a navigation task (minecraft: go to pose), and a stochastic single-object retrieval task (minecraft: collect wood). The authors show that the perceptor gradients algorithm learns much faster than vanilla policy gradient. They also show that the program provides an inductive bias that helps ground the representations to the true state of the agent by manually inspecting the representations and by reconstructing the representation into a semantically coherent scene.

This paper is clear and well-written and I enjoyed reading it. It proposes a nice perspective of leveraging programmatic domain knowledge and integrating such knowledge with a learned policy for planning and control. If the following concerns were addressed I would consider increasing my score.

1. To what extent do the experiments support the authors' claims: Although the existing experiments are very illustrative and clear, they did not seem to me to illustrate that the learned representations are transferable as the authors claimed in the introduction. This perhaps is due to the ambiguous definition of "transferable;" it would be helpful if the authors clarified what they mean by this. Nevertheless, as the paper suggests in the introduction that symbolic representations are less likely to overfit to the training distribution, I would be interested to see an experiment that illustrates the capability of the program-augmented policy to generalize to new tasks. For example, Ellis et al. [1] suggested that the programs can be leveraged to extrapolate to problems not previously seen in the input (e.g. by running the for loop for more iterations). To show the transferability of such symbolic representations, is it possible for the authors to include an experiment to show to what extent the perceptor gradients algorithm can generalize to new problems? For example, is it possible for the proposed approach to train on "Minecraft: Go to Pose" and generalize to a larger map? Or is it possible for the proposed approach to train on one wood block and generalize to more wood blocks?
2. Experiment request: The paper seems to suggest that the "Minecraft: Go to Pose" task and the "Minecraft: Collect Wood" task were trained with an autoencoding perceptor. To more completely assess to what extent the program is responsible for biasing the representations to be semantically grounded in environment, would the authors please provide ablation experiments (learning curves, and visualization of the representations) for these two tasks where only the encoder was used?
3. Question: I am a bit confused by the beta-VAE results in Figure 4. If the beta-VAE is trained to not only reconstruct its input as well as perform a "linear regression between the learnt latent space and the ground truth values" (page 6), then I would have expected that the latent space representations to match the ground truth values much more closely. Would the authors be able to elaborate more on the training details and objective function of the beta-VAE and provide an explanation for why the learned latent space deviates so far from the ground truth?
5. Related work: 
    a) The paper briefly discusses representation learning in computer vision and physical dynamics modeling. However, in these same domains it lacks a discussion of approaches that do use programs to constrain learned representations, as in [1-3]. Without this discussion, my view is that the related work would be very incomplete because program-induced constraints are core to this paper. Can the authors please provide a more thorough and complete treatment of this area?
    b) The approaches that this paper discusses for representation learning have been around for quite a long time, but it seems rather a misrepresentation of the related work to have all but two citations in the Related Work section 2017 and after. For example, statistical constraints on the latent space have been explored in [4-5]. Can the authors please provide a more thorough and complete treatment of the related work?
6. Possible limitation: A potential limitation for decoupling the policy in this particular way is that if the perceptor network produced incorrect representations that are fed into the program, the program cannot compensate for these errors. It would be helpful for the authors to include a discussion about this in paper.
7. How does this scale? As stated in the intro, the motivation for this work is for enabling autonomous agents to learn from raw visual data. Though the experiments in this paper were illustrative of the approach, these experiments assumed that the agent had access to the true state variables of its environment (like position and velocity), and the perceptor network is just inferring the particular values of these variables for a particular problem instance. However, presumably the motivation for learning from raw visual data is that the agent does not have access to the simulator of the environment. How do the authors envision their proposed approach scaling to real world settings where the true state variables are unknown? There is currently not an experiment that shows a need for learning from raw visual data. This is a major concern, because if the only domains that the perceptor gradients algorithm can be applied are those where the agent already has access to the true state variables, then there may be no need to learn from pixels in the first place. This paper would be made significantly stronger with an experiment where 1) learning from raw visual data is necessary (for example, if it is the real world, or if the true state variables were unknown) and 2) where the inductive bias provided by the program helps significantly on that task in terms of learning and transfer. Such an experiment would decisively reflect the paper's claims.
8. Clarity: The paper mentions that it is possible to generate new observations from the latent space. The paper can be made stronger by a more motivated discussion of why generating new observations is desirable, beyond just as a visualization tool. For example, the authors may consider making a connection with the analysis-by-synthesis paradigm that characterizes the Helmholtz machine.

[1] Ellis et al. (<a href="https://arxiv.org/pdf/1707.09627.pdf)" target="_blank" rel="nofollow">https://arxiv.org/pdf/1707.09627.pdf)</a>
[2] Wu et al. (http://papers.nips.cc/paper/6620-learning-to-see-physics-via-visual-de-animation.pdf)
[3] Kulkarni et al. (https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Kulkarni_Picture_A_Probabilistic_2015_CVPR_paper.html)
[4] Schmidhuber (ftp://ftp.idsia.ch/pub/juergen/factorial.pdf)
[5] Bengio et al. (https://arxiv.org/abs/1206.5538)</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1xbCrZgpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning Programmatically Structured Representations with Perceptor Gradients </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJggZnRcFQ&amp;noteId=r1xbCrZgpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1141 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1141 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes the perceptor gradients algorithm to learn symbolic representations for devising autonomous agent policies to act.
The perceptor gradients algorithm decomposes a typical policy into a perceptor network that maps observations to symbolic representations and a user-provided task encoding program which is executed on the perceived symbols in order to generate an action. Experiments show the proposed approach achieves faster learning rates compared to methods based solely on neural networks and yields transferable task related symbolic representations. The results prove the programmatic regularisation is a general technique for structured representation learning. Although the reviewer is out of the area in this paper, this paper seems to propose a novel algorithm to learn the symbolic representations.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">1: The reviewer's evaluation is an educated guess</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hyg6yj_qnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>I tried very hard but I think ultimately failed to understand this paper.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJggZnRcFQ&amp;noteId=Hyg6yj_qnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1141 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 12 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1141 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=Hyg6yj_qnX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The fundamental idea proposed in this paper is a sensible one:  design the functional form of a policy so that there is an initial parameterized stage that operates on perceptual input and outputs some "symbolic" (I'd be happier if we could just call them "discrete") characterization of the input, and then an arbitrary program that operates on the symbolic output of the first stage.

My fundamental problem is with equation 3.  If you want to talk about the factoring of the probability distribution p(a | s) that's fine, but, to do it in fine detail, it should be:
P(a | s) = \sum_sigma P(a, sigma | s) = \sum_sigma P(a | sigma, s) * P(sigma | s)
And then by conditional independence of a from s given sigma
 = \sum_sigma P(a | sigma) * P(sigma | s)
But, critically, there needs to be a sum over sigma!  Now, it could be that I am misunderstanding your notation and you mean for p(a | sigma) to stand for a whole factor and for the operation in (3) to be factor multiplication, but I don't think that's what is going on.

Then, I think, you go on to assume, that p(a | sigma) is a delta distribution.  That's fine.

But then equation 5 in Theorem 1 again seems to mention delta without summing over it, which still seems incorrect to me.

And, ultimately, I think the theorem doesn't make sense because the transformation that the program performs on its input is not included in the gradient computation.  Consider the case where the program always outputs action 0 no matter what its symbolic input is.   Then the gradient of the log prob of a trajectory with respect to theta should be 0, but instead you end up with the gradient of the log prob of the symbol trajectory with respect to theta.

I got so hung up here that I didn't feel I could evaluate the rest of the paper.  

One other point is that there is a lot of work that is closely related to this at the high level, including papers about Value Iteration Networks, QMDP Networks, Particle Filter Networks, etc.  They all combine a fixed program with a parametric part and differentiate the whole transformation to do gradient updates.  It would be important in any revision of this paper to connect with that literature.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJgi_JbSaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to the review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJggZnRcFQ&amp;noteId=BJgi_JbSaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1141 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1141 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Factorisation in Equation 3:
------------------------------------------
Thank you for the close look at the mathematical details of the paper. Equation 3 is meant to represent full factor multiplication rather than marginalisation of \sigma_t. We will clarify this in the paper and hopefully avoid confusing other readers. 

Theorem 1:
-----------------
The purpose of Theorem 1 is to show that REINFORCE can be applied to train the perceptor network. The key intuition behind Theorem 1 is that the program can be thought of as being absorbed in the environment and the task of the agent is to feed it the right inputs. Therefore, considering a program that always outputs action 0, as suggested in the review, is essentially equivalent to an environment which does not take into account the actions of the agent at all. In this case, the gradient of the log probability of the trajectory with respect to the parameters of the policy, in a standard policy gradients setup, would also be non-zero. More importantly, while this scenario is an interesting theoretical edge case it has little, if any, practical implications.    

Additionally, we would like to note that Theorem 1 handles correctly the case when the perceptor is to always output the same symbols as the gradient of the log prob of the symbol trajectory with respect to theta will be 0 as expected. 

Related Work:
----------------------
Works such as Value Iteration Networks, QMDP Networks and Particle Filter Networks are based on the idea of differentiable programs which can express only subset of the problems that a general program can express.
One of the key contributions of the paper is that perceptor gradients can work with general programs as we have demonstrated by directly plugging in programs from standard Python packages. Nevertheless, that is a substantial body of literature that we will update the paper to connect with.

Symbols vs. Discrete variables:
-----------------------------------------------
The perceptor can output both continuous (LQR experiment) and discrete variables (Minecraft experiments) that characterise the raw input data. We call the output of the perceptor symbolic as each output variable (regardless of its domain) has semantic content imposed by the program. Our experiments demonstrate that the perceptor does learn representations which follow the symbolic structure of the program.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJxKv6VUaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Now I see...</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJggZnRcFQ&amp;noteId=HJxKv6VUaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1141 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1141 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">So, if equation (3) is a factor multiply, then we can write it out as:
\[\pi(a \mid s) =  \sum_\sigma P(a \mid \sigma) P(\sigma \mid s)\;\;,\]
which by your assumption of a deterministic program is 
\begin{align*}
\pi(a \mid s) &amp; =  \sum_\sigma I(\rho(\sigma) = a)  P(\sigma \mid s)\\
&amp;  =  \sum_{\{\sigma \mid \rho(\sigma) = a\}}  \psi(\sigma \mid s)
\end{align*}
So
\begin{align}
\nabla_\theta \log \pi(a \mid s) &amp;= \nabla_\theta \sum_{\{\sigma \mid \rho(\sigma) = a\}}  \psi(\sigma \mid s)\\
&amp; = \sum_{\{\sigma \mid \rho(\sigma) = a\}}  \nabla_\theta \psi(\sigma \mid s)
\end{align}
Note that this quantity still depends on which symbols $\sigma$ will
cause your program $\rho$ to generate action $a$.

Now, in equation (6), $\sigma_t^{(i)}$ a particular symbol, which is the one
that was {\em actually} generated by the perceptor at time $t$ on sequence
$i$, right?   Your equation (6) makes sense to  me if $\sigma$ is
actually part of $\tau$.  But it's not.

Of course my example of a program that ignores its input and always
outputs 0 is not interesting practically.  But let's see what happens
here:  
\begin{itemize}
\item The trace will only have $a = 0$.
\item The sequence of symbols $\sigma_t^{(i)}$ will be something
  interesting.
\item In your equation (6), the right-hand side would be independent
  of $\rho$ and generally be non-zero.
\item But, in fact, my equation (1) above would be 0 because $\phi$
  is a probability distribution, and so it should be 0.  And also
  because, intuitively, it should be 0.
\end{itemize}

In your reply to me you said:  ``the key intuition behind Theorem 1 is
that the program can be thought of as being absorbed in the
environment.''  But, in that case, it really {\em is} true that the
$\sigma$ need to be observed, because they are the new ``actions.''

Okay.  I think this is why we end up with different understandings of
what's going on.  

Let me proceed with the rest of the paper under that assumption (but
if that's in fact what's going on here, then you would need to amend
your description of $\tau$ to include $\sigma$.)

Okay.  Sorry to have been dim.  It does all make sense now.

This seems like a completely reasonable idea, though really not all
that surprising.  One might ask whether it would be a
good idea to do value-iteration networks this way, too:  that is,
think of the VIN as part of the environment and just train the models
using reinforce.  I guess not, quite, because the model for VIN is a
static object, not something that varies along the trajectory.  

Then we need to get into the question of whether reinforce is a
sensible algorithm or not, and under what circumstances.  

In any case, I will change my rating and hope the story of my
confusion above is useful to you.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_r1xETQsMTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>My impression of the purpose of Theorem 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJggZnRcFQ&amp;noteId=r1xETQsMTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1141 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1141 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">With respect to how the program's transformation is included in the gradient computation, my understanding from equation 8 is that the point of Theorem 1 is to show that, because the program is a non-differentiable piece, we can essentially push the agent/environment boundary further into the agent, such that the "actions" are the task related symbols, the "states" are the visual observations, and the "agent" is only the perceptor network. Then, from the perspective of the policy parameters, the program essentially becomes part of the environment. Therefore, we can apply REINFORCE to optimize the perceptor network as we would for any other policy.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkl4fe-r6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Purpose of Theorem 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJggZnRcFQ&amp;noteId=Bkl4fe-r6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1141 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1141 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">This is indeed the purpose of Theorem 1 as we have also mentioned in our response to the reviewer.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>