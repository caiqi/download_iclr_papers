<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Engaging Image Captioning Via Personality | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Engaging Image Captioning Via Personality" />
        <meta name="citation_author" content="Kurt Shuster" />
        <meta name="citation_author" content="Samuel Humeau" />
        <meta name="citation_author" content="Hexiang Hu" />
        <meta name="citation_author" content="Antoine Bordes" />
        <meta name="citation_author" content="Jason Weston" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HJN6DiAcKQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Engaging Image Captioning Via Personality" />
      <meta name="og:description" content="Standard image captioning tasks such as COCO and Flickr30k are factual, neutral in tone and (to a human) state the obvious (e.g., “a man playing a guitar”). While such tasks are useful to verify..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HJN6DiAcKQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Engaging Image Captioning Via Personality</a> <a class="note_content_pdf" href="/pdf?id=HJN6DiAcKQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=kshuster%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="kshuster@fb.com">Kurt Shuster</a>, <a href="/profile?email=samuelhumeau%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="samuelhumeau@fb.com">Samuel Humeau</a>, <a href="/profile?email=hexianghu%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="hexianghu@fb.com">Hexiang Hu</a>, <a href="/profile?email=abordes%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="abordes@fb.com">Antoine Bordes</a>, <a href="/profile?email=jaseweston%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="jaseweston@gmail.com">Jason Weston</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Standard image captioning tasks such as COCO and Flickr30k are factual, neutral in tone and (to a human) state the obvious (e.g., “a man playing a guitar”). While such tasks are useful to verify that a machine understands the content of an image,  they are not engaging to humans as captions.   With this in mind we define a new task, Personality-Captions, where the goal is to be as engaging to humans as possible by incorporating controllable style and personality traits.We collect and release a large dataset of 201,858 of such captions conditioned over 215 possible traits.  We build models that combine existing work from (i) sentence representations (Mazaré et al., 2018) with Transformers trained on 1.7 billion dialogue examples; and (ii) image representations (Mahajan et al., 2018) with ResNets trained on 3.5 billion social media images.  We obtain state-of-the-art performance on Flickr30k and COCO, and strong performance on our new task. Finally, online evaluations validate that our task and models are engaging to humans, with our best model close to human performance.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">image, captioning, captions, vision, language</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We develop engaging image captioning models conditioned on personality that are also state of the art on regular captioning tasks.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">5 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_ByecdLulCX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJN6DiAcKQ&amp;noteId=ByecdLulCX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper307 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper307 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ByeaAA3nTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response from Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJN6DiAcKQ&amp;noteId=ByeaAA3nTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper307 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you to all the reviewers for your detailed comments. We will update the manuscript to improve it in light of your comments, and aim to resubmit elsewhere.

In particular we will add:

* a table comparing our new dataset with existing datasets 

* collect multiple captions on the test set for improved multi-reference BLEU metrics  

* a human evaluation baseline using (multi-reference) BLEU metrics

* more explanation of how the data was collected (indeed it was crowdsourced R2)

* more explanation of how and why we curated the set of 215 personality traits in the dataset

* evaluation of whether the caption conformed to the personality, and comparison to humans in terms of this metric as well 

* clarify our statistical significance tests 

* further details of how the personality encoding works in both retrieval and word-by-word generation models (termed generative models in this work)

* further explanation of why we think this task is important — briefly, we are interested in machines that can see and speak, and that can talk to humans in an engaging way as that is part of the dream of AI. Hence, we measure engagingness as evaluated by humans. Moreover, images are nowadays an important vector or communication, e.g. on Whatsapp 8% of the messages exchanged are images (<a href="https://blog.whatsapp.com/10000631/Connecting-One-Billion-Users-Every-Day)." target="_blank" rel="nofollow">https://blog.whatsapp.com/10000631/Connecting-One-Billion-Users-Every-Day).</a> Hence it seems necessary that an engaging dialog agent should be able to speak about an image. Similarly, there are many studies (cited) that show personality in language is more engaging to humans than neutral tone.

* we will describe the human evaluation in more detail

There are also some misunderstandings by the reviewers which we will aim to make clearer in the text:

* R1: We did perform a large number of ablation studies: removing personality types, text encoder types (bag-of-words vs. transformer,  together with full, word only or no pre-training types), and trying different image encoder types. We will make that clearer.   R1 writes “For instance, it would be interesting to see the performance of captioning model trained on ResNeXT-IG-3.5B and COCO dataset.” — but that is actually in Table 4.

* R3 mentions 7 subjective captions. In fact, there are 215 possible traits in the dataset, which cover a rather wide range of personality types, but we could not fit them all in an example figure of course. 

* R3 states that “humans prefer their labelings 50% of the time... sounds as low performance”. This is a misunderstanding we must make clearer, and this result in the paper actually shows the strength of our dataset and our models, so it is shame it was misunderstood. This metric means that given a caption from our model and a caption from a human, another human selected our caption around half the time in a blind test between the two captions averaged over 2500 trials, i.e. we are almost on par with human performance on this task/metric. This is clearly a strong result, and an important part of the paper.

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJxmlBqcnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Important benchmark for an interesting problem. Some key analysis missing</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJN6DiAcKQ&amp;noteId=BJxmlBqcnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper307 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper307 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
This paper introduces a large-scale benchmark for the task of image captioning in which the goal is for the caption to be as engaging as humans as possible. The dataset consists of 200K + captions spanning 200+ personality traits. The dataset includes traits like “Kind”, “Mellow”, “Creative”, “Contradictory” etc.  The authors also analyze the performance of models built on top of recent approaches in image and sentence representations on both caption generation and caption retrieval task.  The paper consists of two main experiments: (1) they evaluate their architecture on MS-COCO and Flickr30K dataset to assess the model’s ability to describe the image factually and in a neutral tone. They show similar performance to existing state-of-the-art approaches on caption-generation task and show best retrieval numbers on Flickr30K dataset. 
(2) They also perform human evaluation of personality-captions and found that humans tend to prefer these captions over the neutral captions

Pros:
- The paper introduces a first large-scale dataset for personality captions that has 215 traits and ~200,000 images. It is significantly bigger than past datasets which consist of a only a few thousand captions covering a small set of categories.
- The analysis of models is done not only on the personality-captions dataset but these models are also evaluated on the traditional COCO dataset. 
- The paper evaluates both retrieval and caption generation models on both these datasets and also perform human evaluation for completeness. 
- The paper is well written and is backed-up well by details mentioned in the supplementary material.

Cons:
- The paper doesn’t analyze and compare the benchmark with existing datasets. For instance, readers might be interested in  statistics of the dataset compared to existing datasets: 
    - Does it produce more diverse captions (are the captions longer) 
    - Is the caption vocabulary bigger than existing dataset? What’s the overlap with existing dataset. 
    - Does it have more unique words per sentence position) 
- It has been shown in the past that collecting multiple captions for the same image leads to more robust evaluation using automatic metrics. However, the dataset contains just one caption per image. 
- The paper also didn’t perform ablation studies on their model to know the source of performance gains. For instance, it would be interesting to see the performance of captioning model trained on ResNeXT-IG-3.5B and COCO dataset. What is the contribution of supervision of more captions coming from the new dataset vs (better?) image features extracted from ResNeXT-IG-3.5B 
- Although the paper does mention human evaluation of which caption is more engaging, there is no human baseline to compare the models to for personality-caption test set using automatic metrics. 


Some other questions/remarks:
- During caption collection, caption-writers were asked to write “a comment” instead of a “caption”. I wonder why this instruction was added? 
- Compared to the StyleNet paper by Gan et al (2017), it seems like the instructions were quite different to collection captions. They observed that collecting captions by just mentioning the style resulted in captions that were not visually grounded. Therefore, they setup the interface such that factual captions are changed to match the style. Can the authors comment on their choice of the interface as compared to the one described in the StyleNet paper?
- There is also some value in evaluating whether the caption conformed to the personality mentioned while generating caption. Although there is a bit of discussion about this in the diversity of captions in the supplementary, I would be curious to see a bit more analysis on this. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1llfZ2Kn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A new dataset for image captioning</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJN6DiAcKQ&amp;noteId=B1llfZ2Kn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper307 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper307 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a new image captioning dataset. The image captions were created in the context of a "personality" prompt, e.g. "happy" or "sad". The dataset is used to train personality-aware image captioning models and caption retrieval models. The models are standard image captioning models or image--sentence ranking models with additional encodings of the personality trait. I am sympathetic to the argument that existing resources for image captioning make it a "neutral" language generation task, and this dataset may be useful for future research in image captioning. But most of the details of the new dataset are relegated to the appendices, and most of the results are devoted to the value of using pre-trained embeddings or CNNs.

Experiments are conducted on the COCO dataset and the the personality captions dataset. The experiments also focus on the role of using more complex image recogniiton models. The results show that using a better visual recognition model leads to improvements in captioning and retrieval performance. This is not surprising and the same basic result has been down in many papers, going back to Donahue et al. (CVPR 2015), who showed that using VGG was better than using CaffeNet on captioning and retrieval tasks.

The human evaluation is commendable but there are insufficient details about how you carried out the experiment. Did you conduct this experiment on a crowd-sourcing platform? How reliable are the results of five human annotators? The difference is claimed to be a "statstically significant result", so please present the test statistic in the paper.

What exactly does it mean that you "learn an embedding for each train and concatenation it with each input of the encoder"? Is the personality trait input to the encoder at every timestep? Is it also used to initialise the hidden state of the encoder? Please provide more details.

There are many references to the language generation model as a "generative" model. I was confused by this term, which I expected to be used to describe an actual generative model of p(x, y), whereas your captioning models estimate p(y|x).

The last sentence of Section 2 is missing a citation "... large-scale pretraining (?) ..."</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HyxilqII27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The authors approach the image captioning problem conditioned on personality "traits"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJN6DiAcKQ&amp;noteId=HyxilqII27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper307 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper307 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
The technical contribution of this paper is reduced to adding a "personality embedding" to approach the image captioning problem (authors also propose a new task and data set, but motivation is not convincing). As side contributions the authors will release a data set on image captioning conditioned on personality "traits".  I think the authors fail at motivating the proposed task, see comments below. Also, they fail at emphasizing the technical contribution of the paper (see below as well). Finally, authors do not put in context their work, in terms of CV studies on personality analysis. 


I think the authors should elaborate more on scenarios/applications were personality-conditioned captions would be of interest to humans!, with reference to the example in Figure 1: why one would a computer to generate a subjective caption biased towards a particular personality aspect (which has to be chosen by the user, I guess)?. There are 7 subjective captions, all of them could be valid, but these are both, subjective and they surely do not cover the whole spectrum one can think of. I am not convinced of the relevance of the task, I mean is an interesting experiment, but what impact will it have in terms of applications? I think this is critical, since one of the contributions of the paper is the new dataset. 

Authors mention that humans prefer their labelings ˜50% of the time (wo statistically significant difference), this actually sounds as low performance, can you please elaborate on why is this a good result?

The authors make use "personality traits" to make reference to a list of adjectives, actions, etc. Why the authors consider these as personality traits? What is the support for that list? There are well established notions of personality traits from the psychology field, why not adhering to them?, authors should also make an effort to put in context their work in terms of personality-analysis efforts from the CV community, see e.g.: Jacques et al. First Impressions: A Survey on Computer Vision-Based Apparent Personality Trait Analysis. ArXiv 2018

Why are missing in Table 5 the results obtained by the ResNet152 image encoder without personality encoder?

The description of the personality encoder in section 4.2 is rather brief!, this is actually the only technical contribution of the paper and the authors fail at emphasizing it. 

How do the authors define engagement to annotators? More details on the human evaluation should be provided to determine the validity of the conclusions derived from this study 

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>