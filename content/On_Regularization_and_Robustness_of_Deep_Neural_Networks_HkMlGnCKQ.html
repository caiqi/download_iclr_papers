<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>On Regularization and Robustness of Deep Neural Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="On Regularization and Robustness of Deep Neural Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HkMlGnC9KQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="On Regularization and Robustness of Deep Neural Networks" />
      <meta name="og:description" content="Despite their success, deep neural networks suffer from several drawbacks: they lack robustness to small changes of input data known as " adversarial="" examples"="" and="" training="" them="" with="" small="" amounts..."="" />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HkMlGnC9KQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>On Regularization and Robustness of Deep Neural Networks</a> <a class="note_content_pdf" href="/pdf?id=HkMlGnC9KQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019on,    &#10;title={On Regularization and Robustness of Deep Neural Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HkMlGnC9KQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Despite their success, deep neural networks suffer from several drawbacks: they lack robustness to small changes of input data known as "adversarial examples" and training them with small amounts of annotated data is challenging.  In this work, we study the connection between regularization and robustness by viewing neural networks as elements of a reproducing kernel Hilbert space (RKHS) of functions and by regularizing them using the RKHS norm. Even though this norm cannot be computed, we consider various approximations based on upper and lower bounds.  These approximations lead to new strategies for regularization, but also to existing ones such as spectral norm penalties or constraints, gradient penalties, or adversarial training. Besides, the kernel framework allows us to obtain margin-based bounds on adversarial generalization.  We study the obtained algorithms for learning on small datasets, learning adversarially robust models, and discuss implications for learning implicit generative models.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">regularization, robustness, deep learning, convolutional networks, kernel methods</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_S1eid00WaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>General response to reviewers</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkMlGnC9KQ&amp;noteId=S1eid00WaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1233 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1233 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">First, we would like to thank the reviewers for their useful remarks and suggestions. We provide general comments here that are relevant for all reviewers, with specific comments for each reviewer in individual replies.

Based on the reviewers' comments, we realize that our original submission focuses too much on establishing links between regularization with the RKHS norm and existing strategies, which we found particularly interesting, rather than highlighting the benefits of the newly obtained strategies. Additionally, some of our observations and insights were a bit preliminary in the submission, particularly regarding robustness and security applications, given that our original main motivation for this work was different, focusing on regularization in small data settings. We hope to better clarify the concerns about novelty here, and welcome further remarks by the reviewers. We will do our best to update the paper accordingly before the end of the discussion period.

** Novelty compared to Bietti and Mairal (2018)

Bietti and Mairal (2018) study theoretical properties of the RKHS only. Here, we provide *practical algorithms* for regularizing usual neural networks using the RKHS norm, which is a major step forward compared to this existing work.  We are also not aware of previous work that considers the RKHS norm for regularization of deep networks in practice. An interesting insight of our work is that this norm is quite large for standard networks trained with SGD, and that explicitly controlling it brings clear benefits.

** Novelty of the regularization strategies

The adversarial perturbation penalty ||f||_M that we introduce in this work is quite different from previous work: (i) it encourages stability of the entire prediction function by considering a separate penalty term in the optimization objective; (ii) it optimizes worst-case stability across the domain, in contrast to other approaches which only optimize this on average over training points. On Cifar10, our method seems most effective in controlling the RKHS norm compared to other methods, where we observe that both the lower bound and spectral norms are controlled together. In contrast, other lower bound methods seem less effective at controlling the upper bound, and this is particularly pronounced in the case of PGD in to our experiments. This is mentioned in the last paragraph of Section 4, and displayed in Figure 2(left).
The approach ||f||_M yields the best accuracy for regularization on some of the small dataset problems we consider, and can achieve the best performance in some regimes of the robustness/generalization trade-off. Additionally, it provides the most useful certified guarantee on adversarial generalization in our experiments (see next bullet point).

Another key difference between our approach and previous ones is that our penalties involve a global optimization problem across the space of inputs X rather than only an average over training samples. This is true both for ||f||_M vs. PGD and for our gradient penalty vs. existing strategies based on gradients. We admittedly did not yet investigate the importance of this local vs. global regularization effect (and we currently only optimize across examples in a mini-batch), which we plan to do in a longer version of the paper. This indeed paves the way to transductive and semi-supervised settings, which we plan to investigate as well.

** Certified guarantees for adversarial generalization, and novelty of our theoretical analysis

We also would like to point out that our original main motivation was to study regularization benefits, while some of our observations regarding robustness and security were somewhat preliminary at the time of submission. Yet, one important aspect of our work in the context of robustness is that controlling the RKHS norm can provide a model with *certified* guarantees on *adversarial* generalization (i.e. test accuracy in the presence of an adversary), as given by our margin bound analysis, although it depends on the RKHS norm which can only be approximated. We note that while margin bounds have been useful to establish (standard) generalization guarantees for neural networks, to our knowledge our work is the first to use similar arguments for bounding adversarial generalization.

Our experiments suggest that the most useful guarantees are obtained for models trained with our penalty ||f||_M, for which the upper and lower bounds are more tightly related than for other methods (see Figure 2). In contrast, while methods like PGD may give improved robustness empirically in some regimes, our experiments on CIFAR10 suggest that the obtained models have large spectral norms, yielding quite weak guarantees on adversarial generalization.
This suggests that the robustness of such models may be only local, so that one may need (possibly costly) verification procedures on each test example in order to guarantee robustness against all adversaries.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1x3WD8ChQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Well written with interesting findings, but limited novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkMlGnC9KQ&amp;noteId=S1x3WD8ChQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1233 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1233 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Regularizing RKHS norm is a classic way to prevent overfitting. The authors
note the connections between RKHS norm and several common regularization and
robustness enhancement techniques, including gradient penalty, robust
optimization via PGD and spectral norm normalization. They can be seen as upper
or lower bounds of the RKHS norm.

There are some interesting findings in the experiments. For example, for
improving generalization, using the gradient penalty based method seems to work
best.  For improving robustness, adversarial training with PGD has the best
results (which matches the conclusions by Madry et al.); but as shown in Figure
2, because adversarial training only decreases a lower bound of RKHS norm, it
does not necessarily decrease the upper bound (the product of spectral norms).
This can be shown as a weakness of adversarial training if the authors explore
further and deeper in this direction.

Overall, this paper has many interesting results, but its contribution is
limited because:

1. The regularization techniques in reproducing kernel Hilbert space (RKHS) has
been well studied by previous literature. This paper simply applies these
results to deep neural networks, by treating the neural network as a big
black-box function f(x).  Many of the results have been already presented in
previous works like Bietti &amp; Mairal (2018).

2. In experiments, the authors explored many existing methods on improving
generalization and robustness. However all these methods are known and not new.
Ideally, the authors can go further and propose a new regularization method
based on the connection between neural networks and RKHS, and conduct
experiments to show its effectiveness.

The paper is overall well written, and the introductions to RKHS and each
regularization techniques are very clear. The provided experiments also include
some interesting findings. My major concern is the lack of novel contributions
in this paper.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Skx8HkyGT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkMlGnC9KQ&amp;noteId=Skx8HkyGT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1233 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1233 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for his comments. We address the comments about novelty in our general response ( <a href="https://openreview.net/forum?id=HkMlGnC9KQ&amp;noteId=S1eid00WaQ" target="_blank" rel="nofollow">https://openreview.net/forum?id=HkMlGnC9KQ&amp;noteId=S1eid00WaQ</a> ), for instance concerning the relationship to previous work, and the regularization penalty ||f||_M we propose. More detailed comments are addressed below.

** weakness of adversarial training

As noted in our general response, our ||f||_M regularization approach empirically yields models with a more useful certified generalization guarantee in the presence of adversaries on Cifar10, while PGD adversarial training would likely require local verification of robustness around each test example, and we are not aware of useful guarantees on adversarial generalization for such models. We agree that this aspect is not clear in the current submission, and we will improve it in the next version.

** relationship with traditional RKHS regularization

There is indeed no question that kernel methods/RKHSs have been widely used for regularization of non-linear functions, for over 20 years now, however these methods typically rely on solving convex optimization problems using the kernel trick, or various kernel approximations (such as random Fourier features). Separately, defining RKHSs that contain neural networks has indeed been the study of previous work, such as Bietti and Mairal (2018) or Zhang et al. (2016; 2017), however these only study theoretical properties of the kernel mapping and the RKHS norm, or derive convex learning procedures to replace training neural networks. Our approach is quite different, in that we leverage these insights to obtain practical regularization strategies for generic neural networks.

** new regularization methods

In addition to the ||f||_M lower bound penalty discussed in our general response, we note that combined approaches based on lower bound + upper bound methods are also novel to the best of our knowledge, and in particular we found combining robust optimization techniques with spectral norm constraints to be quite successful in many of the small data scenarios considered (see Table 1).

We will happily clarify some of these points in an updated version of the paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SylRu6annQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting ideas, but not enough of an independent contribution</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkMlGnC9KQ&amp;noteId=SylRu6annQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1233 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1233 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper looks at adversarial examples from the context of RKHS norms for neural networks.  The work builds conceptually on the work of Bietti and Mairal (2018), who investigate approximate RKHS norms for neural networks (including computation via a specialized convolutional kernel), and Xu et al., (2009) which looks at robustness properties of kernel classifiers.  The authors discuss how the RKHS norm of neural network functions provide robustness guarantees for the resulting classifier, both in terms of a straightforward robustness property for a given example, as well as in terms of generalization guarantees about robustness.

Overall, I think there are some interesting ideas in this work, but ultimately not enough to make a compelling independent paper.  The core issue here is that the RKHS properties are used only in a very minimal manner to actually provide much analysis or insight into the robustness properties of the network.  For example, the upper bound in (8) seems to be central here to illustrating how a bound on the RKHS norm can be upper bounded as a function of the operator l2 norm of the inner weight matrices (though the actual form of the bound isn't mentioned), and the latter term could thus provide a certified bound on the robustness loss of a classifier.  However, there are two big issues here: 1) it's trivial to directly bound the l2 robustness of a classifier by the product of the weight spectral norms and 2) the actual regularization term the authors proposed to use (the sum of spectral norms) is notably _not_ an upper bound on either the robust loss or the RKHS norm; naturally, this penalty along with the constrained version will still provide some degree of control over the actual robustness, but the authors don't connect this to any real bound.  I also think the authors aren't properly acknowledging just how similar this is to past work: the Parseval networks paper (Cisse et al., 2017), for instance, presents a lot of similar discussion of how to bound generalization error based based upon terms involving operator norms of the matrices, and the actual spectral normalization penalty that the authors advocate for has been studied by Miyato et al. (2018).  To be clear, both of these past works (and several similar ones) are of course cited by the current paper, but from a practical standpoint it's just not clear to me what the takeaways should be here above and beyond this past work, other than the fact that these quantities _also_ bound the relevant RKHS norms.  Likewise the generalization bound in the paper is a fairly straightforward application of existing bounds given the mechanics of the RKHS norm defined by previous work.

To be clear, I think the RKHS perspective that the authors advocate for here is actually quite interesting.  I wasn't particularly familiar with the Bietti and Mairal (2018) work, and going through it in some detail for reviewing this paper, I think it's an important directly for analysis of deep networks, including from a perspective of robustness.  But the results here seem more like a brief follow-on note to the past work, not a complete set of results in and of themselves.  Indeed, because the robustness perspective here can largely be derived completely independently of the RKHS framework, and because the resulting training procedures seem to be essentially identical to previously-proposed approaches, the mere contribution of connecting these works to the RKHS norm doesn't seem independently to be enough of a contribution in my mind.

One final, though more minor, point: It's worth pointing out that (globally) bounding the Lipschitz constant seems top stringent a condition for most networks, and most papers on certifiable robustness seem to instead focus on some kind of local Lipschitz bound around the training or test examples.  Thus, it's debatable whether even the lower bound on the RKHS norm is really reasonable to consider for the purposes of adversarial robustness.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJxIJl1za7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkMlGnC9KQ&amp;noteId=HJxIJl1za7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1233 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1233 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for his comments. Our general response ( <a href="https://openreview.net/forum?id=HkMlGnC9KQ&amp;noteId=S1eid00WaQ" target="_blank" rel="nofollow">https://openreview.net/forum?id=HkMlGnC9KQ&amp;noteId=S1eid00WaQ</a> ) details the aspects related to novelty. Further comments are addressed below.

** comparison with Parseval networks + works of Miyato et al.

We agree that a better comparison with the Parseval network paper would be useful. Regarding generalization, the Parseval networks paper seems to only discuss standard generalization performance based on robustness, not *adversarial* generalization (that is, test error in the presence of an adversary), as considered in our work. Also, our bound seems significantly better: whereas the bound in CissÃ© et al. (2017) has an exponential dependence on the dimension due to covering number of the input space (this is a weakness of the generalization bounds from Xu and Mannor (2012), which do not leverage statistical properties of the function class being used), our margin bound has no dependence on the dimension, or only a logarithmic dependence if we use the Rademacher analysis of Bartlett et al. (2017) instead of our kernel framework.
Regarding the improper acknowledgement of Miyato's work, we are a bit surprised by the reviewer's comment: we cite the work of Miyato almost each time we mention spectral regularization and the acknowledgment seems clear to us throughout the paper. However, if the reviewer finds any ambiguous claim in our paper that we would have missed, we would be happy to clarify it.

 ** role of the specific regularizer

The reviewer points out that some of the regularization functions we consider such as the spectral norm penalties, are not based on the precise upper bound we derive. Whereas optimizing a product of spectral norms is impractical, which naturally leads to other variants (sums of spectral norms or constraints), we would like to emphasize that such variants are empirically effective in the sense that the quantities obtained at the end of training---such as the spectral norms, (local or global) Lipschitz constants, and the margins of each datapoint---are controlled. These quantities are also what governs our generalization guarantees. Besides, we also note that many deep architectures with ReLUs (particularly VGG-like, if we ignore bias terms) are homogeneous in the weight matrices, making the relative norms at each layer not crucial (multiplying one layer by a scalar and dividing another by the same scalar leads to an equivalent model). In particular, this justifies using the same value for the spectral norm constraint of each layer.

 ** Usefulness of the RKHS framework

The RKHS framework was quite beneficial in our work because it displays several properties at once:
 (1) clear understanding of regularization and generalization through margin bounds
 (2) makes a clear link between stability/robustness and regularization/generalization by using the RKHS norm and properties of the kernel mapping
 (3) yields practical regularization algorithms through upper and lower bounds

Looking at alternatives, if we consider the product of spectral norms instead of the RKHS norm, then we may have (1) using results of Bartlett et al.(2017) and partly (2) since we can upper bound the Lipschitz constant, however algorithms based on lower bounds are crucially missing, and our experiments suggest that these algorithms are often important for good performance, both for regularization on small datasets and for robustness.
If instead we consider the robust optimization approach, we obtain variants of good algorithms (3) such as PGD or gradient penalties, and perhaps some connections to regularization following Xu et al. (2009), however it is difficult to obtain useful generalization guarantees without defining a precise quantity of model complexity. Additionally, such approaches may favor local over global robustness, particularly with powerful function approximators such as neural networks, which may be undesirable when one wants global guarantees.

** bounding l2 robustness with product of spectral norms

It is indeed easy to upper bound l2 robustness using the product of spectral norms. However, such a robustness guarantee is only useful if this quantity is appropriately controlled during training. In particular, for methods like PGD, we find that such a quantity is poorly controlled on Cifar10, and would thus only provide very weak guarantees.

** on global vs local Lipschitz constants

We agree that in some cases local robustness is enough in practice, however this may come at the cost of having weak guarantees on adversarial generalization, and may require expensive verification procedures locally around each test example for guaranteed robustness, as mentioned in our general response.

We will happily clarify some of these points in an updated version of the paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkxTYDf2i7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkMlGnC9KQ&amp;noteId=rkxTYDf2i7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1233 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">23 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1233 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this paper, the authors consider CNN models from the lens of kernel methods. They build upon past work that showed that such models can be seen to lie in appropriate RKHS, and derive upper and lower bounds for the kernel norm. These bounds can be used as regularizers that help train more robust neural networks, especially in the context of euclidean perturbations of the inputs, and training GANs. They show that the bounds can also be used to recover existing special cases such as spectral norm penalizations and gradient regularization. They derive generalization bounds from the point of view of adversarial learning, and report experiments to buttress their claims.

Overall, the paper is a little confusing. A lot of the times, the result seem to be a derivative of the work by Bietti and Mairal, and looks like the main results in this paper are intertwined with stuff B+M already showed in their paper. It's hard to ascertain what exactly the contributions are, and how they might not be a straightforward consequence of prior work (for example, combining results from Bietti and Mairal; and generalization bounds for linear models). It might be nice to carefully delineate the authors' work from the former, and present their contributions. 

Page 4: Other Connections with Lower bounds: The first line " "we may also consider ... ". This line is vague. How will you ensure the amount of deformation is such that the set \bar{U} is contained in U ?

Page 4 last paragraph: "One advantage ... complex architectures in practice" : True, but the tightness of the bounds *do* depend on "f" (specifically the RKHS norm). It needs to be ascertained when equality holds in the bounds you propose, so that we know how tight they are. What if the bounds are too loose to be practical?

eqn (8): use something else to denote the function 'U'. You used 'U' before to denote the set. 

eqn (12): does \tilde{O} hide polylog factors? please clarify. 



</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJeWhgkfpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkMlGnC9KQ&amp;noteId=SJeWhgkfpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1233 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1233 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for his comments. We discuss the novelty aspects in our general response ( <a href="https://openreview.net/forum?id=HkMlGnC9KQ&amp;noteId=S1eid00WaQ" target="_blank" rel="nofollow">https://openreview.net/forum?id=HkMlGnC9KQ&amp;noteId=S1eid00WaQ</a> ) and will be happy to clarify this in the paper. Further comments are addressed below.

** controlling the amount of deformations

The stability bounds of B+M provide upper bounds on ||Phi(x') - Phi(x)|| (where x' is a deformation of x) based on quantities related to the corresponding diffeomorphism, i.e. the maximum norm and the maximum jacobian norm. For simple classes of deformations these can be computed precisely in terms of the parameters of the deformation, e.g. for translations, rotations, scaling or simple parametric warps. When bounding these away from zero by a certain constant, ||Phi(x') - Phi(x)|| is then included in a centered ball of the RKHS with a radius growing with this constant. This constant then acts as a regularization parameter, just like the size of additive perturbations in the case of adversarial perturbations, and can be tuned by cross-validation.

** tightness of the lower bounds

This is something that we verify empirically in our experiments at the end of training by checking the values of spectral norms as a proxy of the upper bound, and looking at the gap with the lower bound. In particular, when using the ||f||_M penalty, lower and upper bounds seem to be controlled together in our experiments (Figure 2), making the bound useful, in contrast to PGD, for which spectral norms grow uncontrolled when the lower bound decreases. We will further clarify this in the paper.

eqn (8), (12): thanks for pointing these out, we will fix this in the paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>