<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>One Bit Matters: Understanding Adversarial Examples as the Abuse of Redundancy | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="One Bit Matters: Understanding Adversarial Examples as the Abuse of Redundancy" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1enCo0cK7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="One Bit Matters: Understanding Adversarial Examples as the Abuse of..." />
      <meta name="og:description" content="Adversarial examples have somewhat disrupted the enormous success of machine learning (ML) and are causing concern with regards to its trustworthiness: A small perturbation of an input results in..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1enCo0cK7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>One Bit Matters: Understanding Adversarial Examples as the Abuse of Redundancy</a> <a class="note_content_pdf" href="/pdf?id=B1enCo0cK7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019one,    &#10;title={One Bit Matters: Understanding Adversarial Examples as the Abuse of Redundancy},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=B1enCo0cK7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Adversarial examples have somewhat disrupted the enormous success of machine learning (ML) and are causing concern with regards to its trustworthiness: A small perturbation of an input results in an arbitrary failure of an otherwise seemingly well-trained ML system. While studies are being conducted to discover the intrinsic properties of adversarial examples, such as their transferability and universality, there is insufficient theoretic analysis to help understand the phenomenon in a way that can influence the design process of ML experiments. In this paper, we deduce an information-theoretic model which explains adversarial attacks universally as the abuse of feature redundancies in ML algorithms. We prove that feature redundancy is a necessary condition for the existence of adversarial examples. Our model helps to explain the major questions raised in many anecdotal studies on adversarial examples. Our theory is backed up by empirical measurements of the information content of benign and adversarial examples on both image and text datasets. Our measurements show that typical adversarial examples introduce just enough redundancy to overflow the decision making of a machine learner trained on corresponding benign examples. We conclude with actionable recommendations to improve the robustness of machine learners against adversarial examples.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">adversarial examples, information theory, robust neural networks</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A new theoretical explanation for the existence of adversarial examples</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">5 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BkgV6i2ihQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>No real value of the work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1enCo0cK7&amp;noteId=BkgV6i2ihQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper933 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper933 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper is about understanding robustness of neural networks w.r.t. adversarial examples.

The authors argue that a good model should be built upon a small subset of input features, with the rest being ignored as noise. If there are many more noisy features (referred as redundant ones), it is harder to find the good relevant features, hence the model becomes more susceptible to any noise introduced in the unfiltered features. This argument is presented theoretically using the notion of minimal sufficient statistics; it is stated that a model that is robust to adversary attacks must compress the input features to minimal sufficient statistics. As an example, even if there is one bit more than the minimal sufficient statistics, the model can overfit, and so the paper title.

While the paper is a nice read to understand a new perspective about adversarial examples, it doesn't solve the existing issues with neural networks. At present, it doesn't seem like a complete work to be published.

As per the definition of adversarial examples cited in the paper, if x' has a very small l2 norm distance w.r.t. x, while x' having a class label different from x, x' is considered to be an adversarial example. As per the decades of machine learning literature, x's should not be considered an adversary example as such. I suppose it is deemed to be a reasonable definition specific to the known and unknown capabiltiies of neural networks.

Depending upon a perspective, one may also find the argument on redundnancy counter intuitive. Ensemble classifiers take an advantage of redundancy while avoiding the problem of adversarial examples by learning each classifier on a subset of features or a subset of dataset. In that context, to understand adversarial examples in a principle manner as this paper attempts, one may also want to relate to the old literature for bagging, boosting, etc.


</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1e35Awo3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Discuss an important issue, but empirical investigation is not convincing and discussion is at times incorrect</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1enCo0cK7&amp;noteId=S1e35Awo3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper933 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper933 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
In this work, the authors study the problem of adversarial examples in ML from an information-theoretic point of view. They explore connections between feature redundancy and adversarial perturbations. They provide a combination of empirical and theoretical studies to justify this connection.

Quality, Originality and Novelty:
While the high level idea studied in this paper is interesting, however it has been suggested before in prior work [Tsipras et al., 2018].  Thus, I do not find the analysis in this paper novel or convincing. There also seem to be some technical flaws in the examples used to corroborate the claims. The experimental justification provided is lacking, as analysis is largely restricted to simple models and does not include state-of-the-art attacks. 

Specific Comments:

Section 2:
- The authors are misinformed about the lack of prior work to theoretically understand the cause of adversarial examples. Many aspects of this problem have been analyzed in previous work. Some (non-exhaustive) related work - [Papernot et al., 2016, Fawzi et al., 2018, Schmidt et al., 2018, Bubeck et al., 2018, Tsipras et al., 2018].

-  I do not understand the comment on optical illusions, etc. There seems to be a misunderstanding about the definition of adversarial examples. In the standard setting, adversarial examples are defined as small perturbations, which are imperceptible to humans, that cause models to classify otherwise benign inputs incorrectly. Note that the ground truth label for adversarial perturbations is determined by human classification and thus instances such as optical illusions are not adversarial examples since humans do not consider them to be part of the same class.

Section 3:

- The setting discussed by the authors in Figure 2a does not fit in to the standard adversarial threat model. If you consider the colored regions to be ground truth labels for the points then the colored points are not adversarial examples, but are perturbations that truly switch the underlying label of the data point. The goal of adversarially robust machine learning is not to be resilient to arbitrary magnitudes of noise, but specifically small perturbations that do not change the underlying class of a data point. In fact, in the example provided, if an ML model truly learns the linear separators then it is robust to adversarial examples.

- The authors state that “random noise overflows the separation capability of a network”. As mentioned in comment 1 above, the colored points in Figure 2a are not adversarial examples. Hence it is not clear why this comment about *large* random noise applies to imperceptible adversarial perturbations. Further, it is important to note that the “true” decision boundary of the task is not affected by the presence of adversarial examples. In fact, we expect adversarial examples to capture invariances that we believe would be present in a classifier that was able to learn this true boundary. 

- The authors seem to conflate random noise with adversarial perturbations. It is well known that random noise is not an adversarial perturbation for state-of-the-art ML models. 

- Section 3.1: The example about the boolean circuit that the authors provide is extremely confusing and also likely incorrect. It is unclear what threat model the authors assume here. The authors seem to consider inputs for which the model predicts an incorrect output as adversarial examples. But these do not come from benign inputs unless we allow very large perturbations (for instance an l_inf eps = 1). Under such large perturbations, a model with both w_1, w_2 = 0 could also be made to “fail” by flipping x_1 or x_2.

- Section 3.2: 
-&gt; I had a hard time trying to understand the discussion in this section. The authors use phrases such as “bit eraser” and “overflowing decision making” which have not been properly defined in the paper. It is also unclear what the threat model being considered is. It seems as though the authors consider standard ML models as bit erasers that can already disregard redundant inputs. If this is the case, it is not clear to me what the authors mean by: “For a black box adversarial attack, we therefore just need to add enough irrelevant input to overflow this bit erasure function.” Is the attacker allowed to append an arbitrary bits to the input at test time? This is not true of standard attack models where the number of bits of the input data are fixed, and hence not clear why adversarial examples would need more bit erasures. 
-&gt; It is well-known that training with random noise is not enough to build robust models. 
-&gt; The authors make a broad claim about transferability, but do not provide sufficient evidence to support it. Given that we train ML models with millions of parameters on a much smaller set of training samples, it is unclear to me why different models would consistently identify the same set of redundant features.

- Section 3.3: The claim that the existence of adversarial examples is a failure of standard generalization is incorrect. In the standard ML setting, we seek to minimize expected population loss on samples from the distribution. In the adversarial setting, we seek to minimize the loss on *worst-case* perturbations of inputs drawn from the distribution. This is a different and much more challenging statistical problem and likely requires more data [Schmidt et al, 2018] and compute [Bubeck et al, 2018] as has been studied out prior works.

-Section 4: 
--&gt; Section 4.1: I do not entirely understand the setup of this experiment. Do the authors retrain after setting weights to 0? Or do they simply try setting different fractions of weights to zero until the train error hits eps. It is not clear to me that setting random weights to zero is a good way to evaluate the complexity of robust models (i.e., those trained on adversarial examples). It has been noted in prior work that these robust models already tend to be sparser than standard models, so setting random weights to zero could indeed have a larger impact on these models. Did the authors verify how many of the weights are away from zero for robust models before clipping the weights? If these were included, the plots might change. The plots in Figure 4 actually contradict the authors claim that adversarial examples are harder to memorize, since the training accuracy plots look very similar for all datasets. The test accuracy plots are indeed different, but this could be because robust learning is a statistically more complex problem and needs more data as has been studied in prior work [Schmidt et al, 2018]. 
--&gt; Section 4.2: The authors should provide more details as to how they estimate the various statistics, at least in the appendix. It is hard to draw inferences from these tables without details about the procedure to generate these statistics, how these adversarial examples were constructed (what was the epsilon, did the authors use code released by prior art) and what the models were on which these examples were generated.
--&gt; Section 4.3: Similar insights about feature redundancy in robust models have been observed in prior work [Ross et al., 2017, Tsipras et al., 2018].

- Section 5: Using lossy compression as a defense against adversarial examples has been tried in prior art and is known to be unsuccessful [Athalye et al., 2018].


References:

Papernot, Nicolas, et al. "Towards the science of security and privacy in machine learning." arXiv preprint arXiv:1611.03814 (2016).

Fawzi, Alhussein, Hamza Fawzi, and Omar Fawzi. "Adversarial vulnerability for any classifier." arXiv preprint arXiv:1802.08686 (2018).

Bubeck, Sébastien, Eric Price, and Ilya Razenshteyn. "Adversarial examples from computational constraints." arXiv preprint arXiv:1805.10204 (2018).

Schmidt, Ludwig, et al. "Adversarially Robust Generalization Requires More Data." arXiv preprint arXiv:1804.11285 (2018).

Ross, Andrew Slavin, and Finale Doshi-Velez. "Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients." arXiv preprint arXiv:1711.09404 (2017).

Tsipras, Dimitris, et al. "Robustness May Be at Odds with Accuracy." arXiv preprint 	arXiv:1805.12152 (2018).

Athalye, Anish, Nicholas Carlini, and David Wagner. "Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples." arXiv preprint arXiv:1802.00420 (2018).
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rker6Qrj2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The Claim is not supported appropriately. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1enCo0cK7&amp;noteId=rker6Qrj2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper933 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper933 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors try to provide an information-theoretic explanation of adversarial examples and claim that the possibility of making the redundant network is the key-aspect of the existence of adversarial examples. Unfortunately, the claim is not appropriately supported, and little intuition is provided for authors to make better algorithms tackling the adversarial examples.

In the paper, a network with minimal sufficient statistic is claimed to have no adversarial example (Contrapositive of Theorem 1), but the claim is far from making actionable ML processes which is mentioned as the contribution at the end of Section 1. How the practitioners can make ML processes from this claim of minimal sufficient statistic?

The entropy claim does not appropriately support the redundancy claim. Adding any noise will increase the entropy. Adversarial examples can have more entropy than original examples, but data with higher entropy do not necessarily contain adversarial examples. Entropy measure is not the key property of the existence of adversarial examples, and adversarial examples definitely have more entropy because noise is added.

The redundancy claims do not provide any novel intuition (and looks trivial), and the redundancy claim does not help improve actual algorithms.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJgKKD2g2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Questions for 4.2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1enCo0cK7&amp;noteId=rJgKKD2g2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">26 Oct 2018</span><span class="item">ICLR 2019 Conference Paper933 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Very nice paper. A few questions:

How do you compute "compressed size" and "original size"? (Why is the original size for all MNIST images not 784 bytes?)

What is the perturbation budget used for FGSM? Similarly, what is the mean distortion introduced by DeepFool and CW? Is it meaningful to compare a fixed-distortion L_infinity attack (with presumably fairly low success rate) to DeepFool/CW that are unbounded? Or should we only compare the adversarial images to the benign images.

It looks like CW adversarial examples only have 0.027% higher complexity than benign images (and DeepFool not much larger). Is this within the margin of error? (What is the standard deviation?)</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1gvQeH43m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to "Questions for 4.2"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1enCo0cK7&amp;noteId=B1gvQeH43m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper933 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018</span><span class="item">ICLR 2019 Conference Paper933 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your interest in our work and the valuable questions.   

1) In this paper, we regard compression as an estimation of complexity following previous work (Friedland et al., 2018). The "size" actually indicates storage space required in the disk, so the "compressed size" and "original size" are the storage size of compressed images and original images, respectively.

2) In the implementation, we utilized the standard settings in Cleverhans framework (Papernot et al., 2018) for all attack methods. Specifically, the perturbation budget for FGSM is L_infinity = 0.3. The mean distortion introduced by DeepFool and CW are 0.12 and 0.025, respectively.

3) The results of different attacking methods aim to show that our understanding is not specific to a certain attack. For instance, Table 1 shows that CW and DeepFool insert less distortion because the estimated entropy/redundancy is lower -- however, they still insert distortion. This does not indicate which attack is stronger, which requires further exploration. The goal of the paper is to study the relationship between information redundancy and adversarial examples, and the comparison for different attacks is out of the scope of this paper.
We compare adversarial images against benign ones aiming to confirm the information redundancy effect, which is theoretically studied in Theorem 1 (Section 3.3). Adversarial examples always have higher complexity (larger file length), in other words, a necessary condition for the existence of adversarial examples is redundancy. 

4) To ensure the significance of the results, we repeated each experiment three times using different adversarial images crafted with different random seeds, and the std is 0.0997 with a mean of 0.09. 
In our experiments, we explicitly add strict perturbation bound (specified above) and optimize all our adversarial attacks to achieve minimal distortion to demonstrate our theoretical results "information redundancy is the necessary condition for adversarial examples", since if we allow larger perturbation, the information redundancy will obviously exist and therefore support the theory. Even for these adversarial examples with minimal perturbation, we can see that their complexity is always higher than that of benign though sometimes close partially due to the strict perturbation bound.
In addition, we leverage four metrics (entropy (MLE), entropy (JVHW), original size and compressed size) to evaluate and show that the complexity of adversarial instances is always higher than that of benign, while the actual magnitude of difference depends on the properties of specific measurement metric. 
For instance, the “compressed size” will always reduce the difference gap between instances, so it is expected to see smaller distance based on compressed distance compared with other metrics.

Thanks again for your comments, and we will clarify the settings in the revision.

References:
1. Gerald Friedland, Jingkang Wang, Ruoxi Jia, and Bo Li. The Helmholtz Method: Using Perceptual Compression to Reduce Machine Learning Complexity. arXiv preprint arXiv:1807.10569, 2018
2. Nicolas Papernot, Fartash Faghri, Nicholas Carlini, Ian Goodfellow, Reuben Feinman, et al. Technical Report on the CleverHans v2.1.0 Adversarial Examples Library. arXiv preprint arXiv:1610.00768, 2016
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>