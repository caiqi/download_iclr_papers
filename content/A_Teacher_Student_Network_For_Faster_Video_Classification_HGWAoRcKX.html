<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>A Teacher Student Network For Faster Video Classification | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="A Teacher Student Network For Faster Video Classification" />
        <meta name="citation_author" content="Shweta Bhardwaj" />
        <meta name="citation_author" content="Mukundhan Srinivasan" />
        <meta name="citation_author" content="Mitesh M. Khapra" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1GWAoRcKX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="A Teacher Student Network For Faster Video Classification" />
      <meta name="og:description" content="Over the past few years, various tasks involving videos such as classification, description, summarization and question answering have received a lot of attention. Current models for these tasks..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1GWAoRcKX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A Teacher Student Network For Faster Video Classification</a> <a class="note_content_pdf" href="/pdf?id=H1GWAoRcKX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=cs16s003%40cse.iitm.ac.in" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="cs16s003@cse.iitm.ac.in">Shweta Bhardwaj</a>, <a href="/profile?email=msrinivasan%40nvidia.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="msrinivasan@nvidia.com">Mukundhan Srinivasan</a>, <a href="/profile?email=miteshk%40cse.iitm.ac.in" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="miteshk@cse.iitm.ac.in">Mitesh M. Khapra</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Over the past few years, various tasks involving videos such as classification, description, summarization and question answering have received a lot of attention. Current models for these tasks compute an encoding of the video by treating it as a sequence of images and going over every image in the sequence, which becomes computationally expensive for longer videos. In this paper, we focus on the task of video classification and aim to reduce the computational cost by using the idea of distillation. Specifically, we propose a Teacher-Student network wherein the teacher looks at all the frames in the video but the student looks at only a small fraction of the frames in the video. The idea is to then train the student to minimize  (i)  the difference between the final representation computed by the student and the teacher and/or (ii) the difference between the distributions predicted by the teacher and the student. This smaller student network which involves fewer computations but still learns to mimic the teacher can then be employed at inference time for video classification. We experiment with the YouTube-8M dataset and show  that the proposed student network can reduce the inference time by upto 30% with a negligent drop in the performance. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">video classification, efficient computation, knowledge distillation, teacher-student</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Teacher-Student framework for efficient video classification using fewer frames </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_H1x3DhNapm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GWAoRcKX&amp;noteId=H1x3DhNapm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper868 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper868 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJlDQMPshQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Not convinced by the idea of using fewer frames to obtain the same representation for longer frames</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GWAoRcKX&amp;noteId=HJlDQMPshQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper868 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper868 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: The paper proposed an idea to distill from a full video classification model a small model that only receives smaller number of frames. 
Quality: 
- The paper needs to be carefully proofread. For example, "classification less" -&gt; "classification loss".
- I am not fully convinced by the proposed idea. If only a partial number of frames are observed, how could it achieve the same representation in theory? The proposed method can provide sub-optimal solution but the variance of the accuracy reduction might be huge.
- The paper claims the teacher can be any video models and the paper uses a recurrent model as the teacher. However, there are models not recurrent, such as the I3D model which essentially models a small number of frames and then aggregate them together. So for those models, it is hard to see the value of the proposed method.

Clarity: 
- The idea is easy to catch but clarity of the technical part can be improved.
- How is the small model used? Do you directly use partial videos to do classification or still need to aggregate the small models?

Originality: 
- I am not aware of existing works trying to distill a long-frame model to short-frame models.

Significance: 
- The contribution is incremental and the results are not significant. The uniform-k baseline already achieved a decent result while the proposed method added around 3% by increasing the system complication dramatically.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJl5zSWu3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a> Review for "A Teacher Student Network For Faster Video Classification"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GWAoRcKX&amp;noteId=BJl5zSWu3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper868 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper868 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors present a teacher-student network to solve video classification problem, aiming at reducing computational cost. Specifically, the authors proposed two training algorithms, serial and parallel training. They showed the proposed approach can reduce inference time up to 30% on YouTube-8M dataset, taking a step forward to on-device classification.

Related work:
The YouTube-8M dataset has been updated this year, and the team hosted the 2nd video classification challenge and workshop at ECCV'18. As they limited the model size up to 1GB this year, many participants actually applied similar idea (distillation). They have posted a summary paper and all submitted papers, so I recommend the authors to refer those papers and summarize in the related work section.

J. Lee, A. Natsev, W. Reade, R. Sukthankar, G. Toderici. The 2nd YouTube-8M Large-Scale Video Understanding Challenge, ECCV workshop 2018.

See the list of accepted papers in <a href="https://research.google.com/youtube8m/workshop2018/" target="_blank" rel="nofollow">https://research.google.com/youtube8m/workshop2018/</a>

The authors motivated the work from high computational cost with RNN/LSTM-based models, claiming that they are the state-of-the-art. However, there are other computationally cheaper but still powerful approaches, such as temporal convolution or learnable pooling. It will be better to introduce these other lines of work to solve video classification problem.

Experiment:
The authors conducted experiment mostly on YouTube-8M dataset, but did not specify which version they used. Given the dataset stats, it seems like their very first version with 8.2M videos. According to their update note (https://research.google.com/youtube8m/download.html), this version is most noisy, so they recommend to use more recent version. Also, it will be beneficial to use 2017 or 2018 version to compare against Kaggle challenge and corresponding workshop participants (CVPR'17 and ECCV'18, respectively) directly. As the reported score is based on the first version, it is not comparable to the state-of-the-art models in both workshops. (The first-place team achieved ~0.85 and 0.87+ each year, while this paper reports 0.806 as the best score. It is not directly comparable, as the training and eval set are not the same.)

Other than this, the experiment was well designed, and it is also good to report both GAP and MAP. In video classification problem, examples are usually highly skewed among classes, so it is useful to evaluate with MAP as well, in order to verify if it performs well for rare classes as well as common ones. In Table 2, MAP score shows bigger gap than GAP, so it will be interesting to explain this phenomenon in more details.

The teacher model tends to be weaker, compared against the best performers at ECCV'18 participants. To fully take advantage of knowledge distillation, it makes more sense to make the teacher with ensembles of multiple models to maximize its performance. At the workshop, participants had to make the final model size less than 1GB, so it will be interesting to compare the model size (~number of parameters) as well.

Overall, the idea presented in this paper would have been more interesting if it was submitted before ECCV'18. Knowledge distillation for video classification is no longer a novel idea unfortunately, so I encourage the authors to catch up with the recent work published through YouTube-8M workshop at ECCV, and propose more distinguishable work compared against to them.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1xMua182m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Not enough novelty and missing key references</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1GWAoRcKX&amp;noteId=S1xMua182m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper868 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper868 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a knowledge distillation based framework to train an action recognition model with fewer video frames as input. I don’t think the paper is novel enough as a number of knowledge distillation works exist which are closely related; some of them are on training efficient networks for action recognition.

Pros:

•	It is interesting to see how compact deep neural network can be trained with knowledge distillation.

Cons:

•	My main concern is the limited novelty of the work. The knowledge distillation approach proposed in this work (a combination of feature alignment and prediction KL loss) is a standard one and has been applied to a variety of vision problems.
•	In particular, the parallel training has been used in Y. Zhang, T. Xiang, T. Hospedales and H. Lu, "Deep Mutual Learning", CVPR'2018. In fact, that papers shows that if the teacher and student, or peers, teach each other, rather than the teacher-to-student one-way traffic, it is more effective. 
•	For action recognition, this paper: B. Zhang, L. Wang, Z. Wang, Y. Qiao, and H. Wang, “Real-time action recognition with enhanced motion vector cnns,” in CVPR, 2016, proposed something very similar: instead of teacher network having more frames as input, in their work, the teacher network has access to the more expensive optimal flow modality, whilst the student network uses the motion vector as a by-product of video compression (hence free). Apart from this difference, the formulation is very similar. This paper should certainly be cited. 
•	Apart from the novelty, the experiment is also limited in that only one dataset is used. The hierarchical RNN model may be effective on the YouTube-8M dataset, but other models such as the two-stream model from Simonyan and Zisserman and I3D from Carreira and Zisserman are popular on other datasets such as Kinetics. It would be useful to see some additional experiments on another benchmark with a different network architecture. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>