<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>In search of theoretically grounded pruning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="In search of theoretically grounded pruning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SkfQAiA9YX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="In search of theoretically grounded pruning" />
      <meta name="og:description" content="Deep learning relies on resource-heavy linear algebra operations which can be prohibitively expensive when deploying to constrained embedded and mobile devices, or even when training large-scale..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SkfQAiA9YX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>In search of theoretically grounded pruning</a> <a class="note_content_pdf" href="/pdf?id=SkfQAiA9YX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019in,    &#10;title={In search of theoretically grounded pruning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SkfQAiA9YX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Deep learning relies on resource-heavy linear algebra operations which can be prohibitively expensive when deploying to constrained embedded and mobile devices, or even when training large-scale networks. One way to reduce a neural network's resource requirements is to sparsify its weight matrices - a process often referred to as pruning. It is typically achieved by removing least important weights as measured by some salience criterion, with pruning by magnitude being the most popular option. This, however, often makes close to random judgments. In this paper we aim to closely investigate the concept of model weight importance, with a particular focus on the magnitude criterion and its most suitable substitute. To this end we identify a suitable Statistical framework and derive deep model parameter asymptotic theory to use with it. Thus, we derive a statistically-grounded pruning criterion which we compare with the magnitude pruning both qualitatively and quantitatively. We find this criterion to better capture parameter salience, by accounting for its estimation uncertainty. This results in improved performance and easier post-pruned re-training.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_S1xyF75q27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review of "In search of theoretically grounded pruning"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkfQAiA9YX&amp;noteId=S1xyF75q27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper879 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper879 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value"># Summary

The article proposes a variation of magnitude-based pruning for neural networks. In a nutshell, the authors argue that standard MLE asymptotic theory is applicable to deep learning, and propose to use the Wald test statistic as a salience criterion for pruning. Limited experimental results are provided using the LeNet-300-100 and LeNet-5 architectures and the MNIST dataset.

# Assessment

Compression and pruning of neural networks is a topic with a vast wealth of prior work spanning several decades. Nonetheless, the interest in deploying deep learning-based models to devices with limited compute capabilities has kept the topic particularly active in the last years.

Despite the relevance of this paper's topic, unfortunately I do not believe that the methodological contribution is sufficiently novel and significant nor that the experimental results are sufficiently thorough and compelling to recommend the paper for publication.

# Major points

1) Novelty and Significance of the methodological contribution

A large part of the paper is devoted to justify the applicability of the Wald test to deep learning models. Nonetheless, I do not believe the arguments put forward are sufficiently rigorous or original to consider those as a substantial contribution. Perhaps most importantly, I have strong concerns about several of those arguments.

For example, using the Wald test requires identifiability, which is clearly severely violated by most commonly used architectures. While the paper argues that the parameter space can be restricted to contain a single minimum, that hardly resembles the way models are trained in practice. 

The Wald test also requires the training to converge to a local minimum, i.e., the Hessian must be positive definite. Again, I do not believe this assumption holds in many real-world models. Convergence to saddle points often occurs, with the Hessian thus containing negative eigenvalues [1].

As accepted in the Appendix, the differentiability condition is violated by some of the most commonly used activation functions, including ReLUs.

Normality of the score distribution depends on asymptotic arguments. However, many deep learning models have a much larger number of parameters than samples they are trained on. Consequently, it is unclear whether asymptotic results are applicable in the regime in which deep learning-based approaches are commonly used.

Finally, perhaps less importantly, the assumption if i.i.d. samples might also be too strong for many real-world datasets.

The proposed criterion is otherwise a straightforward application of the Wald test.

2) Experimental results

The experimental section is currently substantially lacking in pretty much all aspects, including number of architectures under consideration, number of datasets studied and selection of a representative set of state-of-the-art competitors among the very many related methods published recently (e.g. [2-8], just to mention a small subset). 

Perhaps most importantly, the method does not appear to clearly outperform the state of the art. However, the insufficient experimental results reported in the manuscript render a precise assessment difficult.

# References

[1] Martens, James. "New insights and perspectives on the natural gradient method." arXiv preprint arXiv:1412.1193 (2014).
[2] Guo, Yiwen, Anbang Yao, and Yurong Chen. "Dynamic network surgery for efficient dnns." Advances In Neural Information Processing Systems. 2016.
[3] Dong, Xin, Shangyu Chen, and Sinno Pan. "Learning to prune deep neural networks via layer-wise optimal brain surgeon." Advances in Neural Information Processing Systems. 2017.
[4] Ullrich, Karen, Edward Meeds, and Max Welling. "Soft weight-sharing for neural network compression." International Conference on Learning Representations. 2017.
[5] Molchanov, Dmitry, Arsenii Ashukha, and Dmitry Vetrov. "Variational dropout sparsifies deep neural networks." International Conference on Machine Learning. 2017.
[6] Louizos, Christos, Karen Ullrich, and Max Welling. "Bayesian compression for deep learning." Advances in Neural Information Processing Systems. 2017.
[7] Louizos, Christos, Max Welling, and Diederik P. Kingma. "Learning Sparse Neural Networks through $ L_0 $ Regularization." International Conference on Learning Representations. 2018.
[8] Yang, Yibo, Nicholas Ruozzi, and Vibhav Gogate. "Scalable Neural Network Compression and Pruning Using Hard Clustering and L1 Regularization." arXiv preprint arXiv:1806.05355 (2018).</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SklUhDEq3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper is about obtaining a "fully estimable empirical distribution" for neural networks parameters in order to obtain some saliency or importance criterion for performing theoretically grounded pruning. An interesting idea at first sight, but the work and writing are done carelessly and in a rush.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkfQAiA9YX&amp;noteId=SklUhDEq3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper879 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper879 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Comments
* all conditions are not discussed, for instance, isn't it quite strong to impose that the parameter space \Theta is compact?
* LDH condition: what is \mathcal{N}, it isn't defined
* The statement on top of page 3: "Then, asymptotically, as the sample size n grows to infinity, the model parameters are distributed as (...)" is awkward: is it standard? is it a new result? please consider stating it as a Proposition/Theorem or something, and explain where it comes from.
* Actually, I am skeptical about the above statement itself: it is stated that "the model parameters are distributed as (...)". Does that mean that the parameters are random, in a Bayesian fashion? please state it then. Or the alluded to distribution is that of parameters' estimators? then the statement should be amended, and it should be explained what estimators are considered.
* footnote 3: shouldn't the considered sum be divided by n?
* the paper is full of flawed equations where, I think, parameters are mixed up with estimators. 
    * For instance, Equations (2) and (3) should express the same quantity, $W$, however the former displays the parametersâ€™ covariance matrix, while the latter uses an estimator of it, \hat{Avar}.
    * Same in page 4 with the estimator of the Hessians (H) and of the score covariance matrix (\hat Î£).
    * Same for Equation (6): \hat \theta is suddenly plugged-in in place of \theta_0, all of this conserving the equality sign.
    * Same for Equation (7)


Typos, minor comments and un-precise statements
* the footnotes superscripts are misleading for footnotes 2 and 3; they could be better placed.
* Lindeberg-Lvy should be Lindeberg-LÃ©vy
* after equations (6,7,8) you write "Where all is defined as above": this is fairly un-precise; as you use LaTeX, you should conveniently use some \label and \ref.
* end of section 4.1: MINST dataset, a new dataset?
* top of section 4.2: there are even typos in the proposed name for methodology! : "Walt-based Pruning"
* Many references in the bibliography are incomplete, so that it is impossible to see where they come from (at least seven of them)</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HygCeTJ5n7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good insights on magnitude based pruning but needs more empirical results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkfQAiA9YX&amp;noteId=HygCeTJ5n7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper879 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper879 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a statistical framework for determining parameter importance beyond magnitude based pruning.  The paper derives deep model parameter asymptotic theory to formulate a statistically-grounded pruning criterion which we compare with the magnitude pruning both qualitatively and quantitatively. The paper finds that this criterion to better capture parameter salience, since it takes into consideration the estimation uncertainty. The paper demonstrates the improved results for Lenet in terms of performance and easier post-pruned re-training.

The paper's take on the commonly used magnitude based pruning if appreciated. However, I am not fully convinced that pruning weights by magnitude close to random judgements in the general case as in the paper abstract. While the points made in Section 2 against magnitude based pruning are sound and relevant, it is unclear making such a strong general claim is wise since weights are commonly regularized. In fact, some previous work on pruning has performed these experiments and found that for common networks with default hyper-parameter configurations, weight based pruning performs better than magnitude based pruning. Li et. Al 2017 in the paper, Figure 4.4 compares random pruning with smallest L1 norm pruning and they find that smallest filter pruning has better accuracy than random filter pruning for all layers. Are there any real networks where such results would apply beyond the toy network in Figure 1?

In terms of empirical evaluation, results beyond Lenet such as on CIFAR-10 and Imagenet are required to claim Wald pruning is superior over existing methods, and comparison against other pruning methods such as dynamic pruning will make the paper stronger. It is unclear what to make of the 0.8% improvement in the Lenet results, performing extensive evaluation is necessary to show the performance of Wald pruning.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>