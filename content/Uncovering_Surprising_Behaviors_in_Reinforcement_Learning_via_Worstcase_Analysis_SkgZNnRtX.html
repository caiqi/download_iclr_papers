<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Uncovering Surprising Behaviors in Reinforcement Learning via Worst-case Analysis | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Uncovering Surprising Behaviors in Reinforcement Learning via Worst-case Analysis" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SkgZNnR5tX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Uncovering Surprising Behaviors in Reinforcement Learning via..." />
      <meta name="og:description" content="Reinforcement-learning (RL) agents are typically trained and evaluated according to their performance averaged over some distribution of environment settings. But does the distribution over..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SkgZNnR5tX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Uncovering Surprising Behaviors in Reinforcement Learning via Worst-case Analysis</a> <a class="note_content_pdf" href="/pdf?id=SkgZNnR5tX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019uncovering,    &#10;title={Uncovering Surprising Behaviors in Reinforcement Learning via Worst-case Analysis},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SkgZNnR5tX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Reinforcement-learning (RL) agents are typically trained and evaluated according to their performance averaged over some distribution of environment settings. But does the distribution over environment settings contain important biases? Do these lead to agents that fail in certain cases despite high average-case performance? In this work, we consider worst-case evaluation of agents over environment settings in order to detect whether there are directions in which agents may have failed to generalize. Specifically, we consider a 3D first-person task where agents must navigate procedurally generated mazes, and where RL agents have recently achieved human-level average-case performance. Using a method which can be described as evolution over mazes, we find that despite impressive average-case performance, agents still suffer from catastrophic failures on certain mazes, including some surprisingly simple mazes. Additionally, we find that these failures transfer between different agents and even significantly different architectures. We believe our findings highlight an important role for worst-case evaluation in identifying whether there are directions in which agents have failed to generalize. Our hope is that the ability to automatically identify failures of generalization will facilitate development of more general, robust agents.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Reinforcement learning, Adversarial examples, Navigation, Evaluation, Analysis</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We find environment settings in which SOTA agents trained on navigation tasks display extreme failures suggesting failures in generalizaation.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HylA5R1cp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Overall response to all reviewers on out-of-distribution samples and adapting the training distribution</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkgZNnR5tX&amp;noteId=HylA5R1cp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1424 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1424 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">As adapting the training distribution was a common point raised by all reviewers, we provide this overall response for all reviewers.

We absolutely agree that adapting the training distribution is an important direction, and during the work for this paper we investigated several methods for incorporating out-of-distribution mazes into the training distribution.

Specifically, we tried two approaches related to what the reviewers suggested: 
    - Altered Training: we altered the maze generator so that any adversarial maze could be generated and seen during training, meaning our adversarial search procedure produces in-distribution adversarial mazes. We accomplished this by randomly altering the original mazes, repeatedly using the same Modify function used by our adversarial search procedure, but without selecting for worst agent performance.
    - Adversarial Training: we incorporated a large set of adversarial mazes into the training distribution, including the 500 mazes we used in our transfer analysis experiment. This was achieved by having two sets of mazes (the default distribution and the adversarial distribution) which were then randomly sampled every episode (i.e. 50% of training episodes were on an adversarial maze).

To summarise, we found that -- perhaps surprisingly -- neither of these approaches significantly improved the agent’s robustness to our adversarial search procedure (i.e. numerous adversarial mazes still exist and can be found), and therefore adapting the training distribution in such a way that improves performance in the dimension we are interested in is more challenging than initially thought.

Given this result, we originally did not include these experiments as we felt they were too preliminary and the topic was worth a thorough investigation in subsequent work. However, given the surprise of the result, the further work we have put into these results since submission, and that all reviewers suggested trying it, we will include an additional section in the paper on ‘Adapting the Training Distribution’ where we will include our experimental results.

We list the two key results here while we update the paper:
    - Altered Training: we found that training on altered mazes made it marginally harder for our search method to find adversarial mazes (i.e. it took several more iterations), however overall this did not fix the problem.
    - Adversarial Training: we found that incorporating adversarial examples into the training distribution led to agents performing well on those specific examples, however this did not lead to agents performing better against our adversarial search method.

For Adversarial Training, it is possible that adding many more adversarial mazes into the distribution will lead to more robustness (as is the case for some image classification datasets such as MNIST and CIFAR10). However, this would require significant new techniques for finding adversarial mazes more efficiently, and is another line of inquiry we are currently pursuing.

We also found that there were quantifiable differences between the adversarial and non-adversarial distribution of mazes (e.g. goals in small rooms, long path from the player to the goal). Indeed, as all reviewers mentioned, this motivates adding out-of-distribution / rare mazes into the training distribution. However, importantly, we note that while we identified features which correlated with adversarial mazes, there is not necessarily causation. To investigate causation, we tried handcrafting mazes with the identified features of the adversarial distribution, however we were not able to consistently create mazes which were adversarial. This point of correlation versus causation is one possible reason why adapting the training distribution is more challenging than initially thought.

We believe the addition of this new section will provide a more complete investigation while also opening up a number of interesting research directions for future work. We believe it is likely that, similar to adversarial examples in image classification domains, figuring out how to train agents to be more robust and general is likely to take significant time and effort, and therefore span many papers and works.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1xFkeP5hX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A simple idea with interesting results, but lacking in broader impact</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkgZNnR5tX&amp;noteId=B1xFkeP5hX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1424 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1424 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors present a simple technique for finding "worst-case" maze environments that result in bad performance. The adversarial optimization procedure is a greedy procedure, which alternately perturbs maze environments and selects the maze on which the trained agent performs worst for the next iteration. The authors highlight three properties of these mazes, which show how this adversarial optimization procedure can negatively impact performance.

High-level comments:
- I am unconvinced that many of the observed behaviors are "surprising". The procedure for adversarially optimizing the maps is creating out-of-distribution map samples (this is confirmed by the authors). The function for creating maps built-in to DeepMind Lab (the tool used to generate the random maps used in this paper) has a set of rules it uses to ensure that the map follows certain criteria. Visual inspection of the 'Iteration 20' maps in Figure 2 finds that the resulting adversarial map looks fundamentally different from the 'Initial Candidate' maps. As a result, many of the features present in the adversarial maps may not exist in the initial distribution, and the lack of generalizability of Deep RL has become a relatively common talking point within the community. That being said, I agree with the authors' claims about how this sort of analysis is important (I discuss this more in my second point).
- In my mind, the 'discovery' of the performance on these optimized out-of-distribution samples is, in my mind, not particularly impactful on its own. The Deep RL community is already rather aware of the lack of generalization ability for agents, but are in need of tools to make the agents more robust to these sorts of examples. For comparison, there is a community which researches techniques to robustify supervised learning systems to adversarial examples (this is also mentioned by the authors in the paper). I feel that this paper is only partially complete without an investigation of how these out-of-distribution samples can be used to improve the performance of the agents. The addition of such an investigation has the potential to greatly strengthen the paper. This lack of "significance" is the biggest factor in my decision.
- The first two results sections outlining the properties of the adversarially optimized mazes were all well-written and interesting. While generally interesting, that the less-complex A2CV agent shows better generalization performance than the more-complex MERLIN agent is also not overly surprising. Yet, it remains a good study of a phenomenon I would not have thought to investigate.

Minor comments:
- The paper is very clear in general. It was a pleasure to read, so thank you! The introduction is particularly engaging, and I found myself nodding along while
- Figures are generally excellent; your figure titles are also extremely informative, so good work here.
- Fig 4. It might be clearer to say "adversarially optimized" instead of simplly "optimized" in the (b) caption to be clearer that it the map that is being changed here, rather than the agent. Also, "Human Trajectories" -&gt; "Human Trajectory", since there is only one.
- I am not a fan of saying "3D navigation tasks" for 2.5D environments (but this has become standard, so feel free to leave this unchanged).

This paper is a well-written investigation of adversarially chosen out-of-distribution samples. However, the the high-quality of this narrow investigation still only paints a partial picture of the problem the authors set out to address. At the moment, I am hesitant to recommend this paper for acceptance, due to its relatively low "significance"; a more thorough investigation of how these out-of-distribution samples can be used.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1xuCa1cTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Addressing comments on "surprise" and out-of-distribution samples</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkgZNnR5tX&amp;noteId=B1xuCa1cTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1424 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1424 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your constructive comments. We’re glad you enjoyed reading the paper and hope our responses below address your concerns.

Please see the overall response to all reviewers for a detailed answer on adapting the training distribution which we will include in the paper. Here we will provide answers specific to the points you raised.

&gt; I am unconvinced that many of the observed behaviors are "surprising". 

While surprise is clearly subjective, we, and others we spoke to, found the following results surprising: (1) lack of generalisation to simpler situations, (2) how extreme the failures are (not just that the algorithm is imperfect, but 100x’s reduction in performance), (3) transfer of failures between different agents/architectures (so these failures aren’t just overly-specialised to specific training runs). Importantly, we found that these (arguably surprising) behaviours were also present after we addressed the out-of-distribution concern which we explain in the next point.

&gt; The procedure for adversarially optimizing the maps is creating out-of-distribution map samples.

To address this, we have altered the maze generator so that any adversarial maze can be generated and seen during training, meaning our adversarial search procedure produces in-distribution adversarial mazes. We accomplished this by randomly altering the original mazes using the same Modify function used by our adversarial search procedure.

The result of this change is that our adversarial search procedure takes several more iterations to find a maze as-adversarial as before, however it is still possible to find such mazes and there is no significant improvement in agent performance.

&gt; I feel that this paper is only partially complete without an investigation of how these out-of-distribution samples can be used to improve the performance of the agents.

We agree, and the other reviewers also mentioned this, therefore we will be adding a section on this topic to the paper. We found that using these out-of-distribution samples to improve the performance of the agents is not straightforward, and the approaches we tried (which were also suggested by various reviewers) were not sufficient for significantly improving performance. 

In the case of adding adversarial samples to training, we found that agents learned to solve the specific adversarial samples they were trained on, but did not perform better on adversarial samples overall.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ByxGf2I5nQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Official review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkgZNnR5tX&amp;noteId=ByxGf2I5nQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1424 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1424 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper analyzes the performance of modern reinforcement-learning-based navigation agents by searching for “adversarial” maze layouts in which the agents do not perform well. It turns out that such mazes  exist, and moreover, one can find even relatively simple maze configurations that are easily solved by humans, but very challenging for the algorithms.

Pros:
1) Interesting and relevant topic: it is important not only to push for best results on benchmarks, but also understand the limitations of existing approaches.
2) The paper is well written
3) The experiments are quite thorough and convincing. I especially appreciate that it is demonstrated that there exist simple mazes that can be easily solved by humans, but not by algorithms. The analysis of transferability of “adversarial” mazes between different agents is also a plus.

Cons:
1) I am not convinced worst-case performance is the most informative way to evaluate models. Almost no machine learning model is perfect, and therefore almost for any model it would be possible to find training or validation samples on which it does not perform well. Why is it so surprising that this is also the case for navigation models? Why would one assume they should be perfect? Especially given that the generated “adversarial” mazed lie outside of the training data distribution, seemingly quite far outside. Are machine learning models ever expected to perfectly generalize outside of the training data distribution? Very roughly speaking, the key finding of the paper can be summarized as “several recent navigation agents have problems finding and entering small rooms of the type they never saw during training” - is this all that significant?

To me, the most interesting part of the paper is that the models generalize as well as they do. I would therefore like to see if it is possible to modify the training distribution - by adding “adversarial” mazes, potentially in an iterative fashion, or just by hand-designing a wider distribution of mazes - so that generalization becomes nearly perfect and the proposed search method is not anymore able to find “adversarial” mazes that are difficult for the algorithm, but easy for humans.

2) On a related note, to me the largest difference between the mazes generated in this paper and the classical adversarial images is that the modification of the maze is not constrained to be small or imperceptible. In fact, it is quite huge - the generated mazes are far from the training distribution. This is a completely different regime. This is like training a model to classify cartoon images and then asking it to generalize to real images (or perhaps other way round). Noone would expect existing image classification models to do this. This major difference with classical adversarial examples should be clearly acknowledged.

3) It would be interesting to know the computational requirements of the search method. I guess it can be estimated from the information in the paper, but would be great to mention it explicitly. (I am sorry if it is already mentioned and I missed it)

To conclude, I think the paper is interesting and well executed, but the presented results are very much to be expected. To me the most interesting aspect of the work is that the navigation agents generalize surprisingly well. Therefore, I believe the work would be much more useful if it focused more on how to make the agents generalize even better, especially since there is a very straightforward way to try this - by extending the training set. I am currently in the borderline mode, but would be very happy to change my evaluation if the focus of the paper is somewhat changed and additional experiments on improving generalization (or some other experiments, but making the results a bit more useful/surprising) are added.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJgT7Rk5a7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Addressing questions on perfection, modifying the training distribution, and the computational requirements of the search method</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkgZNnR5tX&amp;noteId=SJgT7Rk5a7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1424 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1424 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your detailed comments. We hope our responses below address your comments.

&gt; 1) Almost no machine learning model is perfect... Why is it so surprising that this is also the case for navigation models? Why would one assume they should be perfect? 

We agree that it is not necessarily surprising that these failure cases exist, and this is not the key result we wanted to highlight as surprising. While surprise is clearly subjective, we, and others we spoke to, found the following results surprising: (1) lack of generalisation to simpler situations, (2) how extreme the failures are (not just that the algorithm is imperfect, but 100x’s reduction in performance), (3) transfer of failures between different agents/architectures (so these failures aren’t just overly-specialised to specific training runs).

&gt; 1) I would therefore like to see if it is possible to modify the training distribution - by adding “adversarial” mazes.

Please see the overall response to all reviewers for a full answer to this question.

Summarising our full answer for your question - yes it is possible to modify the training distribution. However, in the case of adding “adversarial” mazes to the training distribution, we found that while agents learned to perform well on the specific mazes added, they did not perform better on adversarial mazes overall. It is possible that adding many more adversarial mazes into the distribution will lead to more robustness (as is the case for some image classification datasets such as MNIST and CIFAR10), however this would require significant new techniques for finding adversarial mazes more efficiently, and this is another line of inquiry we are currently pursuing.

&gt; 2) The modification of the maze is not constrained to be small or imperceptible. In fact, it is quite huge - the generated mazes are far from the training distribution … . This major difference with classical adversarial examples should be clearly acknowledged.

Please see the overall response to all reviewers for a full answer to this question. 

To address this concern, we have altered the maze generator so that any adversarial maze can be generated and seen during training, meaning our adversarial search procedure produces in-distribution adversarial mazes. We accomplished this by randomly altering the original mazes using the same Modify function used by our adversarial search procedure. The result of this is that the generated mazes are no longer far from the training distribution.

We will add an acknowledgement to the difference between our current analysis and the classical adversarial examples, and highlight how our new section on adapting the training distribution helps address this point.

&gt; 3) It would be interesting to know the computational requirements of the search method. 

Our search method is performed using 10 candidate mazes per iteration, each evaluated 30 times, across 20 iterations. This is a total of 6000 episodes for the entire search procedure, and all episodes within one iteration can be evaluated in parallel (i.e. 20 batches of 300 episodes).  Depending on the desired confidence level and resources available, the number of evaluations can be increased or decreased. Indeed, we found that evaluating each maze only 10 times rather than 30 produced similar results and led to a 3x speed up. In our experiments with 30 evaluations per maze, the entire search procedure took 30 minutes to complete, and only 9 minutes on average to find an adversarial maze where the probability of the agent finding the goal was below 50%. As suggested, we will add an explicit mention of this in the paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SygK7DGMnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkgZNnR5tX&amp;noteId=SygK7DGMnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1424 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1424 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This is an interesting paper, trying to find the adversarial cases in reinforcement learning agents. The paper discusses several different settings to investigate how generalizable the worst-case environment is across different models and conjectured that it comes from the bias in training the agents. Overall the paper is well-written and the experiments seem convincing. I have two questions regarding the presented result.

1. The search algorithm depicted in section 2 is only able to find a local optimum in the environment space. How robust is the result given different initializations?

2. It is briefly discussed in the paper that the failure in certain mazes might come from the structural bias in the training and the “complex” mazes are under-represented in the training dataset. It is hence natural to ask, if the procedure described in this paper can be incorporated to enhance the performance by some simple heuristics like re-weighting the training samples. I think some discussion on this would be beneficial for verifying the conjecture made here.

3. The authors compared the “hardness” of the mazes based on the number of walls in the maze. But it is arguably a good metric as the authors also mentioned visibility and other factors in measuring the complexity of the task. I would like to see more exploration in different factors that accounts for the complexity and maybe compare different agents to see if they are sensitive in the same set of factors. 

To summarize, I like the idea of the paper and I think the result can be illuminating and worth some more follow-up work to understand the RL training in general.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJxuv6y5T7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Addressing questions on robustness, re-weighting training samples, and measures of complexity</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkgZNnR5tX&amp;noteId=rJxuv6y5T7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1424 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1424 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your positive comments. We hope our following responses address the three questions you raised.

&gt; 1. The search algorithm depicted in section 2 is only able to find a local optimum in the environment space. How robust is the result given different initializations?

Great question. We found that our search algorithm is robust to different initialisations

In Figure 3 (Section 3.1), we report the average performance of 50 independent optimisation runs (i.e. 50 different initialisations). Related to your question, in 44/50 (88%) of these runs our search algorithm found an adversarial maze where the agent’s probability of finding the goal was &lt;50% (compared to 98% on the average maze). It is also possible to improve the robustness of our method by increasing the number of candidates considered per iteration (at the cost of increased time/computational requirements).

The 25th, 50th, and 75th percentiles of our optimisation method were as follows:
    - p(reaching the goal): 0.031, 0.136, 0.279
    - number of goals reached: 0.042, 0.136, 0.368

We will include a mention of these in the appendix.

&gt; 2. It is hence natural to ask, if the procedure described in this paper can be incorporated to enhance the performance by some simple heuristics like re-weighting the training samples.

Please see the overall response to all reviewers for a full answer to this question.

Summarising our full answer for your question - yes the procedure described in the paper can be used to re-weight the training samples, and we will include a section to the paper describing these experiments. However, we found that doing this is not sufficient to significantly enhance the performance of agents. Specifically, we found that re-weighting adversarial examples improved performance on those examples, but did not lead to agents improving overall.

&gt; 3. I would like to see more exploration in different factors that accounts for the complexity.

This is an interesting point and relates to our motivation for adapting the training distribution. We investigated a number of different ways for measuring the complexity of mazes, comparing the distribution of various features between adversarial and non-adversarial mazes.

Notably, we found differences in two measures of maze complexity: (1) the shortest path distance between the player’s start location and the goal location, and (2) the complexity of the shortest path to goal defined by the shortest path divided by the straight line distance to the goal. In particular, we found that adversarial mazes have a significantly longer shortest path to the goal on average, as well as a higher path complexity. 

These findings are in part what motivated us to adapt the training distribution, for example by re-weighting training samples which were adversarial. However, we observed minimal improvement doing this, and one of our hypotheses for this is that while the measures above are correlated with mazes being adversarial, they are not necessarily the cause (we discuss this more in our overall response to all reviewers).

We will add these findings to the paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>