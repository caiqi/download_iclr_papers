<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>A Modern Take on the Bias-Variance Tradeoff in Neural Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="A Modern Take on the Bias-Variance Tradeoff in Neural Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HkgmzhC5F7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="A Modern Take on the Bias-Variance Tradeoff in Neural Networks" />
      <meta name="og:description" content="We revisit the bias-variance tradeoff for neural networks in light of modern empirical findings. The traditional bias-variance tradeoff in machine learning suggests that as model complexity grows..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HkgmzhC5F7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A Modern Take on the Bias-Variance Tradeoff in Neural Networks</a> <a class="note_content_pdf" href="/pdf?id=HkgmzhC5F7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 10 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019a,    &#10;title={A Modern Take on the Bias-Variance Tradeoff in Neural Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HkgmzhC5F7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=HkgmzhC5F7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We revisit the bias-variance tradeoff for neural networks in light of modern empirical findings. The traditional bias-variance tradeoff in machine learning suggests that as model complexity grows, variance increases. Classical bounds in statistical learning theory point to the number of parameters in a model as a measure of model complexity, which means the tradeoff would indicate that variance increases with the size of neural networks. However, we empirically find that variance due to training set sampling is roughly constant (with both width and depth) in practice. Variance caused by the non-convexity of the loss landscape is different. We find that it decreases with width and increases with depth, in our setting. We provide theoretical analysis, in a simplified setting inspired by linear models, that is consistent with our empirical findings for width. We view bias-variance as a useful lens to study generalization through and encourage further theoretical explanation from this perspective.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">bias-variance tradeoff, deep learning theory, generalization, concentration</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We revisit empirically and theoretically the bias-variance tradeoff for neural networks to shed more light on their generalization properties.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rygWfqh767" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Global Comment from Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgmzhC5F7&amp;noteId=rygWfqh767"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1248 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1248 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you to all of the reviewers for their time. Most importantly, we have revised Section 2.1 to make it more clear and have added an explicit description of all randomness in Eq. 5 in order to aid in the understanding of our decomposition of variance (main contribution 2). We hope this improved clarity in the preliminaries makes the significance of main contribution 2 more apparent.

Main contributions:
1. Variance decreases with width (along with bias), indicating that it isn’t necessary to trade bias for variance.
2. We perform a deeper study of variance by decomposing the coarse variance into two terms: variance due to training set sampling (like the classical decomposition) and variance due to initialization. We find variance due to sampling is roughly constant with both width and depth (Figure 2).
3. In a simplified setting, inspired by linear models, we provide theoretical analysis in support of our empirical findings for network width.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJlSHgE03Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting paper with some experiments and preliminary results but requires more work </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgmzhC5F7&amp;noteId=SJlSHgE03Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1248 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1248 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies variance-bias tradeoff as a function of depth and width of a neural network. Experiments suggest that variance may decrease as a function width and increase as a function of depth. Some analytical results are presented why this may the case for width and why the necessary assumptions for the depth are violated.

Main comment on experiments: if I am correct the step size for optimization is chosen in a data-dependent way for each size of the network. This is a subtle point since it leads to a data-dependent hypothesis set. In other words, in this experiments for each width we study variance of neural nets that can be found in fixed number of iterations by a step size that is chosen in data-dependent way. It may be the case that as width grows the step size decreases faster and hence hypothesis set shrinks and we observe decreasing variance. This makes the results of experiments with width not so surprising or interesting.

Further comments on experiments: it probably worth pointing out that results for depth are what we would expect from theory in general.

More on experiments: it would be also interesting to see how variance behaves as a function of width for depth other than 1.

On assumptions: it is not really clear why assumptions in 5.2 hold for wide shallow networks at least in some cases. Paper provides some references to prior work but it would be great to give more details. Furthermore, some statements seems to be contradicting: sentence before 5.2.1 seems to say that assumption (a) should hold for deep nets while sentence at the end of page 8 seems to say the opposite.

Overall: I think this paper presents an interesting avenue of research but due to aforementioned points is not ready for publication.  


</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1es552m67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author Response to Reviewer 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgmzhC5F7&amp;noteId=r1es552m67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1248 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1248 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for taking the time to review our paper!


“Main comment on experiments [...] It may be the case that as width grows the step size decreases faster and hence hypothesis set shrinks and we observe decreasing variance”:
Thank you for bringing up this point that leads to this natural hypothesis. Note that the bias is also going down with width. Traditional bias-variance tradeoff thinking associates decreasing bias with a growing hypothesis set. The fact that we see both bias and variance decrease is what’s surprising, as it shows we don’t need to trade bias for variance.

In addition, we do not see that the step sizes are decreasing with width. The step sizes that were used for the decreasing variance in the small data setting (Figure 3a) are provided in Appendix B.1.

Furthermore, note that the same experimental procedure did not lead to decreasing variance with depth. By the same line of reasoning as in the above quote, we would expect to get decreasing variance with depth by having smaller and smaller step sizes with deeper networks. However, our experimental procedure did not yield that.

We hope that these points make it clear that we considered this potential explanation of our results, and we determined that this explanation does not capture the whole story. For more discussion on the justification of our experimental design, see Section 3.3 and Appendix B.2.


“results for depth are what we would expect from theory in general”:
While we agree that variance due to initialization is consistent with orthodoxy and discuss this in Section 4.2, we find the observation that variance due to sampling is roughly constant with depth (Figure 2b) quite surprising. This is because the traditional bias-variance tradeoff is exactly about training set sampling randomness (not optimization randomness). The distinction between these two sources of randomness is key to our deeper level of study (main contribution 2).


On assumptions:
We note that the assumptions are strong (though they have their basis in the referenced literature). Our primary goal is to give a rigorous argument beyond the linear case.

Regarding your request for more details on the assumptions, we’ve updated the paragraph just before Section 5.2.1 with another reference and a more clear explanation: “Sagun et al. (2017) showed that the spectrum of the Hessian for over-parametrized networks splits into (i)  a bulk centered near zero and (ii)  a small number of large eigenvalues, which suggests that learning occurs mainly in a small number of directions.” This hypothesis was also formulated by Advani &amp; Saxe (2017), and this is an active area of research. For example, there was a paper submitted to this ICLR titled “Gradient Descent Happens in a Tiny Subspace” that is entirely dedicated to this line of thinking: <a href="https://openreview.net/forum?id=ByeTHsAqtX" target="_blank" rel="nofollow">https://openreview.net/forum?id=ByeTHsAqtX</a>  We view our analysis (and identification of these assumptions) as a useful contribution because it is consistent with the experimental results for width, and the assumptions have some basis in the literature.

Additionally, we have added more to Section 5.3 to make the intuition more clear for why varying the depth is very different from varying the width, with respect to these assumptions.


On seeming contradiction:
Our reference to Advani &amp; Saxe’s result was indeed very imprecise and seems to result in a contradiction. Thank you for pointing that out. We have updated this in the recent revision to report more clearly their result (just before Section 5.2.1).  Their analysis shows that our assumption (a) holds in deep linear networks under a simplifying assumption on the form of the weights that leads to a full decoupling of the dynamics of the weights at different layers. They claim this simplifying assumption is approximately true for small enough initial weights; and our empirical results suggest it might be increasingly inaccurate with depth.


In closing:
Thank you for your time. We hope you find that our revision addresses your concerns.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkecRSD23m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Report on paper 1248</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgmzhC5F7&amp;noteId=rkecRSD23m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1248 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1248 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper offers a different and surprising view on the bias-variance decomposition. The paper shows, by a means of experimental studies and a simplified theoretical analysis, that variance decreases with the model complexity (in terms of the width of neural nets) , which is opposite to the traditional bias-variance trade-off.

While the conclusion is surprising, it is somewhat consistent with my own observation. However, there are potential confounding factors in such an experimental study that needs to be controlled for. One of these factors is the stability of the training algorithm being used. The variance term (and the bias) depends on the distribution p(theta|S) of the model parameters given data S. This would be the posterior distribution in Bayesian settings, but the paper considers the frequentist framework so this distribution encodes all the uncertainty due to initialisation, sampling and the nature of SGD optimizer being used. The paper accounts for the first two, but how about the stability of the optimiser? If the authors used a different optimizer for training, what would the variance behave then? A comment/discussion along this line would be interesting.

It is said in Section 3.1 that different random seeds are used for estimating both the outer and inter expectation in Eq. 5. Should the bootstrap be used instead for the outer expectation as this is w.r.t. the data? Another point that isn't clear to me is how the true conditional mean y_bar(x) = E(y|x)  is computed in real-data experiments, as this quantity is typically unknown. 

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkxYWjhXTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author Response to Reviewer 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgmzhC5F7&amp;noteId=SkxYWjhXTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1248 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1248 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the positive feedback!

“stability of the training algorithm”:
Thank you for bringing up this other factor to consider. There are 3 sources of randomness when using SGD (initialization, training set, and mini-batch sampling). We do not focus on variance due to mini-batch sampling because the the decreasing variance phenomenon persisted when using batch gradient descent (no randomness due to mini-batching); this result is included in Appendix B.3. In response to your feedback, we have added a footnote on the 3rd page that addresses this: “We do not study randomness from stochastic mini-batching because we found the phenomenon of decreasing variance with width persists when using batch gradient descent (Section 3.3, Appendix B.3).”

Behavior of variance when using different optimizers:
When we use other optimizers such as batch gradient descent and LBFGS, we still find that total variance decreases with width. These experiments are mentioned at the end of Section 3.3 and are included in Appendix B.3. We hope you find these results with other optimizers interesting.

Random seeds for outer and inner expectations in Eq. 5:
We greatly appreciate this comment. We believe there may be a misunderstanding, due to our writing in this section. The outer expectation is for estimation over randomness from training set sampling and the inner expectation is for estimation over randomness from initialization. This is necessary because the two terms from the law of total variance both depend on both sources of randomness; it’s just that they take variances with respect to different random variables. If only one seed were used for the inner expectation (randomness from initialization), we would be estimating a conditional variance (over training set sampling), which is conditioned on one specific initialization. To help make this more clear, we have added the explicit introduction of the random variable I, which denotes the randomness from optimization, in Section 2.1 and have added I to Eq. 5.

“y_bar(x) = E(y|x)” and bias estimation:
You are absolutely right that y_bar(x) = E(y|x) is unknown. We have added this clarification in footnote 4 of the new revision: “Because we don't have access to \bar{y}, we use the labels y to estimate bias. This is equivalent to assuming noiseless labels and is standard procedure for estimating bias (Kohavi and Wolpert, 1996; Domingos, 2000).”

In closing:
Thank you for your time. We hope that we have adequately addressed your questions and hope our revision makes the significance of main contribution 2 more apparent.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1gMIHts37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper needs to show some serious understanding on statistical machine learning</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgmzhC5F7&amp;noteId=S1gMIHts37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1248 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1248 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper suggests to rethink about the bias-variance tradeoff from statistical machine learning in the context of neural networks. Based on some empirical observations, the main claims in this work are that (1) it is not always the case that the variance will increase when we use bigger neural network models (particularly, by increasing the network width); (2) the variance should be decomposed into two parts: one part accounts for the variance caused by random initialization of network parameters/optimization and the other part is caused by "sampling of the training set".

For the first claim is based the empirical observation that increasing the number of hidden units did not cause the incrase of variance (as in figure 1). However, to my understanding, it only means increasing the number of hidden units is probably not a good way to increase the network capacity. In other words, this cannot be used as an evidence that the bias-variance tradeoff is not valid in neural network learning.

For the second claim, I don't like the way that they decompose the variance into two parts. To be clear, the classical bias-variance tradeoff doesn't consider the optimization error as an issue. For a more generic view of machine learning errors, please refer to "The Tradeoffs of Large Scale Learning" (Bottou and Bousquet, 2008). In addition, if the proposed framework wants to include the optimization error, it should also cover some other errors caused by optimization, for example, early stopping and the choice of a optimization algorithm.

Besides these high-level issues, I also found the technical parts of this paper is really hard to understand. For example,

- what is exactly the definition of $p(\theta|S)$? The closely related case I can think about is in the Baysian setting, where we want to give a prior distribution of model (parameter). But, clearly, this is not the case here. 
- similar question to the "frequentist risk", in the definition of frequentist risk, model parameter $\theta$ should be fixed and the only expectation we need to compute is over data $S$
- in Eq. (5), I think I need more technical detail to understand this decomposition.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkxIGRh7T7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author Response to Reviewer 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgmzhC5F7&amp;noteId=BkxIGRh7T7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1248 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018 (modified: 13 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1248 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your feedback! It appears that there may have been an important miscommunication regarding our methodology; we hope our answers below will clarify this. Since clarity in the definitions in Section 2 is of utmost importance for understanding the paper, we’ve also added clarifications in our uploaded revision.


On the two high-level points:

1. On width and capacity: If capacity refers representation power, then increasingly large width networks have increasingly large capacity -- in fact  a wide enough network can fit any dataset [1]. The traditional view of bias-variance tradeoff is that increasingly large capacity models have lower bias and higher variance. This led Geman et al [2[ to claim that wide networks will suffer from high variance. We provide a quote of this claim and a sampling of related quotes from other impactful works in Appendix E. In our work we find that both bias and variance decrease with width, challenging the traditional view that bias and variance are related through a tradeoff merely governed by capacity. 

On effective capacity: We understand your comment as saying that this probably means that the very notion of capacity should be amended beyond simply representation power (e.g by explicitly taking into account optimization and the data). We completely agree with this. This is also the point made in Zhang et al. [4] and related work in the context of generalization gap analysis. We reach the same conclusion through a proper analysis of the variance of these models. To the best of our knowledge, this is new; this is the first study of the variance since [2] and it reaches opposite conclusions.

2. On our variance decomposition:  In contrast to the traditional bias-variance decomposition, which only considers one source of randomness (the training set), we are considering two sources of randomness: randomness from the optimization algorithm (mainly initialization) and randomness from sampling the training set. Going by the definition of optimization error in [3], we completely agree that the classical bias-variance tradeoff does not consider the optimization error. We reason that this is partially because variance due to initialization is 0 in the strongly convex case for a batch optimizer; given a decaying step size schedule, this is true for SGD as well [2, Section 4.2]. However, we are not trying to study the optimization error defined in [3]. We extended the classical bias-variance decomposition (via the law of total variance) to have another term that captures variance due to initialization because in the non-convex setting, the learned function is dependent on initialization. We found this extension yielded insightful results as the two variance terms have importantly different trends (Figure 2).

As we mention in 3.2, our results in the full data setting were the same with or without early stopping. Also, as mentioned at the end of Section 3.3, we find the same trends with other optimization algorithms such as batch gradient descent and PyTorchs implementation of LBFGS (included in Appendix B.3).


On the technical parts:

Thank you for making it clear to us that the definitions in Section 2 were not given with sufficient precision; this feedback is very valuable to us. We have uploaded a revision that we hope will make this perfectly clear.

1. On $p( . |S)$: Given a training set $S$, the learned weight theta depends on the random initialization because of non-convexity. Hence it is not deterministic;  $p(\theta|S)$ is the distribution over the learned weights, conditioned on $S$. We have updated the corresponding  paragraph in Section 2, making explicit the random variable I that denotes initialization and explaining the relationship of the learning algorithm with S and I.

2. On frequentist risk:  it looks like there may be a misunderstanding with our notation. We use the standard notion of frequentist risk, just in a more general context. Unfortunately, the notation \theta usually refers to the population parameter, which may have caused some confusion. Our \theta denotes the learned weights of the neural network. Averaging over them w.r.t $p( . |S)$ amounts to averaging over initializations. Hopefully, our revisions in this section makes this all more clear.

3. On Eq 5: Hopefully our answers above clarify this equation. The variance is with respect to both initialization and data sampling. Eq. 5 then follows from the law of total variance. Please let us know if anything is still unclear.


In closing:
Thank you for your time. We hope you find that our responses and our revision address your concerns.

[1] Neural Networks for Exact Matching of Functions on a Discrete Domain (Shrivastava and Dasgupta, 1990)
[2] Understanding Deep Learning Requires Generalization (Zhang et al, 2017)
[3] The Tradeoffs of Large Scale Learning (Bottou and Bousquet, 2008)
[4] Optimization Methods for Large-Scale Machine Learning (Bottou et al., 2016)</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>