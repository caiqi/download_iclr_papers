<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>On the Geometry of Adversarial Examples | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="On the Geometry of Adversarial Examples" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1lug3R5FX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="On the Geometry of Adversarial Examples" />
      <meta name="og:description" content="Adversarial examples are a pervasive phenomenon of machine learning models where seemingly imperceptible perturbations to the input lead to misclassifications for otherwise statistically accurate..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1lug3R5FX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>On the Geometry of Adversarial Examples</a> <a class="note_content_pdf" href="/pdf?id=H1lug3R5FX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 09 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019on,    &#10;title={On the Geometry of Adversarial Examples},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=H1lug3R5FX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=H1lug3R5FX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Adversarial examples are a pervasive phenomenon of machine learning models where seemingly imperceptible perturbations to the input lead to misclassifications for otherwise statistically accurate models. We propose a geometric framework, drawing on tools from the manifold reconstruction literature, to analyze the high-dimensional geometry of adversarial examples. In particular, we highlight the importance of codimension: for low-dimensional data manifolds embedded in high-dimensional space there are many directions off the manifold in which to construct adversarial examples. Adversarial examples are a natural consequence of learning a decision boundary that classifies the low-dimensional data manifold well, but classifies points near the manifold incorrectly. Using our geometric framework we prove (1) a tradeoff between robustness under different norms, (2) that adversarial training in balls around the data is sample inefficient, and (3) sufficient sampling conditions under which nearest neighbor classifiers and ball-based adversarial training are robust.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">adversarial examples, high-dimensional geometry</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We present a geometric framework for proving robustness guarantees and highlight the importance of codimension in adversarial examples. </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rJgtqzFfaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Common Response to All Reviewers</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1lug3R5FX&amp;noteId=rJgtqzFfaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1090 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1090 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank all the reviewers for their helpful comments. To avoid repetition, we present general comments here and individual comments below. 

In reading the reviews we realized that our introduction was unclear with respect to the contributions of the paper. We have restructured the introduction to appropriately highlight our contributions. The primary contributions of the paper are as follows. First we introduce a geometric framework, where we model classes of data as lying on distinct manifolds. Second we use this framework to show that there exists a tradeoff in robustness under different norms. Third, we show that in theory high codimension plays a role in vulnerability to adversarial examples. Vulnerability to adversarial examples is often attributed to high dimensional input spaces. To our knowledge this is the first work that investigates the role codimension plays in adversarial examples. We give theoretical results that show that even under ideal sampling conditions, state of the art methods, like adversarial training, fail in simple settings. Interestingly we find that different classification algorithms are less sensitive to changes in codimension. In preliminary experiments on synthetic data and on MNIST we provide empirical evidence to support this point. 

Regarding the related work of Wang et al. (ICML 2018) on kNN. We were unaware of the work of Wang et al. and we would like to thank the R2 for bringing this important related work to our attention. In developing the paper we only turned to nearest neighbor as an example of a classification algorithm that is robust to high-codimension. We apologize for the lack of clarity. The work of Wang et.al. is related and we have updated the paper to appropriately contextualize our results with respect to this work. Specifically we have added the following passage to the related work.

“Wang et al. (2018) explore the robustness of k-nearest neighbor classifiers to adversarial examples. In the setting where the Bayes optimal classifier is uncertain about the true label of each point, they show that k-nearest neighbors is not robust if k is a small constant. They also show that if k is asymptotically large, then k-nearest neighbors is robust. Using our geometric framework we show a complementary result: in the setting where each point is certain of its label, 1-nearest neighbors is robust to adversarial examples.”

Approaching the problem from a geometric perspective, we reach the complementary result that 1-nearest neighbors is robust in the setting where each sample is certain of its true label. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SyeHPNA92m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting work, but the theory is not very deep</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1lug3R5FX&amp;noteId=SyeHPNA92m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1090 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1090 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies the geometry of adversarial examples under the assumption that dataset encountered in practice exhibit lower dimensional structure despite being embedded in very high dimensional input spaces. Under the proposed framework, the authors analyze several interesting phenomena and give theoretical results related to the necessary number of samples needed to achieves robustness. However, the theory in this paper is not very deep.

Pros:

The logic of this paper is very clear and easy to follow. Definitions and theories are illustrated with well-designed figures.

This paper shows the tradeoff between robustness under two norm and infinity norm for the case when the manifolds of two classes of data are concentric spheres.

When data are distributed on a hypercube in a k dimensional subspace, the authors show that balls with radius \delta centered at data samples only covers a small part of the ‘\delta neighborhood’ of the manifold. 

General theoretical results on robustness and minimum training set to guarantee robustness are given for nearest neighbor classifiers and other classifiers.

Cons:

Most of the theoretical results in this paper are not very general. The tradeoff between robustness in different norms are only shown for concentric spheres; the ‘X^\epsilon is a poor model of \mathcal{M}^\epsilon’ section is only shown for hypercubes in low dimensional subspaces. 

Section 5 is not very convincing. As is discussed later in the paper, although $X^\delta$ only covers a small part of \mathcal{M}^\delta, robustness can be achieved by using balls centered at samples with larger radius.

Most of the analysis is based on the assumption that samples are perfectly distributed to achieve the best possible robustness result. A more interesting case is probably when samples are generated on the manifold following some probabilistic distributions. 

Theorems given in Section 6 are reasonable, but not very significant. It is not very surprising that nearest neighbor classifier is more robust than ‘x^\epsilon based’ algorithms, especially when the samples are perfectly distributed. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJe00fYzpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1lug3R5FX&amp;noteId=SJe00fYzpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1090 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1090 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review. Please see our new post for common comments. Below we respond to your individual concerns.

Re: ‘X^\epsilon is a poor model of \mathcal{M}^\epsilon’ is only shown for hypercubes

Thank you for pointing out that this section is unclear. The primary mathematical result in Section 5 is in Equation 4, which shows that this phenomena holds for general manifolds under additional conditions. To emphasize this, we have moved this result to the beginning of the section and under an explicit Theorem statement. We originally chose the order in the submission to first provide intuition to reader before introducing the more general result. We plan to keep that body of text but to more clearly contextualize it as an intuitive walkthrough in a special case. We invite additional suggestions for improvements. 

Re: Robustness tradeoff only shown for concentric spheres

It is a good question to ask how often does the L2 decision axis differ from the Linf decision axis. We believe that it is the common case that the L2 decision axis differs from the Linf decision axis, and that this phenomena explains recent results on adversarial robustness under different norms [1]. The result extends easily to, e.g., two concentric cylinders. Consider a two-dimensional axis-aligned cross section; in this cross section, the fact that the optimal decision boundaries differ is a corollary of our result. A similar argument works for intertwined tori. We have updated the paper to include a discussion of this intuition and will update the paper with any additional formal results we develop. We also note that our proof uses the spheres in a proof by construction. We made this choice for readability. This is common in theory on adversarial examples. [2] uses theoretical results about linear classifiers and simple data distributions to provide insight into robustness for adversarial examples.

Additionally, our work provides a new source of problems, motivated by adversarial examples, for the computational geometry community. The CG literature has been motivated by reconstructing manifolds, for which the decision axes under the L2 norm is all that is needed. The CG community has not explored the geometry of max-margin decision boundaries under norms other than L2. 

Re: Using larger balls to achieve robustness. 

We are unsure of the meaning of this crique. In the statement “As is discussed later in the paper … robustness can be achieved by using balls centered at samples with larger radius”, can you clarify which discussion you are referencing?

We consider ball-based learners. In Section 5 we show that the ratio of the volume of the union of balls around the samples to the volume of the tubular neighborhood approaches 0 as the codimension increases. This causes problems for ball-based learners because the measure captured by the balls is 0 in high codimensions. A natural way to remedy this is to use larger balls. However there are limits to how large the balls can be made before they begin to intersect the balls from samples on a different class manifold. Theorem 1 shows that even when ball-based learners use the largest possible balls, they require many more samples to achieve the same amount of robustness. 

Re: Proposed extension to non-uniform distributions on data manifolds. 

We are very interested in the direction you suggested. In future work we intend to combine techniques from the statistics literature with our framework, including sampling according to some distribution on a manifold, to understand the more difficult setting and prove more realistic guarantees for learning algorithms. 

In this paper our first goal is to bridge two disparate communities, leveraging the techniques from the manifold reconstruction literature to provide a different perspective and new tools for the problem of adversarial examples. We wish to understand the simplest version of the problem, which, as shown in our paper, already makes several issues clear, such as the affect of codimension on adversarial robustness. 

Re: Significance of robustness of nearest neighbors versus  ‘x^\epsilon based’ algorithms. 

We apologize for not making the point of the results in Section 6 clear. The importance of Theorem 1 is to show that different classification algorithms have different sampling requirements with respect to robustness. In particular nearest neighbor classifiers require fewer samples to achieve the same level of robustness for a fixed codimension. The ball-based learner is a theoretical model of the adversarial training used in state-of-the-art defenses for adversarial examples [3,4]. We have updated this section to make the importance of our results more clear. 

[1] Towards the first adversarially robust neural network model on MNIST. 
[2] Adversarially robust generalization requires more data. NIPS
[3] Explaining and harnessing adversarial examples. ICLR
[4] Towards deep learning models resistant to adversarial attacks. ICLR</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkxyHuB5n7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Synthetic examples and weak analysis of nearest neighbor classifier</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1lug3R5FX&amp;noteId=rkxyHuB5n7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1090 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1090 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper gives a theoretical analysis of adversarial examples, showing that (i) there exists a tradeoff between robustness in different norms, (ii) adversarial training is sample inefficient, and (iii) the nearest neighbor classifier can be robust under certain conditions. The biggest weakness of the paper is that theoretical analysis is done on a very synthetic dataset, whereas real datasets can hardly be conceived to exhibit similar properties. Furthermore, the authors do not give a bound on the probability that the sampling conditions for the robust nearest neighbor classifier (Theorem 1) will be satisfied, leading to potentially vacuous results.

While I certainly agree that theoretical analysis of the adversarial example phenomenon is challenging, there have been prior work on both analyzing the robustness of k-NN classifiers (Wang et al., 2018 - <a href="http://proceedings.mlr.press/v80/wang18c/wang18c.pdf)" target="_blank" rel="nofollow">http://proceedings.mlr.press/v80/wang18c/wang18c.pdf)</a> and on demonstrating the curse of dimensionality as a major contributing factor to adversarial examples (Shafahi et al., 2018 - https://arxiv.org/abs/1809.02104, concurrent submission to ICLR). I am very much in favor of the field moving in these directions, but I do not think this submission is demonstrating any meaningful progress.

Pros:
- Rigorous theoretical analysis.

Cons:
- Results are proven for particular settings rather than relying on realistic data distribution assumptions.
- Paper is poorly written. The authors use unnecessarily complicated jargon to explain simple concepts and the proofs are written to confuse the reader. This is especially a problem since the paper exceeds the suggested page limit of 8 pages.
- While it is certain that nearest neighbor classifiers are robust to adversarial examples, their application is limited to only very simple datasets. This makes the robustness result lacking in applicability.
- Weak experimental validation. The authors make repeat use of synthetic datasets and only validate their claim on MNIST as a real dataset.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Skx0lEKzam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1lug3R5FX&amp;noteId=Skx0lEKzam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1090 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1090 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review. Please see our new post for common comments. Below we respond to your individual concerns.

Re: Dimensionality in adversarial examples

The review points to a concurrent submission to ICLR [1] as an example of prior work on the relationship between high dimensional input spaces and adversarial examples. 

Our paper is primarily concerned with the effect of codimension on robustness to adversarial examples. In this sense our paper is not about the curse of dimensionality, but rather of codimensionality. Thus, our work is complementary to [1]. We highlight the differences below.

Shafahi et al. model each class as a probability density function defined on some domain. In contrast we define each class as a low dimensional manifold. In their model, our results consider the case where data lies on a measure 0 subset of the embedding space. Their results hold under the condition that “the class distribution is not overly concentrated” [1]. In their discussion the suggest extending their results to measure zero densities by considering a tube around the data. In our framework a robust classifier is one that accurately classifies the tube around the data. Our results show that learning an accurate classifier on the manifold is a fundamentally easier problem than learning an accurate classifier on the tube around the manifold, i.e. a robust classifier.

We also note that the results in [1] are exclusive to spheres and cubes, which exhibit concentration of measure properties. Our paper cites [2] which also considered concentration properties of spheres and how they impact adversarial robustness.

Re: Analysis on synthetic datasets

One of our primary contributions is to exhibit a tradeoff in robustness under different norms. We show that this tradeoff exists in general by exhibiting a setting where the decision axes are not equal. Please see our response to R1 on how often this occurs. We have updated the paper to include this response. The remainder of our theoretical results apply to general manifolds and we have updated the text to make this clear.

Re: Clarity

We are sorry that you found the paper unclear. We made a concerted effort to make the paper readable, but it is clear that we can do better. We also note that this is an interdisciplinary paper, and one of the contributions of our work is to make a very difficult to access field more accessible to adversarial examples researchers. High dimensional geometry is highly counterintuitive. As a result, the field of computational geometry prioritizes clear rigorous formal proof and tools. We attempted to use the simplest tools that we could and we iterated multiple times to cut down on unnecessary definitions.  We also note that R1 commented positively on the clarity of our paper.

We are highly committed to resolving any clarity issues with the paper and will be happy to incorporate any concrete suggestions that you have.

Re: Applicability of nearest neighbor results

As evidenced by the ICML paper that you provided (Wang et al. 2018), the robustness of nearest neighbor classifiers is an active and interesting research question. Wang et al. even provide a modification of the standard nearest neighbors algorithm that they show is more robust in practice.

Our paper is not about nearest neighbor classifiers. As we described in the rebuttal to R1, it is important to understand why nearest neighbor classifiers are more robust in our setting. Nearest neighbors naturally handles high codimension settings because the Voronoi cells are elongated in the normal directions. We have modified the paper to make this point clear.

Re: Concerns on experimental validation  

Our primary contribution is our theoretical results detailed in the summary above. Our experiments complement our theoretical results. Our synthetic training data is intended to explore the predictive power of our model of learners for real algorithms. Our results in Fig. 2 show that our theory for changing the norm predicts when real adversarial approaches fail. The CIRCLES and PLANES datasets show that real algorithms do, in fact, show this vulnerability to codimension. Our experiment on MNIST provides an example of a dataset with non-uniform sampling where nearest neighbor classifiers have fundamentally different performance than an adversarial training approach. We will update the paper to emphasize ways our experiments complement and support our other results. We have considered additional experiments that modify co-dimension for MNIST or the big-MNIST domain from [1] and would be happy to run them if requested.

We would like to highlight the fact that we made careful effort to use state-of-the-art attacks and defenses and followed best practices when running the experiments (e.g. averaging over multiple retrainings).

[1] Shafahi etal, Are adversarial examples inevitable?
[2] Adversarial spheres
[3] Adversarially robust generalization requires more data</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1e3vSFun7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting paper on adversarial examples, but with certain concerns</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1lug3R5FX&amp;noteId=S1e3vSFun7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1090 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1090 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper tried to analyze the high-dimensional geometry of adversarial examples from a geometric framework. The authors explained that there exists a tradeoff between being robust to different norms. They further proved that it is insufficient to learn robust decision boundaries by training against adversarial examples drawn from balls around the training set. Moreover, this paper showed that nearest neighbor classifiers do not suffer from this insufficiency.
 
In general, I think this paper is very interesting and enlightening. The authors analyzed the most robust boundary of norm 2 and norm infinity in different dimensions through a simple example and concluded that the single decision boundary cannot be robust in different norms. In addition, the author started from a special manifold and proposed a bound (ratio of two volumes) to prove the insufficiency of the traditional adversarial training methods and then extended to arbitrary manifold. It is good that this might provide a new way to evaluate the robustness of adversarial training method. However, I have some concerns: 1) Is it rigorous to define the bound by vol_X/vol_pi? In my opinion, the ratio of the volume of intersection (X^\del and \pi^\del) and vol \pi^\del may be more rigorous? 2) I don't know if such bound can be useful or easily applied in other work? In my opinion, it might be difficult, since the volume itself appears difficult to calculate. 
I think the paper is a bit complicated or heavy in mathematics, and not easy to follow (though I believe I have well understood it). Some typos and minor issues are also listed as below. 

Minor concerns:
1. At the end of the introduction, 3 attacking methods, FGSM, BIM, and PGD, should be given their full names and also citations are necessary.
2. Could you provide a specific example to illustrate the bound in Eq. (3), e.g. in the case of d=3, k=1.
3. In Page 7, “Figure 4 (left) shows that this expression approaches 1 as the codimension (d-k) of Pi increases.”  I think, the subfigure shows that the ratio approaches 1 when d and k are all increased.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hye6LVtG6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1lug3R5FX&amp;noteId=Hye6LVtG6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1090 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1090 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review. Please see our new post for common comments. Below we respond to your individual concerns.

Re: Definition of volume ratios.

You're correct that it would not be rigorous to define the bound using \vol X / \vol pi, as \vol X = 0, since the volume of a set of points is 0. We do not define the ratio in this way. Instead we define the ratio as \vol X^\del / \vol pi^\del. 

Your suggestion is indeed a rigorous way to compute this ratio. After some thought, your suggestion is equivalent to ours for this setting. In our setting, since the points are on the manifold pi, \vol X^\del is a subset of \vol pi^del, and so the intersection \vol X^\del \cap \vol pi^\del = \vol X^\del. It follows that (\vol X^\del \cap \vol pi^\del) / \vol pi^\del =  \vol X^\del / \vol pi^\del. We have added this argument to the paper. 

Re: Can volume bounds be applied to other work

We had not considered applying this bound as a metric to evaluate the robustness of a specific adversarial training method. We think this is an interesting direction for future work. Toward this end, one may want to compute the intrinsic dimension of the data manifold. There has been prior work on estimating the dimension of manifolds from samples [1,2,3]. 

In our summary we highlight that our key contribution is to identify the role of codimension on the pervasiveness of adversarial examples. In our paper, the primary purpose of our bounds is to prove that there exist classifiers that perform differently with respect to codimension. The specific values of these bounds show that this difference can, in theory, be exponentially sized. The primary practical use of our bounds is that they suggests that the transition from robust to non-robust is rapid; more like a phase transition, than a gradual shift. 

Re: Could you provide a specific example to illustrate the bound in Eq. (3), e.g. in the case of d=3, k=1.

This is a good idea and we are currently considering the type of figure we might create to illustrate Equation 3. We note that we do provide an illustration in Figure 3 (right) to illustrate Equation 2, which is the more difficult step in deriving Equation 3. We updated the text to make the reference to that figure more prominent. 

Re: Subfigure in Figure 4

Thank you for pointing out that we should have been more clear with our explanation. We can imagine two settings, one where we hold d fixed and increase k, and another where we hold k fixed and increase d. In the first setting, Figure 4 shows that lower dimensional problems are generally easier. This aligns well with results and intuition in the machine learning community. We are trying to draw attention to the second setting, that if we hold k fixed and increase d (and thus increase the codimension) the problem becomes more difficult. 

[1] Dimension Detection by Local Homology
[2] Maximum Likelihood Estimation of Intrinsic Dimension
[3] Estimating Local Intrinsic Dimension with k-Nearest Neighbor Graphs
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>