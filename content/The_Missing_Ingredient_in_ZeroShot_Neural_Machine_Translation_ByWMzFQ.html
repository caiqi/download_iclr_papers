<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>The Missing Ingredient in Zero-Shot Neural Machine Translation | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="The Missing Ingredient in Zero-Shot Neural Machine Translation" />
        <meta name="citation_author" content="Naveen Arivazhagan" />
        <meta name="citation_author" content="Ankur Bapna" />
        <meta name="citation_author" content="Orhan Firat" />
        <meta name="citation_author" content="Roee Aharoni" />
        <meta name="citation_author" content="Melvin Johnson" />
        <meta name="citation_author" content="Wolfgang Macherey" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ByWMz305FQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="The Missing Ingredient in Zero-Shot Neural Machine Translation" />
      <meta name="og:description" content="Multilingual Neural Machine Translation (NMT) systems are capable of translating between multiple source and target languages within a single system. An important indicator of generalization within..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ByWMz305FQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The Missing Ingredient in Zero-Shot Neural Machine Translation</a> <a class="note_content_pdf" href="/pdf?id=ByWMz305FQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=naveenariva%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="naveenariva@gmail.com">Naveen Arivazhagan</a>, <a href="/profile?email=ankurbpn%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="ankurbpn@google.com">Ankur Bapna</a>, <a href="/profile?email=orhanf%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="orhanf@google.com">Orhan Firat</a>, <a href="/profile?email=roee.aharoni%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="roee.aharoni@gmail.com">Roee Aharoni</a>, <a href="/profile?email=melvinp%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="melvinp@google.com">Melvin Johnson</a>, <a href="/profile?email=wmach%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="wmach@google.com">Wolfgang Macherey</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Multilingual Neural Machine Translation (NMT) systems are capable of translating between multiple source and target languages within a single system. An important indicator of generalization within these systems is the quality of zero-shot translation - translating between language pairs that the system has never seen during training. However, until now, the zero-shot performance of multilingual models has lagged far behind the quality that can be achieved by using a two step translation process that pivots through an intermediate language (usually English). In this work, we diagnose why multilingual models under-perform in zero shot settings. We propose explicit language invariance losses that guide an NMT encoder towards learning language agnostic representations. Our proposed strategies significantly improve zero-shot translation performance on WMT English-French-German and on the IWSLT 2017 shared task, and for the first time, match the performance of pivoting approaches while maintaining performance on supervised directions.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Machine Translation, Multi-lingual processing, Zero-Shot translation</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Simple similarity constraints on top of multilingual NMT enables high quality translation between unseen language pairs for the first time.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">10 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_ByeWpdWNam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByWMz305FQ&amp;noteId=ByeWpdWNam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1246 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1246 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SkgaDubE6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Global comment</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByWMz305FQ&amp;noteId=SkgaDubE6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1246 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1246 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you to all the reviewers for your time and helpful feedback. We will try to address all major comments in a future version, especially those around providing more background.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HkeEtybRhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The idea is intuitive, and the paper has many experimental analysis.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByWMz305FQ&amp;noteId=HkeEtybRhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1246 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1246 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The motivation of this paper is clear, since the sentence of same meaning in different languages should have similar representation in the high level latent space. The experimental analysis of the proposed alignment method is detailed. 

1. The method ``Pivot through English``: does it mean to use the baseline model decoding twice,  source-&gt;en and en-&gt;target? If true, because you already have the two src&lt;-&gt;en and tat&lt;-&gt;en NMT systems, you can fully use your model and should consider the baseline as the pivot method, whereas the disadvantage is the doubled latency. So the power of your model is not increased, your contribution is to make it more efficient during inference.
2. Figure 2 is interesting. Even when you are trying to optimize the cosine distance, the actual cosine distance is better when using adversarial training.
3. In your pre-training optimizer, the learning rate is either 1.0 or 2.0. If Adam is used, it is abnormal.
4. Another concern is the paper did not compared with another zero-shot NMT or pivot method, e.g., <a href="https://arxiv.org/pdf/1705.00753v1.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1705.00753v1.pdf</a> . 
5. In section 4.4, the lambda to balance alignment loss is 0.001. Since the cosine distance is usually less than 1. I am wondering how significant this extra regularizer is in such case.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1escB-NpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByWMz305FQ&amp;noteId=H1escB-NpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1246 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1246 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1. Yes, we do use pivoting as our baseline. Note, however, that compared to a pivoting (i.e., two-step) approach where the source is first translated into English and then translated into the target language, translating directly from source to target can lead to better translation quality. This is because information (the source sentence) is passed through only one noisy channel, whereas for the pivot approach it must be passed through two.
2. The cosine distance that we minimize is actually different from the cosine distance that we plot in Figure 2. The one that we minimize is the distance between the pooled encoder representation of a pair of parallel sentences. The distance plotted in figure 2 is the mean distance between the attention context vectors (across all 6 decoder layers) produced when translating from xx-&gt;zz and yy-&gt;zz where xx,yy,zz are semantically equivalent sentences in different languages. We will make this distinction more clear in an updated version.
3. We should have mentioned that we used the transformer learning rate schedule (Vaswani et al 2017). By this, the learning rate (which is 1.0 in our case) is divided by sqrt(model_dim) and sqrt(warmup_steps). The 1.0 and 2.0 is a multiplier to the learning rate schedule, and the peak learning rate is actually around 6e-4.
4. We will add more background in a revision. When addressing the problem of translating between language pairs that have no parallel data, there are 2 different settings that have been studied: zero-resource and zero-shot. Zero-resource MT relies on synthesizing parallel data and using that as supervision while training. Zero-shot translation seeks to measure model generalization in a multilingual NMT system by evaluating its ability to translate between language pairs that were never seen at training time. In this work we restrict ourselves to zero-shot only.
5. The significance of these different loss terms to optimization would be more accurately determined by the contribution to the gradient at each update step. In our case, the alignment loss is directly applied to the encoder, whereas the cross entropy loss from translation is applied to the decoder. The gradients from the cross entropy loss may shrink by the time they back-propagate to the encoder, which might explain why a weight of 0.001 was most effective.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJl3P_eY2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>No proper comparison with previous work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByWMz305FQ&amp;noteId=HJl3P_eY2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1246 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1246 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes an explicit alignment loss for the encoder representations in multilingual NMT, which substantially improves the zero-shot translation performance.

While the paper contains some interesting ideas, I think that it does a poor job at comparing the proposed method with existing techniques. In particular, the paper takes Johnson et al. (2016) as its only baseline, and tends to ignore all the other relevant literature in its experiments and most of the discussion. This makes it difficult to put this work into perspective, and I was left with the impression that it oversells to some extent. I would appreciate if the authors could address my concerns below, and I am open to modify my score accordingly.

First of all, the claim that "the quality of zero-shot translation has lagged behind pivoting through a common language by 8-10 BLEU points" seems deceptive if not wrong. For instance, Chen et al. (2017) (<a href="http://aclweb.org/anthology/P17-1176)" target="_blank" rel="nofollow">http://aclweb.org/anthology/P17-1176)</a> claim that their method "significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs", while your method only matches pivot-based performance. If not a direct comparison, this requires at least some discussion, but the current version does not even cite it. You could argue that, unlike Chen et al. (2018), your method is multilingual, but even from that perspective I think that the paper needs to do a better job at putting the proposed method into context.

In addition to that, Ha et al. (2017) and Lu et al. (2018) also focus on zero-shot multilingual translation as you do. While these two papers are cited, the proposed method is not compared against them. I think that some further discussion is necessary, and a direct comparison would also be desirable.

Sestorain et al. (2018) (https://arxiv.org/abs/1805.10338) also seems like a very relevant reference, but it is not even cited. Again, I think that some discussion (if not a direct comparison) is necessary.

Other comments:

- In your IWSLT experiments, you point out that "the data that we train on is multi-way parallel, and the English sentences are shared across the language pairs", which "may be helping the model learn shared representations with the English sentences acting as pivots". Something similar could also happen in your WMT experiments, as some of the underlying training corpora (e.g. Europarl) are also multi-way parallel. I do not aim to question the experimental settings, which seem reasonable, but this raises some flags to me: the proposed evaluation is not completely realistic, as (some) training corpora are indirectly aligned, and the system could be somehow exploiting that. I think that it would be great to see some further discussion on this and, ideally, having experiments that control this factor would also be desirable.

- Your analysis reveals that, to a great extent, the poor performance of the baseline is caused by the decoder generating its output in the wrong language. While your proposed method does not suffer from this issue, this suggests that the baseline is very weak, as one could think of different hacks to mitigate this issue with minimal effort (e.g. simply filtering an n-best list with language detection should greatly improve results). Going back to my previous point, it seems hard to believe that such a weak baseline represents the current state-of-the-art in the topic as the paper seems to suggest.

- Artetxe et al. (2018) do not use adversarial training with a language detecting discriminator as stated in page 3.

- Artetxe et al. (2018) is duplicated in the references.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rklUU8WVTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Output in wrong language</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByWMz305FQ&amp;noteId=rklUU8WVTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1246 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1246 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Yes, the main problem with zero-shot systems seems to be the language problem. We properly quantify the contribution of this phenomenon to the quality regression in zero-shot systems (section 4.1 and 4.2). Our baseline system is the same as those used in previous zero-shot works (Lu et al 2018, Ha et, al 2017). With our techniques we are able to more effectively solve the language problem than previous approaches. However, as we show in section 4.2, this alone would still leave us lagging behind pivoting. The language agnostic representations our model learns are what allow us to close this gap.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1ghNLbETQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Previous work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByWMz305FQ&amp;noteId=S1ghNLbETQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1246 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1246 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">There are two concepts that may have been confounded in the review, yet need to be carefully distinguished: zero-shot translation vs. zero-resource translation. Below we provide some more discussion on zero-shot and zero-resource which will be added in the revision. Note that in this work we restrict ourselves to the more difficult zero-shot setting.

1) Zero-resource (Firat et al 2016, Chen et al, 2017): 
These approaches generally work by synthesizing data for the desired language pair, xx-yy, from available xx-en or yy-en parallel data. This is often done by translating the English portion of the parallel data into a third language. These are distinct from the zero-shot setting (IWSLT-2017) wherein these kinds of distillation based approaches are specifically restricted. Zero-shot translation quality is used to evaluate model generalization by inspecting its ability to translate between languages that it was not trained for. Without the restriction to not train on parallel data, zero-resource is expected to work better. However, zero-resource approaches also have the downside that, in order to build a model that can translate between N languages, one needs to generate data for N*(N-1)/2 language pairs. Hence, it grows quadratically with the number of languages (in number of models/training procedure). 

2) Zero-shot (IWSLT17, Ha et al 2017, Lu et al 2018): 
Previous works such as Ha et al 2017, have made improvements to the quality of zero-shot primarily by addressing the incorrect language problem. They have used techniques like filtering targets during beam search, alternative methods to feeding in the &lt;2xx&gt; token, and adding an auto-encoding auxiliary task. However even with these techniques, their zero-shot system lagged behind their pivoting baseline by about 5 BLEU. Lu et al, (2018), as we discuss in section 2.2, essentially develop a more sophisticated approach to parameter sharing at the final encoder layer. They rely on the shared representations that fall out of parameter sharing to improve zero-shot translation quality. With their technique, they achieve significant improvements over their baseline, but still lag behind pivoting by 2-9 BLEU points depending on the language pair. Our approach differs from previous work in that we explicitly encourage the learning of language agnostic representations which are crucial to enabling zero-shot translation in a multilingual NMT system. Using these we are for the first time able to bring zero-shot translation on par with pivoting.

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJeUuKi_nQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Maybe an improvement to zero-shot MT, but methodological questions make the results unconvincing</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByWMz305FQ&amp;noteId=rJeUuKi_nQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1246 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1246 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Pros:
- The paper proposes two loss terms to help improve zero-shot translation performance. The proposed supervised alignment loss term is simple and would be straightforward to implement. The proposed adversarial loss term is also conceptually simple, although perhaps a bit difficult to tune.
- Both loss terms seem to improve zero-shot translation performance over the models trained without them.
- The paper is fairly well written. It's easy to follow, although there's a few details that are missing which might make it difficult to reproduce.

Cons:
- There are several methodological concerns that make it difficult to evaluate the correctness/significance of the results. For example, the authors use a non-standard valid/test set for the WMT'14 experiments (newstest2012/13 instead of newstest2013/14) which makes the results difficult to compare to other supervised baselines.
- The results would be more convincing if the authors had evaluated on more language pairs and more standard datasets. Most of the results are based on a model trained on just two language pairs, making it difficult to evaluate how well the approach actually works.
- There is no direct quantitative comparison to any other published results.
- The results presented over the IWSLT dataset are heavily summarized, making it difficult to compare to other work.

Methodological questions:
- In Table 1, why do you use newstest2012 as the valid set and newstest2013 as the test set? Most other work on this dataset (e.g., Vaswani et al.) uses newstest2013 as the dev set and newstest2014 as the test set.
- The baseline "Direct translation" results given in Table 1 are quite a bit lower than those reported in Vaswani et al. In particular, Vaswani uses newstest2013 as their dev set (which you use as test) and report an En-De BLEU of 25.8 in their ablation study, compared to 24.5 in your Table 3. Why is there such a large discrepancy?
- In Section 4.3, when examining the attention context vectors, can you explain more about how you computed this? In particular, each Transformer decoder layer has an attention over the encoder outputs, so for 6 layers you'll actually have 6 attention context vectors, each depending on the previous decoder layer as well, right?

Misc questions/comments:
- Did you try combining the adversarial and pool-cosine loss terms?
- How does your approach compare to adding identity (autoencoder) mappings, as in this WMT paper: <a href="http://www.statmt.org/wmt18/pdf/WMT009.pdf" target="_blank" rel="nofollow">http://www.statmt.org/wmt18/pdf/WMT009.pdf</a>
- You claim that prepending the &lt;tl&gt; token to the source has the same effect as putting it as the first token in the decoder. Did you actually test this or have some reference/citation for this? Since a lot of the problem seems to be getting the decoder to produce the correct output language, it seems possible that changing the method that you specify the desired target language could have a non-negligible effect.
- What does a learning rate of 1.0 mean? In the original Vaswani et al. paper they use Adam and a dynamic learning rate schedule that adjusts based on the number of updates -- the effective learning rate is typically &lt;&lt; 1.0. I believe the peak learning rate that they used is closer to 5e-4.
- Which "language identification tool" do you use?
- Table 4 seems to only give averages over the different zero-shot language pairs. Please add the full results for each language pair in the Appendix.
- In Section 4.4, what do you mean by "scores [...] are suspiciously close to that of bridging"? What do you mean by bridging?

There are also a couple uncited related papers:
- http://aclweb.org/anthology/N18-1032
- http://www.statmt.org/wmt18/pdf/WMT009.pdf</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkgoZwbNpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Other Clarifications</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByWMz305FQ&amp;noteId=rkgoZwbNpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1246 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1246 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">- Section 4.3: Yes, we have 6 sets of context vectors from each decoder layer. We found that the trend shown in figure 2 was similar across all layers and plotted just the mean for brevity. We will add this information in an updated version to make things more clear.
- We have not tried combining different alignment loss terms but have found that they are highly correlated. Minimizing one often leads to lowering the others.
- Lu et al (2018) have separate encoders for each source and target language. They develop a “neural interlingua” layer, which we view as a sophisticated attention layer, that is shared across all language pairs. They show that the parameter sharing in this layer leads to more language agnostic representations at the interface of the encoder and the decode. 
They achieve significant improvements over their baseline but still lag behind pivoting by 2-9 BLEU points depending on the language pair. We suspect that this gap is due to parameter sharing being an insufficient constraint on the system to make it learn language agnostic embeddings. In our approach we explicitly encourage the learning of language agnostic representations by means of an auxiliary loss function. With this, we are for the first time able to bring zero-shot translation on par with pivoting.
- We should have mentioned that we used the transformer learning rate schedule (Vaswani et al 2017). By this, the learning rate (which is 1.0 in our case) is divided by sqrt(model_dim) and sqrt(warmup_steps). The 1.0 and 2.0 is a multiplier to the learning rate schedule, and the peak learning rate is actually around 6e-4.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1lAtIWVaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Methodology and baselines</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByWMz305FQ&amp;noteId=B1lAtIWVaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1246 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1246 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Choice of datasets: 
We were not able to identify a standard dataset that has consistently been used in previous work on zero-shot or zero-resource NMT. The one that comes closest is IWSLT-17 which hosted a shared task on the zero-shot NMT. We report a summarized version of the our results on this dataset and, as suggested by the reviewer, will add the full table of results to an updated version. The IWSLT 17 dataset, however, suffers from problems like being small and being multiway parallel. This makes it unsuitable to evaluate zero-shot quality. To alleviate these issues, we decided to use WMT 14 en&lt;-&gt;de and en&lt;-&gt;fr since these are what most competitive systems are evaluated on (Vaswani et al 2017, Xu et al 2018, Ott et al 2018). To evaluate zero-shot quality, we require 3 way parallel data. Since newstest 2014 is not 3 way parallel, we instead used newstest 2012 and 2013 which meet these requirements.

Baseline quality: 
We agree that more comparisons to assert the quality of the baseline will help. Vaswani et al show 1.3 higher BLEU on newstest 2013 en-&gt;de. This higher performance is because their model (and vocab) is trained for a single translation direction (en-&gt;de). On the other hand, our system is trained on to translate in 4 directions (en&lt;-&gt;de,fr). This increased burden on the model is what causes the above quality regression (Johnson et al 2016, Neubig et al 2018).
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>