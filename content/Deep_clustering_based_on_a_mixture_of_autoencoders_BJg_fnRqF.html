<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Deep clustering based on a mixture of autoencoders | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Deep clustering based on a mixture of autoencoders" />
        <meta name="citation_author" content="Shlomo E. Chazan" />
        <meta name="citation_author" content="Sharon Gannot" />
        <meta name="citation_author" content="Jacob Goldberger" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJg_fnRqF7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Deep clustering based on a mixture of autoencoders" />
      <meta name="og:description" content="In this paper we propose a Deep Autoencoder Mixture Clustering (DAMIC) algorithm. It is based on a mixture of deep autoencoders where each cluster is represented by an autoencoder. A clustering..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJg_fnRqF7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Deep clustering based on a mixture of autoencoders</a> <a class="note_content_pdf" href="/pdf?id=BJg_fnRqF7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=shlomi.chazan%40biu.ac.il" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="shlomi.chazan@biu.ac.il">Shlomo E. Chazan</a>, <a href="/profile?email=sharon.gannot%40biu.ac.il" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="sharon.gannot@biu.ac.il">Sharon Gannot</a>, <a href="/profile?email=jacob.goldberger%40biu.ac.il" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="jacob.goldberger@biu.ac.il">Jacob Goldberger</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">In this paper we propose a Deep Autoencoder Mixture Clustering (DAMIC) algorithm. It is based on a mixture of deep autoencoders where each cluster is represented by an autoencoder. A clustering network transforms the data into another space and then selects one of the clusters. Next, the autoencoder associated with this cluster is used to reconstruct the data-point. The clustering algorithm jointly learns the nonlinear data representation and the set of autoencoders. The optimal clustering is found by minimizing the reconstruction loss of the mixture of autoencoder network. Unlike other deep clustering algorithms, no regularization term is needed to avoid data collapsing to a single point. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">deep clustering, mixture of experts, mixture of autoencoders</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a deep clustering method where instead of a centroid each cluster is represented by an autoencoder</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">5 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HylfjVi3TQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJg_fnRqF7&amp;noteId=HylfjVi3TQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1279 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1279 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJxwEl3g6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Deep clustering based on a mixture of autoencoders</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJg_fnRqF7&amp;noteId=rJxwEl3g6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1279 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1279 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a deep clustering based on a mixture of autoencoders - which follows the well-known idea of a mixture of experts, and the k-means. Different from k-means, in this method, each data point is allocated to a cluster based the representation error if the autoencoder network were used to represent this data point. 

Although the method is developed based on some existing concepts in the literature, the use of mixture of autoencoders for deep clustering is novel. The paper is also very well presented and I enjoyed reading the paper. I have however several questions:

(1) I feel the performance of the algorithm is most likely very dependent on the pre-training for initialisation that was used by the authors. How much is the performance dependent on pre-training? Is it possible to use random initialisation, and what's going to happen if you use random initialisation? 

(2) Do you have to assume that you know how many clusters are in the data? This appears to be assumed known in your experiments. What happen if it is not known?

(3) The proposed method is only compared with the deep clustering baselines, and k-means. Are they state-of-the-art? Have you also considered subspace clustering algorithms, such as the sparse subspace clustering algorithms and low-rank subspace clustering algorithms?

Although the paper is well written, there are still a few typos - for example, "should results in", "Once all the network parameter are", "Autoencodr", "a variant the", etc.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkgbSR4R3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting approach but questionable motivation and inadequate evaluation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJg_fnRqF7&amp;noteId=rkgbSR4R3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1279 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1279 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
Summary:
This paper proposes a deep clustering approach that learns a low-dimensional embedding of the data simultaneously while clustering data using a deep neural network. An autoencoder framework is used to learn embeddings and a joint loss term is used to couple the clustering with the AE loss. Their results on MNIST and newsgroups dataset compare with a couple of other deep clustering approaches and show modest improvements in clustering performance.

Clarity:
The write-up is reasonably clear, but in print the equations are rendered poorly. Some suggestions for improvements are described later.

Originality:
- The idea of learning embeddings while learning a clustering is not novel and doing this using a deep NN is not novel either. But incorporating an AE component is novel. 
- The method builds on the overall idea behind an earlier approach, DEC (ICML, 2016) but has one key difference -- a group of autoencoders (AE) is trained simultaneously with the DNN and used to "weigh" the clustering loss
- The other difference from DEC is in that their deep network is the "encoder" part of a SAE whereas in this paper, the deep network seems to be a general DNN. 

Significance:
- Clustering is an important problem and the authors show some results, but on only three datasets: MNIST, newsgroups, Reuters. They should have included the STL dataset from prior work to make it more comparable.

Main concerns:
- What is the state-of-the-art on MNIST and their other datasets? To be convincing, they should compare with that algorithm rather than just prior deep clustering approaches. (Note: k-means is not the state-of-the-art method for clustering many datasets).
- The motivation behind learning the AEs is not clear as the learned embedding is not used anywhere in the clustering. Instead, the reconstruction loss is used to influence the clustering loss by acting as a "weight". Does this "couple" the two components in a desirable manner? It is not intuitive why a wrongly clustered point will always produce a change to h(xi), rather than to the AE part of the objective? That is, the corresp AE might "accommodate" the wrongly clustered point rather than the clustering modifying. This is a standard problem with non-convex functions involving products of functions/parameters.
- Is the purpose of the AEs just as a means of regularization?
- The authors should report standard error for experiments by trying different train/test splits. This standard ML practice of giving an average performance is being skipped by many papers these days and should be taken seriously for the results to be convincing.
- There is no mention or citation of any of the Variational AE based clustering methods. There are methods using Gaussian mixtures of AEs. These should be cited and compared to (GMVAE, VaDE, DLGMM etc)


Other comments:
- For the architecture of the deep network used to train the non-linear representation, have they tried any other architectures or CNNs?
- The DAE expertise section is not surprising at all. Since the initialization was based on an already clustered set of images, one would expect each DAE to have learned a representation of that digit.
- How will their method perform with noisy data and outliers? Have they tried corrupting the images to see if the approach is robust?
- Details of the DNN should be described early on and the choice of the particular size explained.
- In addition to the shown clustering metrics, another useful metric is AUC-PR (AUC precision-recall curve). Here, precision and recall are computed by considering pairs of data-points and the binary classification task of whether the two belong to the same class. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HygG_BG927" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Comments on the similarity between this work and our prior work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJg_fnRqF7&amp;noteId=HygG_BG927"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Dejiao_Zhang1" class="profile-link">Dejiao Zhang</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1279 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear authors, 

We found the work presented here very interesting. However, we would like to point out the similarity between your work and our previous work of which you are likely not aware: "Deep Unsupervised Clustering Using Mixture of Autoencoders" <a href="https://arxiv.org/abs/1712.07788" target="_blank" rel="nofollow">https://arxiv.org/abs/1712.07788</a>  In this previous work, we also proposed to cluster the data by leveraging a mixture of autoencoders. More specifically, our model consists of two parts: 1) a collection of autoencoders where each autoencoder learns the underlying manifold of a group of similar objects, and 2) a mixture assignment neural network, which takes the concatenated latent vectors from the autoencoders as input and infers the distribution over clusters. 

The only difference we see between these two works, to the best of our understanding, is that we perform classification based on the latent features learned by the autoencoders while in your models, the classification is performed on the raw images. However, these two models share similar objectives that are motivated by the same insights. In particular, we both observe that the objective will achieve its minimum when the classifiers are one-hot vectors and always select the autoencoder with minimum reconstruction error for that sample. One the other hand, our previous work went a step further by introducing the mutual information maximization objective for the classifier, which helps to avoid the trivial solution that the classifier is a constant one-hot vector for all input data, i.e., avoiding the selection of a single autoencoder for all of the data. This, in turn, allows us to cluster the data from scratch with pretraining. We see some benefits of your approach, such as your pretraining step, but we think it would be important to compare in order to understand the benefits more clearly.

At this point, we do hope the authors as well as the reviewers can evaluate this work by taking the above into consideration.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1esAOTZhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting but the performance not good enough. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJg_fnRqF7&amp;noteId=B1esAOTZhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1279 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">27 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1279 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper presents a deep clustering method which represents each cluster with different auto-encoders. The idea is simple but seems interesting. Benefiting from the neural network, the proposed model works in an end-to-end manner. It also can be used to cluster new incoming data without redoing the whole clustering procedure. The paper is clearly written and some experiments are conducted. However, I have some concerns as below:

1. The theoretical detail should be given on why no trivial solution will be given.
2. In experiments, the proposed method outperforms the compared methods. However, I still feel that the experimental comparison may be the biggest disadvantage of this paper as the reported results are remarkably worse than the state-of-the-art results, e.g. VaDe, SpectralNet, etc. which has achieved &gt;91% ACC/NMI on mnist.
3. It is unclear why the proposed method makes sense. Comparing with recent developments such as VaDe, I think that the novelty of this work may be on the borderline of ICLR. 
3. Some writing mistakes: 
   - Table 1: The Deep** Autoencodr MIxture** Clustering (DAMIC) algorithm.
   - It seems that the diagram (a) (b) in Fig.4 is incomplete.
4. DAE#1 and DAE#9 achieve a similar result, could explain more and check your result? Does this indicates that the proposed model cannot cluster '1' and '9' correctly?

[1] Variational deep embedding: A generative approach to clustering
[2] SpectralNet: Spectral Clustering using Deep Neural Networks

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>