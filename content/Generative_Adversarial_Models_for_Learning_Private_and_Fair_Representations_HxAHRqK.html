<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Generative Adversarial Models for Learning Private and Fair Representations | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Generative Adversarial Models for Learning Private and Fair Representations" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1xAH2RqK7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Generative Adversarial Models for Learning Private and Fair..." />
      <meta name="og:description" content="We present Generative Adversarial Privacy and Fairness (GAPF), a data-driven framework for learning private and fair representations. GAPF leverages recent advancements in adversarial learning to..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1xAH2RqK7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Generative Adversarial Models for Learning Private and Fair Representations</a> <a class="note_content_pdf" href="/pdf?id=H1xAH2RqK7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019generative,    &#10;title={Generative Adversarial Models for Learning Private and Fair Representations},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=H1xAH2RqK7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We present Generative Adversarial Privacy and Fairness (GAPF), a data-driven framework for learning private and fair representations. GAPF leverages recent advancements in adversarial learning to allow a data holder to learn "universal" representations that decouple a set of sensitive attributes from the rest of the dataset. Under GAPF, finding the optimal privacy mechanism is formulated as a constrained minimax game between a private/fair encoder and an adversary. We show that for appropriately chosen adversarial loss functions, GAPF provides privacy guarantees against strong information-theoretic adversaries and enforces demographic parity. We also evaluate the performance of GAPF on multi-dimensional Gaussian mixture models and real datasets, and show how a designer can certify that representations learned under an adversary with a fixed architecture perform well against more complex adversaries. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Data Privacy, Fairness, Adversarial Learning, Generative Adversarial Networks, Minimax Games, Information Theory</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We present Generative Adversarial Privacy and Fairness (GAPF), a data-driven framework for learning private and fair representations with certified privacy/fairness guarantees</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rygU8-Cp6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Unconvincing newness,  but a good GAN model to understand Private Presententation Learning, </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xAH2RqK7&amp;noteId=rygU8-Cp6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1590 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1590 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">    The paper authors provide a good overview of the related work to Private/Fair Representation Learning (PRL). Well written, The theoretical approach is extensively explained and the first sections of the paper are easy to follow. The authors demonstrate the model performance on or the GMM, the comparison between theoretical and data driven performance is a good case study to understand the PRL.

We usually expect to see related work in the first sections, in this case it's has been put just before the conclusion. It can be still justified by the need o introduce the  PRL concepts before comparing with other works.
The GMM study case is interesting, but incorporates strong assumptions. Moreover, for a 4 or 8 dimensional GM, 20K data points are more than enough to infer the correct parameter. It would have been more useful if it was used to comapre between the mentioned methods in "Related Work".

There seems to be important parts of the paper that has been put in the appendices: how to solve the constrained problem, Algorithm.... Similarly, some technical details were expanded in the paper body (Network structure).

The authors mentioned the similarities with other works and their model choices that set theirs apart from other. Yet, the paper doesn't provide performance ( accuracy, MI) comparison to other works. There seems to be a strong similarity with Censoring representations with an adversary, Harrison Edwards and Amos Storke (link: <a href="https://arxiv.org/abs/1511.05897)." target="_blank" rel="nofollow">https://arxiv.org/abs/1511.05897).</a> Difference : distortion instead of H divergence, non-generative autoencoders.

Consequently, I question the novelty of the paper's contribution. Without extensive comparison with other methods and especially to similar ones mentioned in the related work, there is little to say about the "state-of-the-artness". Yet, it is important to acknowledge the visible effort behind the paper and how the author(s) managed to leverage the simplicity and power of GANs.

On a lighter note:
A)- the paper mention "state-of-the-art CNNs, state-of-the-art entropy estimators, MI, generative models", for the Machine Learning community, many of these elements have been around for a while now.
B)- "Observe that the hard constraint in equation 2 makes our minimax problem different from what is extensively studied in the machine learning community": I would argue it's not an objective statement.
 


</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJlZpem8p7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Relevant References</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xAH2RqK7&amp;noteId=rJlZpem8p7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1590 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, just pointing out some related papers.

1. Xu, Depeng, Shuhan Yuan, Lu Zhang, and Xintao Wu. "FairGAN: Fairness-aware Generative Adversarial Networks." arXiv preprint arXiv:1805.11202 (2018).
2. Sattigeri, Prasanna, Samuel C. Hoffman, Vijil Chenthamarakshan, and Kush R. Varshney. "Fairness GAN." arXiv preprint arXiv:1805.09910 (2018).</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJlvx8RP67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Relevant References</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xAH2RqK7&amp;noteId=SJlvx8RP67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1590 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018 (modified: 14 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1590 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Anonymous,

Thank you very much for these references. We will add them to our related work in the revised version. 

The two papers you mentioned focus on generating synthetic non-sensitive attributes and labels which ensure fairness while preserving the utility of the data (predicting the label). The synthetic data is generated by a conditional generative adversarial network (GAN) which generates the non-sensitive attributes-label pair given the noise variable and the sensitive attribute. The utility is preserved by generating data that is very similar to the original data. To ensure fairness, the generator generates data samples such that an auxiliary classifier (discriminator) trained to predict the sensitive attribute from the synthetic data performs as poorly as possible.

FairGAN uses two discriminators: one discriminates fake / real non-sensitive attributes-label pair and another discriminates generated data from different sensitive groups. This model ensures demographic parity while preserving data utility (predicting the label). The problem is formulated as an unconstrained minimax game in which the empirical loss function is formulated by a weighted sum of the loss functions of the two discriminators. Fairness GAN is similar to FairGAN. The goal here is to develop a conditional GAN-based model to ensure demographic parity or equality of opportunity in the system by learning to generate a fairer dataset. The authors consider both demographic parity and equality of opportunity as fairness metric. They also formulate the problem as an unconstrained minimax game between the discriminator and the generator. To ensure utility, the Fairness GAN uses three pairs of losses which make sure that the generated non-sensitive attributes-label pair, the non-sensitive attributes alone as well as the non-sensitive attributes conditional on the sensitive attributes to be very similar to the original data. To enforce fairness, they include a pair of losses to encourage either demographic parity or equality of opportunity. 

The methods presented in these papers are very different from our method. First, we are focusing on creating representations of the data for a variety of learning tasks. Second, we use a generative model to cleverly injecting noise where it matters to ensure privacy/fairness rather than generate a fairer synthetic dataset. Third, we consider a constrained minimax game in which we use a distortion constraint to preserve the utility of the learned representation for a variety of learning tasks rather than focusing on a particular label. Fourth, we make precise connections between the data-driven adversarial learning framework and the game- and information-theoretic setting (with knowledge of dataset statistics) and show how the change of the loss function in our framework leads to a variety of information-theoretic adversaries with different powers. Furthermore, we use simulations on Gaussian mixture models to show that the learned representations from a finite number of samples and a computationally bounded adversary (neural networks) performs as good as a representation created by the game-theoretic optimal mechanism which assumes knowledge of dataset statistics and infinite adversarial computational power. Finally, we propose using mutual information estimators to verify that no adversary (regardless of their computational power) can reliably learn the sensitive attribute from the learned representation. We encourage the reader to read the detailed list of contributions below where we attempt to make this clear to ICLR reviewers. There are different ways for enforcing fairness, and our work presents a framework that aids in achieving this goal. More work is needed to be done in this area.

We thank you again for your comment and bringing these references to our attention.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1xBl0nJp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting direction and formulation but no enough novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xAH2RqK7&amp;noteId=S1xBl0nJp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1590 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1590 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper present an adversarial-based approach for private and fair representations. This is done by learned distortion of data that minimises the dependency on sensitive variable while the degree of distortion is constrained. This problem is important, and the analysis from game-theory and information theory perspectives is interesting. However, the approach itself is similar to Edwards &amp; Storkey 2015, and I find the presentation of this paper confusing at a few points. 

First, while both the title and abstract suggest it is about learning representation, the approach might be better considered as data-augmentation. As described a bit later: "...modifying the training data is the most appropriate and the focus of this work". This contradiction with more commonly accepted meaning of representation learning (learning abstract/high level representation of data) is confusing.

Although the authours argued this work is different from Edwards &amp; Storkey 2015, I think they are quite similar. The presented method is almost a special case of this previous work: it seems that one can obtain this model by modifying Edwards &amp; Storkey's model as follows (referring to the equations in Edwards &amp; Storkey's paper): (1) removing the task (Y) dependent loss in eq. 9. (2) assume the encoder transforms X to the same data space so the decoder can be removed, so eq. 7 become equivalent to the distortion measure in this paper. There are other small differences, such as adding noise and the exact way to impose constraint, but I doubt whether the novelty is significant in this case.

Other places that are unclear include: proposition 1 -- what does "demographic parity subject to the distortion constraint" mean? demographic parity was defined earlier as complete independence on sensitive variable, so how can "complete independence" subject to a constraint? In addition, it would be helpful introduce S is binary. This information was delayed to section 3 after the cross-entropy loss that assumes binary S was presented.

Overall, I think this paper is interesting, and the analysis offers insights into related areas. However, the novelty is not enough for acceptance at ICLR, and the presentation can be improved.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HklGflyXTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xAH2RqK7&amp;noteId=HklGflyXTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1590 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1590 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear AnonReviewer3,

Thank you for the detailed comments and observations. We are happy you found our paper and analysis interesting. We understand there is room for improvement in the write up. We are currently working on refining it; even as we do so, we respond here to your comments to address as precisely as we can.

**Confusion about the term “representation learning”**

We agree that the term “learning private and fair representations” might be confused with the widely studied “representation learning” problem -- which we are not tackling in this work. While our framework can be generalized to a setting in which we can learn an arbitrary representation using an encode-decode structure, we are primarily interested in learning representations of the data (of the same dimension/shape/structure) that are fair and private. Thank you for pointing this out. We will fix our writeup to clarify things.

**Difference between our work and Edwards &amp; Storkey 2015 [1]**

Our work departs (quite significantly) from other related works. Here is a list of the important differences. 

1. Our framework is not a special case of Edwards &amp; Storkey 2015. For starters, our formulation is a minimax one subject to a "hard distortion constraint". Their formulation is a weighted combination of three loss functions, and zeroing out one of them (the one that measures how well you do in a given classification task of interest) does not recover our formulation because of the non-convexity/concavity of the minimax problem with respect to the decorrelator/adversary neural network parameters. The hard distortion constraint allows us to directly limit the amount of distortion added to learn the private/fair representation, which is crucial for preserving the utility of the learned representation. Moreover, notice that enforcing the hard distortion constraint calls for a new training process that relies on the Penalty method or Augmented Lagrangian method presented in Appendix C.

2. All the relevant papers have exclusively focused on showing (via experiments) that this approach works well in practice when you design things with a particular classification task in mind (i.e., in a supervised fashion). This requires having access to additional training labels which may be unavailable during the training phase. Our paper shows (via experiments on medium-sized datasets) that this approach works even when the designer does not want to restrict their attention to one classification task (i.e., in an unsupervised fashion). Indeed, our experiments and simulations show that the learned representations work well on classification tasks that haven't (at all) been accounted for in the training process.

3. We make very precise connections between the data-driven adversarial learning framework and the game- and information-theoretic setting (which assumes that the designer has access to the joint distribution between data and sensitive attributes, and the minimax optimization is performed over all theoretically possible randomized decorrelation and adversarial learning strategies). We also show how the change of the loss function in our framework leads to a variety of information-theoretic adversaries with different powers. This is an important novelty because it allows us to generalize conclusions that can be made upon learning representations from a finite number of samples and a computationally bounded adversary to the more important setting of infinite samples (i.e., access to distributional information) and infinite adversarial computational power. Indeed, this is explicitly shown in Section 3 for Gaussian mixture models. Notice that this section shows that decorrelation schemes that are learned in a data-driven fashion against a computationally bounded adversary perform well when evaluated against a maximum a posteriori probability (MAP) adversary that has access to distributional information and knows the applied decorrelation schemes. Further, this shows that there is no gap between the game-theoretically optimal decorrelation schemes and the ones that are learned via a generative neural network for binary variable S. This critical piece where one investigates what guarantees we can get against more potent adversaries is missing from prior works. 

4. Even though we learn our randomized decorrelation neural networks by training against a specific adversarial neural network, the learned decorrelation scheme performs well when evaluated against unseen (more complex) adversarial architectures. To prove this point, we show that the mutual information (MI) between the learned representations and the sensitive attribute is sufficiently small. A sufficiently small MI implies that no attacker (regardless of their computational power) can reliably learn the sensitive attribute from the learned representation (from Fano’s inequality [2]). This is again a novelty that didn't appear in prior works.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1lkEkyQ6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' Response Continued</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xAH2RqK7&amp;noteId=B1lkEkyQ6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1590 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1590 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">5. While prior works have used a classical, non-generative auto-encoder type architecture for the creation of the fair/censored representations, we harness the power of generative models which have the capability to not only compress the data in certain ways but to also cleverly inject noise where it matters. (see Figure 8 and 9 in Appendix E)

6. Our experiments reveal that the learned representations are private/fair even to humans. For instance, on the GENKI dataset, we show how the gender has been stripped off by hiding mustaches, facial hair, lip color etc. (see Figure 4). At the same time, we show that the representations are still useful for other classification tasks. 

**Confusion in demographic parity subject to the distortion constraint**
        	
We would like to clarify what we meant by "demographic parity subject to a distortion constraint." It is well known in the fairness community that enforcing demographic parity (or other notions of fairness) conflicts with the learning of well-calibrated classifiers ([3,4]). To circumvent this issue, we chose to "partially" decorrelate the data up to an allowed distortion. This helps in ensuring that the learned representations are useful in practice for learning good classifiers, while limiting the underlying correlations with the sensitive attributes. Our formulation implies demographic parity if the distortion budget is set to infinity (see the analysis in Appendix B, proof of proposition 1).  

 **Introduce S is binary**

We would like to emphasize that the proposed framework is general and can be used for non-binary sensitive variable. However, in the theoretical analysis, we only consider binary sensitive variable. The analysis can be generalized to the non-binary case. Furthermore, we also consider non-binary sensitive variable in our simulation (the HAR dataset). We will fix our writeup to clarify this.

With all of the above in mind, we hope we have made a case for the innovation in our work and convinced you to reevaluate your assessment of our work. We are happy to further discuss and clarify any concerns you may still have. 

References:
[1] Harrison Edwards and Amos Storkey, Censoring representations with an adversary, In Proceedings of the International Conference on Learning Representations, San Juan, Puerto Rico, May 2016.

[2] Thomas M Cover and Joy A Thomas, Elements of information theory, John Wiley &amp; Sons, 2012.

[3] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel, Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, pp. 214–226. ACM, 2012.

[4] Moritz Hardt, Eric Price, Nathan Srebro, Equality of opportunity in supervised learning. In Advances in neural information processing systems, pp. 3315–3323, 2016.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HklEKy1XaQ" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xAH2RqK7&amp;noteId=HklEKy1XaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper1590 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_SJggqV6v37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Formalization of data driven GAN driven fairness methods.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xAH2RqK7&amp;noteId=SJggqV6v37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1590 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1590 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors describe a framework of how to learn a "fair" (demographic parity) representation that can be used to train certain classifiers, in their case facial expression and activity recognition. The method describes an adversarial framework with a constraint that bounds the distortion of the learned representation compared to the original input.

Clarity:
The paper is well written and easy to follow. The appendix is rather extensive though and contains some important parts of the paper, though the paper can be understood w/o it.

I didn't quite follow Sec 3. It is a bit sparse on the details and the final conclusion isn't entirely clear. It also isn't clear to me how general the conclusions drawn from the Gaussian mixture model are for more complex cases.

Novelty:
Adversarial fairness methods are not new, but in my opinion the authors do a good job of summarizing the literature and formalizing the problem. I am not fully familiar with the space to judge if this is enough novelty.

Using the distortion constraint is interesting and seems to work according to the experiments. Generally though, I think that distortion can be a very restrictive constraint. One could imagine representations with a very high distortion (e.g. by completely removing the sensitive attribute) and predictive qualities equivalent to the original representation. Some further discussion of this would be good.

Experiments:
The experiments are somewhat limited, but show the expected correlations (e.g. distortion vs predictiveness). 

Overall, I do believe that this work is in the right direction in this more and more popular area of great importance. I also think that contributions compared to other works could be made more clear, as well as additional experiments and discussions of the shortcomings of this approach may be added.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1xyb2JXp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xAH2RqK7&amp;noteId=H1xyb2JXp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1590 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1590 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear AnonReviewer1,
 
Thank you for the detailed comments and observations. We address your concerns below.

**Clarity of section 3** 

We are currently working on rewriting section 3 to make it more accessible. This section shows that decorrelation schemes learned in a data-driven fashion against a computationally bounded adversary perform well when evaluated against a maximum a posteriori probability (MAP) adversary that has access to distributional information and knows the applied decorrelation schemes. We evaluate the learned decorrelation scheme in the following three steps:
 
1. We learn the decorrelation scheme in a data driven fashion using synthetic Gaussian mixture data. 
2. We evaluate the performance of the learned decorrelation scheme under a strong adversary who has access to dataset statistics, knows the learned decorrelation scheme, and can compute the MAP decision rule.
3. We compare the performance of the learned scheme with the game-theoretic optimal one. 

The first step can be done for any dataset but the last two steps can only be done for data that we have access to its distribution. Since the distribution of a real dataset is very difficult to obtain. We assume that the public variable follows a Gaussian mixture model conditioned on the value of the sensitive variable. In this case, we can compute the game-theoretic optimal decorrelation scheme and the optimal decision rule of the strong adversary. We agree that the conclusion drawn from the Gaussian mixture model is limited and may not generalize to more complex model. But this serves as a good sanity check, especially given that Gaussian mixture models have been used in many areas [1]. 

**Novelty of this paper**

We would like to list the novelty of this paper and highlight the important contributions as follows.  

1. All the relevant papers have exclusively focused on showing (via experiments) that this approach works well in practice when you design things with a particular classification task in mind (i.e., in a supervised fashion). This requires having access to additional training labels which may be unavailable during the training phase. Our paper shows (via experiments on medium-sized datasets) that this approach works even when the designer does not want to restrict their attention to one classification task (i.e., in an unsupervised fashion). Indeed, our experiments and simulations show that the learned representations work well on classification tasks that haven't been accounted for. 

2. We make very precise connections between the data-driven adversarial learning framework and the game- and information-theoretic setting (which assumes that the designer has access to the joint distributions between data and sensitive attributes, and the minimax optimization is performed over all theoretically possible randomized decorrelation and adversarial learning rules). We also show how the change of the loss function in our framework leads to a variety of information theoretic adversaries with different powers. This is an important novelty because it allows us to generalize conclusions that can be made upon learning representations from a finite number of samples against a computationally bounded adversary to the more important setting of infinite samples (i.e., access to distributional information) and infinite adversarial computational power. Indeed, this is explicitly shown in Section 3 for Gaussian mixture models. Notice that this section shows that decorrelation schemes that are learned in a data-driven fashion against a computationally bounded adversary perform well when evaluated against a maximum a posteriori probability (MAP) adversary that has access to distributional information and knows the applied decorrelation schemes. Further, this shows that there is no gap between the game-theoretically optimal decorrelation schemes and the ones that are learned via a generative neural network for binary variable S. 

3. Different from previous works where the objective is modeled as a weighted combination of loss functions and distortion penalty, our formulation is a minimax game subject to a "hard distortion constraint". This allows us to directly limit the amount of distortion added to learn the representation, which is crucial for preserving the utility of the learned representation. Moreover, notice that enforcing the hard distortion constraint calls for a new training process that relies on the Penalty method or Augmented Lagrangian method presented in Appendix C.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HyluRikQpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' Response Continued</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xAH2RqK7&amp;noteId=HyluRikQpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1590 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1590 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
4. Even though we learn our randomized decorrelation neural networks by training against a specific adversarial neural network, the learned decorrelation scheme performs well when evaluated against unseen (more complex) adversarial architectures. To prove this point, we show that the mutual information (MI) between the learned representations and the sensitive attribute is sufficiently small. A sufficiently small MI implies that no attacker (regardless of their computational power) can reliably learn the sensitive attribute from the learned representation (from Fano’s inequality [2]). This is again a novelty that didn't appear in prior works.

5. While prior works have used a classical, non-generative auto-encoder type architecture for the creation of the fair/censored representations, we harness the power of generative models which have the capability to not only compress the data in certain ways but to also cleverly inject noise where it matters (see Figure 8 and 9 in Appendix E).

6. Our set of experiments reveal that the learned representations are provably private/fair. For instance, on the GENKI dataset, we show how the gender has been stripped off by hiding mustaches, facial hair, lip color etc. (see Figure 4). At the same time, we show that the representations are still useful for other classification tasks. 

**Shortcomings of the distortion function**

Regarding the concern about the use of distortion function, we want to point out that we are focusing on publishing datasets or meaningful representations that can be “universally” used for a variety of learning tasks which may not be known at the stage of publishing. The goal of our distortion constraint is to limit the perturbation of the data when trying to decorrelate the sensitive variable from the public variable. Thus, this distortion constraint preserves the utility of the learned representation of the data for other unknown machine learning tasks. For certain machine learning tasks, it is possible that the features related to the labels are orthogonal to the features related to the sensitive attributes. In this case, there exists a representation which completely removes the sensitive attribute with a very high distortion while the predictive qualities is still equivalent to the original representation. However, for publishing the learned representation of the data, we have to ensure this representation can also be used for a variety of learning tasks. Therefore, we impose a distortion constraint on the data to ensure that the learned representation does not deviate too much from the original data. We will fix the write up and add more discussions to this topic. 

**Limited experiments and some further discussions**

You are correct that our simulations are limited (greyscale images and motion sensor data). We are currently working on presenting more simulation results and will post a revised version with new experimental results and detailed discussions of our contributions and shortcomings of this approach soon. 

References:
[1] Kazuho Watanabe and Sumio Watanabe, Stochastic complexities of Gaussian mixtures in variational Bayesian approximation, Journal of Machine Learning Research, 7:625–644, 2006.

[2] Thomas M Cover and Joy A Thomas, Elements of information theory. John Wiley &amp; Sons, 2012.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1eI1DyBhX" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xAH2RqK7&amp;noteId=H1eI1DyBhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper1590 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>