<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Opportunistic Learning: Budgeted Cost-Sensitive Learning from Data Streams | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Opportunistic Learning: Budgeted Cost-Sensitive Learning from Data Streams" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=S1eOHo09KX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Opportunistic Learning: Budgeted Cost-Sensitive Learning from Data..." />
      <meta name="og:description" content="In many real-world learning scenarios, features are only acquirable at a cost constrained under a budget. In this paper, we propose a novel approach for cost-sensitive feature acquisition at the..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_S1eOHo09KX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Opportunistic Learning: Budgeted Cost-Sensitive Learning from Data Streams</a> <a class="note_content_pdf" href="/pdf?id=S1eOHo09KX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019opportunistic,    &#10;title={Opportunistic Learning: Budgeted Cost-Sensitive Learning from Data Streams},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=S1eOHo09KX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=S1eOHo09KX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">In many real-world learning scenarios, features are only acquirable at a cost constrained under a budget. In this paper, we propose a novel approach for cost-sensitive feature acquisition at the prediction-time. The suggested method acquires features incrementally based on a context-aware feature-value function. We formulate the problem in the reinforcement learning paradigm, and introduce a reward function based on the utility of each feature. Specifically, MC dropout sampling is used to measure expected variations of the model uncertainty which is used as a feature-value function. Furthermore, we suggest sharing representations between the class predictor and value function estimator networks. The suggested approach is completely online and is readily applicable to stream learning setups. The solution is evaluated on three different datasets including the well-known MNIST dataset as a benchmark as well as two cost-sensitive datasets: Yahoo Learning to Rank and a dataset in the medical domain for diabetes classification. According to the results, the proposed method is able to efficiently acquire features and make accurate predictions. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Cost-Aware Learning, Feature Acquisition, Reinforcement Learning, Stream Learning, Deep Q-Learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">An online algorithm for cost-aware feature acquisition and prediction</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BJxJxn2l0Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Summary of changes</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1eOHo09KX&amp;noteId=BJxJxn2l0Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper99 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper99 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
We thank the reviewers for the constructive comments and suggestions. We believe that the suggested revisions enhanced the scientific quality of the manuscript significantly.

The summary of revisions is as follows:

- We added clarifications to different parts of the paper including: loss functions, algorithm box comments, explanation of the algorithm, implementation details, and so forth.

- A new section is included in the revised version which is dedicated to ablation study of: (i) certainty measurement used in this paper, (ii) the suggested representation sharing method, (iii) the proposed reward function, and (iv) the performance of the proposed method under different enforced budgets (see Section 4.3.1).

- We presented and discussed plots showing the performance of the algorithm during the operation at each episode.

- We added discussions on how to handle bundled features, how to represent missing values, the impacts of the annealing strategy, etc.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rygaWIL0h7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>nice results but limited novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1eOHo09KX&amp;noteId=rygaWIL0h7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper99 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper99 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper presents a RL approach for sequential feature acquisition in a budgeted learning setting, where each feature comes at some cost and the goal is to find a good trade-off between accuracy and cost. Starting with zero feature, the model sequentially acquires new features to update its prediction and stops when the budget is exhausted. The feature selection policy is learned by deep Q-learning. The authors have shown improvements over several prior approaches in terms of accuracy-cost trade-off on three datasets, including a real-world health dataset with real feature costs.

While the results are nice, the novelty of this paper is limited. As mentioned in the paper, the RL framework for sequential feature acquisition has been explored multiple times. Compared to prior work, the main novelty in this paper is a reward function based on better calibrated classifier confidence. However, ablations study on the reward function is needed to understand to what extent is this helpful.

I find the model description confusing. 
1. What is the loss function? In particular, how is the P-Network learned? It seems that the model is based on actor-critic algorithms, but this is not clear from the text.
2. What is the reward function? Only immediate reward is given.
3. What is the state representation? How do you represent features not acquired yet?

It is great that the authors have done extensive comparison with prior approaches; however, I find more ablation study needed to understand what made the model works better. There are at least 3 improvements: 1) using proper certainty estimation; 2) using immediate reward; 3) new policy architecture. Right now not clear which one gives the most improvement.

Overall, this paper has done some nice improvement over prior work along similar lines, but novelty is limited and more analysis of the model is needed.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HkgGRh3gAQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>To Reviewer 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1eOHo09KX&amp;noteId=HkgGRh3gAQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper99 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper99 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Thank you for reviewing the manuscript and helpful comments. Please find a point-to-point response to your comments in the following.

--------------------------------------------
*Comment: “1. What is the loss function? In particular, how is the P-Network learned? It seems that the model is based on actor-critic algorithms, but this is not clear from the text.”

The loss function used for the P-Network is a cross-entropy loss as a typical loss used for classification tasks. For training the Q-Network we use mean squared error (MSE) between the estimated reward and the observed reward values. Please note that the replay buffer is used to sample batches of feature vectors, labels, actions, and reward values required to measure P- and Q-Network losses.

We added the following clarification to the revised paper (see Sec. 3.2):
“Cross-entropy and mean squared error (MSE) loss functions were used as the objective functions for the P and Q networks, respectively.”

--------------------------------------------
*Comment: “2. What is the reward function? Only immediate reward is given.”

The reward function which is suggested by this paper is presented in Eq. (7). Here, we are using epsilon-greedy explorations and Bellman equations to fit the action-value function. It allows the general formulation of non-immediate and accumulated rewards through a discount factor.
Intuitively, the suggested reward function in Eq. (7) measures the expected change of model hypothesis corresponding to each feature acquisition action.

--------------------------------------------
*Comment: “3. What is the state representation? How do you represent features not acquired yet?”

In this paper, each state is the current feature vector containing values for features that are acquired at that state. 
From the first paragraph of Section 3.1:
“At each point, the current state is defined as the current realization of the feature vector (i.e., $\bm{x}_i^t$) for a given instance.”

We use NaN (not a number) values to internally represent the features that are not available. However, the implementation we use replaces the NaN values with zeros during the forward/backward computation. We believe it is an efficient approach compared to using separate mask vectors to represent missing features as it reduces the memory and I/O overheads.

We included a brief explanation in the revised version (See Sec. 3.2):
“Note that, in our implementation, for efficiency reasons, we use NaN (not a number) values to represent features that are not available and impute them with zeros during the forward/backward computation.”

--------------------------------------------
*Comment: “It is great that the authors have done extensive comparison with prior approaches; however, I find more ablation study needed to understand what made the model works better.”


Thank you for suggesting this. In the revised version, we added a new subsection (Sec. 4.3.1) to the results section entitled “ablation study”. In summary, it presents an ablation study and comparisons of:

- Using the MC-Dropout certainty versus the uncalibrated softmax estimates. We compared the accuracy of the estimated certainty values achieved as well as the overall impact on the feature acquisition performance (see Fig. 5a and Fig. 5b of the revised version). As it can be seen from these figures, the idea of using MC-Dropout certainty plays a crucial rule in the performance of the proposed method.

- Demonstrating the effectiveness of the suggested representation sharing between the P and Q networks (see Fig. 6) demonstrating that the representation sharing would result in a faster convergence.

- We added an analysis of the suggested method under different enforced budget constraints (see Fig. 7). According to results, the suggested method is able to efficiently operate at different enforced budget constraints.

- Regarding other ablation analysis suggested by the reviewer, we have comparisons of the suggested approach (OL) and a basic reinforcement learning based method (RL-based) in the comparison results presented in Section 4.2. Due to space considerations, in the revised version, we discussed this case in the ablation study section without reiterating the plots and by referring to Fig. 2 and Fig. 4b (see Sec 4.3.1):
“A comparison between the suggested feature-value function (OL) in this paper with a traditional feature-value function (RL-Based) was presented in Figure 2 and Figure 4b. Here, RL-Based method is using a similar architecture and learning algorithm as the OL, while the reward function is simply the negative of feature costs for acquiring each feature and a positive value for making correct predictions. As it can be seen from the comparison of these approaches, the reward function suggested in this paper results in a more efficient feature acquisition.“ 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJgr7KZahQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting approach with a confused exposition</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1eOHo09KX&amp;noteId=HJgr7KZahQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper99 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper99 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">I like the approach, however: I consider the paper to be poorly written.  The presentation needs to be improved for me to find it acceptable.

It presents a stream-oriented (aka online) version of the algorithm, but experiments treat the algorithm as an offline training algorithm.  This is particularly critical in this area because feature acquisition costs during the "warm-up" phase are actual costs, and given the inherent sample complexity challenges of reinforcement learning, I would expect them to be significant in practice.  This would be fine if the setup is "we have a fixed offline set of examples where all features have been acquired (full cost paid) from which we will learn a selector+predictor for test time".

The algorithm 1 float greatly helped intelligibility, but I'm left confused.  
  * Is this underlying predictor trained simultaneously to the selector?  
        * Exposition suggests yes ("At the same time, learning should take place by updating the model while maintaining the budgets."), but algorithm block doesn't make it obvious.
        * Maybe line 21 reference to "train data" refers to the underlying predictor.
  * Line 16 pushes a value estimate into the replay buffer based upon the current underlying predictor, but:
        * this value will be stale when we dequeue from the replay buffer if the underlying predictor has changed, and 
        * we have enough information stored in the replay buffer to recompute the value estimate using the new predictor, but
        * this is not discussed at all.

Also, I'm wondering about the annealing schedule for the exploration parameter (this is related to my concern that the
algorithm is not really an online algorithm).  The experiments are all silent on the "exploration" feature acquisition cost.  Furthermore I'm wondering: when you do the test evaluations, do you set exploration to 0?

I also found the following disturbing: "It is also worth noting that, as the proposed method is
incremental, we continued feature acquisition until all features were acquired and reported the average
accuracy corresponding to each feature acquisition budget."  Does this mean the underlying predictor was trained on data 
that it would not have if the budget constraint were strictly enforced?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJelzRnl0X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>To Reviewer 1 (1/2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1eOHo09KX&amp;noteId=rJelzRnl0X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper99 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper99 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Thank you for reviewing the manuscript and helpful comments. Please find a point-to-point response to your comments in the following.
-----------------------------------------------------------------------------------------------
* Is this underlying predictor trained simultaneously to the selector?
* Exposition suggests yes ("At the same time, learning should take place by updating the model while maintaining the budgets."), but algorithm block doesn't make it obvious.

Yes, the predictor is trained jointly with the feature-value estimator. In algorithm block, Line 22 is related to updating P and Q networks at the same time on a training batch sampled from the relay memory. In order to clarify this, we have added the following comment to the algorithm box in the revised version:
“update P, Q, and target Q networks using train-batch // Jointly train P &amp; Q”

-----------------------------------------------------------------------------------------------
* Maybe line 21 reference to "train data" refers to the underlying predictor.
* Line 16 pushes a value estimate into the replay buffer based upon the current underlying predictor, but:
    * this value will be stale when we dequeue from the replay buffer if the underlying predictor has changed, and
    * we have enough information stored in the replay buffer to recompute the value estimate using the new predictor, but
    * this is not discussed at all.

Here, train data refers to a batch of samples from the experience replay memory. Each item in the replay memory is a tuple of: a feature vector before and after the acquisition, the action taken, reward received for that action, and the ground-truth label corresponding to that feature vector (see Line 16 of the algorithm box).

Regarding the reviewer’s concern about issues with having stale predicted values, in this paper, we prevent this by storing the ground-truth label in the replay buffer and by recomputing the predictions before updating the parameters. This way, we always use the most up-to-date results. However, as the ground truth label may not be available during the feature acquisition, in our final implementation, we use a temporary buffer to store experiences without the label and we push them along with the ground truth label as soon as the feature acquisition is finished and the ground-truth label is available. To simplify the presentation of the algorithm, we decided to omit the temporary buffering trick from the algorithm box and assumed that the labels are available. If the reviewer believes that including this in the algorithm would be helpful, we would be glad to include this.

We have discussed this in the second-to-last paragraph of Section 3.1:
“It is worth noting that, in Algorithm 1, to simplify the presentation, we assumed that ground-truth labels are available at the beginning of each episode. However, in the actual implementation, we store experiences within an episode in a temporary buffer, excluding the label. At last, after the termination of the feature acquisition procedure, a prediction is being made and upon the availability of label for that sample, the temporary experiences along with the ground-truth label are pushed to the experience replay memory.”
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HyeFo62e07" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>To Reviewer 2 (2/2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1eOHo09KX&amp;noteId=HyeFo62e07"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper99 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper99 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
*Comment: Also, I'm wondering about the annealing schedule for the exploration parameter (this is related to my concern that the algorithm is not really an online algorithm). The experiments are all silent on the "exploration" feature acquisition cost. Furthermore I'm wondering: when you do the test evaluations, do you set exploration to 0?

In an online learning setup, data becomes available sequentially and the goal for an online learner is to update its hypothesis as more data is being observed. There are two main considerations for an online method. First, data is not provided or can be stored as a batch. Second, the hypothesis should be refined incrementally as more observations take place. 

Regarding the reviewer’s concern about annealing, annealing is a standard approach widely used in the literature helping early steps of optimization. We believe that the suggested algorithm is online because, initially, there is no viable alternative strategy to follow due to the limited number of samples. However, as we observe more samples, we anneal the random decisions and try to use the captured knowledge instead. In this respect, the suggested algorithm is online according to the definition above.

Regarding the exploration probability used in our experiments: during the training and validation phase, we use the random exploration mechanism. However, for the comparison of the results with other work in the literature, as they are all offline methods, we decided to not to do the exploration.

In order to address the reviewer’s comment, we added the following explanation to the revised paper:
“During the training and validation phase, we use the random exploration mechanism. However, for the comparison of the results with other work in the literature, as they are all offline methods, the random exploration is not used during the feature acquisition.”

-----------------------------------------------------------------------------------------------
*Comment: I also found the following disturbing: "It is also worth noting that, as the proposed method is incremental, we continued feature acquisition until all features were acquired and reported the average accuracy corresponding to each feature acquisition budget." Does this mean the underlying predictor was trained on data that it would not have if the budget constraint were strictly enforced?

In order to address the reviewer’s concern, we conducted experiments using different enforced budgets (see Fig. 7). In summary, according to our experiments, the suggested method is able to efficiently operate at different enforced budget constraints.

We have also included the following discussion to the paper:
“Figure 7 shows the performance of the OL method having various limited budgets during the operation. Here, we report the accuracy-cost curves for 25%, 50%, 75%, and 100% of the budget required to acquire all features. As it can be inferred from this figure, the suggested method is able to efficiently operate at different enforced budget constraints.”
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SklBdpngRX" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1eOHo09KX&amp;noteId=SklBdpngRX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper99 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1g0t0mMnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>budgeted feature acquisition to train networks - seems similar to RADIN</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1eOHo09KX&amp;noteId=H1g0t0mMnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper99 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper99 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a novel method for budgeted cost sensitive learning from Data Streams.
This paper seems very similar to the work of Contrado’s RADIN algorithm which similarly evaluates sequential datapoints with a recurrent neural network by adaptively “purchasing” the most valuable features for the current datapoint under evaluation according to a budget. 

In this process, a sample (S_i) with up to “d” features arrives for evaluation.  A partially revealed feature vector x_i arrives at time “t” for consideration.  There seems to exist a set of “known features” that that are revealed “for free” before the budget is considered (Algorithm 1).  Then while either the budget is not exhausted or some other stopping condition is met features are sequentially revealed either randomly (an explore option with a decaying rate of probability) or according to their cost sensitive utility.  When the stopping condition is reached, a prediction is made.  After a prediction is made, a random mini-batch of the partially revealed features is pushed into replay memory along with the correct class label and the P. Q, and target Q networks are updated.

The ideas of using a sequentially revealed vector of features and sequentially training a network are in Contrado’s RADIN paper.   The main novelty of the paper seems to be the use of MC dropout as an estimate of certainty in place of the softmax output layer and the methods of updating the P and Q networks.
The value of this paper is in the idea that we can learn online and in a cost sensitive way.  The most compelling example of this is the idea that a patient shows up at time “t” and we would like to make a prediction of disease in a cost sensitive way.  To this end I would have liked to have seen a chart on how well this algorithm performs across time/history.  How well does the algorithm perform on the first 100 patients vs the last 91,962-91,062 patients at what point would it make sense to start to use the algorithm (how much history is needed).

Am I correct in assuming there are some base features that are revealed “for free” for all samples?  If so how are these chosen?  If so how does the number of these impact the results?  

In Contrado’s RADIN paper the authors explore both the MNIST dataset and others, including a medical dataset “cardio.”  Why did you only use RADIN as a comparison for the MNIST dataset and not the LTRC or diabetes dataset?  Did you actually re-implement RADIN or just take the numbers from their paper?  In which case, are you certain which MNIST set was used in this paper? (it was not as well specified as in your paper).

With respect to the real world validity of the paper, given that the primary value of the paper has to do with cost sensitive online learning, it would have been better to talk more about the various cost structure and how those impact the value of your algorithm.  For the first example, MNIST, the assumed uniform cost structure is a toy example that equates feature acquisition with cost.  The second example uses computational cost vs relevance gain.  This would just me a measure of computational efficiency, in which case all of the computational cost of running the updates to your networks should also be considered as cost.  With respect to the third proprietary diabetes dataset, the costs are real and relevant, however there discussion of these are given except to say that you had a single person familiar with medical billing create them for you (also the web address you cite is a general address and does not go to the dataset you are using). 

 In reality, these costs would be bundled.  You say you estimate the cost in terms of overall financial burden, patient privacy and patient inconvenience.  Usually if you ask the patient to fill out a survey it has multiple questions, so for the same cost you get all the answers.  Similarly if you do a blood draw and test for multiple factors the cost to the patient and the hospital are paid for the most part upfront.  It is not realistic to say that the cost of asking a patient a questions is 1/20th of the cost of the survey.  The first survey question asked would be more likely 90-95% of the cost with each additional question some incremental percentage.  To show the value of your work, a better discussion of the cost savings would be appreciated.             
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HklsxJ6e07" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>To Reviewer 3 (1/2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1eOHo09KX&amp;noteId=HklsxJ6e07"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper99 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper99 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Thank you for reviewing the manuscript and helpful comments. Please find a point-to-point response to your comments in the following.

-----------------------------------------------------------------------------------------------
* Comment: “The ideas of using a sequentially revealed vector of features and sequentially training a network are in Contrado’s RADIN paper.”

We agree with the reviewer that having sequentially revealed vectors is in common between the earlier work by Contrado (RADIN) and the current study (OL). However, we believe that RADIN and OL are significantly different from each other in the idea, architecture, and implementation. Specifically:

- RADIN approaches the problem by looking into the feature acquisition process as a time sequence of acquisitions. However, the suggested method is modeling the utility of actions given the current state regardless of the previous actions. From this perspective, RADIN can be considered a time-series approach, while OL is a reinforcement learning approach using a time-invariant policy.

- RADIN defines a cost function consisting of two terms weighted by a hyper-parameter: a classification loss and a feature acquisition cost. However, the introduced method in this paper is using the variations of model uncertainty as a value function of eq (7) being used in making decisions.

- RADIN is using a recurrent neural network (RNN) architecture, while OL is based on reinforcement learning and deep Q learning algorithms.

- The suggested method is designed to operate as an online learning algorithm, while RADIN is not studying this case.

-----------------------------------------------------------------------------------------------
* Comment: “I would have liked to have seen a chart on how well this algorithm performs across time/history. How well does the algorithm perform on the first 100 patients vs the last 91,962-91,062 patients at what point would it make sense to start to use the algorithm (how much history is needed).”

Thank you for suggesting this. We have included a new section to discuss this (see Section 4.3.2 and Fig. 8ab). 

We have also added the following explanation in the results section (see Section 4.3.2):
“Figure 8a and 8b demonstrate the validation accuracy and AUACC values measured during the processing of the data stream at each episode for the MNIST and Diabetes datasets, respectively. As it can be seen from this figure, as the algorithm observes more data samples, it achieves higher validation accuracy/AUACC values, and it eventually converges after a certain number of episodes. It should be noted that, in general, convergence in reinforcement learning setups is dependent on the training algorithm and parameters used. For instance, the random exploration strategy, the update condition, and the update strategy for the target Q network would influence the overall time behavior of the algorithm. In this paper, we use conservative and reasonable strategies as reported in Section 3.2 that results in stable results across a wide range of experiments.”

-----------------------------------------------------------------------------------------------
*Comment: “Am I correct in assuming there are some base features that are revealed “for free” for all samples? If so how are these chosen? If so how does the number of these impact the results?”

In our experiments, we are not assuming any feature will be available for free. However, the formulation presented in this paper accommodates the case where features are available for free. In order to clarify this issue and prevent any confusion to our readers, we added the following explanation to the paper:

“In this algorithm, if any features are available for free we include them in the initial feature vector; otherwise, we start with all features not being available initially.”
Also, the algorithm box is revised by adding a comment to Line 4:
“$x_i^t$ &lt;- known features of  S_i // if there are any features available”
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJexCC2xR7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>To Reviewer 3 (2/2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1eOHo09KX&amp;noteId=rJexCC2xR7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper99 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper99 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
*Comment: “In Contrado’s RADIN paper the authors explore both the MNIST dataset and others, including a medical dataset “cardio.” Why did you only use RADIN as a comparison for the MNIST dataset and not the LTRC or diabetes dataset? Did you actually re-implement RADIN or just take the numbers from their paper? In which case, are you certain which MNIST set was used in this paper? (it was not as well specified as in your paper).”

We have compared our results with Contrado’s results as reported on the RADIN paper. The reason behind this was the fact that RADIN is consisting of many components and parameters which makes reproducing their results for our comparisons with RADIN very difficult. We would be glad to include comparisons with RADIN on other datasets, if the reviewer could point us to an open source implementation of RADIN.
Regarding the reviewer’s comment on which samples of the MNIST was used for training/validation/test: we use the standard MNIST separation using the provided train set for our train and validation, and the MNIST test set is used for testing the suggested algorithm.

-----------------------------------------------------------------------------------------------
*Comment: “With respect to the real world validity of the paper, given that the primary value of the paper has to do with cost sensitive online learning, it would have been better to talk more about the various cost structure and how those impact the value of your algorithm...”

We agree with the reviewer that it is very important to consider cost structures in real-world scenarios. However, a deep study of any specific cost structure (e.g., in specific healthcare problems) is itself an area of research and any problem would require an in-depth study. In this paper, we introduced a general formulation for the problem for cost-sensitive feature acquisition from stream data that is evaluated on different applications. However, a deeper study of any specific cost structure would require integrating domain expertise and is out of the scope of this study.

-----------------------------------------------------------------------------------------------
*Comment: “the web address you cite is a general address and does not go to the dataset you are using”

The web address provided contains links to the dataset download page (the “Questionnaires, Datasets, and Related Documentation” option on the left sidebar). Additionally, we plan to publish the dataset preprocessing source code to help other future work to reproduce and compare our results.


-----------------------------------------------------------------------------------------------
*Comment: “In reality, these costs would be bundled...To show the value of your work, a better discussion of the cost savings would be appreciated.”

The current formulation presented in this paper allows for having bundled feature sets. In this case, each action would be acquiring a bundle and the reward function is evaluated for the acquisition of this bundle by measuring the variations of uncertainty before and after acquiring the bundle. As suggested by the reviewer, we have added a discussion on this to the revised paper:
“In our experiments, for the simplicity of presentation, we assume that all features are independently acquirable at a certain cost, while in many scenarios, features are bundled and acquired together (e.g., certain clinical measurements). However, it should be noted that the current formulation presented in this paper allows for having bundled feature sets. In this case, each action would be acquiring each bundle and the reward function is evaluated for the acquisition of the bundle by measuring the variations of uncertainty before and after acquiring the bundle.”
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>