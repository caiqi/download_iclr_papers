<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Deepström Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Deepström Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJlSQnR5t7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Deepström Networks" />
      <meta name="og:description" content="Recent work has focused on combining kernel methods and deep learning. With this in mind, we introduce Deepström networks -- a new architecture of neural networks which we use to replace top dense..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJlSQnR5t7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Deepström Networks</a> <a class="note_content_pdf" href="/pdf?id=BJlSQnR5t7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019deepström,    &#10;title={Deepström Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BJlSQnR5t7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Recent work has focused on combining kernel methods and deep learning. With this in mind, we introduce Deepström networks -- a new architecture of neural networks which we use to replace top dense layers of standard convolutional architectures with an approximation of a kernel function by relying on the Nyström approximation. 
Our approach is easy highly flexible. It is compatible with any kernel function and it allows exploiting multiple kernels. 
We show that Deepström networks reach state-of-the-art performance on standard datasets like SVHN and CIFAR100. One benefit of the method lies in its limited number of learnable parameters which make it particularly suited for small training set sizes, e.g. from 5 to 20 samples per class. Finally we illustrate two ways of using multiple kernels, including a multiple Deepström  setting, that exploits a kernel on each feature map output by the convolutional part of the model.    </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">kernels, Nyström approximation, deep convnets</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A new neural architecture where top dense layers of standard convolutional architectures are replaced with an approximation of a kernel function by relying on the Nyström approximation.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_B1lDCi9_3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>I'll need better argument for the significance of this paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlSQnR5t7&amp;noteId=B1lDCi9_3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1358 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1358 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this work, the author(s) proposed to replace the random feature approximation with Nystrom approximation for kernelized deep neural networks.

The contribution of this work seems pretty incremental to me.  It's well known that Nystrom usually provides a more succinct/concise approximation of the Gram matrix. And its performance also critically depends on the choice of inducing points, which reminds me there is a lack of discussion on this matter in the paper. Often times, Cholesky  factorization works even better compared with Nystrom according to my own experience. 

Kernelizing deep networks is a  challenging task on its own, and the reality is that it often does nothing more than slowing down the computation without tangible improvements. It takes much more than ``Nystromizing'' to convince me. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HkgbXRe1h7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nystroem meets deep learning - review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlSQnR5t7&amp;noteId=HkgbXRe1h7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1358 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1358 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">DEEPSTRÖM NETWORKS

Quality: good
Originality: original
Significance: relevant for ICLR
Pros: - interesting idea -- see detailed comments
Cons: a number of open issues in the presentation -- see detailed comments

The paper is based on some concepts to link non-linear data representations
by means of non-linear kernel mappings, with the deep learning approach.
The presented concept is compared to former methods going into this direction
and random fourier feature approaches. Additionally an adaptive scheme is
suggested where the internally used landmark matrix (which forms a kind
of mahalanobis like weighting matrix to the data) is adapted. 

I think the main idea of the paper is interesting but the presentation
lacks a number of details in the description/parametrization and the
experimental design. 

- 'Indeed, since the Nyström approximation uses an explicit feature map from the data kernel matrix, it is not restricted to a specific kernel function and not limited only to
   RBF kernels, as in Fastfood approximation.' -- well yes an no, Nystroem works only effectively if the intrinsic dimensionality of the data is reasonable small - otherwise
   you need a lot of disjunct landmarks. Additionally the parameterization - how many landmarks and which one is not so easy - in this sense random fourier features are easier to
   handle. Further the random fourier features are not restricted to RBF kernel and not even to shift-invariant ones - there are a few proposals 
- The 'feature map' obtained by the Nystroem approximation is only helpful as long as the number of landmarks is reasonable small - otherwise an out of sample extension (mapping
  new test points) in the spanned feature space may not be very reliable
- please correct the spell errors like 'using ful training sets' (full); 'In (24), wo architectures' -- two?
- random subsampling of the landmarks can lead to rather high approximation errors if the number of landmarks is small - there are some proposal to address this using leverage score sampling and other
  see e.g. work of K. Zhang, Suykens, C. Musco
- 'it is restricted to the Gaussian kernel' - well this is not true - see comments above (but I agree that there may not have been done much work to show this in practice, 
  and it is probably not possible to be used for - all - kinds of psd kernels)
- 'Singular Value Decomposition (SVD) of K 11 . In the case where the size of the subsample L, m,is large, the computational complexity of the SVD is O(m 3 ).'
  -- well - yes and no, with Nystroem you would expect that N is much ! larger than m - than the m^3 may still be somewhat ok. To learn the weightings on the fly in your
  adaptive approach will address the complexity issue but may in parts degenerate performance - now it is a question what the user is more interested on
- You mainly ignore the point how the landmarks have to be selected - random is (often) not a very reasonable choice - see respective work around this
  In your datasets the problem may not show up because you have images only which have very strong characteristics and are intrinsically very low dimensional.
  --&gt; I think you have to add another section here - evaluating either different sampling schemes or just use a more state of the art one like leverage score sampling 
  (getting I think O(N^2) complexity); you should also address how you find a good guess for the number of landmarks
- I suggest to use also additional non-image data. Basically one motivation in your approach is to go beyond the classical deep learning data representation 
  - this may not be so interesting for image recognition problems
- I may suggest to spend a small subsection to a complexity analysis (including parameter optimization) of the various methods
- Figure 2 - what do you mean by number of parameters? - there are a few strong fluctuations (fully connected, addative deep linear) - why
- please increase the level of details of the accuracy axis in Figure 2 - so far it basically has 10% steps
- if I have not missed it you did not say how the parameter \sigma (or \gamma) in the rbf kernel has been determined (same for other parametric kernels)
- multiple kernels - well how did you combine the kernels (there are many options) 
- there are number of typos in the refs (see e.g. ref 14); some other references are very incomplete e.g. ref 15
- plots should be (prefered) close to the respective text (the appendix is a bit strange here)
  
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1gk8F8iiQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>review of Deepstrom networks</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlSQnR5t7&amp;noteId=r1gk8F8iiQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1358 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">22 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1358 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Paper 1358 considers the problem of reducing the cost of fully-connected layers in deep architectures. For this purpose, the deep fried architecture was recently introduced. It involves replacing a fully connected layer by a structured projection layer. Followed by a cos/sin non-linearity, this actually leads to an approximation of the RBF kernel. 

Paper 1358 aims at extending this work beyond the RBF kernel. For this purpose, paper 1358 uses the Nyström approximation. It involves computing an explicit embedding based on a subset of training samples {x_i, i=1…m}. Given a sample x, the embedding is the multiplication of the sample kernel vector [k(x,x_1}, …, k(x, x_m)] with a projection matrix. Two variants are proposed: 
1)	the projection matrix is the inverse of the m x m sample kernel matrix;
2)	the projection matrix is learned through backpropagation.

The approach is validated on four public benchmarks.

On the positive side: the paper is rather clear and easy to follow.

On the negative side:
-	The novelty of the proposed approach is modest. All in all, it feels very much like applying the Nyström approximation on top of convnet features. Indeed, to the best of my understanding, the gradients were not back-propagated through the convolutional layers. This point is actually unclear and should be clarified. 
-	In this sense, the term “deepström” is somewhat misleading. This seems to indicate that the proposed approach is deep. However, if one compounds the matrices W and lc into a single matrix, then the propose Nyström module has a single hidden layer.
-	The claim is made that the deep-fried module is restricted to approximating the RBF kernel. This is incorrect. Other kernels such as the arc-cosine kernel can be approximated by replacing the sin/cos non-linearity by a reLu.
-	The experiments are lacking in some respects.
o	Straightforward non-deep baselines are missing. For instance, the authors could have applied the “efficient additive kernels” of Vedaldi and Zisserman (CVPR’10) on top of convnet features. 
o	It is unclear why a single hidden layer was used for the dense architecture baseline. Is this because the results did not improve with more? Or because the proposed deepström architecture has a single hidden layer?
o	In the result figures, it is unclear whether the # parameters on the x-axis is the total number of stored parameters, including the samples {x_i, i=1…m}, or just the estimated parameters, i.e. W and lc. 
o	There is no result at Imagenet scale, contrary to the original deep-fried convnet paper.
o	At the end of section 4.1, the authors spend a lot of time explaining that the purpose is not to compete with the state-of-the-art. Yet, in the third paragraph of section 4.2, they claim that their approximation reaches the state-of-the-art. So which is it?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>