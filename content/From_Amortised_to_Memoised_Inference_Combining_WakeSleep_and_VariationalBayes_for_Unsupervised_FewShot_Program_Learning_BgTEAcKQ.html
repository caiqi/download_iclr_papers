<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>From Amortised to Memoised Inference: Combining Wake-Sleep and Variational-Bayes for Unsupervised Few-Shot Program Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="From Amortised to Memoised Inference: Combining Wake-Sleep and Variational-Bayes for Unsupervised Few-Shot Program Learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1gTE2AcKQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="From Amortised to Memoised Inference: Combining Wake-Sleep and..." />
      <meta name="og:description" content="Given a large database of concepts but only one or a few examples of each, can we learn models for each concept that are not only generalisable, but interpretable? In this work, we aim to tackle..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1gTE2AcKQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>From Amortised to Memoised Inference: Combining Wake-Sleep and Variational-Bayes for Unsupervised Few-Shot Program Learning</a> <a class="note_content_pdf" href="/pdf?id=B1gTE2AcKQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019from,    &#10;title={From Amortised to Memoised Inference: Combining Wake-Sleep and Variational-Bayes for Unsupervised Few-Shot Program Learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=B1gTE2AcKQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Given a large database of concepts but only one or a few examples of each, can we learn models for each concept that are not only generalisable, but interpretable? In this work, we aim to tackle this problem through hierarchical Bayesian program induction. We present a novel learning algorithm which can infer concepts as short, generative, stochastic programs, while learning a global prior over programs to improve generalisation and a recognition network for efficient inference. Our algorithm, Wake-Sleep-Remember (WSR), combines gradient learning for continuous parameters with neurally-guided search over programs. We show that WSR learns compelling latent programs in two tough symbolic domains: cellular automata and Gaussian process kernels. We also collect and evaluate on a new dataset, Text-Concepts, for discovering structured patterns in natural text data.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">wake-sleep, variational, amortised inference, hierarchical bayes, program learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We extend the wake-sleep algorithm and use it to learn to learn structured models from few examples, </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_r1goBvVshX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A nice proposed idea and new dataset, but with confusing presentation and evaluation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1gTE2AcKQ&amp;noteId=r1goBvVshX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1494 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1494 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper introduces a modification to the wake-sleep algorithm that introduces into the recognition network a cache which holds a small number of high-quality proposed latent variables, and uses discrete sampling from this cache instead of using draws from the recognition network directly. It's a nice idea, but unfortunately the presentation is quite unclear, and the experiments do not really succeed in isolating the effect of this particular contribution.

There is an extensive introduction and background, which aims to situate this work, but overall it is a bit confusing (and strays a bit afield, e.g. why is there a description of SGD in connection to deep meta-learning? noting that this paper is nearly two pages over length…).  

- Much language is devoted to the idea that a variational autoencoder setting is "better" than wake-sleep for learning deep generative models, because it provides a single unifying objective function. This shows up also in language like in table 2 ("the correct update for q", whereas the wake-sleep update "may be heavily biased"). This doesn't make much sense. There may well be empirical reasons to prefer the performance of a VAE as opposed to wake-sleep, but the issue of whether it "will converge on a good model" is clearly an issue in VAEs as well (see, e.g., the "hacks" necessary to achieve convergence to a non-spurious optima in [1]). This paper's own experiments seem to show that, for these discrete models, achieving convergence is a non-issue for wake-sleep.

- In section 2.3 it is claimed this "highlights a bias-variance tension". This doesn't make any sense; the "sleep" update to q in wake-sleep is its own update, and not a biased approximation to the ELBO; there is nothing inherently wrong with the dual objectives used in the wake-sleep agorithm. Each is easily motivated as maximizing a lower bound on the marginal likelihood; the sleep update could alternately be motivated as performing variational inference, simply with a alpha-divergence as the objective.

- There are claims here that the challenges to good VAE performance are due to inaccurate inference networks, but this is not clear at all (see, e.g. [2]).

- Wake-sleep seems to be presented primarily as a straw-man, with little effort to actually benchmark against it or consider its merits. In particular, reweighted wake-sleep [3] and related wake-wake algorithms [4] should address any potential bias concerns, at modest increase in computational cost.

- Given that much of the focus is on how this paper uses a direct estimate of an ELBO, in constrast to wake-sleep, it is completely baffling that this objective function never once appears in the main text of the paper! The only statement of it is in table 2 (which is an incomplete objective, and doesn't include M) and implicitly in the algorithm block itself. Figure 2 is nice as a sales pitch, but it doesn't replace actually writing down the objective functions used for training!

- Why are the latent variables z_i refered to as "programs" in section 2.3? This is simply a latent variable, and is a free choice for the model designer depending on problem domain. If the point here is that this is a discrete latent variable, or in some sort of combinatorial / structured space, then say that. 

- There is some confusion here about what is a "prior" in section 3. A prior is not something which is "learned" empirically from data as part of the training (empirical Bayes procedures notwithstanding), A prior is literally a prior belief on parameters. Estimating a marginal distribution p(z), isnot learning a prior, it is doing density / distribution estimation. That's also fine, but (as noted) likely additional regularization is needed. In the VAE setting, the phrase "marginal posterior" is often used for q(z) = 1/N \sum_i q(z_i | x_i), i.e. marginalizing over the data.

- The experiments are all nice examples of discrete domains, and I like the new dataset as a testbed for fitting regular expressions a lot.

In general, I think this paper would benefit from a much more expanded and clear presentation of the contents of section 3. At the moment, the algorithm is only understandable from a close read of the algorithm block. The core novel idea here is the introduction of the "memory" block into the wake-sleep algorithm, but this is only briefly described. Some experiments here which isolate the impact of this memory block on training would be great — for example, how does performance vary given different sizes of the memory? In the experiments, there is not a huge performance gap between the proposed approach and wake-sleep; it would be good to also compare reweighted wake sleep as a comparison. It would also be good to run the importance sampled variant of the proposed approach (or, even, a variant which directly marginalizes over the contents of the memory). In terms of framing, I don't really see what this paper has to do with "few-shot program learning", aside from the fact that the test examples happen to be structured domains (it is arguable whether e.g. the compositional kernels are "programs").


[1] Bowman et al, Generating Sentences from a Continuous Space
[2] Shu et al, Amortized Inference Regularization
[3] Bornschein and Bengio, Reweighted wake-sleep
[4] Le et al, Revisiting Reweighted Wake-Sleep

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SyxVCRCFnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Biased particle-based gradient estimator; no comparison to related algorithms</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1gTE2AcKQ&amp;noteId=SyxVCRCFnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1494 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1494 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper introduces a new training algorithm for generative latent variable models called Wake-Sleep-Remember (WSR). The authors note that it is related to Wake-Sleep (WS) and is supposed to improve on it. 

WSR uses samples from p(x, z) to train the recognition model q(z|x), similarly to WS.  The authors note that the gradient estimator of decoder parameters in WS is biased with respect to the ELBO, because samples are taken from q(z|x) instead of intractable p(z|x). To alleviate this issue, in WSR samples from q are cached (remembered), and p(x, z) is trained on samples from the cache instead of being trained on samples from q directly, which is the case in WS. In particular, the cache has a fixed size of k samples per data point, and samples are weighted according to the log probability under the model p(x, z). Whenever any of the samples in the cache is less probable under the model than a new sample from q (for that data point), it gets replaced by that new sample. The authors argue that this procedure reduces the bias of the decoder update of WS, because the decoder is trained only on “good” samples. 

It is also noted in §2.3¶2 that optimising the ELBO in VAEs and WS relies on the ability of q to match the true posterior exactly - a failure to do so leads to a modification of p.  IWAE [1] and RWS [2] alleviate this issue by importance sampling. Interestingly, WSR could also mitigate this issue, but authors do not comment on that.

WSR is strongly related to RWS, but the authors seem to have missed the connection. Both algorithms require evaluation of p(x, z) on k samples from q at every training iteration. The only difference seems to be that WSR can populate the cache with samples that have very high probability under the model and can reuse the same sample from the cache multiple times to train p. RWS, on the other hand, draws new samples at every training iteration. While both algorithms feature the same computational cost, the approach of RWS is based on importance sampling and the resulting gradient estimator is unbiased (but see [3]). This is unlike WSR, in which samples in the cache are updated with samples that have higher log probability under the model. This can lead to a scenario where the cache contains samples from only one mode of the posterior, or even where it is filled with virtually the same sample replicated k times. This leads to a biased gradient estimator.

The proposed algorithm is evaluated on three problems: (1) learning cellular automata, (2) composing gaussian process kernels and (3) discovering text concepts, all of which involve discrete latent variables. WSR is compared WS, a VAE with a control variate and a WSR model without a recognition model. The last baseline corresponds to a lesioned version of WSR to gauge how important the cache (memory) is. WSR achieves the highest quantitative results and produces the best looking qualitative results from all considered baselines. Experimental evaluation would be sufficient, if not for the fact that it does not feature comparison to other particle-based inference algorithms like IWAE or RWS, which are closely related to WSR. It is hard to say whether WSR is useful despite its biased gradient estimates, since both IWAE and RWS tend to achieve much better results than VAE and WS, respectively [4].

The paper is generally well written, but it is confusing in a few places. For instance, the “memory” used in WSR is a type of cache and is unrelated to other (working/external/episodic) memory used in literature - it is purely algorithmic, and not architectural component. It would be clear if it was treated as such. For instance, I the “memory” column in Table 2 confusing.  Moreover, it calls the latent variable “z” a stochastic program, which is not necessary - it is just a discrete latent variable. “ELBo” is typically written as “ELBO”.

It is an interesting paper, but the authors does not seems to realize that they have introduced significant biased by updating the cache at every training iteration, nor do they notice links to RWS. I believe the paper should be rejected, as it might otherwise be confusing for inexperienced readers.

[1] Burda, Y., Grosse, R.B., &amp; Salakhutdinov, R. (2015). Importance Weighted Autoencoders. CoRR, abs/1509.00519.
[2] Bornschein, J., &amp; Bengio, Y. (2014). Reweighted Wake-Sleep. CoRR, abs/1406.2751.
[3] Tucker, G., Lawson, D., Gu, S., &amp; Maddison, C. J. (2018). Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives. arXiv preprint arXiv:1810.04152.
[4] Le, T.A., Kosiorek, A.R., Siddharth, N., Teh, Y.W., &amp; Wood, F. (2018). Revisiting Reweighted Wake-Sleep. CoRR, abs/1805.10469.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HklZJXXK27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea. Presentation is unclear and requires more work.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1gTE2AcKQ&amp;noteId=HklZJXXK27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1494 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1494 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this work the authors propose a learning algorithm for inferring generative programs and the concepts associated with the input data. Specifically, the idea is originated from amortised inference where a generative model (the mapping from the latent to the data) is learned simultaneously with the reverse mapping, i.e., the recognition network, in a joint optimisation framework following variational Bayes. In this work the authors further introduce a memory buffer of highly probable latent samples from the recognition model in order to avoid evaluating programs that are unlikely to have generated the data. The main challenge of the algorithm is the handling of the discrete nature of the latent state.

In general, as having little experience with programs and automata I found the paper genuinely interesting, but also very confusing. On the positive side, the authors managed to nicely motivate their work and tried to make a direct connection to popular approaches like the VAE and the sleep and wake algorithm. However, even after putting a lot of effort in understanding the paper, I am still not confident that I managed to extract all the important information out of the manuscript.

* More specifically, the paper lacks in clarity and the presentation of the main methodology needs improvement. The proposed algorithm is presented in a rather convoluted way and lacks a proper formulation. Apart from the general equation of the ELBO the rest of the methodology looks more like a guide to follow rather than a scientific presentation of a novel idea. The authors need to put significantly more effort in communicating better their work. It cannot be the case that out of a 10-page paper the whole idea is summarised/compressed in the last two small paragraphs of page 4.

* In these paragraphs (end of page 4) the authors describe the proposed idea of having a memory buffer to store latent samples. They also refer to their joint p(z, x). I have a couple of questions here. First of all, what is the form of it? Furthermore, do we use this probability as weights in order to later resample from the memory to train the generative model?

* Also, in Algorithm 1 different parameters in different distributions are all denoted as \theta. This is very confusing. I really struggle to follow the algorithm as none of the involved distributions is properly defined, and we do not even know what and how many parameters do we need to optimise. I am also confused with the flow of the algorithm. Is it a single objective that we optimise in one go as the gradient update suggests? Are we optimising in turns? What happened to the KL term of the variational bound? When/where is the prior been updated?

* Regarding the update of the recognition network in the sleep phase. Due to the nature of the optimisation scheme the recognition network is being updated after having access to the optimal (up to that point) variational parameters (do you learn a q(z)?) and prior p(z). This is definitely acceptable, however, it would be fair to do a similar two-step update for the VAE. A similar idea has been studied in [Krishnan et al. 2018].
[Krishnan et al. 2018] “On the challenges of learning with inference networks on sparse, high-dimensional data”. AISTATS 2018

* In the beginning of Section 3 the authors argue that their work considerably differs from a VAE, because the proposed algorithm tries to recover a distribution of a latent program that generates the observations rather than a latent vector. To my understanding---correct me if I am wrong but it is nowhere mentioned in the main text apart from the caption of Figure 1---a latent program is just a categorical distribution that spits out the alphabet needed to construct the observations. To me this is exactly the same as what VAEs do with just using a categorical variational distribution. The only difference is the discrete nature of the latent variable and the fact that you also try to learn the prior p(z), which you refer to it as inductive bias. Can the authors please comment on that?

* Regarding the discrete latent state and optimisation of VAE the authors use the REINFORCE algorithm. Can a trick similar to Gumbel-softmax [Jang et al. 2017] be exploited for the purposes of this work. Does it make any sense? If yes what are the implications?
[Jang et al. 2017]. “Categorical Reparameterization with Gumbel-Softmax”. ICLR 2017

* In the experiment in Figure 4 you compare wallclock time of the algorithms. In such a plot I would expect to see different algorithms terminating at different time. This is not the case in the plot, although you explicitly mention that the proposed algorithm requires several times more computation per iteration. Can you please explain? Also, there is no text explaining Figure 7.

---Minor issues---
* Page 3: “needn’t” → “need not”
* Page 7: “marginal likelihood p(x|z)” --&gt; this is not the marginal
* Page 7: “estimates a lower bound on p(x|z)” → the lower bound is on the marginal p(x).

Overall, based on my detailed comments above I believe that the paper is not ready for publication. Thus my recommendation is to reject.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>