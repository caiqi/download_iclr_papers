<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Ain't Nobody Got Time for Coding: Structure-Aware Program Synthesis from Natural Language | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Ain't Nobody Got Time for Coding: Structure-Aware Program Synthesis from Natural Language" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJe-Sn0ctm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Ain't Nobody Got Time for Coding: Structure-Aware Program Synthesis..." />
      <meta name="og:description" content="Program synthesis from natural language (NL) is practical for humans and, once technically feasible, would significantly facilitate software development and revolutionize end-user programming. We..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJe-Sn0ctm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Ain't Nobody Got Time for Coding: Structure-Aware Program Synthesis from Natural Language</a> <a class="note_content_pdf" href="/pdf?id=BJe-Sn0ctm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019ain't,    &#10;title={Ain't Nobody Got Time for Coding: Structure-Aware Program Synthesis from Natural Language},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BJe-Sn0ctm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Program synthesis from natural language (NL) is practical for humans and, once technically feasible, would significantly facilitate software development and revolutionize end-user programming. We present SAPS, an end-to-end neural network capable of mapping relatively complex, multi-sentence NL specifications to snippets of executable code. The proposed architecture relies exclusively on neural components, and is built upon a tree2tree autoencoder trained on abstract syntax trees, combined with a pretrained word embedding and a bi-directional multi-layer LSTM for NL processing. The decoder features a doubly-recurrent LSTM with a novel signal propagation scheme and soft attention mechanism. When applied to a large dataset of problems proposed in a previous study, SAPS performs on par with or better than the method proposed there, producing correct programs in over 90% of cases. In contrast to other methods, it does not involve any non-neural components to post-process the resulting programs, and uses a fixed-dimensional latent representation as the only link between the NL analyzer and source code generator. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Program synthesis, tree2tree autoencoders, soft attention, doubly-recurrent neural networks, LSTM, nlp2tree</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We generate source code based on short descriptions in natural language, using deep neural networks.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_r1xvolu9nX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting ideas, but no ablation experiments to attribute usefulness of the ideas</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJe-Sn0ctm&amp;noteId=r1xvolu9nX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1516 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1516 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes the Structure-Aware Program Synthesis (SAPS) system, which is an end-to-end neural approach to generate snippets of executable code from the corresponding natural language descriptions. Compared to a previous approach that used search in combination of a neural encoder-decoder architecture, SAPS relies exclusively on neural components. The architecture uses a pretrained GloVe embedding to embed tokens in the natural language description, which are then embedded into a vector representation using a bidirectional-LSTM. The decoder uses a doubly-recurrent neural network for generating tree structured output. One of the key ideas of the approach is to use a single vector point in the latent space to represent the program tree, where it uses a tree2tree autoencoder to pre-train the tree decoder. The results on the NAPS dataset show an impressive increase in accuracy of about 20% compared to neural-only baselines of the previous approach.

While overall the SAPS architecture achieves impressive practical results, some of the key contributions to the design of the architecture are not evaluated and therefore it makes it difficult to attribute the usefulness and impact of the key contributions. For example, what happens if one were to use pre-trained GloVe embeddings for embedding NL specifications in Polosukhin &amp; Skidanov (2018). Such and experiment would lead to a better understanding of how much gain in accuracy one can obtain just by using pre-trained embeddings.

One of the key ideas of the approach is to use a tree2tree autoencoder to train the latent space of program trees. The decoder weights are then initialized with the learnt weights and fine-tuned during the end-to-end training. What happens if one keeps the decoder weights fixed while training the architecture from NL descriptions to target ASTs? Alternatively, if one were to not perform auto-encoding based decoder pre-training and learn the decoder weights from scratch, how would the results look?

Another key point of the paper is to use a soft attention mechanism based on only the h^latent. What happens if the attention is also perform on the NL description embeddings? Presumably, it might be difficult to encode all of the information in a single h^latent vector and the decoder might benefit from attending over the NL tokens.

It was also not clear if it might be possible to perform some form of a beam search over the decoded trees to possibly improve the results even more? 

There are also other datasets such as WikiSQL and Spider for learning programs from natural language descriptions. It might be interesting to evaluate the SAPS architecture on those datasets as well to showcase the generality of the architecture.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJgAOA8q2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reasonable model but unclear results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJe-Sn0ctm&amp;noteId=BJgAOA8q2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1516 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1516 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value"># Summary

This paper introduces a model called SAPS for the task of mapping natural language descriptions of programs to the AST tree of the corresponding program. The model consists of a variation of a double recurrent neural network (DRNN) which is pre-trained using an autoencoder. The natural language description is turned into a latent vector using pretrained word embeddings and a bidirectional stacked LSTM. The final model consists of training this sentence embedding model jointly with the decoder of the autoencoder.

# Quality

The authors introduce a reasonable model which achieves good performance on a relevant task. The results section contains a fair amount of numbers which give some insight into the performance of the model. However, the results make it hard to compare the proposed model with other models, or with other training procedures.

For example, the Seq2Tree model that is shown in table 2 was not necessarily intended to be used without a search algorithm. It is also not mentioned how many parameters both models have which makes it hard to judge how fair of a comparison it is. (I couldn't find the dimensionality of the encoder in the text, and the decoder dimensionality is only shown in figure 2.)

The model proposed in this work uses decoder pretrained in an autoencoder setting. No results are shown for how the model performs without pretraining. Pretraining using autoencoders is a technique that fell out of favor years ago, so it seems worthwhile to investigate whether or not this pretraining is necessary, and if so, why and how it aids the final performance.

It is unclear to me what type of robustness the authors are trying to show in table 5. The use of robustness here is not clear (robustness is often used to refer to a network's susceptability to adversarial attacks or perturbations of the weights). It also seems that the type of "simple replacements" mentioned are very similar to the way the examples were generated in the first place (section 4 of Polosukhin). If the goal is to measure generalization, why do the authors believe that performance on the test set alone is not a sufficient measure of generalization?

Some smaller comments and questions:

* In section 4 you mention training the sentence-to-tree and the sentence-to-vector mappings. Isn't the sentence-to-vector model a subset of the sentence-to-tree model? Should I interpret this as saying that, given the pretrained decoder and the glove embeddings, you now train the entire model jointly? Or do you first learn the mapping from sentences to the latent space, and only then finetune the entire model?
* The attention mechanism is not actually an attention mechanism: Attention mechanisms are used to reduce a variable number of elements to a single element by learning a weighting function and taking a weighted sum. My understanding is that in this case, the input (the latent representation) is of fixed size. The term "gating function" would be more appropriate.
* You specify that the hidden states of the decoder are initialized to zero, but don't specify what the cell states are initialized to.

# Clarity

The writing in the paper is passable. It lacks a bit in structure (e.g., I would introduce the problem and dataset before introducing the model) and sometimes fails to explain what insights the authors draw from certain results, or why certain results are reported. Take table 3 as an example: As a reader, I was confused at first why I should care about the reconstruction performance of the autoencoder alone, considering its only purpose is pretraining. Then, when looking at the numbers, I am even more confused since it is counterintuitive that it is harder to copy a program than it is to infer it. At the end of the paragraph the authors propose an explanation (the encoder isn't as powerful as the decoder) but leave it unclear as to why these numbers were being reported in the first place.

In general, the paper would do well to restructure the text so that the reader is explained what the goal of the different experiments is, and what insights should be drawn from them.

A variety of smaller concerns and comments:

* Please reduce and harmonize the terminology in the paper: the terms latent-to-AST, NLP2Tree, NLP2Vec, tree2tree/tree-to-tree, sentence-to-tree, sentence-to-vector, NL-to-latent, and spec-to-latent all appear in the paper and several of them are redundant, making it significantly harder to follow along with the text.
* Avoid citing the same work multiple times within a paragraph; cite only the first use and use prose to make clear that future references are to the same work.
* Formula 14 has h_i^{(pred)} on both sides of the quation, and is used in the definition of A as well. I am assuming these two terms are actually the h_i^{(pred)} from equation 8, but this should be made clear in the notation.
* Why does figure 1 have boxes for "NL specification", "NL spec.", and "NL query"? In general, the boxes inconsistently seem to represent both values and operations.
* It is never explicitly stated that Seq2Tree is the model from the Polosukhin et al. paper, which is a bit confusing.
* Parameters is missing an -s in the first paragraph of section 4.
* It is said that regularization is applied to layer normalization, which I assume means that the regularization is applied to the gain parameters of the layer normalization.
* It says "like a in the above example" when table 1 is rendered at the bottom of the page by Latex.

# Originality and significance

The paper introduces model variations that the authors claim improve performance on this particular program synthesis problem. In particular, in the DRNN decoder the hidden state is never reset for the "horizontal" (breadth-first order) decoder, and each node is only allowed to attend over the latent representation of the program. The authors claim this means their model "significantly diverges from [other] works", which seems hyperbolical.

The main contribution of this work is then the performance on the program synthesis task of Polosukhin and Skidanov. However, the model fails to improve on the Seq2Tree-guided search approach, so its main claimed benefit is that it is trained end-to-end. Although there is a strong trend in ML research to prefer end-to-end systems, it is worthwhile to ask when and why end-to-end systems are preferred. It is often clear that they are better than having separately learned components that are combined later. However, this does not apply to the model from Polosukhin et al., which consists of a single learned model being used in a search, which is a perfectly acceptable technique. In comparison, translations in neural machine translation are also produced by performing a beam search, guided by the probabilities under the model.

The paper would be significantly stronger if it could show that some alternative/existing method (e.g., a standard DRNN, or a Seq2Tree model with the same number of parameters, or a non-pretrained network) would fail to solve the problem where the authors' proposed method does not. However, the single comparison with the Seq2Tree model does not show this.

# Summary

Pros:

* Reasonable model, extensive results reported
* Decently written

Cons:

* Unclear how the performance compares to other models
* Not well justified why end-to-end methods would be better than guided-search based methods
* Model architectural differences seem relatively minor compared to original DRNN
* The pretraining using an autoencoder and the use of pretrained word embeddings seems arbitrary and is not critically evaluated
* Lack of coherent story to several results (the autoencoder performance, robustness analysis)</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1lYrsiB3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Autoencoder used for program synthesis</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJe-Sn0ctm&amp;noteId=B1lYrsiB3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1516 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1516 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The submission proposes to combine a tree2tree autoencoder with a sequence encoder for natural language. It uses the autoencoding objective to appropriately shape the latent space and train the decoder, and then uses a second training step to align the output of a sequence encoder with the input for the tree decoder. Experiments on a recent dataset for the natural language-to-code task show that the proposed model is able to beat simple baselines.

There's much to like about this paper, but also many aspects that are confusing and make it hard to tease out the core contribution. I'm trying to reflect my understanding here, but the authors could improve their paper by providing an explicit contribution list. Overall, there seem to be three novel things presented in the paper:
(1) (Pre)training the (program) tree decoder using an autoencoder objective
(2) The doubly-recurrent tree decoder, which follows a different signal propagation strategy from most other approaches.
(3) An "attention" mechanism over the point in latent space (that essentially rescales parts of the decoder input)

However, the experiments do not evaluate these contributions separately; and so their relative merits remain unclear. Primarily, I have the following questions (for the rebuttal, and to improve the paper):

Re (1):
 (a) Does the pre-training procedure help? Did you evaluate joint end-to-end training of the NL spec encoder and the tree decoder? 
 (b) The auto-encoder objective would allow you to train on a larger corpus of programs without natural language specifications. Arguably, the size of the dataset is insufficient for most high-capacity deep learning models, and as you use word embeddings trained on a much larger corpus...), you could imagine training the autoencoder on an additional corpus of programs without NL specs. Did you attempt this?

Re (2): 
 (a) The tree decoder is unusual in that (one) part of the recurrence essentially enforces a breadth-first expansion order, whereas almost all other approaches use a depth-first technique (with the only exception of R3NN, as far as I remember). You cite the works of Yin &amp; Neubig and Rabinovich et al.; did you evaluate how your decoder compares to their techniques? (or alternatively, you could compare to the absurdly complex graph approach of Brockschmidt et al. (arxiv 1805.08490)))
 (b) Ablations on this model would be nice: How does the model perform if you set the horizontal (resp. the vertical) input to 0 at each step? (i.e., ablations to standard tree decoder / to pure BFS)

Re (3): This is an unusual interpretation of the attention mechanism, and somewhat enforced by your choice (1). If you run an experiment on end-to-training (without the autoencoder objective), you could use a standard attention mechanism that attends over the memories of the NL encoder. I would be interested to see how this would change performance.

As the experimental evaluation seems to be insufficient for other researchers to judge the individual value of the paper's contribution, I feel that the paper is currently not in a state that should be accepted for publication at ICLR. However, I would be happy to raise my score if (some) of the questions above are answered; primarily, I just want to know if all of the contributions are equally important, or if some boost results more than others.


Minor notes:
- There are many spelling mistakes ("snipped" for "snippet", "isomorhpic", ...) -- running a spell checker and doing a calm read-through would help with these details.
- page1par2: Writing specifications for programs is never harder than writing the program -- a program is a specification, after all. What you mean is the hardness of writing a /correct/ and exact spec, which can be substantially harder. However, it remains unclear how natural language would improve things here. Verification engineers will laugh at you if you propose to "ease" their life by using of non-formal language...
- page1par3: This (and the rest of the paper) is completely ignoring the old and active field of semantic parsing. Extending the related work section to compare to some of these works, and maybe even the experiments, would be very helpful.
- page2par3 / page6par4 contradict each other. First, you claim that mostly normal english vocabulary is used, with only occasional programming-specific terms; later you state that "NL vocabulary used in specifications is strongly related to programming". The fact that there are only 281 (!!!) unique tokens makes it very doubtful that you gain anything from using the 1.9million element vocab of GLoVe instead of direct end-to-end training...
- page4par3: You state "a reference to a previously used variable may require 'climbing up' the tree and then descending" - something that your model, unlike e.g. the work of Yin &amp; Neubig, does not support. How important is this really? Can you support your statement by data?
- page5, (14) (and (13), probably): To avoid infinite recursion, the $h_i^{(pred)}$ on the right-hand-side should probably be $h_{i-1}^{(pred)}$

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>