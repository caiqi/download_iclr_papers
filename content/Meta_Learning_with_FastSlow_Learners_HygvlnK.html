<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Meta Learning with Fast/Slow Learners | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Meta Learning with Fast/Slow Learners" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Hygvln09K7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Meta Learning with Fast/Slow Learners" />
      <meta name="og:description" content="Meta-learning has recently achieved success in many optimization problems. In general, a meta learner g(.) could be learned for a base model f(.) on a variety of tasks, such that it can be more..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Hygvln09K7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Meta Learning with Fast/Slow Learners</a> <a class="note_content_pdf" href="/pdf?id=Hygvln09K7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019meta,    &#10;title={Meta Learning with Fast/Slow Learners},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Hygvln09K7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Meta-learning has recently achieved success in many optimization problems. In general, a meta learner g(.) could be learned for a base model f(.) on a variety of tasks, such that it can be more efficient on a new task. In this paper, we make some key modifications to enhance the performance of meta-learning models. (1) we leverage different meta-strategies for different modules to optimize them separately: we use conservative “slow learners” on low-level basic feature representation layers and “fast learners” on high-level task-specific layers; (2) Furthermore, we provide theoretical analysis on why the proposed approach works, based on a case study on a two-layer MLP. We evaluate our model on synthetic MLP regression, as well as low-shot learning tasks on Omniglot and ImageNet benchmarks. We demonstrate that our approach is able to achieve state-of-the-art performance.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">computer vision, meta learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We applied multiple meta-strategy to improve meta-learning performance on base CNNs. </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Bke71ZD5nX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The paper addresses fast/slow learning modules in deep networks. Good paper. Needs more clarity/work. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hygvln09K7&amp;noteId=Bke71ZD5nX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1085 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1085 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The overall contribution makes sense. Consider solving a linear system i.e., learning an unknown matrix. Splitting it into two components (like in NMF or MMF) and learning each separately gives more control on the conditioning of the matrices. This is the basis of residual networks (at least the theory for linear resnets). Within this, the technical/theoretical results presented in the paper are sensible. Couple of issues: 
1) Where are we breaking the slow/fast learners in terms of the depth of the network? I.e., How many of the layers are slow? Does this break point influence the overall convergence? 
2) It is unclear what the aim of simulations is? The reported figures are not conveying useful information. It makes sense to do a repeatability experiment here with multiple sets of simulated datasets. 
3) Put confidence intervals on the results (table/figure). 
4) What is the nature and choice of g()? The evaluations uses LSTM but will the structure of g() influence the rate of learning? 
5) The authors should choose a better reference than miracle for the </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1gXHkH5hm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting treatment to meta-learning with fast/slow learning components. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hygvln09K7&amp;noteId=S1gXHkH5hm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1085 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1085 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">[Summary]
The paper presents a novel learning framework for meta-learning that is motivated by neural learning process of human over long periods. Specifically, the process of meta-learning is divided into a slow and a fast learning modules, where the slowly-learnt component accounts for low-level representation that is progressively optimized over all data seen so far to achieve generalization power, and the fastly-learnt component is supposed to pick up the target in a new task for quick adaptation. It is proposed that meta-learning should focus on capturing the meta-information for the fast learning module, and leave the slow module being updated steadily without task-specific adaptation. Theoretical analysis is presented on a linear MLP examples to shed some light on the properties of the proposed algorithm. Results on both synthetic dataset and benchmarks justify the theoretical observation and advantages.               

Pros
Novel treatment and formulation of meta-learning from the perspective of fast and slow  learning process
Cons
Some interesting cases not tested
Presentation could be improved 

[Originality]
The paper approaches the recently popular meta-learning from a novel perspective by decomposing the learning process into slow and fast ones. 

[Quality]
Overall,  the paper is well motivated and implemented with both theoretical study and empirical justification. There are a few questions / areas for further improvements, though:
- It seems that to initialize the slow module, another set of data is needed to pretrain it before the actual meta-learning takes place to learn to optimize the fast learner (as opposed to other meta-learning methods where all parameters in a base model were meta-learnt over the meta training set). How does this affect the performance? E.g., what if the slow module is only updated over the meta-training set (still without reinitialization across different batches) without pre-training?
- In the current formulation, the base model is decomposed into two distinct (slow and fast) modules. What is the rule to decide which layers should belong to slow or fast modules? How does different choice affect the performance? Can we decompose the base model into finer granularities for different learning behaviors? E.g., a third module module in-between the fast and slow ones that follows medium learning pace.          
- The theoretical study can be better organized. The proofs can be left in appendix to make room for more discussion on conclusions, non-linear and / or non-Gaussian cases.     
- The write-up can be improved too at some places: proper reference at line 4 of section 1 is missing; \phi in (1) is not well defined, as well as “SOA” in section 2;

[Clarity]
The paper is generally clearly written, with a few places to improve (see comments above).

[Significance]
The paper brings in an interesting perspective to meta-learning. It can also inspire more follow-up work to better understand the problem.   
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJlSVKRY2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Training slow and fast learners using different strategies is an interesting idea. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hygvln09K7&amp;noteId=SJlSVKRY2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1085 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1085 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">[Summary:]
This paper presents a meta-learning architecture where the slow learner is trained by SGD and the fast learner is trained according to what the meta-learner guides. CNN is split into two parts: (1) bottom conv layers devoted to learn meaningful representation, which is referred to as slow learner; (2) top-fully connected layers involving task-specific fast learners. As in [Andrychowicz et al., 2016], the meta-learner guides the training of task-specific learners. In addition, slow learners are trained by SGD. The motivation is that low-level features should be meaningful everywhere while high-level features should vary wildly. They introduce “miracle representations” and prove that fast/slow learning on a two-layer linear network should converge to somewhere near this miracle representation. They evaluate on few-shot classification benchmarks to evaluate how well this fast/slow meta-learning approach works.

[Strengths:]
The paper has a clear motivation. It is easy to read. Training slow/fast learners using different strategies is an interesting idea. 

[Weaknesses:]
- The technique used in this work is a mix of SGD and  [Andrychowicz et al., 2016].
- The analysis is limited to a simple two-layer linear network. It is not clear whether this analysis is carried over to the proposed deep nets. 
- Quantitative results did not compare to recent results such as Reptile[1] or MT-Nets[2].

[Specific comments:]
- The current work is an improvement over [Andrychowicz et al., 2016], claiming that training conv layers and fully-connected layers with different strategies improves the generalization. I am wondering why the comparison to [Andrychowicz et al., 2016] is missing. You can use (fully) pre-trained CNN (which already learns meaningful representation using a huge amount of data) in the framework of [Andrychowicz et al., 2016]. 
-As one of the points of the paper is that this meta-learning strategy enables life-long learning, it would have been nice to see an experiment using this, where the distribution of tasks changes as time goes on.
-The paper says SOA(State Of the Art); I think the term SOTA(State Of The Art) is more commonly used.
-The use of the term “miracle” keeps changing(miracle solution, miracle representation, miracle W, miracle knowledge); the paper would be clearer if only one “miracle X” was defined and used as these are all essentially saying the same thing.

References
[1]<a href="https://arxiv.org/abs/1803.02999" target="_blank" rel="nofollow">https://arxiv.org/abs/1803.02999</a>
[2]https://arxiv.org/abs/1801.05558

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>