<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>On Accurate Evaluation of GANs for Language Generation | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="On Accurate Evaluation of GANs for Language Generation" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rJMcdsA5FX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="On Accurate Evaluation of GANs for Language Generation" />
      <meta name="og:description" content="Generative Adversarial Networks (GANs) are a promising approach to language generation. The latest works introducing novel GAN models for language generation use n-gram based metrics for evaluation..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rJMcdsA5FX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>On Accurate Evaluation of GANs for Language Generation</a> <a class="note_content_pdf" href="/pdf?id=rJMcdsA5FX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019on,    &#10;title={On Accurate Evaluation of GANs for Language Generation},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rJMcdsA5FX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Generative Adversarial Networks (GANs) are a promising approach to language generation. The latest works introducing novel GAN models for language generation use n-gram based metrics for evaluation and only report single scores of the best run. In this paper, we argue that this often misrepresents the true picture and does not tell the full story, as GAN models can be extremely sensitive to the random initialization and small deviations from the best hyperparameter choice. In particular, we demonstrate that the previously used BLEU score is not sensitive to semantic deterioration of generated texts and propose alternative metrics that better capture the quality and diversity of the generated samples. We also conduct a set of experiments comparing a number of GAN models for text with a conventional Language Model (LM) and find that none of the considered models performs convincingly better than the LM.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">GANs, Evaluation, Generative Models</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We discuss how to evaluate GANs for language generation, propose a protocol and show that simple Language Models achieve results as good as GANs.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_B1x_zGoKa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJMcdsA5FX&amp;noteId=B1x_zGoKa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper379 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper379 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank the reviewers for their insightful comments and feedback.

We agree that it is well known that metrics like BLEU have important deficiencies. However, they are still widely used to claim that a given model is SOTA, which is something the community needs to move away from, and we believe further evidence can help. Human evaluations are also not sufficient as it is almost impossible to capture diversity for the unconditional text generation setting, beyond detecting severe mode collapse, as also noted by R3. Our goal was to choose a metric that is capable of capturing both quality and diversity in automated manner.

We agree that it would be nice to have more text GAN models implemented and compared in the same setting. Each model reproduction corresponds to a very large amount of work, and it is not feasible to have one group reproduce all of them. Ideally a community effort would be possible to organize, but that's beyond the scope of this work. We instead focused on some representative publications; the main value of our paper is on pointing out some of the pitfalls of common evaluation protocols and metrics, and propose a methodology that future works can take inspiration from. The  models that we evaluate are examples of the fact that conclusions can be different if one follows a more careful experimental setup.

Regarding comments that some of our contributions have been known to the community, e.g. that reporting single best value is misleading: we agree with that these issues have been raised before, however this approach to evaluation of language GANs is still widely accepted in the literature. In addition, these concerns have been dominantly voiced in the Computer Vision community and has not made their way to the Language Generation works. We are convinced that it is important to explicitly state and demonstrate these issues when working in the Language domain.

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hyl9e1IihX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Experimental paper studying an important problem with insufficient / unsurprising conclusions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJMcdsA5FX&amp;noteId=Hyl9e1IihX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper379 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper379 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper tackles the problem of evaluation of language generation models and particularly focuses on the comparison between GAN-based language models (GAN-LM) vs likelihood-based language models (MLE-LM). Studying the behaviour of current evaluation metrics for language generation as well as finding new ones is an important research topic. I believe that this paper makes a step in the right direction but the magnitude of that step may be insufficient for publication. I appreciate the efforts but I find most of the findings about BLEU not being a good metric and characteristic of reverse PPL rather unsurprising. The majority of the paper is dedicated to describing models / metrics which are well-known instead of performing more solid experimental evaluation (Results start at page 6). Instead, the authors could have focused more on the study of FD for language generation.

-- Details

-"Our main finding is that, when compared carefully, a conventional neural Language Model performs at least as well as any of the tested GAN models", however the authors don't compare with the recent MaskGAN model, which, according to (<a href="https://arxiv.org/abs/1801.07736)" target="_blank" rel="nofollow">https://arxiv.org/abs/1801.07736)</a> outperforms MLE variants.

- "We demonstrate that previously used n-gram matching, such as BLEU scores, is an insufficient metric": the fact that BLEU is not ideal for evaluation natural language generation has been pointed out in multiple related papers (e.g. https://arxiv.org/abs/1603.08023) and thus is not surprising.

- "We find that reporting results from the best single run or not performing sufficient tuning introduces significant bias in the reported results": as the authors point out in related works, the variance in GAN results which hinders meaningfulness of the reported results is a also a well-known problem (e.g. https://arxiv.org/pdf/1711.10337.pdf), therefore cannot be considered as a contribution.

- The observed behaviour (sensitivity to mode collapse, word swap, word removal) of the "reverse PPL" metric is pretty much expected, but I agree some experimental results are still interesting.

- On the contrary, I liked the study on the FD metric but I would have loved the paper to focus more on the study of the behaviour of that metric: for example, by examining the robustness under different base models, while the authors only test with the model by Conneau et. al, 2017.

- It would have been good to train a state-of-the-art language model architecture, e.g. AWD-LSTM, and to control regularization. I cannot see if the MLE-LM model is overfitting or not.

-- Style remarks:

- Moving the figures closer to the paragraph where they are described avoids the reader the burden of going back and forth through the paper.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkxM_MoYTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Regarding testing a number of embedding models for use with FD</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJMcdsA5FX&amp;noteId=BkxM_MoYTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper379 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper379 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have also experimented with two sequence embedding models from Universal Sentence Encoder (<a href="https://arxiv.org/abs/1803.11175)" target="_blank" rel="nofollow">https://arxiv.org/abs/1803.11175)</a>  and found behavior of FD to be generally the same regardless of the base model. The results are presented in Figure 1, row 2. We agree that a better discussion of this point will be helpful.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkxfeqp5hX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Valuable goal, but very limited executions with incorrect claims</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJMcdsA5FX&amp;noteId=rkxfeqp5hX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper379 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper379 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper sets out to improve the evaluation of GAN models as e.g. the previously used BLEU score is not sensitive to semantic deterioration of generated texts. The paper claims to “propose alternative metrics that better
capture the quality and diversity of the generated samples”.


Strengths:
-	The paper has a valuable goal
-	Some of the evaluations are interesting.


Weaknesses:
1.	The claim of the paper to “propose alternative metrics that better capture the quality and diversity of the generated samples” is not met in multiple ways:
a.	The paper seems not to propose any new metrics but evaluate existing ones.
b.	The metrics are not extensively compared to human judgments, e.g. by computing correlation. In fact, Figure 5 suggests that they are not very well correlated.
c.	The diversity is not explicitly studied on generated text samples.
2.	The paper concludes that the human eval “assigns better scores to the Language Model”, which is incorrect as Seq gan scores 3.49 vs. 3.37 for language model (even if the seq gan has higher variance).
3.	The metrics are not very well defined, e.g. with formulas, although this is one of the central points of the paper. e.g. what are the reference the blue score is computed against?

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJlB6-it67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Regarding metrics being not very well defined</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJMcdsA5FX&amp;noteId=SJlB6-it67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper379 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper379 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We agree that it would be best to have a self-contained paper, but we have opted for referencing papers introducing metric as they have a much better introduction and discussions of these metrics. The reference used to compute BLEU scores is the full validation set. This is an accepted reference in the language GANs community and one of our goals is to show that it leads to misleading results.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkg6QeB5hX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>this paper seems to be timely for this line of work </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJMcdsA5FX&amp;noteId=rkg6QeB5hX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper379 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper379 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Contributions:

The main contribution of this paper is the study of the currently adopted evaluation metrics for textual GAN models. It was shown that BLEU and Self-BLEU scores used by previous work are insufficient to evaluate textual GAN models, and the authors propose that Frechet Distance and reverse Language Model scores can be a good complement to the above BLEU score evaluations. 

Detailed Comments:

(1) Novelty: It seems to me that this paper is timely, as developing GAN models for text generation gains more and more attention in the research community, and it is indeed much needed to provide good evaluation methods. The proposed new metrics seem proper, and the observation that most GAN models do not yield obviously better results than conventional LM is also insightful. 

(2) Presentation: This paper is generally well-written and easy to follow. However, when discussing related work in section 3.1, I think one literature [*] is missed. It uses annealed softmax to approximate argmax for textual GAN. 

[*] Adversarial Feature Matching for Text Generation, ICML 2017

(3) Evaluation: The experiments are generally well-executed, with some questions listed below.

Questions:

(1) I have some concerns in terms of human evaluation. Though human evaluation is the golden metric, it seems that presenting individual sentences to human raters does not account diversity into consideration. Therefore, systems that generate high quality samples but with less diversity will get a high score in terms of human evalution. Can the authors provide some discussion on this? And if this is the case, how will this change the conclusions in this paper?

(2) I understand why the authors use simplified GAN models for evaluation. However, if the models are not simplified, what the performance will be for LeakGAN and MaskGAN, for example? This seems to be relatively easy to evaluate since the code is open sourced. 

Minor issues:

(1) I think the citation format needs to be changed. For example, in many places, it is more natural to use "(Hassan et al., 2018)" than "Hassan et al. (2018)" for example. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>