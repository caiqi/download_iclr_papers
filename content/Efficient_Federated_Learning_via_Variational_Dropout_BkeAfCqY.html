<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Efficient Federated Learning via Variational Dropout | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Efficient Federated Learning via Variational Dropout" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BkeAf2CqY7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Efficient Federated Learning via Variational Dropout" />
      <meta name="og:description" content="As an emerging field, federated learning has recently attracted considerable attention.&#10;  Compared to distributed learning in the datacenter setting, federated learning&#10;  has more strict constraints on..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BkeAf2CqY7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Efficient Federated Learning via Variational Dropout</a> <a class="note_content_pdf" href="/pdf?id=BkeAf2CqY7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019efficient,    &#10;title={Efficient Federated Learning via Variational Dropout},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BkeAf2CqY7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">As an emerging field, federated learning has recently attracted considerable attention.
Compared to distributed learning in the datacenter setting, federated learning
has more strict constraints on computate efficiency of the learned model and communication
cost during the training process. In this work, we propose an efficient
federated learning framework based on variational dropout. Our approach is able
to jointly learn a sparse model while reducing the amount of gradients exchanged
during the iterative training process. We demonstrate the superior performance
of our approach on achieving significant model compression and communication
reduction ratios with no accuracy loss.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">federated learning, communication efficient, variational dropout, sparse model</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">a joint model and gradient sparsification method for federated learning</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HJlEpB7ZaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Lack of comparisons</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkeAf2CqY7&amp;noteId=HJlEpB7ZaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1313 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1313 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper applies variational dropout to reduce the communication cost of distributed training of neural networks. The authors do experiments on mnist, cifar10 and svhn datasets. The technique is simple and easy to understand.

However, I think this paper has some problems.
1. Novelty
Applying variational dropout for model compression is not a new idea. As this paper only combines variational dropout and distributed training, I think the novelty is a little thin.

Reference:
Louizos, Christos, Karen Ullrich, and Max Welling. "Bayesian compression for deep learning." Advances in Neural Information Processing Systems. 2017.

2. Comparisons
Need to compare with other methods that reduce the amount of communication by sparsifying gradients. 

3. Larger dataset &amp; networks
Cifar10 and SVHN is not interesting for distributed training. The data / model size differs significantly with the data / models that really needs distributed training, e.g., ImageNet. The authors should at least do ImageNet and a few state-of-the-art models to make the paper convincing. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rylaMW4kpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A simplistic approach for improving cost of federated learning with unimpresive performance</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkeAf2CqY7&amp;noteId=rylaMW4kpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1313 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1313 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In federated learning the data are kept privately on each device, gradients from each device are sent to the server, and the server sends back the averaged gradients to each device to update model weights. The authors propose an algorithm that reduces communication costs by sending sparse gradients from device to server and back. The sparsification relies on the variational dropout and sets partial derivatives to zero if the weights are sufficiently uncertain, where the uncertainty threshold is a hyperparameter. As a by-product of the algorithm, the size of the model is also reduced somewhat. The proposed algorithm is evaluated on several benchmark data sets using several benchmark neural network architectures and the results indicate a reduction in communication costs are compared to a non-sparsified variant of federated learning 

Strengths:
â€¢	The algorithm jointly leads to compression of a neural network and reduction in communication cost of federated learning.
â€¢	The paper is well organized and clearly written.

Weakness:
â€¢	The novelty is low: the paper is a straightforward application of the variational dropout paper [Molchanov et al., 2017], the federated learning setup is standard , and the architecture in Figure 1 is very similar to distributed deep learning [see Felix Sattler 2018].
â€¢	There are some questions related to threshold T: if it is set too high, the model will not be compressed and communication cost will be low, but if it is set too low, the accuracy could be significantly impacted. It seems reasonable to apply an adjustable threshold, but the paper is not discussing this issue.
â€¢	Some details are missing: how to determine threshold T in algorithm 1. is the model compressed in each iteration, is the model compressed using the constant threshold T?
â€¢	The paper does not compare with other state of the art federated learning algorithms: as listed in the related work, there are papers that compress the model, and also papers that sparsify gradients. Is the proposed variational dropout better in compression models or reducing communication cost compared to them?
â€¢	The reported improvements in communication cost and reductions in model size are relatively minor
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1le8m0ph7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkeAf2CqY7&amp;noteId=H1le8m0ph7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1313 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I enjoying reading this paper and find the authors reduce the communication cost from the perspective of the weight rather than the gradients side. It is a new framework and hopes it can gets in. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1elAazthX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Several technical flaws. Experiments are not representative of federated learning.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkeAf2CqY7&amp;noteId=B1elAazthX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1313 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1313 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper combines distributed optimization algorithm with variational dropout to sparsify the gradients sent to master server from local learners.

My major concerns are related to technical issues with the algorithm and overly simplified treatment of federated learning. I think almost every section of this paper has problems of varying severeness. I will give comments on every section in the order of its appearance in the paper.

Introduction:
"Federated Learning uses some form of distributed stochastic gradient descent". Distributed SGD is certainly one of the many approaches to federated learning, but it is hardly suited for the definition of federated learning.

"Each device then updates the model parameters using the averaged gradients" - this seems somewhat unconventional to me to be presented in the introduction. More often global server performs the update and sends the parameters back to local learners. The two are equivalent if communication is performed after each iteration, however it is unclear how to implement this scheme if communication frequency needs to be restricted.

While model size constraint and communication constraint are important in federated learning, there are multiple other challenges. In particular, data is often unbalanced and non-iid, which is fully ignored by the authors. Privacy and communication frequency are another important aspects of federated learning. Even if authors choose to only address a subset of federated learning challenges, the problem has to be fully described.

Preliminaries:
"One popular approximation approach is variational inference, which uses a parametric distribution to approximate the true posterior distribution (Kingma &amp; Welling, 2013)." While work of Kingma &amp; Welling is certainly important, I do not think it is appropriate as a stand alone citation in the context of the sentence. Variational inference has been studied for over 20 years by many researchers and this should be acknowledged with appropriate references in the general sentence chosen by the authors.

"In traditional dropout training, either Bernoulli or Gaussian noises are added to the weights" - dropout is not an additive noise. Weights are multiplied by the corresponding noise variables. This mistake appears at least one more time in the preliminaries section.

Eq. 3: I can hardly find it useful to provide equation with several numerical values of unknown origin. Indeed looking at the corresponding reference I could not find an equation with same numbers. Please elaborate or give literature reference with specific equation number.

Algorithm section:
Equation for the gradient of $\alpha$ does not seem to make any sense. It essentially implies that after $\alpha_{ij}&gt;T$, corresponding $\alpha_{ij}$ will never be updated again as its gradient is zeroed out. In other words, if the weight is ever masked by dropout, it can never appear again. This issue seems to be confirmed by the later comment "Second, Î¸ij is suppressed by large Î±ij and it can hardly grow back unless DKL in (1) is removed."

"since Î± is associated with Î¸ which is synchronized every iteration during SGD, Î± will thus be forced to be almost the same across all the devices" - I do not see why this is the case. Especially if the authors were to consider imbalanced non-iid partitioning, local Î± can easily diverge from each other. Moreover, if Î± differ across local learners, there is no reason for the average of sparse gradients, computed on the server, to be sparse. This seems to defeat the whole purpose of the proposed method since the communication from server to local devices will be inefficient.

Experiments:
"five datasets that fit the federated learning setting" - datasets considered can hardly be associated with federated learning in my opinion. They can be used to simulate federated learning scenario by partitioning with class and size imbalance. Instead "Each dataset is randomly and evenly divided into N non-overlapping parts for N devices", which, as I mentioned earlier, is not representative of federated learning. Moreover I would encourage to consider larger number of devices for the experiments.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>