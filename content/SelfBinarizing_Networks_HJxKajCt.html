<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Self-Binarizing Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Self-Binarizing Networks" />
        <meta name="citation_author" content="Fayez Lahoud" />
        <meta name="citation_author" content="Radhakrishna Achanta" />
        <meta name="citation_author" content="Pablo Márquez-Neila" />
        <meta name="citation_author" content="Sabine Süsstrunk" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HJxKajC5t7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Self-Binarizing Networks" />
      <meta name="og:description" content="We present a method to train self-binarizing neural networks, that is, networks that evolve their weights and activations during training to become binary. To obtain similar binary networks..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HJxKajC5t7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Self-Binarizing Networks</a> <a class="note_content_pdf" href="/pdf?id=HJxKajC5t7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=fayez.lahoud%40epfl.ch" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="fayez.lahoud@epfl.ch">Fayez Lahoud</a>, <a href="/profile?email=radhakrishna.achanta%40epfl.ch" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="radhakrishna.achanta@epfl.ch">Radhakrishna Achanta</a>, <a href="/profile?email=pablo.marquez%40artorg.unibe.ch" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="pablo.marquez@artorg.unibe.ch">Pablo Márquez-Neila</a>, <a href="/profile?email=sabine.susstrunk%40epfl.ch" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="sabine.susstrunk@epfl.ch">Sabine Süsstrunk</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We present a method to train self-binarizing neural networks, that is, networks that evolve their weights and activations during training to become binary. To obtain similar binary networks, existing methods rely on the sign activation function. This function, however, has no gradients for non-zero values, which makes standard backpropagation impossible. To circumvent the difficulty of training a network relying on the sign activation function, these methods alternate between floating-point and binary representations of the network during training, which is sub-optimal and inefficient. We approach the binarization task by training on a unique representation involving a smooth activation function, which is iteratively sharpened during training until it becomes a binary representation equivalent to the sign activation function. Additionally, we introduce a new technique to perform binary batch normalization that simplifies the conventional batch normalization by transforming it into a simple comparison operation. This is unlike existing methods, which are forced to the retain the conventional floating-point-based batch normalization. Our binary networks, apart from displaying advantages of lower memory and computation as compared to conventional floating-point and binary networks, also show higher classification accuracy than existing state-of-the-art methods on multiple benchmark datasets.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Binarization, Convolutional Neural Networks, Deep Learning, Deep Neural Networks</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A method to binarize both weights and activations of a deep neural network that is efficient in computation and memory usage and performs better than the state-of-the-art.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HklHr8GsTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxKajC5t7&amp;noteId=HklHr8GsTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper823 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper823 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJlDEyzo67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Very Similar Work Published Last year</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxKajC5t7&amp;noteId=rJlDEyzo67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper823 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Authors,

I am afraid an extremely similar work has already been published about a year ago [1]. There might be differences, though, at a first glance it appears the teo approaches are identical. I would urge the authors to take a look at [1] and identify how their work differs from it and include a discussion in their related works section (at least).

Cheers.

[1] Sakr, Charbel, et al. "True Gradient-Based Training of Deep Binary Activated Neural Networks Via Continuous Binarization." 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hyx3pFqK2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Self-Binarizing Networks</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxKajC5t7&amp;noteId=Hyx3pFqK2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper823 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper823 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This is a good paper on an important topic of low precision representation in neural network weights and signal representations. As we move more and more into making neural networks deployed in low power applications such as mobile and wearable devices, this topic is of increasing importance. This is an area explored by other researchers -- and the manuscript does a good job of surveying related work. The novel contribution in the paper is to hide neural network weights and activation at nodes behind use hyperbolic tangent functions, thereby working in a continuous space in which differentiation is straightforward. The sharpness of hyperbolic tangent is gradually increased (in an ad-hoc way every epoch), so that gradually everything become binary. A clearly written paper. However, the empirical work reported is somewhat weak: (a) there is no uncertainty provided in the results of Table 1. When the differences in performance quoted are so small, it is important to give uncertainties (by cross validation) so that a reader can judge the significance of the results; (b) very little effort is made to carry out an analysis of errors; where do the gains come from? If, for example, one looks at the confusion matrix of the ten-class problem, can one identify where the differences are -- are they random or is there anything systematic one can pull out? Lack of error analysis is particularly striking when one notes that in both the CIFAR10 and CIFAR100 problems, there are instances where the binary low precision method actually outperforms the full precision method. How is this so? (c) following on from this, what does the paper bring to this particular conference on "learning representations"? Is there anything we can pull out from the work of representing features in a binary space that is specific to the problems considered? 
In conclusion, the paper has a novel idea I like, it is explained clearly, but the work has to mature a bit more in terms of  empirical work and interpretation of results.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1xlRZsE3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Not enough contribution and not very clear experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxKajC5t7&amp;noteId=S1xlRZsE3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper823 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper823 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presnets a different way to train a binary network. Instead of using the sign function for the forward pass and the straight trough approximation for the backward pass, as in previous work, it uses continuation, i.e. weights and activations are converted to the range [-1,1] with a hyperbolic tangent, with a multiplicative factor that initially is low, and then though iterations becomes larger and forces the output of htan to be close to the binary values. The paper also uses a different way to compute batch normalization (BN), which manages to use 8 bits fixed point instead of 32 bits floating point. Results are shown on CIFAR 10, 100 and ImageNet, and compared to previous approaches.

Pros:
- The paper is well written
- The idea of fully avoiding float32 is intriguing, but it would be much more interesting if it can be done also at training time.

Cons:
- Reading the introduction gives a high expectation about a method that can actually learn directly on binary values. Then, in practise, the real contribution of the paper is just a different binarization of the network and a different binary BN.
- The first contribution of the paper is to use continuation instead of straight trough for using back-propagation. To me it makes a lot of sense, but in the experimental part I could not clearly see if the improvement in performance is due to this representation of the binarized BN.
- The second contribution of the paper is on batch nomralization. The authors show another way to compute jointly the network activation and batch normalization using a 8 bit fixed point instead of a 32 bit floating point. This can help on practical implementations to use the network on machines that do not have floating point operations. However, I do not see this contribution enough for a ICLR paper.
- The presentation of the results seems biased. In tab. 1, the authors show that their approach requires only 1 bit for the weights and 1 bit for their binary BN. However, they also use a 8 bit fixed point for BN, but it is not shown. 
- In Fig. 3, the improvement in computational cost and memory is only for the binarized BN. Globally I think that this is not affecting much the entire network speed and memory. Can the authors say more about that? 

Global Evaluation:
I thinks that a representation of a binary net based on a continuation approach is quite interesting. However, in the experimental results the authors did not really clarify if the obtained improvements are really due to this part or not. More experiments and an ablation study is necessary. The second contribution about binarized BN does not seem very important to me. It can reduce the BN computational cost and memory consumption, but globally,in terms of the full network it does not change much.


Additional comments:
- Fig. 1 is quite confusing. There are many bars and it is not clear what they represent in the network.
- I do not think it is necessary to describe all the method for reducing networks computation in related work
- in related work: "which requires only one bit to represent" needs an object or a passive form.
- missing the very relevant reference to: "How to Train a Compact Binary Neural Network with High Accuracy?" Wei Tang, Gang Hua, Liang Wang.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJlOUBBEnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Simple but not vey novel bianrization method for weights and activations</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxKajC5t7&amp;noteId=HJlOUBBEnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper823 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper823 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose to use a scaled hyperbolic tangent function (soft quantization function) to mimic the hard quantization function, and gradually sharpen it during the training process to diminish the gap between the soft and hard binarization functions. The authors also propose a more efficient batch normalization method.

This method is simple yet general as the soft quantization function can be used for both weights and activations, which is a desirable property. One major concern is that the proposed continuous relaxation training trick is previously studied and  used in  applications like hashing and this may not be treated as an inspiring technical contribution.  Moreover, some more recent state-of-the-art approaches achieve better results (e.g., [1]) than XNOR-NET, and discussions on these methods may further improve the proposed method. Yet another concern is that the accuracy results  (top1-37.84/top5-64.06) for XNOR-NET (Bw=1, Ba=1, Bbn=32) in this paper is much lower that the reported (top1-44.2/top5-69.2) in the original XNOR-NET paper. Even the results of the proposed method is much lower than the original XNOR-NET results. This may raise concern on the efficacy of the proposed method whether the proposed method is good or not. Can the authors use the same setting as the original XNOR-NET paper and do the comparison?

[1]. Towards accurate binary convolutional neural network, NIPS2017. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ryeHDVEhsX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Relevant Work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxKajC5t7&amp;noteId=ryeHDVEhsX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">23 Oct 2018</span><span class="item">ICLR 2019 Conference Paper823 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">There is a work from this year's ICML [1]. They tried to use Gumbel-Softmax to push the activations to near binary values.

[1] Towards Binary-Valued Gates for Robust LSTM Training, <a href="https://arxiv.org/abs/1806.02988," target="_blank" rel="nofollow">https://arxiv.org/abs/1806.02988,</a> ICML 2018</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJg4WhCH9X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Is there any performance on modern network architecture?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxKajC5t7&amp;noteId=BJg4WhCH9X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Oct 2018</span><span class="item">ICLR 2019 Conference Paper823 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Nice paper :) I'm curious that if there is any performance of self-binarizing network on modern network architecture such as ResNet?</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1eeUVu49X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Regarding "Self-Binarizing"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxKajC5t7&amp;noteId=H1eeUVu49X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Oct 2018 (modified: 06 Oct 2018)</span><span class="item">ICLR 2019 Conference Paper823 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I think the proposed "Self-Binarizing" idea is a widely used training trick in other tasks especially in Hashing, where only the last layer is binarized for fast image retrieval. I only list some references that used exactly the same approach (use tanh(.) to approximate sign(.)).

[1]: HashNet: Deep Learning to Hash by Continuation, ICCV2017.
[2]: <a href="https://arxiv.org/pdf/1802.07437.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1802.07437.pdf</a>
................</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkxUHqihtX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Combined comparison batch norm and sign activation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxKajC5t7&amp;noteId=rkxUHqihtX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Evgenii_Zheltonozhskii1" class="profile-link">Evgenii Zheltonozhskii</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Sep 2018</span><span class="item">ICLR 2019 Conference Paper823 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The idea similar to BinaryBN was described in FINN (<a href="https://arxiv.org/abs/1612.07119)," target="_blank" rel="nofollow">https://arxiv.org/abs/1612.07119),</a> section 4.2.2 (batchnorm-activation as threshold). Probably, it should be added to references?</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>