<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1xWcj0qYm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="On the Minimal Supervision for Training Any Binary Classifier from..." />
      <meta name="og:description" content="Empirical risk minimization (ERM), with proper loss function and regularization, is the common practice of supervised classification. In this paper, we study training arbitrary (from linear to..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1xWcj0qYm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data</a> <a class="note_content_pdf" href="/pdf?id=B1xWcj0qYm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019on,    &#10;title={On the Minimal Supervision for Training Any Binary Classifier from Only Unlabeled Data},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=B1xWcj0qYm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Empirical risk minimization (ERM), with proper loss function and regularization, is the common practice of supervised classification. In this paper, we study training arbitrary (from linear to deep) binary classifier from only unlabeled (U) data by ERM. We prove that it is impossible to estimate the risk of an arbitrary binary classifier in an unbiased manner given a single set of U data, but it becomes possible given two sets of U data with different class priors. These two facts answer a fundamental question---what the minimal supervision is for training any binary classifier from only U data. Following these findings, we propose an ERM-based learning method from two sets of U data, and then prove it is consistent. Experiments demonstrate the proposed method could train deep models and outperform state-of-the-art methods for learning from two sets of U data.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">learning from only unlabeled data, empirical risk minimization, unbiased risk estimator</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Three class priors are all you need to train deep models from only U data, while any two should not be enough.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">14 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_r1gZR2ahaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting contribution and a thorough review of the existing work in the field</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xWcj0qYm&amp;noteId=r1gZR2ahaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper508 AnonReviewer5</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper508 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose an unbiased estimator that allows for training models with weak supervision on two unlabeled datasets with known class priors. The theoretical properties of the estimator are discussed and an empirical evaluation shows promising performance.

The paper provides a thorough overview of the related work.
The experiments compare to the relevant baselines.

Minor remarks:

The writing seems like it could be improved in multiple places and the main thing that makes some sections of the paper hard to follow is that the concepts often get mentioned and discussed before they are formally defined/introduced. Concepts that are introduced via citations should also be explained even if not in-depth.

Figure 2: the curves suggest that the models should have been left to train for a longer time - some of the small PN and small PN prior-shift risks are still decreasing

Figure 2: the scaling seems inconsistent - the leftmost subplot in each row doesn’t start at (0,0) in the lower left corner, unlike the other subplots in each row - and it should probably be the same throughout - no need to be showing the negative space.

Figure 2: maybe it would be good to plot the different lines in different styles (not just colors) - for BW print and colorblind readers

For small PN and small PN prior-shift, the choice of 10% seems arbitrary. At what percentage do the supervised methods start displaying a clear advantage - for the experiments in the paper?

When looking into the robustness wrt noise in the training class priors, both are multiplied by the same epsilon coefficient. In a more realistic setting the priors might be perturbed independently, potentially even in a different direction. It would be nice to have a more general experiment here, measuring the robustness of the proposed approach in such a way.

5.2 typo: benchmarksand ; datasetsfor</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ryxIBivipQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting paper covering proofs as well as experiments on a newly defined unbiased risk estimator for unlabeled classification</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xWcj0qYm&amp;noteId=ryxIBivipQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper508 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper508 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a methodology for training any binary classifier from only unlabeled data. They proved that it is impossible to provide an unbiased estimator if having only a single set of unlabeled data, however, they provide an empirical risk minimization method for only two sets of unlabeled data where all the class priors are given. Some experiments and comparisons with state-of-the-art are provided, together with a study on the robustness of the method.

pros:

- The paper is clear, and it provides an interesting proven statement as well as a methodology that can be applied directly. Because they show that only two sets with different (and known) priors are sufficient to have an unbiased estimator, the paper has a clear contribution.
- The impact of the method is a clear asset, because learning from unlabeled data is applicable to a large number of tasks and is raising attention in the last years.
- The large literature on the subject has been well covered in the introduction.
- The importance made on the integration of the method to state-of-the-art classifiers, such as the deep learning framework, is also a very positive point.
- The effort made in the experiments, by testing the performance as well as the robustness of the method with noisy training class priors is very interesting. 

remarks:

- part 4.1 : the simplification is interesting. However, the authors say that this simplification is easier to implement in many deep learning frameworks. Why is that?
- part 4.2 : the consistency part is too condensed and not clear enough.
- experiments : what about computation time?
- More generally, I wonder if the authors can find examples of typical problems for classification from unlabeled data with known class priors and with at least two sets?

minor comments:
- part 1: 'but also IN weakly-supervised learning'
- part 2. related work : post- precessing --&gt; post-processing
- part 2. related work : it is proven THAT the minimal number of U sets...
- part 2. related work : In fact, these two are fairly different --&gt; not clear, did you mean 'Actually, ..' ?
- part 4.1 : definition 3. Why naming l- and l+ the corrected loss functions? both of them integrate l(z) and l(-z), so it can be confusing.
- part 5.1 Analysis of moving ... closer: ... is exactly THE same as before.
- part 5.2 : Missing spaces : 'from the webpage of authors.Note ...' and 'USPS datasetsfor the experiment ...' </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BkgahCMcpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>thorough review of material and refinement that surpasses the state-of-the-art; needs some more development on real-world experiment or justified statement to defer issues until later paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xWcj0qYm&amp;noteId=BkgahCMcpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper508 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper508 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: 
The authors introduce the task of learning from unlabeled data clearly and concisely with sufficient reference to background material. They propose a learning approach, called UU, from two unlabeled datasets with known class priors and prove consistency and convergence rates. Their experiments are insightful to the problem, revealing how the two datasets must be sufficiently separated and how UU learning outperforms state-of-the-art approaches. The writing is clear and the idea is an original refinement of earlier work, justified by its exceeding state-of-the-art approaches. However, the paper needs more experimentation.  

Further details:
While the introduction and set-up is long, it positions the paper well by making it approachable to someone not directly in the subject area and delineating how the approach differs from existing theory. The paper flows smoothly and the arguments build sequentially. A few issues are left unaddressed:
- How does the natural extension of UU learning extend beyond the binary setting? 
- As the authors state, in the wild the class priors may not be known. Their experiment is not completely satisfying because it scales both priors the same. It would be more interesting to experimentally consider them with two different unknown error rates. If this were theoretically addressed (even under the symmetrical single epsilon) this paper would be much better. 
- In Table 2, using an epsilon greater than 1 seems to always decrease the error with a seeming greater impact when theta and theta' are close. This trend should be explained. In general, the real-world application was the weakest section. Expounding up on it more, running more revealing experiments (potentially on an actual problem in addition to benchmarks), and providing theoretical motivation would greatly improve the paper. 
- In the introduction is is emphasized how this compares to supervised learning but the explanation is how this compares to unsupervised clustering is much more terse. Another sentence or two explaining why using the resulting cluster identifications for binary labeling is inferior to the "arbitrary binary classifier" would help. It's clear in the author's application because one would like to use all data available, including the class priors, for classification. 

Minor issues: 
-At the bottom of page 3 the authors state, " In fact, these two are fairly different, and the differences are reviewed and discussed in Menon et al. (2015) and van Rooyen &amp; Williamson (2018). " It would be clearer to immediately state the key difference instead of waiting until the end of the paragraph. 
- In the first sentence of Section 3.1 "imagining" is mistyped as "imaging."
- What does "classifier-calibrated" mean in Section 3.1? 
- In Section 3.1, "That is why by choosing a model G, g∗ = arg ming∈G R(g) is changed as the target to which" was a bit unclear at first. The phrase "is changed as the target to which" was confusing because of the phrasing. Upon second read, the meaning was clear. 
- In the introduction it was stated "impossibility is a proof by contradiction, and the possibility is a proof by construction." It would be better to (re)state this with each theorem. I was immediately curious about the proof technique after reading the theorem but no elaboration was provided (other than see the appendix). The footnote with the latter theorem is helpful as it alludes to the kind of construction used without being overly detailed.
- In section 5.2, in the next to last sentence of the first paragraph there are some issues with missing spaces. 
- Some more experiment details, e.g. hyperparameter tuning, could be explained in the appendix for reproducibility. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJgzB6LbAX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you so much for your time and constructive comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xWcj0qYm&amp;noteId=rJgzB6LbAX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper508 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper508 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We are trying to expand Table 2 by adding noises of different directions to the training class priors (experiments on MNIST are finished but experiments on CIFAR-10 are quite slow; we will update the submission later). Please find our detailed responses below.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkxn-AIbC7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>How does the natural extension of UU learning extend beyond the binary setting?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xWcj0qYm&amp;noteId=rkxn-AIbC7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper508 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper508 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Great question! A similar question has been addressed in a previous reply entitled “Relationship to learning from label proportions (LLP) and natural extensions to k classes with k U sets”. The main message can be summarized as follows. LLP makes use of the mean operator technique for linear-odd losses, and it can naturally handle k classes with k U sets but it cannot learn nonlinear classifiers. The proposed method makes use of the risk rewrite technique from learning with noisy labels, and it can naturally learn nonlinear classifiers but it cannot handle k classes with k U sets.

We think the technical difficulty is how to connect the k-class learning problem to learning with noisy labels. For binary classification, this connection is obvious: we regard the U set with larger class prior as the corrupted positive class and the U set with smaller class prior as the corrupted negative class. For multi-class classification with multiple U sets where all class priors are given, we can construct combinatorial many mappings from a U set to a corrupted class, and we lack a measure of the quality of these mappings. This should be the first step for extending this paper beyond binary classification.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SyeJk0I-A7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Real-world applications</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xWcj0qYm&amp;noteId=SyeJk0I-A7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper508 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper508 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Two sets of U data with different class priors may be collected from different places or time points. For example, considering morbidity rates, they can be potential patient data collected from urban and rural areas; considering food preferences, they can be potential customer data collected from the Northern and Southern China; likewise, considering approval rates, they can be unlabeled voter data collected in two years.

Note that in the seminal paper on learning from label proportions “N. Quadrianto, A. J. Smola, T. S. Caetano, and Q. V. Le. Estimating labels from label proportions. JMLR, 2009”, there are many potential applications in areas like e-commerce, politics, spam filtering and improper content detection. The two problem settings are different yet closely related, and thus those can also be our potential applications.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJlyh6UW07" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Why using the resulting cluster identifications for binary labeling is inferior to the “arbitrary binary classifier”?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xWcj0qYm&amp;noteId=rJlyh6UW07"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper508 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper508 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Note that given only U data for training, the most straightforward idea is to use clustering, in particular discriminative clustering which is also known as “unsupervised classification”. This solution is usually suboptimal. A minor reason is that clustering methods are not always compatible with state-of-the-art deep models, but there are two major reasons.

First, successful translation of clustering results into classification results exclusively relies on an assumption, namely one cluster exactly corresponds to one class. If we have one cluster formed by a few geometrically close classes, or one class formed by several geometrically separated clusters (as in our experiments), this assumption would be violated and we would fail in translating clusters into meaningful classes. It may happen that clustering results are perfect while classification results are poor.

Second, clustering must introduce additional geometric or information-theoretic assumptions (for example, by following the large margin principle and the information-maximization principle). The learning objectives of clustering methods are built upon these additional assumptions. It is very difficult to measure the distance or similarity according to the geometry for complex data in high-dimensional spaces. On the other hand, we employ ERM and rely on the same assumptions of supervised deep learning: the smoothness assumption for supervised learning and the composition-of-factors assumption for deep learning (Section 5.11.2, the DL book). Therefore, we prefer ERM to clustering methods.

BTW, the argument was that using clustering is inferior to ERM rather than using clustering is inferior to arbitrary binary classifier. The difference between learning objectives is more critical than the difference between models to be learned.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1leKaUW0Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Meaning of “classification calibrated”</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xWcj0qYm&amp;noteId=H1leKaUW0Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper508 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper508 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Classification-calibrated loss functions are defined in “P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classification, and risk bounds. Journal of the American Statistical Association, 2006.” Briefly speaking, using such a surrogate loss guarantees that under mild assumption on the model, the learned model will converge to the Bayes optimal classifier, as the number of training data goes to infinity. Almost all popular losses are classification calibrated---actually, a monotonic and differentiable loss should be classification calibrated, if its gradient at zero is negative.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_H1xTSAEqnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice conceptual contribution + thoroughly executed; however dense writing, and in a crowded area</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xWcj0qYm&amp;noteId=H1xTSAEqnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper508 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper508 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies the weak supervision setting of learning a general binary classifier from two unlabeled (U) datasets with known class balances. The authors establish that this is possible by constructing an unbiased estimator, analyze its convergence theoretically, and then run experiments using modern image classification models.

Pros:
- This work demonstrates, theoretically and empirically, a simple way to train generic models using only the known class balances of several sets of unlabeled data (having the same conditional distributions p(x|y))---a very interesting configuration of weak supervision, an increasingly popular and important area

- The treatment is thorough, proceeding from establishing the minimum number of U datasets, constructing the estimator, analyzing convergence, and implementing thorough experiments

Cons:
- This is a crowded area (as covered in their related work section). As they cite, (Quadrianto et al., 2009) proposed this setting and considered linear models for k-wise classification.  Moreover, the two U datasets with known class balances can equivalently be viewed as two weak / noisy label sources with known accuracies.  Thus this work connects to many areas- both in noisy learning, as they cite heavily, but also in methods (in e.g. crowdsourcing and multi-source weak supervision) where several sources label unlabeled datasets with unknown accuracies (which are often estimated in an unsupervised fashion).

- The overall clarity of the paper's writing could be improved. For example, the introduction and related work sections take up a large portion of the paper, but are very dense and heavy with jargon that is not internally defined upfront; for example "risk rewrite" is introduced in paragraph 2 with no internal definition and then used subsequently throughout the paper (this defn would be simple enough to give: in the context of this paper, "risk rewrite" means a linear combination of the class-conditional losses; or more generally, the expected loss w.r.t. distribution over classes...).  Also intuition could be briefly given about the theorem proof strategies.

- The difference between the two class distributions over the U datasets seems like an important quantity (akin, in e.g. weak supervision / crowd source modeling papers, to quantity of how bounded away from random noise the labelers are). This is treated empirically, but would be stronger to have this show up in the theory somewhere.

- Other prior work here has handled k classes with k U sets; could have extended to cover this setting too, since seems natural

Overall take: This learning from label proportions setting has been covered before, but this paper presents it in an overall clean and general way, testing it empirically on modern models and datasets, which is an interesting contribution.

Other minor points:
- The argument for / distinction between using eqns. (3) and (4) seems a bit ad hoc / informal ("we argue that...").  This is an important point...
- Theorem 1 proof seems fine, but some intuition in the main body would be nice.
- What does "classification calibrated" mean?
- Saying that three U sets are needed, where this includes the test set, seems a bit non-standard?  Also I'm confused- isn't a labeled test set used?  So what is this third U set for?
- The labels l_+ and l_- in Defn. 3 seem to imply that the two U sets are positive vs. negative; but this is not the case, correct…?
- Stating both Lemma 5 and Thm 6 seems unnecessary
- In Fig. 2, seems like could have trained for longer and perhaps some of the losses would have continued decreasing?  In particular, small PN?  Also, a table of the final test set accuracies would have been very helpful.
- More detail on experimental protocol would be helpful: what kind of hyperparameter tuning was done? repeated runs averaging?  It seems odd, for example in Fig. 3, that the green lines are so different in (a) vs. (c), and not in the way that one would expect given the decrease in theta
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkeeoqWha7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you so much for your time and constructive comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xWcj0qYm&amp;noteId=rkeeoqWha7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper508 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper508 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We are still working on the experiments for extending the number of max epochs for training from 200 to 500 and investigating whether standard supervised learning with limited data can be significantly better than now. After that we will go to improve the clarity of the paper following your comments. Please find our detailed responses below.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkx7Fs-ham" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Relationship to crowdsourcing and learning with noisy labels (LNL)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xWcj0qYm&amp;noteId=rkx7Fs-ham"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper508 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper508 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for pointing out that crowdsourcing is also related to this paper! We are conjecturing that data generation processes in crowdsourcing depend heavily on non-expert labelers, while data generation/corruption processes in LNL are more theoretical/statistical. In LNL, when p(y|x) is corrupted, we may first draw x_i from p(x) and then manipulate y_i according to corrupted p(y|x). However, when p(x|y) is corrupted, we have to first pick up y_i and then draw corrupted x_i directly from corrupted p(x|y), since here p(x) is different in clean and corrupted joint densities. In this sense, the problem settings seem not very related. We will carefully check this issue later since we are not very familiar with crowdsourcing. Could you please recommend a few crowdsourcing and multi-source learning papers for our reference?

Note that CCN noise model (where p(y|x) is corrupted) has no covariate shift, and MCD noise model (where p(x|y) is corrupted) has inevitable covariate shift. The empirical studies cited in this paper from the computer vision society, if not CCN-based, basically assume that noisy labels are from p(y_noisy|x,y_clean) and thus again has no covariate shift. This paper is the first to experimentally show that unbiased risk estimators originally designed for no-covariate-shift case don’t work in covariate-shift case. As a consequence, this paper is related to but still fairly different from the majority of LNL papers. To the best of our knowledge, this paper is the fourth paper going along this specific direction (after three papers from COLT 2013, TAAI 2013 and ICML 2015).
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJgWwjZ3T7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Relationship to learning from label proportions (LLP) and natural extensions to k classes with k U sets</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xWcj0qYm&amp;noteId=BJgWwjZ3T7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper508 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper508 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the suggestion, but extending this paper to binary or k classes with k U sets is beyond the scope of the current paper. It seems that the problem setting of LLP is more general than this paper since LLP can make use of k U sets to learn a binary classifier. Nevertheless, the goal of learning or the model to be learned should be taken into account too: the goal of LLP is to learn a linear model whereas our goal is to learn either linear or deep model. Existing LLP methods based on ERM, i.e., “N. Quadrianto, A. J. Smola, T. S. Caetano, and Q. V. Le. Estimating labels from label proportions. JMLR, 2009”, cannot learn any nonlinear model.

Here we distinguish three cases of extending A to B where A is some component in some existing method. The first case is that A can be naturally extended to B but the authors didn’t know/realize it. The second case is that extending A to B has no problem theoretically but the performance can be quite poor practically. The third case is that A cannot be extended to B theoretically.

For example, extending the logistic loss in LLP to any linear-odd loss (i.e., l(z) - l(-z) = -z; see [1]) is the first case. Extending the linear model in many LNL methods to deep models is the second case---the mathematical derivations suggest they are model-independent, but the performances turn out to be quite poor because deep models are much better at memorizing noisy labels than linear models. Finally, extending the linear model in LLP to deep models is the third case---this can be explained by the proof of Theorem 3 in [1], where the key observation is y*g(x)=g(y*x) if g is linear in its parameters; as a result, only the expectation of y*x needs to be estimated that is known as the mean operator and is the technique of LLP. Therefore, LLP cannot benefit from deep learning unless it can get rid of the mean operator.

On the other hand, the proposed method shares the technique for designing unbiased risk estimators in LNL (i.e., risk rewrite). For learning a binary classifier, we proved that one U set is not enough but two U sets are enough. However, given (more than) three U sets for training, how to meaningfully incorporate all U sets is an open question. Now we cannot say it belongs to the second or third case, but we are sure it is not the first case.

[1] G. Patrini, F. Nielsen, R. Nock, M. Carioni. Loss factorization, weakly supervised learning and label noise robustness. ICML, 2016.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJeZEjbhTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Theoretical analysis of the difference between two U sets for training</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xWcj0qYm&amp;noteId=rJeZEjbhTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper508 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper508 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">This has been explored in Theorem 6, where the estimation error bound of the proposed method is linear in alpha and alpha’. By the definitions of alpha and alpha’, we can see that alpha and alpha’ are both non-negative and no more than 4/(theta-theta’) under the assumption that theta&gt;theta’. Thus, the larger theta-theta’ is, the better the proposed method performs. This theoretical result is consistent with our empirical results shown in Figures 2 and 3.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJepWiW3pm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Answers to other minor points</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xWcj0qYm&amp;noteId=rJepWiW3pm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper508 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper508 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Q: Meaning of “classification calibrated”
A: Classification-calibrated loss functions are defined in “P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classification, and risk bounds. Journal of the American Statistical Association, 2006.” Briefly speaking, using such a surrogate loss guarantees that under mild assumption on the model, the learned model will converge to the Bayes optimal classifier, as the number of training data goes to infinity. Almost all popular losses are classification calibrated---actually, a monotonic and differentiable loss should be classification calibrated, if its gradient at zero is negative.

Q: Saying that three U sets are needed, where this includes the test set, seems a bit non-standard
A: Sorry for the confusion. To be clear, *three class priors* are needed, i.e. two for the training distributions and one for the test distribution. However, *two U sets* corresponding to two training distributions are needed, and we don’t need any training data from the test distribution.

Q: The labels l_+ and l_- in Definition 3 seem to imply the two U sets are positive vs. negative; but this is not the case, correct?
A: Yes, the two U sets are completely unlabeled, that is, neither positive nor negative. We followed the notation suggested by the area of learning with noisy labels which caused this confusion. In this context, the U set with larger class prior is regarded as *corrupted positive* data and the U set with smaller class prior is regarded as *corrupted negative* data. We will clearly explain this notation in the revised version.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>