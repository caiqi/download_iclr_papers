<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>SUPERVISED POLICY UPDATE | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="SUPERVISED POLICY UPDATE" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SJxTroR9F7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="SUPERVISED POLICY UPDATE" />
      <meta name="og:description" content="We propose a new sample-efficient methodology, called Supervised Policy Update (SPU), for deep reinforcement learning. Starting with data generated by the current policy, SPU formulates and solves..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SJxTroR9F7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>SUPERVISED POLICY UPDATE</a> <a class="note_content_pdf" href="/pdf?id=SJxTroR9F7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019supervised,    &#10;title={SUPERVISED POLICY UPDATE},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SJxTroR9F7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We propose a new sample-efficient methodology, called Supervised Policy Update (SPU), for deep reinforcement learning. Starting with data generated by the current policy, SPU formulates and solves a constrained optimization problem in the non-parameterized proximal policy space. Using supervised regression, it then converts the optimal non-parameterized policy to a parameterized policy, from which it draws new samples. The methodology is general in that it applies to both discrete and continuous action spaces, and can handle a wide variety of proximity constraints for the non-parameterized optimization problem. We show how the Natural Policy Gradient and Trust Region Policy Optimization (NPG/TRPO) problems, and the Proximal Policy Optimization (PPO) problem can be addressed by this methodology. The SPU implementation is much simpler than TRPO. In terms of sample efficiency, our extensive experiments show SPU outperforms TRPO in Mujoco simulated robotic tasks and outperforms PPO in Atari video game tasks.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Deep Reinforcement Learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">first posing and solving the sample efficiency optimization problem in the non-parameterized policy space, and then solving a supervised regression problem to find a parameterized policy that is near the optimal non-parameterized policy.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">22 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rygWJZz16X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A very interesting approach to constraint policy optimization that uses nonparametric relaxation with subsequent projection</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxTroR9F7&amp;noteId=rygWJZz16X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper130 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper130 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes to perform a constraint optimization of an approximation of the expected reward function for unparameterized policy with subsequent projection of the solution to the nearest parameterized one. This approach allows fast ("nearly closed form") solutions for nonparametric policies and leads to an increase in sample efficiency.

The proposed approach is interesting and the results are promising.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">9: Top 15% of accepted papers, strong accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJetuHsN6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to your comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxTroR9F7&amp;noteId=BJetuHsN6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper130 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper130 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you so much for your comments! We are more than happy to answer any thing about the paper should you have more comments in the future.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Skx4C43dnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Strong similarities to previous work with no comparison</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxTroR9F7&amp;noteId=Skx4C43dnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper130 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper130 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors formulate policy optimization as a two step iterative procedure: 1) solving a constrained optimization problem in the non-parameterized policy space, 2) using supervised regression to project this onto a parameterized policy. This approach generally applies to both continuous and discrete action spaces and can handle a variety of constraints. Their primary claims is that this approach improves sample-efficiency over TRPO on Mujoco tasks and over PPO on Atari games.

The method proposed in the paper has strong similarities with existing methods, but lacks comparisons with these approaches. The authors have not clearly demonstrated that SPU provides novel insights beyond the existing literature. I'm happy to change my score if the authors can convince me otherwise.

Main comments:

The focus of the paper is sample-efficiency, but the intro restricts to the on-policy setting. The authors should justify this choice. It is well known that off-policy algorithms (e.g., SAC for continuous control and Implicit Quantile Networks for Atari) are much more sample-efficient.

In Sec 4, what is the advantage of breaking the problem up into these 3 steps versus directly trying to solve (9),(10)? In fact, if we convert (10) into a penalty and take the derivative, we arrive at nearly the same gradient as (17). As this is central to the SPU framework, this needs to be justified.

MPO (Abdolmaleki et al. 2018) is very closely related to SPU. It is unclear if SPU provides any additional insights or benefits over MPO. This needs to be discussed and compared.

The experimental section could be strengthened by:
* Given the similarity to SPU, comparisons to MPO and GAC should be made, or clear justification for why they are not comparable must be given.
* Why is the comparison on Mujoco to TRPO in the main text and the comparison to PPO relegated to the appendix? It would make more sense to compare to PPO, so the authors need to justify this decision.
* The results on Mujoco are quite poor compared to state-of-the-art methods (e.g., SAC). The authors should justify why we should care about their results.

Comments:

In Sec 2, the authors should be careful about the discounting. For example, they are almost surely not having A_{it} approximate \gamma^t A^{\pi_{\theta_k}}, rather A^{\pi_{\theta_k}}.

In Sec 2, the KL is denoted as KL(\pi || pi_k), but in the text is described as the KL from \pi_k to \pi (reversed). From the equations, it appears that is an error, and it should read KL from \pi to \pi_k.

In Sec 3, the description of NPG/TRPO is not accurate. The main goal of NPG/TRPO work was to establish monotonic improvement.

In Sec 3, computational speed is cited as a major deficit of GAC, especially the solving linear systems with the Hessian (wrt to the actions). This seems rather surprising. Inverting a 1000x1000 matrix on a modern computer takes &lt;1 second, so it doesn't seem like this should be the limiting step for any of the problems encountered.

The KL penalty version of PPO seems closely related to SPU. Can the authors mention differences with that version of PPO in the related work?

In Sec 4, step iii is described as supervised learning. Can the authors elaborate on why? I would typically think of the other direction as supervised learning as that leads to MLE.

In Sec 5.1, what is the justification/reasoning for setting \tilde{\lambda_{s_i}} = \lambda and introducing the indicator functions?

Sec 5.2 is not evaluated and Sec 5.3 produces inferior results, so it may make sense to move these to the appendix. Otherwise, the authors should explain situations where we would expect these to be useful or provide some additional insight. It also should be noted that the proximity constraints in TRPO/PPO follow from a theoretical argument and are not arbitrary choices.

Sec 5.3 seems to deviate quite a bit from the SPU framework. In addition to the differences pointed out in the text, the "supervised" loss changes too. Can the authors justify/explain the reasoning for these changes?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Syl9f8jVTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to your comments (general reply)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxTroR9F7&amp;noteId=Syl9f8jVTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper130 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper130 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your thorough comments!

We argue that SPU has the following advantages:

-  It is mathematically principled. 

-  It is mathematically straightforward. It is much simpler and more understandable than MPO and GAC. The technique is comprehensible by undergraduates. Simplicity in RL algorithms has its own merits. This is especially useful when non-expert wants to apply RL algorithms to solve their problems, which is becoming a trend. The step-by-step description and implementation of SPU in only slightly more involved than PPO.

-  It is versatile since a wide variety of constraint types can be used. SPU provides a general framework, allowing practitioners to try out different constraint types. For example, the TRPO paper mentions that enforcing the disaggregated constraint is preferable to enforcing the aggregated constraints. However, for mathematical conveniences, they choose to work with the aggregated constraints: “While it is motivated by the theory, this problem is impractical to solve due to the large number of constraints. Instead, we can use a heuristic approximation which considers the average KL divergence” [6]. In our paper, we show that the SPU framework allows us to solve the optimization problem with the dis-aggregated constraint exactly and also experimentally demonstrates that doing so helps to bring performance of SPU to be higher than TRPO.

-  It is desirable to have a unified algorithm that applies to both continuous and discrete problems. In our understanding, GAC and SAC do not apply to the discrete case. MPO has only made preliminary progress with the discrete case. 

-  To our knowledge, among all the on-policy algorithms, it gives the best performance for the continuous cases. Deeply understanding the on-policy case is beneficial to the community. In future work we will consider combining SPU with replay buffers. 

-  It is a general approach to solving DRL problems. The algorithms tested in our paper are specific instances of this approach. In both the GAC and MPO paper, working in the non-parameterized policy space is a by-product of applying the main ideas in those papers to DRL.

We will cite and discuss SAC and MPO paper in related work. We also argue that SAC is not directly comparable to SPU since SAC tunes environment-specific parameter (reward scaling). In SPU, the hyper-parameters are shared and fixed across all environments. The neural networks used in the SAC paper are also 4 times larger than the neural networks used in our work and the versions of PPO and TRPO that we compared against. Also, the performance of SPU and SAC can not be compared by looking at the graphs in the respective papers since the Mujoco environments used in the SAC paper is version 1 while we used version 2.  

That being said, in our paper, we want to compare SPU against algorithms that operate under the same constraints, one of which is being on-policy. Thus, the focus of the paper will remain comparing SPU with other on-policy schemes. We justify why in more detailed below. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1lMx0hgAm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re:</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxTroR9F7&amp;noteId=B1lMx0hgAm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper130 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper130 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Is the claim that it is more principled than other approaches?

"The technique is comprehensible by undergraduates. Simplicity in RL algorithms has its own merits. This is especially useful when non-expert wants to apply RL algorithms to solve their problems, which is becoming a trend." and "deeply understanding the on-policy case is beneficial to the community." are both reasonable motivations for your work. I hope you can rework the introduction to motivate the work by that reasoning instead of the current version.

It's unclear why SAC wouldn't apply to the discrete case. However, it would be quite similar to MPO at that point.  Yes, I agree that directly comparing to the SAC paper graphs is not possible, but multiple open source versions of SAC exist, including from the authors of the original paper.

The fact that GAC and MPO derive the non-parameterized policy space as a by-product of applying higher level ideas is a positive.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1g0iDsNp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>References used in our reply</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxTroR9F7&amp;noteId=S1g0iDsNp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper130 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper130 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[1] Human-level control through deep reinforcement learning. <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf" target="_blank" rel="nofollow">https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf</a>

[2] Sample Efficient Actor-Critic with Experience Replay https://arxiv.org/pdf/1611.01224.pdf

[3] https://github.com/voot-t/guide-actor-critic/blob/8af08348a83f5f0cf32f70347fd987fb4391e53d/GAC_learner.py#L166

[4] https://github.com/voot-t/guide-actor-critic/blob/8af08348a83f5f0cf32f70347fd987fb4391e53d/GAC_learner.py#L245

[5] https://github.com/voot-t/guide-actor-critic/blob/8af08348a83f5f0cf32f70347fd987fb4391e53d/GAC_learner.py#L231

[6] TRPO. https://arxiv.org/pdf/1502.05477.pdf</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJl0FDoEpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>On the computational speed of GAC</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxTroR9F7&amp;noteId=HJl0FDoEpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper130 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper130 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">“In Sec 3, computational speed is cited as a major deficit of GAC, especially the solving linear systems with the Hessian (wrt to the actions). This seems rather surprising. Inverting a 1000x1000 matrix on a modern computer takes &lt;1 second, so it doesn't seem like this should be the limiting step for any of the problems encountered.”

We profiled GAC and identified these major computation bottlenecks. At every policy and q-network update step, GAC:

-  Forms empirical estimate of the Q-value by sampling multiple actions from the action distribution and running a forward pass for each action (30% of total computational time) [3] 

-  Minimizes the dual function to obtain the dual variables using SLSQP (20% of total computational time) [4]

-  Forms taylor approximation of the Hessian (10% of computational time) [5]
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BkerOwoV6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Comparison against SAC and state-of-the-art approach</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxTroR9F7&amp;noteId=BkerOwoV6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper130 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper130 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">“Given the similarity to SPU, comparisons to MPO and GAC should be made, or clear justification for why they are not comparable must be given.
* Why is the comparison on Mujoco to TRPO in the main text and the comparison to PPO relegated to the appendix? It would make more sense to compare to PPO, so the authors need to justify this decision.
* The results on Mujoco are quite poor compared to state-of-the-art methods (e.g., SAC). The authors should justify why we should care about their results.”

Response: We do not claim state of the art results. Rather, we hope our paper ignites interests in separating finding the optimal policy into a two-step process: finding the optimal non-parameterized policy, and then parameterizing this optimal policy. As such, we wanted to compare with algorithms operating under the same algorithmic constraints, one of which is being on-policy. Also, both SAC and Implicit Quantile Networks for Distributional Reinforcement Learning have a long of history of development and we argue this is why they are state-of-the-arts method. In contrast, SPU is among the first few papers to consider working in the non-parameterized space. 

It is a general trend in RL that the performance of an on-policy algorithm can be substantially improved by incorporating off-policy training. For example, experience replay was used to make Q-learning the state-of-the-art approach in DRL [1]. Subsequently, experience replay was also incorporate into actor-critic methods, which brought its performance to being equal to Q-learning method [2]. We thus leave the extension of our approach to off-policy training to future work. We also invite the community to join us in making this extension. To help with this effort, we will release code for push-button replication of the main results in the paper.

We included the graphs for Atari in the appendix because there are too many of them. We have included figure 2 to provide for a high-level overview of the performance of SPU vs PPO. We will rewrite the explanation of figure 2 to make this clearer.

We acknowledge that solving the model-free DRL problem as a two-step procedure of first solving a non-parameterized problem and then a projection onto the parameter space has independently been proposed in the GAC paper and the MPO paper. We will be sure to give the MPO paper proper credit in the revision. 

We acknowledge SAC and other off-policy techniques can do better than SPU, which is on policy. We thank the reviewer for pointing out to us the SAC paper. We will make sure to cite SAC and discuss its benefit in our related work section. However, we argue that by solving the same problem that TRPO tries to solve and out-performing it with the only change being working in the non-parameterized space, we have demonstrates that our approach leads to experimental gain in a controlled experimental setting.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJlGckTeAm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: Comparison against SAC and state-of-the-art approach </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxTroR9F7&amp;noteId=BJlGckTeAm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper130 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper130 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">But, why are you introducing this constraint? There needs to be a clear motivation for why this two-step procedure would be a good thing to do. You motivate the work by sample-efficiency concerns, but the proposed approach is far from the state-of-the-art in sample-efficiency. A reader of your paper will be confused as to why they should care about your results. Reworking the motivation to justify your approach will greatly strengthen the paper.

Your approach is more similar to PPO than TRPO. So, the Mujoco PPO results should be in the main-text, not the Mujoco TRPO results. 


</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1lNNDiVaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>On MPO</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxTroR9F7&amp;noteId=B1lNNDiVaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper130 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper130 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">“MPO (Abdolmaleki et al. 2018) is very closely related to SPU. It is unclear if SPU provides any additional insights or benefits over MPO. This needs to be discussed and compared.”

Response: MPO uses much more sophisticated machinery, namely, Expectation Maximization to address the DRL problem. Our approach is more straightforward, and has been designed to handle different types of trust region constraints in a natural manner. Also, the approach naturally applies to discrete (Atari) as well as continuous (Mujoco). Also, we argue that working in the non-parameterized space is a general framework to solving DRL problem and demonstrates its theoretical and experimental benefits. In MPO, working in the non-parameterized space is a by-product of applying EM to DRL problems.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJlSEZ6gC7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: On MPO </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxTroR9F7&amp;noteId=rJlSEZ6gC7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper130 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper130 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The fact that MPO is motivated by a high level idea and the algorithm naturally follows from it is a positive.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJldMPoV6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>On the advantage of working in the non-parameterized policy space</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxTroR9F7&amp;noteId=BJldMPoV6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper130 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper130 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">“In Sec 4, what is the advantage of breaking the problem up into these 3 steps versus directly trying to solve (9), (10)? In fact, if we convert (10) into a penalty and take the derivative, we arrive at nearly the same gradient as (17). As this is central to the SPU framework, this needs to be justified.”

Response: A central aspect of SPU is its versatility, including to be able to handle both aggregated and disaggregated constraints. The three-step procedure allows for this diversity. Also, obtaining the optimal solution for the non-parameterized problems sheds significant insight on the problem. Also, the form of the gradient being nearly the same is not the same. If we convert (10) into a penalty and take the derivative, it is true we arrive at nearly the same gradient as (17). This is in fact one of the approach tried in the PPO paper, which they demonstrate does not work as well as the clipping approach in PPO. However, we show that in SPU, the form of gradient in (17) is superior to PPO.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1x3hIAlRX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: On the advantage of working in the non-parameterized policy space </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxTroR9F7&amp;noteId=S1x3hIAlRX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper130 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper130 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The key difference seems to be the disaggregated constraints, but these are added by per-state acceptance identity function, which as far as I can tell is an ad-hoc addition to account for the disaggregated constraints, unrelated to the two-step approach. Other than that, Equation 18 is exactly what you get from taking the derivative of the penalty version of the objective. If that is not correct, can the authors clarify? 

I agree that the authors do demonstrate that the disaggregated constraints are beneficial.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_Sylkwyddn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>SUPERVISED POLICY UPDATE</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxTroR9F7&amp;noteId=Sylkwyddn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper130 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper130 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Overall this paper is ok. The algorithm seems novel, but is clearly very closely related to other things in the literature. The paper is also let down by poor exposition in several areas. The numerical results seem reasonably strong, at least against relatively old baselines.

Equation 8 is crucial to the final algorithm, but is presented with no proof or explanation.

Just above theorem 1 the sentence does not parse "Further, for each s, let λs be the solution to ", firstly there is no 'solution' to an equation, secondly should it be λs or pi?

The discussion following theorem 1 is very messy and hard to follow and the notation is horrendous. I'm confused as to why the indicator function in the 'disaggregated' update only includes states for which the constraint is already satisfied, what about the states where it is not? I presume this is because you initialize from the previous policy, but this seems very approximate and even worse updating the parameters for one state might significantly move the policy in some other state meaning large violations are possible and not dealt with.

The connections to the papers 'MAXIMUM A POSTERIORI POLICY OPTIMISATION' and 'Relative Entropy Policy Search' should be mentioned, as another commenter said previously.

I don't think TRPO/PPO is SOTA anymore, so maybe these baselines aren't particularly interesting.

Figure 2 is incomprehensible.

Two of the references are repeated (Schulman et al, Wang et al).

The appendices include long lists of equalities with no explanation (e.g. appendix B), how is a reader meant to reasonably follow those steps? Each non-trivial equality needs a sentence explaining what was used to get it.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJxWSOjNaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to your comment</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxTroR9F7&amp;noteId=HJxWSOjNaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper130 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper130 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you so much for your detailed and thoughtful comments, we will revise the paper and figures based on your comments to better explain ourselves. 

Below, we address some of the specific concerns you have for our paper:

“Equation 8 is crucial to the final algorithm, but is presented with no proof or explanation.”


Equation 8 is a well-known result with detailed proofs provided in [1], [2]. We felt it more appropriate to refer readers to these works rather than repeating the results in the appendix.

-------------------------------------------------------------------------------------------------------------------------

“Just above theorem 1 the sentence does not parse "Further, for each s, let λs be the solution to ", firstly there is no 'solution' to an equation, secondly should it be λs or pi?”

Just above Theorem 1, note that 𝜋^𝝀 is defined as a function of 𝝀. We then define 𝝀_s as the  𝝀 that makes the value of the KL divergence equal to epsilon.

-------------------------------------------------------------------------------------------------------------------------

“The discussion following theorem 1 is very messy and hard to follow and the notation is horrendous. I'm confused as to why the indicator function in the 'disaggregated' update only includes states for which the constraint is already satisfied, what about the states where it is not? I presume this is because you initialize from the previous policy, but this seems very approximate and even worse updating the parameters for one state might significantly move the policy in some other state meaning large violations are possible and not dealt with.”

Please note that the indicator function in Equation (18) is applied to each state in the trajectories sampled by the current policy. The indicator function helps us to take into account the disaggregated constraints (14) in the optimization problem (12)-(14). 

Please also note that the original policy improvement bound from TRPO [1] [2] were proved using the disaggregated constraints. We would like to quote the TRPO paper: “This problem imposes a constraint that the KL divergence is bounded at every point in the state space.” In the TRPO, the disaggregated constraint is too unwieldy to work with and they thus choose to work with the average KL divergence (the aggregated KL constraints in our case). In our paper, we work directly with the disaggregated constraint, but make the approximation that we only enforce the disaggregated constraints for the states in the trajectories sampled by the current policy. 

Thank you for your comment here! We will rewrite the discussion to better explain our reasoning regarding the indicator function.

-------------------------------------------------------------------------------------------------------------------------

“The connections to the papers 'MAXIMUM A POSTERIORI POLICY OPTIMISATION' and 'Relative Entropy Policy Search' should be mentioned, as another commenter said previously.”

In the revision, we will include a discussion of MPO and Relative Entropy Policy Search and their relationship to our work. 

-------------------------------------------------------------------------------------------------------------------------

“I don't think TRPO/PPO is SOTA anymore, so maybe these baselines aren't particularly interesting.”

We acknowledge that recent work such as SAC has show to improve performance on TRPO and PPO, however we would like to point out that the main focus of our paper is to explore the idea of separating finding the optimal policy into a two-step process: finding the optimal non-parameterized policy, and then parameterizing this optimal policy. As such, we wanted to compare with algorithms operating under the same algorithmic constraints, one of which is being on-policy. It is a general trend in RL that the performance of an on-policy algorithm can be improved by incorporating off-policy training (such as in SAC), We thus leave the extension of our approach to off-policy training to future work. We also invite the community to join us in making this extension. To help with this effort, we will release code for push-button replication of the main results in the paper.

Thank you again for your input, we look forward to further discussions.


Reference

[1] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In International Conference on Machine Learning, pp. 22–31, 2017.

[2] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning, pp. 1889–1897, 2015.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Hkej9pzf5Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Derivation from Eq.17 to Eq.18</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxTroR9F7&amp;noteId=Hkej9pzf5Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper130 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, 
Thank you for the great paper and the detailed explanation. This is so far the best ICLR paper that I read. However, I didn't quite follow the simplification from Eq. 17 to Eq. 18. In Eq.7, you would penalize the advantage term when KL is violated. But in Eq.18, you just zero out the gradient when the KL is violated?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bklxxyuz9Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarifications</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxTroR9F7&amp;noteId=Bklxxyuz9Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper130 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Oct 2018</span><span class="item">ICLR 2019 Conference Paper130 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi,

We're happy you enjoyed reading our paper. To ensure we answer your questions correctly, we would like to clarify the following: in the sentence "In Eq.7, you would penalize the advantage term when KL is violated.", are you referring to equation 17 instead of equation 7? </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkefVwRfcm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarifications on Clarifications </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxTroR9F7&amp;noteId=SkefVwRfcm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Oct 2018</span><span class="item">ICLR 2019 Conference Paper130 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Sorry my bad. I meant "In Eq.17, you would penalize the advantage term when KL is violated."</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HyximF2Q9m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Answer</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxTroR9F7&amp;noteId=HyximF2Q9m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper130 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Oct 2018</span><span class="item">ICLR 2019 Conference Paper130 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">In eq 17, penalizing the advantage term comes from the derivation of the gradient of the KL (as shown in appendix B1). In eq 18, for each sampled state s_i, the contribution of the gradient coming from s_i is zeroed out if the KL constraint for s_i is violated. Hope that answers your question!</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_B1ggSKrCFQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Supervised policy update</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxTroR9F7&amp;noteId=B1ggSKrCFQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Sep 2018</span><span class="item">ICLR 2019 Conference Paper130 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hello,

It seems there are potential algorithmic similarities with a paper from  ICLR2018:

Maximum a-Posteriori Policy Optimisaiton
<a href="https://arxiv.org/pdf/1806.06920.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1806.06920.pdf</a>

Here  the policy optimization has two steps:

1- E-Step: Computing non-parametric policies (See equations 7,8 and 9 of the paper)
2- M-step: Using supervised learning to fit a parametric policy to non-parametric ones from E-step (see equation 12).

Please also see "Relative Entropy Policy Search (AAAI2009)":

https://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/viewFile/1851/2264

Thank you,




  


</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkgyRK2Q5m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Answer</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxTroR9F7&amp;noteId=BkgyRK2Q5m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper130 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Oct 2018</span><span class="item">ICLR 2019 Conference Paper130 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi,

Thank you for your comment! Our paper does indeed involve first finding a non-parameterized policy, then using supervised learning to fit a parameterized policy to the nonparametric one, similar to what you described in the maximum a posteriori optimization paper. However, the way with which we derived the non-parametric and parametric policies are fundamentally different, instead of an inference-based approach, our algorithms were derived from a proximal optimization perspective. Our approach can address a wide-variety of different definitions of proximity, including aggregated and disaggregated KL proximity, reverse KL proximity, L-infinity proximity. Each definition of proximity leads to a different non-parameterized optimal policy.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkx-7eNJnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: Answer</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxTroR9F7&amp;noteId=rkx-7eNJnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018</span><span class="item">ICLR 2019 Conference Paper130 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the reply. 

2-step optimization process in MPO [1] has been derived from a single inference principle. And the derivation naturally gives guidance on which direction of KL should be used in E-step which accounts for deriving the non-parametric policies. I think given the tight connections between MPO [1], REPS [2]  and your method in terms of update rules, a discussion on similarities and differences would be informative.

Thank you, 

[1] <a href="https://arxiv.org/pdf/1806.06920.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1806.06920.pdf</a>
[2] https://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/viewFile/1851/2264
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>