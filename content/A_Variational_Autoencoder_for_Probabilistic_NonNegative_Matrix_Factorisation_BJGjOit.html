<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJGjOi09t7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="A Variational Autoencoder for Probabilistic Non-Negative Matrix..." />
      <meta name="og:description" content="We introduce and demonstrate the variational autoencoder for probabilistic non-negative matrix factorisation (PAE-NMF). We design a network which can perform NMF and add in aspects of a VAE to make..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJGjOi09t7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation</a> <a class="note_content_pdf" href="/pdf?id=BJGjOi09t7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019a,    &#10;title={A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BJGjOi09t7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We introduce and demonstrate the variational autoencoder for probabilistic non-negative matrix factorisation (PAE-NMF). We design a network which can perform NMF and add in aspects of a VAE to make the coefficients of the latent space probabilistic. By restricting the weights in the final layer of the network to be non-negative and using the non-negative Weibull distribution we produce a probabilistic form of NMF which allows us to generate new data and find a probability distribution that effectively links the latent and input variables. We demonstrate the effectiveness of PAE-NMF on three heterogeneous datasets: images, financial time series and genomic.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Non-negative matrix factorisation, Variational autoencoder, Probabilistic</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BklPXKoVh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>lacking motivation and comparisons</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJGjOi09t7&amp;noteId=BklPXKoVh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper385 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper385 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper is generally well-written (lacking details in some sections though). My main criticism is about the lack of motivation for nonnegative VAE and lack of comparison with NMF.

Comments:
- the motivation of the proposed methodology is not clear to me. What is the interest of the proposed auto-encoding strategy w.r.t NMF ? There is no experimental comparison either. Besides the probabilistic embedding (which exists in NMF as well), is there something PAE-NMF can do better than NMF ? There is probably something, but the paper does not bring a convincing answer.
- the paper missed important references to nonnegative auto-encoders, in particular:
<a href="https://paris.cs.illinois.edu/pubs/paris-icassp2017.pdf" target="_blank" rel="nofollow">https://paris.cs.illinois.edu/pubs/paris-icassp2017.pdf</a>
- the review of probabilistic NMF works is limited, see e.g.,
https://paris.cs.illinois.edu/pubs/smaragdis-spm2014.pdf
- more details are needed about inference in Section 2.4

Minor comments:
- the notations z and h are sometimes confusing, what about using h every where ?
- it’s not clear to me how the first term in (1) is equal to the second term in (2</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HklILgbxCX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author reply to reviewer</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJGjOi09t7&amp;noteId=HklILgbxCX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper385 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for the review. We will answer each comment in turn.

Comment 1: the motivation of the proposed methodology is not clear to me. What is the interest of the proposed auto-encoding strategy w.r.t NMF ? There is no experimental comparison either. Besides the probabilistic embedding (which exists in NMF as well), is there something PAE-NMF can do better than NMF ? There is probably something, but the paper does not bring a convincing answer.
Our response: Although probabilistic methods for NMF have been developed, even a full Bayesian framework still faces the problem that, for the vast majority of problems where NMF is used, we have little idea about what is the appropriate prior. We would therefore be forced to do model selection or introduce hyperparameters and perform an inference (maximum likelihood or Bayesian) based on the evidence. However, as the posterior is such cases is unlikely to be analytic this is likely to involve highly time consuming Monte Carlo. In doing so we would expect to get results close to those we obtain. However, for machine learning algorithms to be of value they must be practical. Our approach, following a minimum description length methodology, provides a principled method for achieving automatic regularisation. Because it fits within the framework of deep learning it is relatively straightforward and quick to implement (using software such as Keras or pytorch with built-in automatic differentiation, fast gradient descent algorithms, and GPU support). In addition our approach provides a considerable degree of flexibility (e.g. in continuous updating, allowing exogenous data, etc.), which we believe might be much more complicated to achieve in a fully probabilistic approach. Obviously we failed to properly articulate these advantages in our original submission and will correct this when we submit our corrections.

In terms of experimental comparison – that is always challenging in unsupervised learning, especially with limited space to go deeply into the comparison. We could add equivalent NMF results to some of the figures we provide, however, we do not think it would add significantly to the purpose of the paper which is to introduce the method and idea behind PAE-NMF.

Comment 2: the paper missed important references to nonnegative auto-encoders, in particular:
<a href="https://paris.cs.illinois.edu/pubs/paris-icassp2017.pdf" target="_blank" rel="nofollow">https://paris.cs.illinois.edu/pubs/paris-icassp2017.pdf</a> (https://paris.cs.illinois.edu/pubs/paris-icassp2017.pdf)
Our response: This is a very fair point, we are happy to add in this reference to this paper.

Comment3 : the review of probabilistic NMF works is limited, see e.g.,
https://paris.cs.illinois.edu/pubs/smaragdis-spm2014.pdf
Our response: We are happy to extend our review of probabilistic NMF in this paper but should note that we are at the limit of 8 pages at the moment and there will always be a limitation on what can be added.

Comment:4  more details are needed about inference in Section 2.4
Our response: we certainly could write considerably more but as our method is based upon the work of VAEs we did not want to go over work that has already been done and discussed. Also, we are at the edge of the page limit. 

Minor comments:
Minor Comment: the notations z and h are sometimes confusing, what about using h every where ?
Our response: The use of z was due to its familiarity to researchers with a background in VAEs. If this was confusing we are happy to make this change whilst altering our paper.

Minor Comment: it’s not clear to me how the first term in (1) is equal to the second term in (2
Our response: this should have been an approximation sign rather than an equals sign. We are assuming (not an unreasonable assumption) that the errors, the gap between v-hat and v, will be approximately Gaussian. This is a common assumption made in nearly all VAEs.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rylhntPTs7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Experimental evaluations are mostly qualitative</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJGjOi09t7&amp;noteId=rylhntPTs7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper385 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">24 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper385 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper replaces the Gaussian latent variables in standard VAE with the Weibull distribution and therefore presents a VAE solution to nonnegative matrix factorization, under squared Euclidean distance loss function. The literature review summarizes four related work very well. The adopted Weibull distribution provides a tractable inverse CDF function and analytical form of the KL divergence, facilitating VAE inference. In particular, the effects of the entropy term are discussed in detail.  Experiments illustrate the generated data from the model,  the learned part-based dictionaries, and the distributions of latent variables from similar data points.  

Questions: 

1. What is the updating rule for W_f? Is it multiplicative?  In Sec 2.4, The value of W_f is kept to be nonnegative by "setting negative terms to zero". Does it mean once one entry is set to zero, it would never be positive in the sequential gradient steps? 

2. Although the proposed model is claimed to be probabilistic, the L2 loss function in equation (2) implies that the data generated from the model could be negative. How would the proposed approach handle other loss function of NMF such as KL (e.g., under Poisson assumption)?  

3. The nonnegative variant sounds interesting, but the experimental results are quite limited. It is unclear how the proposed approach would compare to other probabilistic NMF models and algorithms, or the standard VAE as a generative model. It seems the proposed method can do as good as NMF or VAE in some aspects. This begs the question of when would the proposed approach be superior to others and in what aspect? 

Minor: 
In some places where the parameter are constrained to be nonnegative, it would be more clear to use notations such as R_+ instead of R.  </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1l_jl-xRX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author response to reviewer</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJGjOi09t7&amp;noteId=S1l_jl-xRX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper385 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for the review. We will answer each comment in turn.

Comment 1. What is the updating rule for W_f? Is it multiplicative? In Sec 2.4, The value of W_f is kept to be nonnegative by "setting negative terms to zero". Does it mean once one entry is set to zero, it would never be positive in the sequential gradient steps?
Our response: we use a gradient descent method, not a multiplicative update rule which means that values of Wf which have gone to zero can become non-zero again. We did not discuss this in our paper as it is standard in neural networks to use gradient descent (or variants of) methods, while in NMF methods such as Lee and Seung’s multiplicative update is commonly used. We will add a brief mention of the update method in our resubmitted paper.

Comment 2. Although the proposed model is claimed to be probabilistic, the L2 loss function in equation (2) implies that the data generated from the model could be negative. How would the proposed approach handle other loss function of NMF such as KL (e.g., under Poisson assumption)?
Our response: we are a little unclear about the first part of this comment. The data, v-hat, generated by the model cannot be negative because the output v-hat=Wf*h where h is draw from the Weibull distribution, which is non-negative, and Wf is forced to remain non-negative by the algorithm. It is certainly true that the difference between v-hat and v can be positive or negative. We have not investigated the use of the KL divergence for the error term but we see no obvious reason why this would not work effectively.

Comment 3. The nonnegative variant sounds interesting, but the experimental results are quite limited. It is unclear how the proposed approach would compare to other probabilistic NMF models and algorithms, or the standard VAE as a generative model. It seems the proposed method can do as good as NMF or VAE in some aspects. This begs the question of when would the proposed approach be superior to others and in what aspect?
Our response: here we refer to our reply to Comment 2 from the first reviewer and copy the same comment below. The experiments are there to back up the idea which is the main part of the paper. It is challenging in many unsupervised learning problems to conclusively show the technique is effective. In this case we were trying to demonstrate some key features of our method.

Although probabilistic methods for NMF have been developed, even a full Bayesian framework still faces the problem that, for the vast majority of problems where NMF is used, we have little idea about what is the appropriate prior. We would therefore be forced to do model selection or introduce hyperparameters and perform an inference (maximum likelihood or Bayesian) based on the evidence. However, as the posterior is such cases is unlikely to be analytic this is likely to involve highly time consuming Monte Carlo. In doing so we would expect to get results close to those we obtain. However, for machine learning algorithms to be of value they must be practical. Our approach, following a minimum description length methodology, provides a principled method for achieving automatic regularisation. Because it fits within the framework of deep learning it is relatively straightforward and quick to implement (using software such as Keras or pytorch with built-in automatic differentiation, fast gradient descent algorithms, and GPU support). In addition our approach provides a considerable degree of flexibility (e.g. in continuous updating, allowing exogenous data, etc.), which we believe might be much more complicated to achieve in a fully probabilistic approach. Obviously we failed to properly articulate these advantages in our original submission and will correct this when we submit our corrections.

Minor Comments:
In some places where the parameter are constrained to be nonnegative, it would be more clear to use notations such as R_+ instead of R.
Our response: a fair point which could add to clarity – we will make this change in our manuscript.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1xMbCCbsX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Well written, interesting new idea, modest technical contribution, limited demonstration.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJGjOi09t7&amp;noteId=H1xMbCCbsX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper385 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper385 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">TITLE
A VARIATIONAL AUTOENCODER FOR PROBABILISTIC NON-NEGATIVE MATRIX FACTORISATION

REVIEW SUMMARY

Well written, interesting new idea, modest technical contribution, limited demonstration.

PAPER SUMMARY

The paper presents an approach to NMF within a variational autoencoder framework. It uses a Weibull distribution in the latent space. 

QUALITY

The work appears technically sound except for minor typos. 

CLARITY

Overall the paper is a pleasure to read. Only the presentation of the standard vae could be more clear.

ORIGINALITY

The method is (to my knowledge) novel. 

SIGNIFICANCE

I think this paper is a significant contribution. I feel I have learned something from reading it, and am motivated to try out this approach. I believe there should be a wide general interest. The technical contribution is perhaps somewhat modest, as the paper fairly straightforwardly includes non-negativity in a vae setting, but I think this is a good idea. The demonstration of the algorithm is also quite limited - I would have enjoyed seeing this applied to some more reaslistic, practical problems, where perhaps the quantification of uncertaincy (which is one of the main benefits of a vae-based nmf) would come more directly into play. 

FURTHER COMMENTS

page 3

The presentation of the VAE objective is a bit oblique. The statement "they require a different objectiv function" is not wrong, but also not very precise. The equality in eq. (2) is incorrect (I assume this is meant to be a stochastic approximation, i.e. the expectation over q approximated by sampling?)

"with \hat v  the reconstructed vector" Not clear. I assume \hat v is reconstructed from a sample from q given v ?

There is a typo in eq. 3. The first factor in the second to last term should be (lambda_1/\lambda_2)^(k_2)

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1xSbZZl0X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author response to reviewer</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJGjOi09t7&amp;noteId=S1xSbZZl0X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper385 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank this reviewer for their generous review.

Comment: The presentation of the VAE objective is a bit oblique. The statement "they require a different objectiv function" is not wrong, but also not very precise. The equality in eq. (2) is incorrect (I assume this is meant to be a stochastic approximation, i.e. the expectation over q approximated by sampling?)
Our response: the first point here is very valid – that is an imprecise sentence and we will improve it. The second point is also correct,  we should not have had an equality there as it is an approximation, we will also fix that in the manuscript.

Comment: "with \hat v the reconstructed vector" Not clear. I assume \hat v is reconstructed from a sample from q given v ?
Our response: yes that is correct, we will add a sentence about how vhat is formed there.

Comment: There is a typo in eq. 3. The first factor in the second to last term should be (lambda_1/\lambda_2)^(k_2)
Our response: Thank you very much for spotting that typo, we will correct it.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>