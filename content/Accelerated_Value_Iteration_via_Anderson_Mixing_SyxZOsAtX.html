<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Accelerated Value Iteration via Anderson Mixing | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Accelerated Value Iteration via Anderson Mixing" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SyxZOsA9tX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Accelerated Value Iteration via Anderson Mixing" />
      <meta name="og:description" content="Accelerating reinforcement learning methods is an important and challenging topic. We introduce the Anderson acceleration technique into the value iteration and develop an accelerated value..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SyxZOsA9tX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Accelerated Value Iteration via Anderson Mixing</a> <a class="note_content_pdf" href="/pdf?id=SyxZOsA9tX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019accelerated,    &#10;title={Accelerated Value Iteration via Anderson Mixing},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SyxZOsA9tX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Accelerating reinforcement learning methods is an important and challenging topic. We introduce the Anderson acceleration technique into the value iteration and develop an accelerated value iteration algorithm Anderson Accelerated Value Iteration (A2VI). We further apply our method to Deep Q-learning algorithm and propose Deep Anderson Accelerated Q-learning (DA2Q) algorithm. Our approach can be viewed as an approximation of the policy evaluation by interpolating on historical data. A2VI is more efficient than classical modified policy iteration methods. We provide a theoretical analysis of our algorithm and conduct experiments on both toy problems and Atari games. Both the theoretical and empirical results demonstrate the effectiveness of our algorithm. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Reinforcement Learning</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BJgqyzu-67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Another paper on Anderson acceleration for RL</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxZOsA9tX&amp;noteId=BJgqyzu-67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Matthieu_Geist1" class="profile-link">Matthieu Geist</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper329 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">This ICLR submission is a very interesting paper.

In addition to the reference provided by two of the reviewers, the authors might be interested by this (very recent) workshop paper, that also proposes to use Anderson acceleration for RL : <a href="https://arxiv.org/abs/1809.09501" target="_blank" rel="nofollow">https://arxiv.org/abs/1809.09501</a>

The paper is much more preliminary than this ICLR submission (eg, no convergence analysis, extension to deep reinforcement learning only briefly outlined), but it provides some complementary things (notably, a partial and empirical discussion of the actual speed of convergence of the sequence of greedy policies for accelerated VI).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SyxldX402Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxZOsA9tX&amp;noteId=SyxldX402Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper329 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper329 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This is a very well-written paper which proposed a way to accelerate the value-iteration of MDP. The method is the so-called "Anderson-Mixing" method. It replaces the policy evaluation step by solving a smaller linear equation: find a linear combination of a few historical values to represent the value of the current policy. The paper also presents a very nice explanation of why such a modification of VI accelerates VI. The paper also extends the method to DQN and shows a very nice acceleration. The experiments are convincing and interesting.

I only have two concerns: 

1) In section 4, the convergence proof is shown but the contraction is only gamma. This is the same as the original VI. Of course, this is the worst case best bound. Is it possible to show a result that the modified-VI is always better than the original VI?

2) In section 4, the dependence on k has not been studied. But k actually critically affects the time complexity. Is it possible to obtain convergence proof depending on k?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Syg7ehHc3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Extension to the Approximate DP case needed</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxZOsA9tX&amp;noteId=Syg7ehHc3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper329 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper329 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper introduces the "Anderson mixing" ideas from the broader literature on general fixed-point problems to the specific problem of finding the fixed-point to the Bellman optimality equations for a Markov Decision Processes. The general idea is to summarizes the history of previous iterates (value functions in this case) by finding of convex combination which also minimizes the residuals. The authors provide a solution for when an iterate is no longer representable by a convex combination of the recent history by simply bypassing the interpolation step and replacing it with a usual value iteration step. Using the intuition developed in the MDP case, they then adapt their DP algorithms to the learning case by substituting exact (tabular) value functions with deep function approximators. Experimental results are presented in 3 games from the ALE environment.

The jump from the DP formulation to the learning case is rather abrupt, and lacks sufficient motivation. The way the paper is currently structured is 50-50: 50% of the contribution is the DP view of the proposed method while the remaining half comes from the deep formulation (and experiments). I think that I would have preferred to see the entire paper being dedicated to the DP point of view, followed by a more principled Approximate DP analysis in the simpler linear case. Dedicating the remaining of the paper to the deep formulation almost feels like a missed opportunity to fully developing the theory initiated in the first section. But then of course the price to pay would be a paper which would be less aligned with the "representation learning" aspect of the conference. My main concern is that extending this technique to the deep setting mare involve some serious interference with other mechanisms already at play. It is very difficult to explain if the observed improvement come from the underlying DP basis or as a secondary effect of architectural and algorithmic considerations. 

To my knowledge, this is the first attempt at using Anderson mixing in the MDP framework. However, I would appreciate if the authors could survey previous attempts (if any) by other authors, or more generally existing results in the literature on non-linear fixed-point methods.  You may find relevant work by consulting the recent Zhang, Oâ€™Donoghue and Boyd paper (2018). 

# Detailed comments

&gt; Puterman 2014

The 2014 edition is likely to be a re-print of the 1994 which is commonly cited. I would double-check to see if there is any difference in the content between the 2014 and 1994 edition. If not (and just a re-print) I would cite the 1994 edition which is more widely recognized. 

&gt; Citations for VI and PI
You should cite Bellman 1957 and Howard 1961 (not Puterman). For exact references, see bibliographical remarks in Puterman. 

&gt; Citation for Modified policy iteration

Please cite original paper(s) by Puterman and Brumelle ~1978. See bibliographical remarks in Puterman 1994 (or 2014) for the origins of MPI. 

&gt;  via the Neumann expansion

truncated

&gt; computationally inefficient for complex decision problems

Compared to what? More efficient than full PI for sure

&gt; Page 2, notation for $\Gamma_\pi$ vs $\Gamma$

I suggest using a different notation for the (linear) policy evaluation operator vs the Bellman optimality one. The subscript "_\pi$ is easy to miss. 

&gt; converges much faster with K

Define K

&gt; In most cases, we can

In reinforcement learning, we can

&gt; value iteration can be finished

Finished ? 

&gt; value iteration can be finished by estimating Î“(v) through sampling.

We are no longer in the realm of DP, but more stochastic approximation methods. This isn't quite VI anymore. I would be more careful when jumping from one setting to the other.

&gt; provided the sampling estimations are accurate enough

The approach described so far does not involve any sampling. 

&gt; This modification is based on the observation that the recent successive policies do not

So far, the mixing equations (3) and (4) only describe the evaluation case. You haven't mentioned yet how you plan to combine this into a more general control algorithm where successive (changing) policies are generated.

&gt; the solution can be written explicitly as

Please cite where this comes from (or provide proof inline or appendix)

&gt; while PI is similar to Newtonâ€™s method

Cite Puterman and Brumelle for the original work on showing the connection between PI and Newton's method. 

&gt; except that the tangent line is replaced with a secant line.

Please explain this intuition: how you obtain this geometric interpretation.
Also, the secant method being an analogue to quasi-Newton methods, and policy iteration being Newton's method, there is an opportunity to better develop and explain those parallels.


</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BklSoqjdhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Accelerated Value Iteration via Anderson Mixing</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxZOsA9tX&amp;noteId=BklSoqjdhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper329 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper329 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper seems like a nice idea, but I'm not sure if it's ready for publication. It seems that the main contribution of this paper is the DA2Q algorithm, since the A2VI algorithm is a straightforward application of AA to VI. However the numerical examples are very weak, only 3 games are tested, and the results are not that strong. Furthermore in Figure 3 with the results it's not clear what the 'Time' axis is.

Smaller comments:

It seems like this recent paper should be cited:
<a href="https://arxiv.org/abs/1808.03971" target="_blank" rel="nofollow">https://arxiv.org/abs/1808.03971</a>
it includes value iteration as an example, both in theory and in practice.

I think that lemma 1 is a direct consequence of the fact that PI has finite convergence (this is easily seen since there are finite policies and it converges). 

In the contraction for PI what is K?

With the constraints as specified after equation 5 it is no longer Anderson acceleration. The convex combination constraint is just the standard alpha &gt;= 0 constraint.

Rejection step seems very onerous, how often does it occur in practice?

Note that a simple application of AA to VI would not have the problem that it needs to "Jump out of the subspace".

DA2Q algorithm as printed is very complicated, can it be simplified somehow? Just focusing on the novel steps would help.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>