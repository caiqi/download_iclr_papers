<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Meta-Learning Probabilistic Inference for Prediction | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Meta-Learning Probabilistic Inference for Prediction" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HkxStoC5F7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Meta-Learning Probabilistic Inference for Prediction" />
      <meta name="og:description" content="This paper introduces a new framework for data efficient and versatile learning. Specifically:&#10;  1) We develop ML-PIP, a general framework for Meta-Learning approximate Probabilistic Inference for..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HkxStoC5F7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Meta-Learning Probabilistic Inference for Prediction</a> <a class="note_content_pdf" href="/pdf?id=HkxStoC5F7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 13 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019meta-learning,    &#10;title={Meta-Learning Probabilistic Inference for Prediction},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HkxStoC5F7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=HkxStoC5F7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">This paper introduces a new framework for data efficient and versatile learning. Specifically:
1) We develop ML-PIP, a general framework for Meta-Learning approximate Probabilistic Inference for Prediction. ML-PIP extends existing probabilistic interpretations of meta-learning to cover a broad class of methods. 
2) We introduce \Versa{}, an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass. \Versa{} substitutes optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training.
3) We evaluate \Versa{} on benchmark datasets where the method sets new state-of-the-art results, and can handle arbitrary number of shots, and for classification, arbitrary numbers of classes at train and test time. The power of the approach is then demonstrated through a challenging few-shot ShapeNet view reconstruction task.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">probabilistic models, approximate inference, few-shot learning, meta-learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Novel framework for meta-learning that unifies and extends a broad class of existing few-shot learning methods. Achieves strong performance on few-shot learning benchmarks without requiring iterative test-time inference.   </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rJe6s67O6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your reviews and comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkxStoC5F7&amp;noteId=rJe6s67O6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper441 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper441 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Reviewers,

Many thanks for your detailed comments and suggestions. We really appreciate the time and effort you have put into reading our paper. Your comments are both insightful and constructive, and we believe have contributed to improving the quality of our paper.

We have uploaded a revised version of the paper, incorporating your comments and suggestions. Below, we address each of your reviews individually.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Byxh3JK9nQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A novel meta-learning framework</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkxStoC5F7&amp;noteId=Byxh3JK9nQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper441 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper441 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes both a general meta-learning framework with approximate probabilistic inference, and implements an instance of it for few-shot learning. First, they propose Meta-Learning Probabilistic inference for Prediction (ML-PIP) which trains the meta-learner to minimize the KL-divergence between the approximate predictive distribution generated from it and predictive distribution for each class. Then, they use this framework to implement Versatile Amortized Inference, which they call VERSA. VERSA replaces the optimization for test time with efficient posterior inference, by generating distribution over task-specific parameters in a single forward pass. The authors validate VERSA against amortized and non-amortized variational inference which it outperforms. VERSA is also highly versatile as it can be trained with varying number of classes and shots.

Pros
- The proposed general meta-learning framework that aims to learn the meta-learner that approximates the predictive distribution over multiple tasks is quite novel and makes sense.
- VERSA obtains impressive performance on both benchmark datasets for few-shot learning and is versatile in terms of number of classes and shots.
- The appendix section has in-depth analysis and additional experimental results which are quite helpful in understanding the paper.

Cons
- The main paper feels quite empty, especially the experimental validation parts with limited number of baselines. It would have been good if some of the experiments could be moved into the main paper. Some experimental results such as Figure 4 on versatility does not add much insight to the main story and could be moved to appendix.
- It would have been good if there was some validation of the time-performance of the model as one motivation of meta-learning is rapid adaptation to a test-time task. 

In sum, since the proposed meta-learning probabilistic inference framework is novel and effective I vote for accepting the paper. However the structure and organization of the paper could be improved by moving some of the methodological details and experimental results in the appendix to the main paper. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hye5i0QOpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>More experiments to main text + timing experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkxStoC5F7&amp;noteId=Hye5i0QOpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper441 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper441 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">“It would have been good if some of the experiments could be moved into the main paper. … the structure and organization of the paper could be improved by moving some of the methodological details and experimental results in the appendix to the main paper.”

We agree that a significant portion of interesting content has been relegated to the appendix in our submission. Much of this, of course, has to do with space constraints. However, we have addressed this in the revised version in line with your suggestions by (i) moving the appendix containing the toy-data experimentation to the main body of the paper (see Section 5.1), and (ii) moving some methodological details from the appendix in to the experiments section (see Section 5).

“It would have been good if there was some validation of the time-performance of the model as one motivation of meta-learning is rapid adaptation to a test-time task. “

We strongly agree that the issue of performance timing is of great interest, and it is useful and important to validate this experimentally. We were originally hesitant to add any timing results as code released with research papers is often optimized for correctness as opposed to speed. That said, we measured the test time performance of both MAML (as implemented in the authors'  publicly available repository at <a href="https://github.com/cbfinn/maml)" target="_blank" rel="nofollow">https://github.com/cbfinn/maml)</a> and Versa in 5-shot 5-way experiments on mini-ImageNet, using the same architectures for both. We found Versa to achieve 5x speed up compared to MAML, while achieving significantly better accuracy (see Table 3). We have amended the paper to include this experimental data (see Section 5.2 for details). We believe this data demonstrates the performance gains achieved by relieving the need for test time optimization procedures.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_r1llzOxFhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review for Meta-Learning Probabilistic Inference for Prediction</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkxStoC5F7&amp;noteId=r1llzOxFhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper441 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper441 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents two different sections:
1. A generalized framework to describe a range of meta-learning algorithms.
2. A meta-learning algorithm that allows few shot inference over new tasks without the need for retraining. The important aspect of the algorithm is the context independence assumption between posteriors of different classes for learning weights. This reduces the number of parameters to amortize during meta-training. More importantly, it makes it independent of the number of classes in a task, and effectively doing meta-training across class inference instead of each task. The idea sounds great, but I am skeptical of the justification behind the independence assumption which, as per its justifications sounds contrived and only empirical. 

Overall, I feel the paper makes some progress in important aspects of meta-learning.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1xx7y4dTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>On justification for the context-indepence assumption</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkxStoC5F7&amp;noteId=r1xx7y4dTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper441 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper441 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">“The important aspect of the algorithm is the context independence assumption between posteriors of different classes for learning weights. … The idea sounds great, but I am skeptical of the justification behind the independence assumption which, as per its justifications sounds contrived and only empirical.”

We thank the reviewer for imploring us to think more carefully about this point. We share the concern that providing only an empirical justification for the context independent assumption is slightly troubling. We have therefore considered this more carefully, and have found that there is a principled justification of this design choice, which is best understood through the lens of density ratio estimation [i, ii]. 

Results from Density Ratio Estimation [i, ii] show that an optimal softmax classifier learns the ratio of the densities

    Softmax(y=k | x) = p(x | y=k) / Sum_j p(x | y=j)

assuming equal a priori probability for each class. Our system follows his optimal form by setting:

            log p(\tilde{x} | y=c) proportional h_theta ( \tilde{x})^T w_c

where w_c ~ q_phi (w | {x_n ; y_n=c} ) for each class in a given task. Here {(x_n, y_n)} are the few-shot training examples, and $\tilde{x}$ is the test example. This argument states that under ideal conditions (i.e., we can perfectly estimate p(y=c | x) ), the context-independent assumption is correct, and further motivates our design.

We have amended the paper to include this argument (see Appendix B). We thank the reviewer for pointing to this important issue, and we hope that this alleviates some of their concerns.

[i] - S. Mohamed. The Density Ratio Trick. The Spectator (Blog). 2018
[ii] - M. Sugiyama, T. Suzuki, and T. Kanamori. Density ratio estimation in machine learning. 2012
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Syewq7hpoX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Few-shot learning, based on amortized inference network for parameters of logistic regression head models. Uses learning criterion based on predictive distributions on train/test splits. Extensive comparison, achieves state-of-the-art despite simpler setup than many competitors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkxStoC5F7&amp;noteId=Syewq7hpoX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper441 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">24 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper441 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
This work tackles few-shot (or meta) learning from a probabilistic inference viewpoint. Compared to previous work, it uses a simpler setup, performing task-specific inference only for single-layer head models, and employs an objective based on predictive distributions on train/test splits for each task (rather than an approximation to log marginal likelihood). Inference is done amortized by a network, whose input is the task training split. The same network is used for parameters of each class (only feeding training points of that class), which allows an arbitrary number of classes per task. At test time, inference just requires forward passes through this network, attractive compared to non-amortized approaches which need optimization or gradients here.

It provides a clean, decision-theoretic derivation, and clarifies relationships to previous work. The experimental results are encouraging: the method achieves a new best on 5-way, 5-shot miniImageNet, despite the simple setup. In general, explanations in the main text could be more complete (see questions). I'd recommend shortening Section 4, which is pretty obvious.

- Quality: Several interesting differences to prior work. Well-done experiments
- Clarity: Clean derivation, easy to understand. Some details could be spelled out better
- Originality: Several important novelties (predictive criterion, simple model setup, amortized inference network). Closely related to "neural processes" work, but this happened roughly at the same time
- Significance: The few-shot learning results are competitive, in particular given they use a simpler model setup than most previous work. I am not an expert on these kind of experiments, but I found the comparisons fair and rather extensive

Interesting about this work:
- Clean Bayesian decision-theoretic viewpoint. Key question is of course whether
   an inference network of this simple structure (no correlations, sum combination
   of datapoints, same network for each class) can deliver a good approximation to
   the true posterior.
- Different to previous work, task-specific inference is done only on the weights of
   single-layer head models (logistic regression models, with shared features).
   Highly encouraging that this is sufficient for state-of-the-art few-shot classification
   performance. The authors could be more clear about this point.
- Simple and efficient amortized inference model, which along with the neural
   network features, is learned on all data jointly
- Optimization criterion is based on predictive distributions on train/test splits, not
   on the log marginal likelihood. Has some odd consequences (question below),
   but clearly works better for few-shot classification

Experiments:
- 5.1: Convincing results, in particular given the simplicity of the model setup and
   the inference network. But some important points are not explained:
   - Which of the competitors (if any) use the same restricted model setup (inference
      only on the top-layer weights)? Clearly, MAML does not, right? Please state this
      explicitly.
   - For Versa, you use k_c training and 15 test points per task update during
      training. Do competitors without train/test split also get k_c + 15 points, or
      only k_c points? The former would be fair, the latter not so much.
- 5.2: This seems a challenging problem, and both your numbers and reconstructions
   look better than the competitor. I cannot say more, based on the very brief
   explanations provided here.
   The main paper does not really state what the model or the likelihood is. From
   F.4 in the Appendix, this model does not have the form of your classification
   models, but psi is input at the bottom of the network. Also, the final layer has
   sigmoid activation. What likelihood do you use?
   One observation: If you used the same "inference on final layer weights" setup
   here, and Gaussian likelihood, you could compute the posterior over psi in closed
   form, no amortization needed. Would this setup apply to your problem?

Further questions:
- Confused about the input to the inference network. Real Bayesian inference would
   just see features h_theta(x) as inputs, not the x's. Why not simply feed features in
   then?
   Please do improve the description of the inference network, this is a major
   novelty of this paper, and even the appendix is only understandable by reading
   other work as well. Be clear how it depends on theta (I think nothing is lost by
   feeding in the h_theta(x)).
- The learning criterion based on predictive distributions on train/test splits seem
   to work better than ELBO-like criteria, for few-shot classification.
   But there are some worrying aspects. The marginal likelihood has an Occam's
   razor argument to prevent overfitting. Why would your criterion prevent overfitting?
   And it is quite worrying that the prior p(psi | theta) drops out of the method
   entirely. Can you comment more on that?

Small:
- p(psi_t | tilde{x}_t, D_t, theta) should be p(psi_t | D_t, theta). Please avoid a more
   general notation early on, if you do not do it later on. This is confusing
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkxV4A7_am" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Responding to your review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkxStoC5F7&amp;noteId=rkxV4A7_am"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper441 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper441 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">“Which of the competitors (if any) use the same restricted model setup (inference only on the top-layer weights)?”

To the best of our knowledge, almost all the competing methods adapt the entire network for new tasks. We have amended the paper to clarify this point (see Section 5.2).

“Do competitors without train/test split also get k_c + 15 points, or only k_c points?”

To the best of our knowledge, all methods we compare to use train/test splits, with the exception of the VI methods referenced in Table 1. The VI methods used the same number of observations at train time (i.e., the data available to all methods was identical).

“The main paper does not really state what the model or the likelihood is [in the ShapeNet experiments]. From F.4 in the Appendix, this model does not have the form of your classification models, but psi is input at the bottom of the network. Also, the final layer has sigmoid activation. What likelihood do you use?”

The terseness of the ShapeNet model details was a result of space constraints. We have amended the paper to include additional explanatory details (see Section 3). You are correct in observing that psi plays a different role from the classification case, namely as an input to the image-generator. The likelihood we used is Gaussian, the sigmoid activation ensures that the mean is between 0 and 1, reflecting the constraints on pixel-intensities. Your observation that using top-layer weights would allow us to perform exact inference is very insightful. We decided to use an architecture that passed the latent parameters underlying each shape instance through multiple non-linearities, but it would be very interesting to compare to the simpler baseline that you suggest. As this is a significant undertaking, we will leave it to future work,

“Real Bayesian inference would just see features h_theta(x) as inputs, not the x's. Why not simply feed features in then? … Be clear how it depends on theta (I think nothing is lost by feeding in the h_theta(x)).”

Thank you for suggesting this cleaner way of presenting our work. We agree with your observations on the input to the inference network. We have amended Fig. 2 accordingly, and have improved the descriptions in Section 3.

“The marginal likelihood has an Occam's razor argument to prevent overfitting. Why would your criterion prevent overfitting?”

The mechanism preventing overfitting in our criterion is the meta train / test splits, which explicitly encourages the model to generalize from the training observations to the test data. Methods based on held-out sets, like cross validation, are known to favor models which are more complex than those favoured by Bayesian model comparison [i, ii]. However, as is empirically demonstrated in the experimental section, our proposed criterion consistently outperformed variational objectives.

“It is quite worrying that the prior p(psi | theta) drops out of the method entirely. Can you comment more on that?”

This is a subtle point that we view as both a feature and a bug. It is a feature in the sense that a prior is learned implicitly through the sampling procedure (as is shown for example in the simple Gaussian experiment -- see Section 5.1). This can be compared to VI, for example, where the prior enters through a KL regularization term which often favours underfitting. It is a bug if, for example, the user has a priori knowledge about the parameters that they would like to leverage. In this case, it could be possible to use synthetic training data to incorporate such knowledge into the scheme.  However, for the predictive purposes explored in this work, we did not find that the lack of prior posed an issue.


[i] - C. E. Rasumessen and Z. Ghahramani. Occam’s razor. 2001.
[ii] - I. Murray and Z. Ghahramani. A note on the evidence and Bayesian Occam’s razor. 2005.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>