<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Metric-Optimized Example Weights | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Metric-Optimized Example Weights" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SklgHoRqt7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Metric-Optimized Example Weights" />
      <meta name="og:description" content="Real-world machine learning applications often have complex test metrics, and may have training and test data that follow different distributions.  We propose addressing these issues by using a..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SklgHoRqt7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Metric-Optimized Example Weights</a> <a class="note_content_pdf" href="/pdf?id=SklgHoRqt7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019metric-optimized,    &#10;title={Metric-Optimized Example Weights},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SklgHoRqt7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Real-world machine learning applications often have complex test metrics, and may have training and test data that follow different distributions.  We propose addressing these issues by using a weighted loss function with a standard convex loss, but with weights on the training examples that are learned to optimize the test metric of interest on the validation set. These metric-optimized example weights can be learned for any test metric, including black box losses and customized metrics for specific applications.  We illustrate the performance of our proposal with public benchmark datasets and real-world applications with domain shift and custom loss functions that balance multiple objectives, impose fairness policies, and are non-convex and non-decomposable.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_ryeamta0hQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>good initiative, not mature enough</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SklgHoRqt7&amp;noteId=ryeamta0hQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper55 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper55 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose to optimize a black-box (validation/test) metric by learning to re-weight the training examples. The weights are calculated from a linear model on an auto-encoder-computed embedding, and the parameters of the linear model is found by an Gaussian-Process-Regression-(UCB)-guided global optimization procedure. Experimental results demonstrate that the learnt weights outputs uniform weights.

The paper is well-written with clear motivations. Nevertheless, the paper is not mature due to the following reasons:

(1) The alternatives are not carefully compared/discussed for the key components of the proposed framework.
   (1a) Why need the linear model? What if the weights are not calculated from the linear model parameters, but optimized by GPUCB directly?
   (1b) Why need GPUCB? What if we just do random search or standard simulated annealing for global optimization?
   (1c) Following (1a) and (1b), what if the weights are optimized directly through random search?
   (1d) What if, in the case of MNIST, class weights are used instead of example weights
   (1e) How sensitive is the proposed framework in terms of hyperparameters like p and q, and perhaps other GP parameters?

(2) No comparison on the standard-but-challenging metrics like F1. The authors state in the experiments that it is not the focus of the work, but I do believe that the comparison is meaningful to help understand whether the proposed framework is close to the state-of-the-art in those standard metrics. Otherwise the baseline (uniform weights) is arguably just too weak.

(3) Is the framework just overfitting the validation data set by reusing it to evaluate multiple times? Are there overfitting behavior observed during the reuses?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Byg37fo2hm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting problem. I think more discussion and experiments to understand better potential overfitting issues are needed here.  I'm intrigued that the approach works as well as it does on the examples given, so there may be something interesting here.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SklgHoRqt7&amp;noteId=Byg37fo2hm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper55 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper55 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Pros:
- Addresses several interesting and important problems all at once: covariate shift, concept drift, mismatch between training loss and test loss.
- Fairly simple and elegant solution.
- Multiple examples of the method working.
- Clearly written

Cons:
- Examples don't feel like full-fledged machine learning examples, where you  tune your learning algorithm on the validation set, as well as the example weights (their approach).  
- Needs discussion of potential overfitting issues (see comments below).

Comments:
- I think one area that is underexplored in this paper is overfitting.  One issue is overfitting the validation data by having more complicated weight functions. For example, in Figure 3(b), as the embedding dimension goes beyond 14, it seems like the error metric gets worse.  Would be interesting to see a plot of the validation error metric alongside this test error metric.  Also, what happens when we use even larger embedding dimensions -- that should clarify whether this is an overfitting situation, or just a random chance fluctuation.
- In machine learning contexts, it's standard to try many different ML methods (or at least network architectures) with various hyperparameter settings and regularization methods, yet this isn't discussed at all in the paper.  Would you use the search for alpha as an inner loop in your model search and hyperparameter selection process?  I'd expect there could be additional issues with overfitting the validation set as you used more complicated models.  I think not discussing or investigating this makes the examples feel a little bit more like toys.  I think tuning your learning algorithm settings on a validation set is pretty intrinsic to machine learning approaches.
- Relatedly, you say "we impose no regularization on the model parameters"... does this include things like early stopping, dropout, or other things that are used to prevent overfitting?  This seems just part of the "no hyperparameter tuning" setting of the paper.  
- In the introdution you say "MOEW . . . reshapes the total loss function to better match the testing metric.  This is similar to the idea of basis expansion, where we approximate the metric function using a linear combination of per example loss functions".  You make a similar statement in the conclusion. This is an interesting idea, but it doesn't seem to represent what you're doing. This explanation suggests that you are fitting \alpha's so that the objective function value approximates the validation metric for each theta.  But that's not what you're doing, right?
- In 3.2, you say that c "is a constant that normalizes the weights over a (batch from) the training set T".  Why would you renormalize per batch?  This seems to potentially negate the effect of the reweighting, especially for small batches.
- It might have been interesting to see if there was any significant differences between hinge loss and cross-entropy loss for the binary case.
- In the MNIST experiment, am I correctly understanding that the difference between the 300 uniform weighted models you tried was the random initialization of the weights? Was there a lot of variation in performance among these trials?  Folk wisdom makes me think there would not be large performance differences.  
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Bke6g1Hc37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper motivates using a nonlinear function based on an autoencoded representation to derive training sample weights to optimize any test metric. It seems like a significant contribution, particularly for analyzing datasets where training and validation data are known to be drawn from different distributions.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SklgHoRqt7&amp;noteId=Bke6g1Hc37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper55 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper55 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This proposal learns training data weights that optimize any given test metric.
They do this by learning both a weighting function and a classifier. They 
iteratively train an ensemble of K {weighting_k,classifier} pairs, and
selecting the pair that presents the best best metric-of-interest value over
the test set (over all iterations) as the final output.

The paper provides a large set of references that I found useful, and is
clearly written.

Each training iteration of their MOEW algorithm first optimizes classifiers 
over all training examples, for each of the K sample weightings.  Each
weighting gives rise to one converged classifier, whose metric-of-interest is
evaluated on the validation set.  Given K sets of weighting functions and their
metric-of-interest values, new parameters for the weighting functions are
generated.

The weighting function choice is a simple function, based on a linear transform
of autoencoder features, a normalization factor, and factor to account for
differing label frequencies in training and test data.  The low-dimensional
auto-encoding of {data,label} pairs is trained once, using the training data.
The weighting function parameters are a linear transformation matrix. They
also have a sigmoid non-linearity in their weighting function.

If I understand correctly, their approach seems to generalize the importance
sampling methods that they reference by using a nonlinear combination of autoencoder
features.  They reference importance sampling methods that, for example, use 
Gaussian kernel basis functions to model training/test example densities.
While they do provide extensive references to previous approaches, I would have
enjoyed a clearer explanation of main differences between their weighting
function and ones that have previously been used in importance sampling.

Their main contribution seems to be the procedure (Alg. 2) used to updated the
weighting function parameters.  They first fit a model that predicts
metric-of-interest values given all previous weighting function parameters used
in the algorithm.  Then they use this model to generate the next set of
weighting function parameters.  Their method adapts a Gaussian Process
Upper-Confidence Bound to batch-wise processing.  Within-batch and between
batch exploration of the weighting function parameters is controlled by two
parameters, which they held fixed at values corresponding to +/-1 for a normal
distribution.

Only the validation set need be iid with the test set, so their method seems
quite general.

Their MNIST results use very simple networks to show that learning weighting
functions has the most benefit when classifier networks are severely
under-parameterized.  Their wine price example addresses the choice of
embedding dimension, and they found their error metric decreased from 52% with
uniform weighting, to around 46% as they approached ~10 dimensions in the
autoencoded {data,label} representation of the training data.  They then use a
small crime dataset with a complicated test metric measuring *fairness*, based
on dividing the dataset into 4 quantiles based on white population. Using
MOEW they could improve the fairness metric with little effect on the accuracy
metric.  They could also preset thresholds to achieve very good fairness on the
training data and use MOEW to maximize the accuracy metric, which resulting
in improving both fairness and accuracy compared to uniform sample weighting.

For spam blocking and web page quality, I would like to see a little more 
interpretation of their results. They again show improvements using MOEW to
provide weights for training data.  This time, Table 2 presents a comparison
with importance sampling, but the methodology they used for importance sampling
is not well described.  I'd like to understand where this improvement came
from. What model/procedure was used for importance weighting? Gaussian rbfs?  
Do the authors attribute the dramatic improvement for MOEW for Spam Blocking 
simply to having a more flexible sample distribution function?  Or is it mainly
due to their adapting to the test metric?  Then for Web Page Quality, would it
be correct to conclude that the old and new web pages (training vs
validation/test) are actually fairly similary distributed?

What values of B and K are reasonable values in Alg. 1?  When applied to larger
problems, where do the authors feel the bottlenecks in Algs 1 and 2 will lie?
Do the authors find that retuning the classifier for different weighted samples
to take a lot of time?

Pros:
 - tests on several datasets
 - seems fairly generally applicable.
 
Cons:
 - To reproduce, I guess I'd need to adapt existing GP-UCB code from python or
   C++, and I'm not sure how easy this would be. Releasing code to reproduce
   the results might be nice.
 - Better understanding of differences wrt. typical Importance Sampling methods
   would nice.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>