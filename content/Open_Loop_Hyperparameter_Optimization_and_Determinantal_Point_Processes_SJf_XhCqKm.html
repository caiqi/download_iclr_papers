<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Open Loop Hyperparameter Optimization and Determinantal Point Processes | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Open Loop Hyperparameter Optimization and Determinantal Point Processes" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SJf_XhCqKm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Open Loop Hyperparameter Optimization and Determinantal Point..." />
      <meta name="og:description" content="Driven by the need for parallelizable hyperparameter optimization methods, this paper studies open loop search methods: sequences that are predetermined and can be generated before a single..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SJf_XhCqKm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Open Loop Hyperparameter Optimization and Determinantal Point Processes</a> <a class="note_content_pdf" href="/pdf?id=SJf_XhCqKm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019open,    &#10;title={Open Loop Hyperparameter Optimization and Determinantal Point Processes},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SJf_XhCqKm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Driven by the need for parallelizable hyperparameter optimization methods, this paper studies open loop search methods: sequences that are predetermined and can be generated before a single configuration is evaluated. Examples include grid search, uniform random search, low discrepancy sequences, and other sampling distributions.
In particular, we propose the use of k-determinantal point processes in  hyperparameter optimization via random search. Compared to conventional uniform random search where hyperparameter settings are sampled independently, a k-DPP promotes diversity.  We describe an approach that transforms hyperparameter search spaces for efficient use with a k-DPP. In addition, we introduce a novel Metropolis-Hastings algorithm which can sample from k-DPPs defined over any space from which uniform samples can be drawn, including spaces with a mixture of discrete and continuous dimensions or tree structure. Our experiments show significant benefits  in realistic scenarios with a limited budget for training supervised learners, whether in serial or parallel.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">hyperparameter optimization, black box optimization</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We address fully parallel hyperparameter optimization with Determinantal Point Processes. </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SylYFraThX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>small number of hyperparameters, comparison with spearmint not strong enough</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJf_XhCqKm&amp;noteId=SylYFraThX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1373 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1373 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">I reviewed the same paper last year. I am appending a few lines based on the changes made by authors.

The authors propose k-DPP as an open loop (oblivious to the evaluation of configurations) method for hyperparameter optimization and provide its empirical study and comparison with other methods such as grid search, uniform random search, low-discrepancy Sobol sequences, BO-TPE (Bayesian optimization using tree-structured Parzen estimator) by Bergstra et al. (2011). The k-DPP sampling algorithm and the concept of k-DPP-RBF over hyperparameters are not new, so the main contribution here is the empirical study. 

The first experiment by the authors shows that k-DPP-RBF gives better star discrepancy than uniform random search while being comparable to low-discrepancy Sobol sequences in other metrics such as distance from the center or an arbitrary corner (Fig. 1).

The second experiment shows surprisingly that for the hard learning rate range, k-DPP-RBF performs better than uniform random search, and moreover, both of these outperform BO-TPE (Fig. 2, column 1).

The third experiment shows that on good or stable ranges, k-DPP-RBF and its discrete analog slightly outperform uniform random search and its discrete analog, respectively.

I have a few reservations. First, I do not find these outcomes very surprising or informative, except for the second experiment (Fig. 2, column 1). Second, their study only applies to a small number like 3-6 hyperparameters with a small k=20. The real challenge lies in scaling up to many hyperparameters or even k-DPP sampling for larger k. Third, the authors do not compare against some relevant, recent work, e.g., Springenberg et al. (<a href="http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf)" target="_blank" rel="nofollow">http://aad.informatik.uni-freiburg.de/papers/16-NIPS-BOHamiANN.pdf)</a> and Snoek et al. (https://arxiv.org/pdf/1502.05700.pdf) that is essential for this kind of empirical study.

COMMENTS ON THE CHANGES SINCE THE LAST YEAR

I am not convinced by the comparison with Spearmint added by the authors since the previous version. It is unclear to me if the comparison of wall clock time and accuracy holds for larger number of hyperparameters or against Spearmint with more parallelization.

In addition the authors do not compare against more recent work, e.g., 

@INPROCEEDINGS{falkner-bayesopt17,
 author    = {S. Falkner and A. Klein and F. Hutter},
 title     = {Combining Hyperband and Bayesian Optimization},
 booktitle = {NIPS 2017 Bayesian Optimization Workshop},
 year      = {2017},
 month     = dec,
}

@InProceedings{falkner-icml-18,
  title =        {{BOHB}: Robust and Efficient Hyperparameter Optimization at Scale},
  author =       {Falkner, Stefan and Klein, Aaron and Hutter, Frank},
  booktitle =    {Proceedings of the 35th International Conference on Machine Learning (ICML 2018)},
  pages =        {1436--1445},
  year =         {2018},
  month =        jul,
}</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BklWLzRl07" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for both rounds of review you have provided for our paper. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJf_XhCqKm&amp;noteId=BklWLzRl07"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1373 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1373 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We greatly appreciate both rounds of review that you've provided for our paper. We took the first review quite seriously, and it seems that you missed some of the changes we made in response to your review and others.  Specifically, addressing items that have changed that were mentioned in your first review: 
The first experiment actually shows dispersion, not discrepancy. While we include star discrepancy in the appendix (in Figure 5, not Figure 1), we argue in section 3 that dispersion is a better measure of the quality of a point set for optimization (though discrepancy has been the tool of choice for previous work). We include a theorem which bounds the optimization error with dispersion, a connection which discrepancy lacks, and show our approach outperforms the Sobol sequence and uniform sampling. We believe this connection will encourage future work to move away from evaluating open loop methods with discrepancy, and to use dispersion instead.
The third experiment does not address discretization error. Instead, it is another hyperparameter optimization experiment on a different search space, again showing that samples from a K-DPP outperform uniform samples, the Sobol sequence, and BO-TPE.

We believe our method for defining a K-DPP over tree-structured, mixed discrete-continuous spaces is novel, as is the sampling algorithm we introduce in Algorithm 2. We know of no other approach that can draw K-DPP samples from tree-structured, mixed discrete-continuous domains. Previous work using K-DPPs for hyperparameter optimization (Kathuria et al., 2016, Wang et al., 2017) discretize continuous domains, then use a known algorithm to sample from a discrete (non-tree structured) base set. Your first review mentions a plot which showed the error from discretizing the search space, empirically motivating this contribution.

To address the listed reservations: we do find it surprising that samples from a K-DPP match or outperform the Sobol sequence in our synthetic measures, as the Sobol sequence was designed specifically to perform well. Additionally, it has become perhaps the most frequently used approach (e.g. without function evaluation results, Spearmint returns the Sobol sequence, while our results indicate that it should return K-DPP samples instead).

We agree that scaling into large K and D is important, but that isn't the focus of this work, and there is a large body of work on improving space and time complexity of GPs which is directly applicable to our approach.

When considering which pieces of recent work to compare against, we emphasize that our work is not trying to answer the question, "Is active learning helpful?" by comparing active learning approaches against our open loop approach; instead, we focus on comparing against other non-active learning approaches. For example, Hyperband starts by uniformly sampling K hyperparameter assignments, then (partially) training and evaluating models with those assignments. This work does not advocate replacing Hyperband with a single draw from a K-DPP (i.e., replacing an active learning strategy with a non-active learning strategy), but it does argue that the uniform sampling step in Hyperband be replaced with a draw from a K-DPP (replacing an open loop strategy with another, better one). All of your suggested comparisons are against active learning approaches.

We do include experiments comparing against Spearmint (an active learning approach), though this is meant to illustrate the large cost in optimization time that active learning entails. We appreciate the suggestion to compare against Spearmint with more parallelization, but note that in our experiments we compared against the most parallel possible Spearmint configuration (as well as a number of others). Any active learning strategy (excluding Hyperband-style evaluations of partially trained models) will take at least twice as long in expectation as a fully-parallel non-active learning strategy like a K-DPP to train and evaluate models with a set of hyperparameter assignments, so we expect the results in Figure 4 to hold for any number of hyperparameters.

Thank you again for your review, we look forward to further discussion.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Bke7SBzq2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting and clear idea on using DPPs for hyperparameter search</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJf_XhCqKm&amp;noteId=Bke7SBzq2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1373 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1373 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">- This paper proposes an approach to get samples with high dispersion for hyperparameter optimisation. 
- It theoretically motivates the use of Determinantal Point Processes in yielding such samples.
- Further, an iterative mixing algorithm is proposed to handle continuous and discrete sample space.
- Experiments on finding hyperparameter for sentence classification are presented. In terms of accuracy, it performs better than other open-loop methods. In comparison to closed-loop methods, it yields parameter settings with comparable performance but with gains in wall clock time.
- The distinction from close-loop approaches makes it easy to parallelise.


This paper is novel in its modelling of hyperparameter optimisation with DPP and the theoretical justification and experiments have been clearly presented. It would be interesting to explore the practicability of the method on more large-scale experiments on image related tasks.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkl41X0xC7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>How else can we improve?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJf_XhCqKm&amp;noteId=rkl41X0xC7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1373 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1373 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank AnonReviewer2 for their thoughtful review. 

We have been convinced by the synthetic and experimental results (on two hyperparameter search spaces) of the efficacy and generality of our approach, and ask what experiments on an image dataset would contribute beyond the current experimental results?

We look forward to further discussion, especially if there are any other points we can clarify, or additional ways you suggest we could improve our work.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rklkvgsL27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>recommending rejection because of lack of analyses and questionable novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJf_XhCqKm&amp;noteId=rklkvgsL27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1373 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1373 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose to use k-DPP to select a set of diverse parameters and use them to search for a good a hyperparameter setting. 

This paper covers the related work nicely, with details on both closed loop and open loop methods. The rest of the paper are also clearly written. However, I have some concerns about the proposed method.
- It is not clear how to define the kernel, the feature function and the quality function for the proposed method. The choices of those seem to have a huge impact on the performance. How was those functions decided and how sensitive is the result to hyperparameters of those functions?
- If the search space is continuous, what is the mixing rate of Alg. 2? In practice, how is "mixed" decided? What exactly is the space and time complexity? I'm not sure where k log(N) comes from in page 7.  
- Alg. 2 is a straight forward extension of Alg. 1, just with L not explicitly computed. I think it would have more novelty if some theoretical analyses can be shown on the mixing rate and how good this optimization algorithm is. 

Other small things:
- citation format problems in, for example, Sec. 4.1. It should be \citep instead of \cite. 
- it would be good to mention Figure 2 in the text first before showing it. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkeUrUReAX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your review.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJf_XhCqKm&amp;noteId=BkeUrUReAX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1373 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1373 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank AnonReviewer3 for their review, and address their points in order. 

We experimented with a number of different kernels, including cosine similarity (the most common approach used with K-DPPs), a kernel built using Levenshtein distance, and the RBF kernel. We found our results to be quite robust to the choice of the kernel -- we saw the K-DPP outperform the other approaches on our hyperparameter optimization experiments for all three. Similarly, we experimented with a number of different bandwidths for the RBF kernel, and found that as long as the bandwidth was large enough that the points interacted each other, it performed well. Our experiments focused on the presented feature function, as it was the most natural, but we expect any feature function which allows the points to be repulsed from one another (through the kernel) would behave similarly. In this work, we set the quality function to be 1, and have left learning the quality function to future work.

If the search space is continuous, the mixing rate of Alg. 2 is not known. In practice, the MCMC algorithm is quite fast (a small fraction of the expense of training the models) so we ran the algorithm for 10x as long as the expected mixing rate if the space had been discrete, though our synthetic and real experimental results indicated that it was mixed significantly earlier.
The k log(N) term appears when analyzing the difference between Algorithm 1 and Algorithm 2: computation and storage of the NxN kernel matrix L. In the discrete case, Algorithm 1 requires computing all of L, which has time and space complexity of O(N^2). Algorithm 2, instead of constructing L directly, only uses a submatrix of L computed on the fly. It runs for O(N log(N)) steps, and at each step computes and stores at most O(k) additional distances, leading to a total of O(Nk log(N)) time and space complexity (with a max of O(N^2) once it computes all of L). Therefore, Algorithm 2 has better complexity when O(Nk log(N)) &lt; O(N^2), or when k log(N) &lt; N. Otherwise, Algorithm 1 and 2 have the same time and space complexity. We will include a clarification in the paper, please let us know if this is still unclear.

While we agree Algorithm 2 is a straightforward extension, it is an important one for the community. Other work that has used K-DPPs for hyperparameter optimization (Kathuria et al., 2016, Wang et al., 2017) has been restricted to non-tree structured domains, and has discretized continuous spaces to be discrete so they could use existing sampling algorithms, which we experimentally found to hurt performance. We introduce the ability to sample from more realistic hyperparameter spaces.

Thank you for pointing out the small changes, they will be updated. 

The title of your review mentions worries about novelty, but as we mentioned in a reply to another review, we believe this approach (drawing samples from tree-structured, mixed discrete and continuous spaces in the open loop regime) and analysis (including dispersion calculation) are novel. We welcome further discussion, especially of more specific novelty concerns that may arise, and look forward to further suggestions on how to improve our paper or clarifications we can provide.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>