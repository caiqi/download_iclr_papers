<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Optimization on Multiple Manifolds | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Optimization on Multiple Manifolds" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HJerDj05tQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Optimization on Multiple Manifolds" />
      <meta name="og:description" content="Optimization on manifold has been widely used in machine learning, to handle optimization problems with constraint. Most previous works focus on the case with a single manifold. However, in..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HJerDj05tQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Optimization on Multiple Manifolds</a> <a class="note_content_pdf" href="/pdf?id=HJerDj05tQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019optimization,    &#10;title={Optimization on Multiple Manifolds},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HJerDj05tQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Optimization on manifold has been widely used in machine learning, to handle optimization problems with constraint. Most previous works focus on the case with a single manifold. However, in practice it is quite common that the optimization problem involves more than one constraints, (each constraint corresponding to one manifold). It is not clear in general how to optimize on multiple manifolds effectively and provably especially when the intersection of multiple manifolds is not a manifold or cannot be easily calculated. We propose a unified algorithm framework to handle the optimization on multiple manifolds. Specifically,  we integrate information from multiple manifolds and move along an ensemble direction by viewing the information from each manifold as a drift and adding them together. We prove the convergence properties of the proposed algorithms. We also apply the algorithms into  training neural network with batch normalization layers and achieve preferable empirical results.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Optimization, Multiple constraints, Manifold</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">This paper introduces an algorithm to handle optimization problem with multiple constraints under vision of manifold.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">5 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_ByxNEz2p27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>a novel algorithm on optimization on multiple manifolds</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJerDj05tQ&amp;noteId=ByxNEz2p27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper259 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper259 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a novel algorithm for optimization on multiple manifolds. The moving direction fuses gradient information from each manifolds via correlation. More importantly, the convergence is guaranteed.

However, the empirical results seems not very good compared to SGD.

My concerns: 

1) How you ensure each step is descent? 

2) How is the performance of the proposed algorithm compared to the ADMM which is well-suited for this problem.

The presentation needs to be improved.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hye2YPOu37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The problems addressed in the paper are interesting and crucial from both practical and theoretical perspectives. However, there are various major mathematical, conceptual and algorithmic problems with this paper.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJerDj05tQ&amp;noteId=Hye2YPOu37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper259 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper259 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
The title is misleading, since only two particular manifolds are studied in this work. In addition, the proposed methods cannot be applied to a larger or a general class of manifolds. Therefore, you should update the title.

There are multiple problem definitions proposed in the paper. They are not compatible with each other and also with the proposed methods. In addition, some of the proposed problem definitions are incorrect, as explained below:

You should be more precise about the definition of the manifold you consider in this paper. For example, in equation (1), please define your manifold of interest more precisely checking some standard textbooks.

Please define intersection of manifolds, what do you mean by which intersection of which type of manifolds?

In the contribution (1); the paper does not introduce an algorithm to deal with optimization on with multiple manifolds, but for a particular type of individual manifolds.

In the contribution (2): It is not clear why and how the proposed method can be applied to optimization on manifolds with momentum (what do you mean by use of momentum here?), and regularization (what do you mean by regularization?). There are many problems with this claim, but you can simply consider that applying momentum and regularization will affect the geometry of loss landscape.

Definition of retraction is not precise, please fix it.
What is L in equation (2)?

Please define neighborhood in U_x in Lemma 2.1.

What is || ||in Lemma 2.1?

As you also noticed on page 3, an intersection of manifolds may not be a manifold. Then, your proposed first problem (1) fails. Therefore, you should completely change your claims on your problem definitions and contributions.

What do you mean by “We add a drift which contains information from the other manifold to the original gradient descent on manifold”? What is “the information from the manifold”? In equations (3) and (4), you just apply optimization on manifolds individually. 

How do you compute/determine a_k^(1) and a_k^(2)? How do they affect the theoretical and experimental results?
In your claim “From the construction of bk, we can see that the smaller the correlation between gradf(xk) and hk is, the smaller effect the information from M2 brings”, it is not clear how “the information from M2” affects? First, again, what is “the information”? Second, b_k^(1) and b_k^(2) are computed for individual manifolds separately. Then, how “the information” make an effect?

 In Theorem 2.2, what do you mean by “then xk convergence to a local minimizer“?

What is &lt;,&gt; in Theorem 2.2?

What is ^ in Theorem 2.3?

What is v in proof 6?

What is an engine value?

What does P (1) xk gradf(yk) denote in computation of h_k? For example, gradf(yk) is a vector on tangent space of the second manifold at yk. Then, how do you project orthogonally this projected vector to the tangent space of the first manifold at xk? 

They may be completely different geometries, and such an “orthogonal projection” may not exist in general. Then, how do you compute and calculate that projection?

All the theoretical results given in the paper are not about convergence of parameters on a manifold at the intersection or product of manifolds but for an individual manifold. For example, x and y belong to manifolds M1 and M2, and convergence results is about x. How are they related to parameters at the intersection or product of manifolds?

The statements regarding batch normalization are confusing and also sound incorrect:

Do you apply batch normalization on weights on BN(w)?

Please explain what you mean by “BN(w) has same image space on G(1, n) and St(n, 1)“. There are not such results in the papers Cho &amp; Lee (2017); Huang et al. (2017) you cited for these results.

What do you mean by “applying optimization on manifold to batch normalization problem”?

In your statement “However, the property of these two manifold implies that we can actually optimize on the intersection of two manifolds”. Please explain how does this property imply this result more precisely?

Please define “Grassmann manifold G(1, n)“ more precisely. In your notation, together with explanation of the notation for St(n,p), G(1,n) is like a set of 1xn dimensional row vectors, while St(n,1) is an nx1 dimensional column vector, Then, their intersection is an empty set and your proposal for optimization on a vector on their intersection is wrong. 

Notation and definitions used in (9) are wrong and confusing. Please check and revise them.

In the whole paper, the problem, method, solutions, theorems, and contributions are proposed for optimization using parameters which belong to intersection of some manifolds. Then, suddenly, you start considering optimization on product manifolds, and give the results for that;

What does the statement “Then we apply Algorithm 1 to update parameters, which means we optimize on a product manifold” mean?

What do “G(1, k1) × · · · G(1, kn)” and “St(k1, 1) × · · · St(kn, 1)” denote?

Don’t you perform optimization on intersection of manifolds? Why do you ignore your original problem and methods, and consider this problem? 

In addition, how do you use your Algorithm 1 for optimization on product manifolds? Optimization on intersection on manifolds and product manifolds are completely different problems. If they are same or related to each in particular cases in your specific definitions, then you should provide these definitions more precisely.

What do you mean by optimization on product manifold of weights of all layers? If you compute a product manifold for spaces of all layers, then you simply perform a shallow optimization on a huge matrix containing millions of dimensions according to this definition. First, how do you do that? Second, how can you train a large network using this approach?

In the experiments, please first give variance of errors. These results are statistically insignificant.

Which problem is solved to perform these experiments is not also clear (see above).

The results reported in the paper are also not good, may be due to the mathematical and algorithmic  problems and errors mentioned above. Please clarify them, and provide additional results, especially using other datasets (small scale mnist and large scale imagenet), and networks (mlp, vgg, resnet etc.)

Related work is also incomplete, such that many traditional and recent work on optimization on multiple manifolds are omitted. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">1: Trivial or wrong</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1l_Mpupo7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The problem considered in this paper is interesting. However, there are quite a few fundamental errors about Riemannian optimization. Therefore, I donot think this paper can be published at this stage.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJerDj05tQ&amp;noteId=B1l_Mpupo7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper259 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">24 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper259 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper considers an optimization problem defined on the intersection of multiple manifolds and the intersection is not a manifold. An optimization algorithm is proposed and its convergence analysis is given. An experiment of neural network with batch normalizatiion is used to demonstrate the performance of the algorithm.

The problem considered in this paper is interesting. However, there are quite a few fundamental errors about Riemannian optimization. In addition, the convergence analysis is not complete. See more details below. Therefore, I donot think this paper can be published at this stage.

*) P2, Section 2.1, line 2: The statement "A manifold is a subspace of R^n$ is not true in general.
*) P2, Section 2.1, line 7: The statement "manifold is not a linear space" is not true in general. A manifold can be a linear space, such as the vector space R^n.
*) P2, Section 2.1, below (2): The statement "Riemannian gradient is the orthogonal projection of gradient \nabla f(x) ..." is not true in general.
*) P3, (3) and (4): what is the definition of $h_k^{(1)}$ and $h_k^{(2)}$. Are they arbitrary or the ones given on Page 4?
*) P4, Theorem 2.3: the iterates {x_k} converges in the sense that \|gradf(x_k)\| goes to 0. Does {x_k} go to the intersection of the two manifolds \mathcal{M}_1 and \mathcal{M}_2? To complete the proofs, the author may need to show that \|gradf(y_k)\| goes to 0 and {x_k} and {y_k} have the same limit.
*) P5, the grassmann manifold with p = 1: G(1, n), is called projective space, and the Stiefel manifold with p = 1: St(n, 1) is called the unit sphere.
*) P6, the discussion of the intersection of G(1, n) and St(n, 1) does not make sense to me. G(1, n) is a quotient manifold, which is not a submanifold of R^n. Given a quotient manifold, the typical way in optimization framework is to choose representation of the quotient manifold. Fortunately, the projective space has a global orthogonal section, which is the unit sphere. In other words, G(1, n) is diffemorphisic to the unit sphere St(n, 1), and even can be isometric if appropriate Riemannian metrics are used on G(1, n) and St(n, 1). Therefore, I don't understand the notion of the intersection of G(1, n) and St(n, 1).


</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HklxSrfT5X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>gradient descent on manifold</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJerDj05tQ&amp;noteId=HklxSrfT5X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Oct 2018</span><span class="item">ICLR 2019 Conference Paper259 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
The optimization with many constraints is usually a very difficult problem.  I don't fully understand details in your paper, still have a question on the iterate relation in the paper that x_{k+1} = Retr_x(-1/L(grad(f(x_k)))).  Say if the constraints are all equalities, then these constraints will confined the feasible domain to many hyperplanes. And then when we calculate the Riemannian gradient of f(x), how to make sure that the update point x_{k+1} locates inside the feasible domain? Or say how to guarantee that the update point x_{k+1} always move on manifolds?

</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJgEyG_ZjX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>reply to gradient descent on manifold</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJerDj05tQ&amp;noteId=rJgEyG_ZjX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Oct 2018 (modified: 18 Oct 2018)</span><span class="item">ICLR 2019 Conference Paper259 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your attention to this paper. For the manifold optimization with a single manifold, to ensure that the iterative point x_{k+1} always moves on manifold, we need the operator $Retr_{x}$: $Retr_{x}(v)$ is a map from $T_{x}\mathcal{M}$ to manifold $\mathcal{M}$, where $T_{x}\mathcal{M}$ is the tangent space at point $x\in\mathcal{M}$. Since ${\rm grad}f(x)$ is on $T_{x}\mathcal{M}$, $x_{k+1} = Retr_x_{k}(-1/L({\rm grad}f(x_k))$ must live on manifold.
In fact, the iterative points obtained by the updating rule $x_{k+1} = Retr_{x_{k}}(v_{k})$ locate on manifold as long as $v_{k}\in T_{x_{k}}\mathcal{M}$. Since the updating rule in our paper satisfy $v_{k}\in T_{x_{k}}\mathcal{M}$, iterates must live on the corresponding manifold.
For the case with multiple manifold where the optimization region corresponds to the intersection of these manifolds ,  existing methods cannot guarantee that iterates live on the intersection of manifolds, because intersection of manifolds may not be a manifold. They can only reach minima on each manifold rather than the intersection of them. In contrast, our method heuristically sets the updating direction on one manifold as the manifold gradient perturbed by the gradient information from other manifolds. This updating rule renders the common minima which is shared by all the manifolds, if there exists, as the only stable points of our algorithm. This is a heuristic argument rather than a theoretical analysis.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>