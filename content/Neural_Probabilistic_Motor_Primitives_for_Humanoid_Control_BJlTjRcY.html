<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Neural Probabilistic Motor Primitives for Humanoid Control | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Neural Probabilistic Motor Primitives for Humanoid Control" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJl6TjRcY7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Neural Probabilistic Motor Primitives for Humanoid Control" />
      <meta name="og:description" content="We focus on the problem of learning a single motor module that can flexibly express a range of behaviors for the control of high-dimensional physically simulated humanoids. We propose a..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJl6TjRcY7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Neural Probabilistic Motor Primitives for Humanoid Control</a> <a class="note_content_pdf" href="/pdf?id=BJl6TjRcY7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019neural,    &#10;title={Neural Probabilistic Motor Primitives for Humanoid Control},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BJl6TjRcY7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=BJl6TjRcY7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We focus on the problem of learning a single motor module that can flexibly express a range of behaviors for the control of high-dimensional physically simulated humanoids. We propose a latent-variable model that bottlenecks a timeshifted, sensory-motor mapping. We show that it is possible to train this model entirely offline to compress thousands of expert policies and learn a motor primitive embedding space. The trained neural probabilistic motor primitive system can perform one-shot imitation of whole-body humanoid behaviors, robustly mimicking unseen trajectories. Critically, we demonstrate that it is also straightforward to train controllers to reuse the learned motor primitive space to solve tasks, and resulting movements can be relatively naturalistic. To support the training of our model, we compare two approaches for offline policy cloning, including an experience efficient method which we call linear feedback policy cloning. We encourage readers to view a supplementary video (<a href="https://youtu.be/44tPXdUCc-g" target="_blank" rel="nofollow">https://youtu.be/44tPXdUCc-g</a> ) summarizing our results.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Motor Primitives, Distillation, Reinforcement Learning, Continuous Control, Humanoid Control, Motion Capture, One-Shot Imitation</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Neural Probabilistic Motor Primitives compress motion capture tracking policies into one flexible model capable of one-shot imitation and reuse as a low-level controller.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SygmMXpJAm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>replies from reviewers to author responses?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJl6TjRcY7&amp;noteId=SygmMXpJAm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper843 Area Chair1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper843 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">This paper has seen detailed reviews and detailed responses by the authors. Thank you to all.

Reviewers:  please do provide further feedback based on the authors replies, 
and note whether it changes your evaluation and your score for the paper.
Also note that a revised draft has been submitted. 
Your input is greatly appreciated, as the opinions are mixed and they focus on different aspects of the work.

For revision differences of the revised draft: 
select "Show Revisions" on the review page, and then select the check-boxes for the versions you wish to compare.  

-- area chair</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Sylz2k9o6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Revised draft posted.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJl6TjRcY7&amp;noteId=Sylz2k9o6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper843 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper843 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">In response to reviewer feedback, we have revised our abstract and contributions portion of the introduction to better communicate the focus of the paper.  We consider the neural probabilistic motor primitive module to be the primary contribution and LFPC as an auxiliary contribution.  As judged by reviewer reception, this did not come across as intended.  We hope the revision better reflects this.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJxBuUYY6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Overall response to reviewers</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJl6TjRcY7&amp;noteId=HJxBuUYY6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper843 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper843 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank all reviewers for their time and comments. 

We would like to emphasize that there are two contributions in the work.  The focal motivation is the production of a single trained motor architecture which can execute and reuse motor skills of a large, diverse set of experts with minimal manual segmentation or curation.  The architecture that we develop permits one-shot imitation as well as reuse of low-level motor behaviors in the context of new tasks. 

Our main results involve one-shot imitation and motor reuse, using our trained module for a humanoid body with relatively high action DoF.  We believe this novel architecture enables more generic behavior and motor flexibility than other work involving learning to control physically simulated humanoids.  

AnonReviewer3 and AnonReviewer1 essentially restrict criticism of the work to the LFPC approach, which is only one aspect of our research contribution. We address these concerns in detail below. But we would also encourage the reviewers to assess the quality and novelty of the core architectural contributions as well as the quality of the experimental results.  We are not aware of previous work for control of a physically simulated humanoid that demonstrates a learned module that can execute many behavioral skills and permits reuse.  
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1eS0BTpnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The idea is oversimplified, which may limit its applications.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJl6TjRcY7&amp;noteId=S1eS0BTpnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper843 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper843 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper mainly focuses the imitation of expert policy as well as compression of expert skills via a latent variable model. Overall, I feel this paper is not quite readable, albeit that the prosed methods are simple and straightforward. 

As one major contribution of this paper, the authors introduce a first-order approximation to estimate the action of an expert, where perturbations are considered. However, this linear treatment could yield large errors when the residuals in (1) are still large, which is very common in high-dimensional and highly-nonlinear cases. Specifically, the estimation of “J” could be hard. In addition, just below (1), the authors mention (1) yields a “stabilized policy”, so what do you mean “stabilized”?

Another crucial issue lies on the treatment of “\Delta(s)”, which is often unknown and hard to modeled, Thus, various optimal controllers are introduced so as to obtain robust controllers. Similarly, in (9) it is also difficult to decide what is “suitable perturbation distribution”.

Overall, the linear treatment in (2) and assumption on “\Delta(s)” in (5) actually oversimplify the imitation learning problem, which may not be applicable in real robot applications.

Others small comments:
-Section 2.1 could be moved to supplementary material or appendix, as this part is indeed not a contribution.

- in (5), it should be “-J_{i}^{*}”
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HkllTLYYaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJl6TjRcY7&amp;noteId=HkllTLYYaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper843 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper843 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Concerning LFPC, we note that bipedal locomotion is highly nonlinear and despite this, the linear feedback policy empirically works rather robustly (despite the high-D observation space) as shown in section 3.1.  The term linear-feedback-stabilized policy, refers to the linear feedback policy in equation 2, which is stabilized with linear feedback (relative to the naive open-loop policy that simply executes a fixed sequence of actions). 

We consider it clear from our results that time-indexed linear feedback policies suffice to capture the behavior of experts around nominal trajectories in our setting. Correspondingly, LFPC is capable of transferring expert functionality. We would like to point out that in our scenario there is no need to estimate J -- it is simply the Jacobian of a neural network with respect to the inputs which is readily available in standard neural network languages (see eqn 2). 

There seems to be some confusion about delta s -- it has very little to do with the “various optimal controllers” and indeed we state in the paper (page 4) that the approach is fairly insensitive to precise selection of this distribution. One possible reason for this is that the distribution does not matter much as long as it covers the states visited by the linear feedback policy which appears to stay pretty close to the nominal trajectory.

Finally, the reviewer expresses concerns with respect to the applicability of our approach to the real robot setting. Our paper primarily targets the control of simulated physical humanoids and we do not make any further claims.  However recent approaches in a similar imitation learning setting have been shown to be effective for real robots (e.g. Laskey et al. 2017), so we do believe, as we speculate in the discussion that this is a plausible direction for future work.

We thank the reviewer for spotting a typo in equation 5 which we will correct.

References:
Laskey, M., Lee, J., Fox, R., Dragan, A. and Goldberg, K., 2017. Dart: Noise injection for robust imitation learning. arXiv preprint arXiv:1703.09327.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1gTWJZ6hX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Sound approach, but very similar to prior work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJl6TjRcY7&amp;noteId=S1gTWJZ6hX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper843 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper843 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper tackles the problem of distilling large numbers of expert demonstrations into a single policy that can both recreate original demonstrations in a physically-simulated environment and humanoid platform, and to generalize to novel motions. Towards this, the paper presents two approaches learn policies from expert demonstrations without involving costly closed loop RL training, and distilling these individual experts into a shared policy by learning latent time-varying codes.

The paper is well-written and the method is well-evaluated in the scope that it is proposed. Both components of the proposed approach have previously been explored in the literature - there is extensive work on learning local controllers for physics based evironments from demonstrations in both open loop and closed loop settings as well as work on mixtures of these controllers in machine learning, robotics and computer graphics communities. While the paper proposes these two components as a contribution, I would like to see a more detailed argument of what this work contributes over previous such approaches. 

Another part  where I wish the paper could make a more compelling argument is that distilled policy can perform non-trivial generalization. Target following is a good illustrative example, but has been showcased by multitude of prior work. The paper talks about compositionality, and it would have been compelling to see examples of that if the method can achieve it. For example, simultaneously performing locomotion skills with upper body manipulation skills is something mixture of expert demonstrations approaches still struggle with and it would have been great to see this paper investigate the approach on this problem. 

Overall, this is a sound and well-written submission, but the existence of very related prior work with similar capabilities makes me reluctant to recommend this paper.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJlsxPYYam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJl6TjRcY7&amp;noteId=HJlsxPYYam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper843 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper843 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for appreciating the difficult problem we’re tackling.  However, we disagree with the reviewer about the level of similarity between this work and previous work.  We have discussed a number of relationships between this work and existing approaches in the robotics, ML, and graphics communities. As far as we are aware, no existing work learns a rich embedding space for physics-based control.  For kinematic sequence modeling, there is abundant work in computer graphics that learns to blend/reuse/compose movement trajectories (e.g. Holden et al. 2017).  To our knowledge, for the much more challenging problem of flexible physics-based control, there is no prior work which results in a robustly reusable skill space that is as comprehensive in scope as what was demonstrated here.  We would sincerely appreciate references of any previous papers that the reviewer thinks overlap in terms of successfully demonstrating the learning of a skill space which is reusable for physics-based control, especially for humanoids.

One-shot imitation has been demonstrated by a few groups in the past couple years for mounted robotic arms. But we are aware of considerably less work (primarily Wang et al. 2017; discussed in the paper) in which humanoids perform one-shot behaviors.  The reason this is difficult in the physics-based case is that the humanoid must balance and remain upright in addition to imitating the demonstration.  Moreover, while one-shot imitation is the core systematic test of the model, since the architecture was trained for this setting, we emphasize that the demonstration of reuse is considerably more interesting to us.  After producing this module, a fresh HL policy can learn to set the “intention” of the LL controller and produces fairly human-like behaviors by reusing the learned skill space.  We selected the go-to-target task because we wanted to heavily tax the LL movement space by demanding sudden, jerky changes of movement and what resulted were strikingly human-like movement changes, with only a very simple reward (reward = 0 everywhere except when target is reached) and no additional constraints on the human-likeness of the behavior.  While simpler bodies can solve this problem from scratch, for a complex humanoid, the movements produced by learning from scratch are most definitely very non-human-like in general.

Simultaneously reusing the upper body for manipulation while having lower body locomote is indeed a great challenge problem for future work. We have already included imitation of arm movements in our evaluation but our training distribution does not contain any manipulation demonstrations. We are optimistic that this approach can scale to this setting, but it is beyond the scope of the present paper.  We do believe, that what we have demonstrated here advances the state of the art for reusable physics-based locomotion behaviors. 

References: 
Holden, Daniel, Taku Komura, and Jun Saito. "Phase-functioned neural networks for character control." ACM Transactions on Graphics (TOG) 36, no. 4 (2017): 42.

Ziyu Wang, Josh S Merel, Scott E Reed, Nando de Freitas, Gregory Wayne, and Nicolas Heess. Robust imitation of diverse behaviors. In Advances in Neural Information Processing Systems, pp. 5320–5329, 2017.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJeCcTtms7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Concerns with proposed approach and results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJl6TjRcY7&amp;noteId=HJeCcTtms7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper843 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper843 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper considers the problem of transferring motor skills from multiple experts to a student policy. To this end, the paper proposes two approaches: (1) an approach for policy cloning that learns to mimic the (local) linear feedback behavior of an expert (where the expert takes the form of a neural network), and (2) an approach that learns to compress a large number of experts via a latent space model. The approaches are applied to the problem of one-shot imitation from motion capture data (using the CMU motion capture database). The paper also considers an extension of the proposed approach to the problem of high-level planning; this is done by treating the learned latent space as a new action space and training a high-level policy that operates in this space. 

Strengths:
S1. The supplementary video was clear and helpful in understanding the setup.
S2. The paper is written in a generally readable fashion.
S3. The related work section does a thorough job of describing the context of the work.  

However, I have some significant concerns with the paper. These are described below. 

Significant concerns:
C1. My biggest concern is that the paper does not make a strong case for the benefits of LPFC over simpler strategies. The results in Figure 3 demonstrate that a linear feedback policy computed along the expert's nominal trajectory performs as well as (and occasionally even better than) LPFC. This is quite concerning.
C2. Moreover, as the authors themselves admit, "while LPFC did not work quite as well in the full-scale model as cloning from noisy rollouts, we believe it holds promise insofar as it may be useful in rollout-limited settings...". However, the paper does not present any theoretical/experimental evidence that would suggest this.
C3. Another concern has to do with the two-step procedure for LPFC (Section 2.2), where the first step is to learn an expert policy (in the form of a neural network) and the second step is to perform behavior cloning by finding a policy that tries to match the local behavior of the expert (i.e., finding a policy that attempts to produce similar actions as the expert policy linearized about the nominal trajectory). This two-step procedure seems unnecessary; the paper does not make a case for why the expert policies are not chosen as linear feedback controllers (along nominal trajectories) in the first place.
C4. The linearization of the expert policy produced in (1) may not lead to a stabilizing feedback controller and could easily destabilize the system. It is easy to imagine cases where the expert neural network policy maintains trajectories of the system in a tube around the nominal trajectory, but whose linearization does not lead to a stabilizing feedback controller. Do you see this in practice? If not, is there any intuition for why this doesn't occur? If this doesn't occur in practice, this would suggest that the expert policies are not highly nonlinear in the neighborhood of states under consideration (in which case, why learn neural network experts in the first place instead of directly learning a linear feedback controller as the expert policy as suggested in C3?)
C5. I would have liked to have seen more implementation details in Section 3. In particular, how exactly was the linear feedback policy along the expert's nominal trajectory computed? Is this the same as (2)? Or did you estimate a linear dynamical model (along the expert's nominal trajectory) and then compute an LQR controller? More details on the architecture used for the behavioral cloning baseline would also have been helpful (was this a MLP? How many layers?)

Minor comments:
- There are some periods missing at the end of equations (eqs. (1), (2), (6), (8), (9)).</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJlewPYtpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJl6TjRcY7&amp;noteId=SJlewPYtpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper843 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper843 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for their detailed discussion of the LFPC method and address concerns below. However, as pointed out in the introductory remarks this is only one aspect of the paper and we would also like to encourage the reviewer to include our main result, the neural probabilistic motor primitive module in their assessment.

Addressing the concerns about LFPC in turn:
C1: For single-behavior experts, indeed we intended Fig 3 to indicate (perhaps surprisingly) that linear-feedback policies perform well, and that LFPC can transfer that level of performance into a new neural network (from a single rollout of behavior).  For a single behavior, this is merely a validation that the new neural network can be as robust as even the linear feedback policy. Our real aim is to be able to distill many experts into a single network as we demonstrate subsequently.

C2: Both LFPC and the behavioral cloning baseline were able to train the NPMP and permit skill reuse, but in our specific one-shot imitation comparisons the behavior-cloning approach performed better.  Behavioral cloning from arbitrary amounts of data is an arbitrarily strong baseline.  The two considerations that motivate LFPC are that we can store fewer data from experts and that we can query fewer trajectories from the expert system (in settings where rollouts are costly, such as real platforms).

C3, C4:  The general setting for our approach is that we assume the existence of experts that perform single behaviors -- as of late, this is a reasonable assumption, enabled by previous research (e.g. Liu et al. 2010, 2015, 2018, Merel et al. 2017, Peng et al. 2018).  What has not been done prior to this work is to exhibit single policies capable of flexibly generating a wide range of skills, and this is the problem we are focusing on.  For our purposes, it is not critical how experts are obtained, and this paper does not advocate any particular way of generating expert policies.  That being said, neural network experts have been successfully trained in some recent work, so we expected it would work, and a priori, it was not obvious that directly training a linear feedback policy might suffice.  Moreover, in preliminary experiments done when beginning this work (not reported here), we found that it can be quite data inefficient to directly train a time-indexed linear feedback policy for tracking motion capture using RL, we believe due to lack of parameter sharing across timesteps, so we did not pursue this further.    

Nevertheless our single-behavior expert transfer experiments demonstrated empirically that linear feedback policies extracted from the expert neural networks were essentially as performant as RL-trained neural network experts in terms of robust tracking of single behaviors (Fig. 3).  That linear feedback policies work as well here is a statement about the dynamics of the environment and the complexity of the behaviors (i.e. that the behaviors here are sufficiently unimodal).  It seems, for a wide range of stereotyped behaviors, the policies required to execute the behaviors might be “surprisingly simple”, depending on your initial preconceptions.  

C5: In contemporary neural network languages, it is straightforward to compute the Jacobian of the actions w/ respect to observation inputs.  As described in eqn 2, this directly provides the linearization of the policy when evaluated at the nominal trajectory.  In section 3.1, we use the same network architecture for cloning from noisy rollouts and from the linear feedback policy (MLP with two hidden layers: 1024, 512; we can add this detail to the text).


References:
Liu, L., Yin, K., van de Panne, M., Shao, T. and Xu, W., 2010. Sampling-based contact-rich motion control. ACM Transactions on Graphics (TOG), 29(4), p.128.

Liu, L., Yin, K. and Guo, B., 2015, May. Improving Sampling‐based Motion Control. In Computer Graphics Forum (Vol. 34, No. 2, pp. 415-423).

Libin Liu and Jessica Hodgins. Learning basketball dribbling skills using trajectory optimization and deep reinforcement learning. ACM Transactions on Graphics (TOG), 37(4):142, 2018. 

Merel, Josh, Yuval Tassa, Sriram Srinivasan, Jay Lemmon, Ziyu Wang, Greg Wayne, and Nicolas Heess. "Learning human behaviors from motion capture by adversarial imitation." arXiv preprint arXiv:1707.02201 (2017).

Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. arXiv preprint arXiv:1804.02717, 2018.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>