<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>The Deep Weight Prior | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="The Deep Weight Prior" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ByGuynAct7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="The Deep Weight Prior" />
      <meta name="og:description" content="Bayesian inference is known to provide a general framework for incorporating prior knowledge or specific properties into machine learning models via carefully choosing a prior distribution. In this..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ByGuynAct7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The Deep Weight Prior</a> <a class="note_content_pdf" href="/pdf?id=ByGuynAct7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 14 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019deep,    &#10;title={Deep Weight Prior},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=ByGuynAct7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=ByGuynAct7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Bayesian inference is known to provide a general framework for incorporating prior knowledge or specific properties into machine learning models via carefully choosing a prior distribution. In this work, we propose a new type of prior distributions for convolutional neural networks, deep weight prior, that in contrast to previously published techniques, favors empirically estimated structure of convolutional filters e.g., spatial correlations of weights. We define deep weight prior as an implicit distribution and propose a method for variational inference with such type of implicit priors. In experiments, we show that deep weight priors can improve the performance of Bayesian neural networks on several problems when training data is limited.  Also, we found that initialization of weights of conventional networks with samples from deep weight prior leads to faster training.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">deep learning, variational inference, prior distributions</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">An empirical prior for convolutional layers in Bayesian neural networks that improves learning on small datasets.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_S1l_qeRt3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Not a strong paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByGuynAct7&amp;noteId=S1l_qeRt3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper999 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper999 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper considers modeling convolutional neural network by a Bayes method. The prior for the weights is considered in which the weights from various layers, input and output channels are assumed to be independent.  A varational method is considered to approximate the posterior distribution of the weights of CNN.  It looks to me that the prior distribution is a fairly standard product which may not perfectly suitable for CNN. Also the validity of the proposed variational method needs further evaluation. Below I summarize my concerns more technically.

1. CNN essentially has a tree structure, i.e., each layer can be viewed as the parent of the next layer. So the consecutive layers should have a sort of dependence. Also, the weights based on the same input channel should all inherit features of that channel. Based on these considerations, is it really reasonable to assume that the random weights are independent?
I agree that independence assumption makes the model and computation easier, but the prior itself should reflect the possible dependence structure of the channels.

2. The KL divergence might not be tractable and so the proposed variational method replaces it with an upper bound. This method highly depends on the assumption that the upper bound of the KL divergence is accurate. Otherwise it is hard to tell that the method really approximates the authentic variational method very well. It would be great if the accuracy of the upper bound can be further evaluated (theoretically and numerically).





</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1e5OxjKpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByGuynAct7&amp;noteId=H1e5OxjKpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper999 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper999 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank you for the thoughtful review and questions about prior factorization and tightness of L^{aux}, we will address raised concerns below:

1) Is it really reasonable to assume that the random weights are independent?  

Before our work, priors for convolutional layers in Bayesian Deep Learning had a fully-factorized structure, namely, all weights were treated independently. We use more general factorization, now at least there are spatial correlations between the weights in a kernel, but not yet between kernels and between layers. The method allows us to use more flexible priors and can be potentially generalized to more complex dependencies. We discuss the factorization in (Section 6) and consider this direction for the future work. 

2) This method highly depends on the assumption that the upper bound of the KL divergence is accurate.  

Indeed, our method depends on the accuracy of the upper bound on KL-divergence or equivalently a gap between an intractable variational lower bound L and a proposed variational lower bound L^{aux}. As we show in (Appendix A, eq. 13), maximization of the L^{aux} minimizes the gap L - L^{aux} = E_q(w) [KL(r(z | w) || p(z | w))] by adjusting parameters of reversed model r(z | w), in other words, the gap reduces as reversed model r(z | w) gets more accurate. The bound is tight if and only if the auxiliary distribution coincides with the exact posterior distribution of the implicit weight model, i.e., r(z | w) = p(z | w). A huge piece of literature on ELBOs relies on this type of argument. 

At the same time, there already exists a number of works on applications and analysis of tightness of variational bounds (for instance [1, 2]). Therefore, we did not address the underlined issue directly. One of the most straightforward approaches to providing more accurate variational bounds is to employ importance weighted bounds introduced in [3], which allow to trade off sample complexity and tightness of variational bounds. We have added the estimates of approximation gap using importance weighted bound to Appendix F, however, these figures should be treated with caution, as IWAE still gives us a lower bound on L.

[1] On the Quantitative Analysis of Decoder-Based Generative Models, <a href="https://arxiv.org/abs/1611.04273" target="_blank" rel="nofollow">https://arxiv.org/abs/1611.04273</a>
[2] Tighter Variational Bounds are Not Necessarily Better, https://arxiv.org/abs/1802.04537
[3] Importance Weighted Autoencoders, https://arxiv.org/abs/1509.00519</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HkebAXKun7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Solid Idea with Focused Experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByGuynAct7&amp;noteId=HkebAXKun7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper999 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper999 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:

This paper proposes the â€˜deep weight priorâ€™: the idea is to elicit a prior on an auxiliary dataset and then use that prior over the CNN filters to jump start inference for a data set of interest.  Both explicit and implicit priors are considered, with the latter having the benefit of increased flexibility but having the drawback of a lack of a parametric form to plug in to the ELBO.  The authors address this last point by extending the ELBO appropriately.  Experiments are performed testing the priorâ€™s ability to capture trained filters (Figure 1), provide a good initialization (Figure 2), improve sample efficiency (Figure 3), improve training speed (Figure 4).  

Pros:

I like this paper: it is a intuitive idea, and the experiments explore exactly what one would hope to gain from the prior (i.e. better initialization, improved sample efficiency).  I find the paper clearly written and to have a logical flow.  Furthermore, I think eliciting priors---while so crucial in more traditional Bayesian modeling---has been mostly overlooked by the Bayesian ML community, and this paper clearly shows that there are gains to be had from a fairly straightforward procedure.  


Cons:

The only potential issue with the paper is the use of the implicit prior, as it complicates variational inference, requiring the extension to the ELBO described in Section 3.2.  As far as I can tell, all experiments use the implicit priors.  I would have liked to have seen an experiment using a parametric prior (eg Gaussian) that shows what gains the implicit prior provides.  Or is it simply a matter of memory efficiency?  


Other comments:

-- Nice first sentence in the introduction!  I like how itâ€™s a general statement but immediately focuses the readerâ€™s attention to the paperâ€™s topic.

-- While it doesnâ€™t say so explicitly, the paper seems to imply it is the first to use implicit priors.  Some previous work that uses some form of implicit prior includes:

Runs a chain to refine the prior: Alex Lamb et al. "GibbsNet: Iterative Adversarial Inference for Deep Graphical Models." Advances in Neural Information Processing Systems. 2017.

Optimizes a NN implicit prior based on an invariance objective: Eric Nalisnick and Padhraic Smyth. "Learning priors for invariance." International Conference on Artificial Intelligence and Statistics. 2018.

Defines implicit priors over functions through samplers: Chao Ma, Yingzhen Li, and JosÃ© Miguel HernÃ¡ndez-Lobato. "Variational Implicit Processes." arXiv preprint arXiv:1806.02390 (2018).


Evaluation:  I recommend this paper for acceptance.  It is a sensible idea with pointed experimental validation.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkxoNliK6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByGuynAct7&amp;noteId=SkxoNliK6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper999 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper999 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank you for the thoughtful review and useful points related to parametric priors and literature on implicit priors, we will address your questions below:

1) I would have liked to have seen an experiment using a parametric prior (eg Gaussian) that shows what gains the implicit prior provides. Or is it simply a matter of memory efficiency? 

We had also evaluated a multivariate Gaussian prior, that shares parameters across all kernels within the specific layer (Gaussian prior over kernels). This prior, however, leads to marginally worse performance in both the likelihood of the generative model and the final accuracy comparing to the VAE-based approximation. We will append these results in the Appendix.

2) While it doesnâ€™t say so explicitly, the paper seems to imply it is the first to use implicit priors.  Some previous work that uses some form of implicit prior includes

You are right, a number of works have already used implicit priors. We clarified explicitly that the paper is not proposing to use implicit priors but propose a new inference technique that is compatible with Stochastic Gradient Variational Lower Bound. These papers are indeed relevant and we will definitely cite them.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rylsLCoBnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Deep weight prior</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByGuynAct7&amp;noteId=rylsLCoBnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper999 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper999 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper considers learning informative priors for convolutional neural network models based on fits to data sets from similar problem domains.  For trained networks on related datasets the authors use autoencoders to obtain an expressive prior on the filter weights, with independence assumed between different layers.  The resulting prior is generative and its density has no closed form expression, and a novel variational method for dealing with this is described.  Some empirical comparisons of the deep weight prior with alternative priors is considered, as well as a comparison of deep weight samples for initialization with alternative initialization schemes.  

This is an interesting paper.  It is mostly clearly written, but there is a lack of detail in Section 4 that makes it hard for me, at least, to understand exactly what was done there.  I think the originality level of the paper is high.  The issue of informative priors in these complex models seems wide open and the authors provide an interesting approach both conceptually and computationally.  I did wonder whether there was any link between the suggested priors and the idea of modelling the current and related data sets used in constructing the prior jointly, with data set specific parameters given an exchangeable prior?  This would be a standard hierarchical modelling approach.  Such an approach would not be computationally attractive, I just wondered if there is some conceptual link with the current method being an approximation of that approach in some sense.  In Section 4.1, it seems that for the trained networks on the source datasets, point estimates of the filter weights are treated as data for learning the variational autoencoder - is that correct?  Could you model dataset heterogeneity here as well?  Presumably the p_l(z) density is N(0,I)?  Details of the inference and reconstruction networks are sketchy.  In Section 4.2, you say that the number of filters is proportional to the scale parameter k and that you vary k.  What scale parameter do you mean?  


</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkl9WeiYa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer1 </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByGuynAct7&amp;noteId=rkl9WeiYa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper999 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper999 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank you for the thoughtful review with interesting suggestions, we will address your questions and concerns below:

1) A lack of detail in Section 4

We had significantly improved Section 4. Subsection 4.1 has been rewritten and moved to Section 3, as it has more to do with the method rather than experiments. The description of experiments also has been improved. 

2)  I did wonder whether there was any link between the suggested priors and the idea of modelling the current and related data sets used in constructing the prior jointly, with data set specific parameters given an exchangeable prior? â€¦ I just wondered if there is some conceptual link with the current method being an approximation of that approach in some sense. 

The suggestion indeed seems to be a generalization of the current approach, that however may be closer to empirical Bayes, since the prior distribution p(W) can be adopted using the current dataset. In the paper, we do not consider any explicit model of a dataset e.g., dataset-embeddings, but it would be an interesting direction for research.

3) In Section 4.1, it seems that for the trained networks on the source datasets, point estimates of the filter weights are treated as data for learning the variational autoencoder - is that correct?  

Yes, this is correct.

4) Could you model dataset heterogeneity here as well? 

If we understand the question correctly, given the current model, the answer is no. Currently, we do not explicitly use kernels heterogeneity e.g., (a) the estimated variance of each training kernel, or (b) the problem from which each training kernel comes from, but using these additional factors could be an interesting topic of investigation.

3) Presumably the p_l(z) density is N(0,I)?

Yes, this is correct, we described the form of the prior p(z) in VAE section, but it is worth to add it explicitly at the description of our model. 

3) In Section 4.2, you say that the number of filters is proportional to the scale parameter k and that you vary k.  What scale parameter do you mean?  

We scale a number of filters on every convolutional layer by k (in our experiments k can be equal to 1/16, 1/8, 1/4, 1/2 or 1). This allows us to track how the size of the model influence our method.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>