<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Multiple-Attribute Text Rewriting | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Multiple-Attribute Text Rewriting" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1g2NhC5KQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Multiple-Attribute Text Rewriting" />
      <meta name="og:description" content="The dominant approach to unsupervised " style="" transfer"="" in="" text="" is="" based="" on="" the="" idea="" of="" learning="" a="" latent="" representation,="" which="" independent="" attributes="" specifying="" its="" "style".="" this="" paper..."="" />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1g2NhC5KQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Multiple-Attribute Text Rewriting</a> <a class="note_content_pdf" href="/pdf?id=H1g2NhC5KQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019multiple-attribute,    &#10;title={Multiple-Attribute Text Rewriting},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=H1g2NhC5KQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=H1g2NhC5KQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">The dominant approach to unsupervised "style transfer" in text is based on the idea of learning a latent representation, which is independent of the attributes specifying its "style". In this paper, we show that this condition is not necessary and is not always met in practice, even with domain adversarial training, that explicitly aims at learning such  disentangled representations. We thus propose a new model that controls several factors of variation in textual data where this condition on disentanglement is replaced with a simpler mechanism based on back-translation. Our method allows control over multiple attributes, like gender, sentiment, product type, etc., and a more fine-grained control on the trade-off between content preservation and change of style with a pooling operator in the latent space. Our experiments demonstrate that the fully entangled model produces better generations, even when tested on new and more challenging benchmarks comprising reviews with multiple sentences and multiple attributes.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">controllable text generation, generative models, conditional generative models, style transfer</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A system for rewriting text conditioned on multiple controllable attributes</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">14 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Hkgb4vEFpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Convergence question in equation. (1)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1g2NhC5KQ&amp;noteId=Hkgb4vEFpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1489 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the interesting work. The results look amazing. I have a question about the loss function (Equation. 1). The loss function only consists of the reconstruction loss and another type reconstruction loss related to the back-translation, and there is no adversarial loss or classification loss to regularize the generated styles. How do you guarantee the generated sentences have correct styles?

I can imagine that there is a local minimum of Equation. 1, where the decoders completely ignores the input style embedding and directly copy the input sentence.  In this case, no matter which style you used, the input and output are the same, and the loss is zero. I'm wondering how do you prevent this situation happens?

Looking forward to seeing the answers!</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkgGgYEt6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>the denoising matters here</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1g2NhC5KQ&amp;noteId=SkgGgYEt6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1489 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1489 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The first part of the loss function (1), the denoising auto-encoder part, would help prevent the situation described. Since the input would have noise added, the simple copy operation can not be learned directly. But I still prefer the authors would give some discussions and maybe quantitative results regarding this.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1xk7NH9pX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The denoising auto-encoder part still couldn't guarantee the styles</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1g2NhC5KQ&amp;noteId=H1xk7NH9pX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1489 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the comment! Yes, the denoising auto-encoder part could prevent the directly copying. However, it still couldn't guarantee the generated styles. If the auto-encoder totally ignores the style embedding and only learns to reconstruct the input sentences (even with noises), the equation. (1) is still converged.  Hope the authors would discuss this issue. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HklLDIyapm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Regarding the convergence of the model</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1g2NhC5KQ&amp;noteId=HklLDIyapm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1489 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1489 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your question. As AnonReviewer3 mentioned, simply copying input sentences wouldn’t satisfy the auto-encoding part of equation (1), as noise has been added to sentences. However, it would indeed satisfy the back-translation loss.

The idea of denoising here is that by removing random words from a sentence, we hope to remove words that are required to infer the style.
For instance, if the input sentence is: “this place is awful”
and that the noised sentence becomes: “this place is &lt;BLANK&gt;”,
the model will be trained to recover “this place is awful”
from: (“this place is &lt;BLANK&gt;”, ATTRIBUTE=NEGATIVE)

Since there might be a lot of occurrences of “this place is amazing” in the dataset, the model will have to learn to consider the provided attribute in order to give a high probability to “awful” without penalizing the perplexity on the positive reviews.

The general argument is that the decoder needs to learn to use the attribute information whenever the input to the system is very noisy. This applies as well when inputs come from the back-translation process. Noisy inputs are produced in the back-translation process at the beginning of training when the model is insufficiently trained and does not generate well, and when generations are produced at high temperature. When using high softmax temperatures, the model tends to exhibit lower content preservation and higher attribute transfer since the generated samples are very noisy and it is therefore more difficult to recover the original input in the back-translation process while the decoder is forced to better leverage the attribute information.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_rylWfjH0hQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Empirical comparison to (Hu el al., 2017)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1g2NhC5KQ&amp;noteId=rylWfjH0hQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1489 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the interesting work. It'd be nice to see an empirical comparison of this work to (Hu el al., 2017) which has released code here: <a href="https://github.com/asyml/texar/tree/master/examples/text_style_transfer." target="_blank" rel="nofollow">https://github.com/asyml/texar/tree/master/examples/text_style_transfer.</a> Based on my experience, (Hu el al., 2017) is usually a strong baseline on many datasets.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HygNdVy66m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Results of the comparison</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1g2NhC5KQ&amp;noteId=HygNdVy66m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1489 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1489 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comment. We’ve added a comparison with Hu et al., 2017 in the revised paper using the code you mentioned. We found that this model obtained a good accuracy / BLEU score, but with pretty high perplexity. We’ve also added a reference to Yang et al in the related work section, thank you for pointing this out.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BkxKA_vRnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>missing references</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1g2NhC5KQ&amp;noteId=BkxKA_vRnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1489 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">There are also some relevant works that are missing in the references such as:
Unsupervised Text Style Transfer using Language Models as Discriminators by Yang etc al.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1la-jxRhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper presents a model for text rewriting for multiple attributes. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1g2NhC5KQ&amp;noteId=B1la-jxRhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1489 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1489 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a model for text rewriting for multiple attributes, for example gender and sentiment, or age and sentiment. The contributions and strengths of the paper are as follows. 

* Problem Definition
An important contribution is the new problem definition of multiple attributes for style transfer. While previous research has looked at single attributes for rewriting, "sentiment" for example, one could imagine controlling more than one attribute at a time. 

* Dataset Augmentation
To do the multiple attribute style transfer, they needed a dataset with multiple attributes. They augmented the Yelp review dataset from previous related paper to add gender and restaurant category. They also worked with microblog dataset labeled with gender, age group, and annoyed/relaxed. In addition to these attributes, they modified to dataset to include longer reviews and allow a larger vocabulary size. In all, this fuller dataset is more realistic than the previously release dataset.

* Model
The model is basically a denoising autoencoder, a well-known, relatively simple model. However, instead of using an adversarial loss term as done in previous style transfer research, they use a back-translation term in the loss. A justification for this modeling choice is explained in detail, arguing that disentanglement (which is a target of adversarial loss) does not really happen and is not really needed. The results show that the new loss term results in improvements.

* Human Evaluation
In addition to automatic evaluation for fluency (perplexity), content preservation (BLEU score), and attribute control (classification), they ask humans to judge the output for the three criteria. This seems standard for this type of task, but it is still a good contribution.

Overall, this paper presents a simple approach to multi-attribute text rewriting. The positive contributions include a new task definition of controlling multiple attributes, an augmented dataset that is more appropriate for the new task, and a simple but effective model which produces improved results.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkekG-1TaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for the review!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1g2NhC5KQ&amp;noteId=BkekG-1TaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1489 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1489 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review. We are glad to see that you liked the paper and it's contributions.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1ll1KQ5hX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good work but better presentation needed</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1g2NhC5KQ&amp;noteId=S1ll1KQ5hX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1489 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1489 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This work proposes a new model that controls several factors of variation in textual data where the condition on disentanglement is replaced with a simpler mechanism based on back-translation. It allows control over multiple attributes, and a more fine-grained control on the trade-off between content preservation and change of style with a pooling operator in the latent space.

One of the major arguments is it is unnecessary to have attribute-disentangled latent representations in order to have good style-transferring rewriting. In Table 2, the authors showed that "a classifier that is separately trained on the resulting encoder representations has an easy time recovering the sentiment" when the discriminator during training has been fooled. Is there any difference between the two discriminators/classifiers? If the post-fit classifier on top of the encoder representation can easily predict the correct sentiment, there should be enough signal from the discriminator to adapt the encoder in order to learn a more disentangled representation. On the other hand, this does not answer the question if a "true" disentangled representation would give better performance. The inferior performance from the adversarially learned models could be because of the "entangled" representations.

As the author pointed out, the technical contributions are the pooling operator and the support for multiple attributes since the loss function is the same as that in (Lample et. al 2018). These deserve more elaborated explanation and quantitative comparisons. After all, the title of this work is "multiple-attribute text rewriting". For example, the performance comparison between the proposed how averaged attribute  embeddings and simple concatenation, and the effect of the introduced trade-off using temporal max-pooling.

How important is the denoising autoencoder loss in the loss function (1)? From the training details in the supplementary material, it seems like the autoencoder loss is used as "initialization" to some degree. As pointed out by the authors, the main task is to get fluent, attribute-targeted, and content-preserving rewriting. As long as the "back-translation" gives expected result, it seems not necessary to have "meaningful" or hard "content-preserving" latent representations when the generator is powerful enough.

I think the last and most critical question is what the expected style-transferred rewriting look like. What level or kind of "content-preserving" do we look for? In Table 4, it shows that the BLEU between the input and the referenced human rewriting is only 30.6 which suggest many contents have been modified besides the positive/negative attribute. This can also be seen from the transferred examples. In Table 8, one of the Male example: "good food. my wife and i always enjoy coming here for dinner. i recommend india garden." and the Female transferred rewriting goes as "good food. my husband and i always stop by here for lunch. i recommend the veggie burrito". It's understandable that men and women prefer different types of food even though it is imagination without providing context. But the transfer from "dinner" to "lunch" is kind of questionable. Is it necessary to change the content which is irrelevant to the attributes?


Other issues:
- Towards the end of Section 3, it says that "without back-propagating through the back-translation generation process". Can you elaborate on this and the reason behind this choice?
- What does it mean by "unknown words" in "... with 60k BPE codes, eliminating the presence of unknown words" from Section 4?
- There is no comparison with (Zhang et. al. 2018), which is the "most relevant work".
- In Table 4, what is the difference among the three "Ours" model?
- In Table 4, the perplexity of "Input Copy" is very high compared with generated sentences.
- In Table 7, what does the "attention" refer to?
- In the supplementary material, there are lambda_BT and lambda_AE. But there is only one lambda in the loss function (1).
- Please unify the citation style.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1eBLQ1pT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your review and raising interesting questions about this work. (part 2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1g2NhC5KQ&amp;noteId=r1eBLQ1pT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1489 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1489 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">“I think the last and most critical question is what the expected style-transferred rewriting look like. What level or kind of "content-preserving" do we look for?” - This is a great question, and a fundamental open research problem which, as far as we know, does not have a clear answer in existing literature. In our paper,  we view this line of research as looking for better ways to generate rewrites of text along certain directions, and exactly the “kind” of what content is being preserved would ideally be one of the “knobs” that a system can control. The phrase “style transfer” is useful to refer to previous work that have adopted it from the image domain, but its framing is a bit narrow for the scope of rewriting types our work addresses. We believe that the trade-off between attribute control and content preservation should depend on two factors 1) the eventual use case of such a system (and style transfer is one use case, but another one would be to obtain more “interesting” and varied generations by augmenting a retrieval system with rewriting capabilities in a controllable way, and 2) the nature of attributes being controlled. Firstly, in contrast to previous work, we present means to control this inherent trade-off in the form of a latent-space pooling operator which can adapted to a particular use case. Secondly, the proposed method is fundamentally one that learns an unsupervised mapping between two or more domains of text, and the nature of the learned mapping will certainly depend on the nature of the domains. For example, it is often possible to map between the positive and negative domains by replacing a few words or small phrases and as a result, we can expect our models to preserve a lot of the input. By contrast, attributes such as one’s age aren’t as “local” and might require rewriting more content to successfully be altered. In that case, the content that is being preserved might be the general structure of the text, its sentiment, etc. To make the trade-off clearer, we have added a figure to the manuscript showing how it varies across training (Fig. 1 in the appendix); we also include illustrations of rewrites at different trade-off levels in Table 13.

“Towards the end of Section 3, it says that "without back-propagating through the back-translation generation process". Can you elaborate on this and the reason behind this choice?” - Back-propagating through the back-translation process would require computing gradients through a sequence of discrete actions since generations are sampled from the decoder. While this may be achieved via policy-gradient methods such as REINFORCE or other approximations like the Gumbel-softmax trick, these have been known to perform very poorly in high dimensional action spaces due to high variance of the gradient estimates. This approach also has the disadvantage of biasing the model towards the degenerate solution of copying the input while ignoring attribute information entirely to satisfy the cycle-consistency objective, since the gradients flow through the entire cycle, which is what we observed in practice.

“What does it mean by "unknown words" in "... with 60k BPE codes, eliminating the presence of unknown words" from Section 4?” - We meant that by using BPE, we can operate without replacing infrequent words with an &lt;unk&gt; token -- we do not have unknown words because these are decomposed into subword units that belong to the BPE dictionary.

“what is the difference among the three "Ours" model?” - These models differ in the choice of hyperparameters (pooling kernel width and back-translation temperature) to demonstrate our model’s ability to control the content preservation vs attribute control trade-off. We have clarified this in the table caption.

“the perplexity of "Input Copy" is very high compared with generated sentences.” - This is true and we believe that this is a consequence of the fact that there is more diversity in the input reviews than in typical generations from ours and other systems. This lack of diversity is typical for models decoding with beam search, which leads to "mode seeking behavior" wherein the output generations contain fragments that occur most frequently in the training set. This results in the pre-trained LM assigning high likelihoods to these samples.

“what does the "attention" refer to?” - The row in Table 7 that corresponds to "-attention" refers to a model that was trained without an attention mechanism in a vanilla sequence-to-sequence fashion, using the last hidden state of the encoder by concatenating it to the word embeddings at every time step of the decoder.

“In the supplementary material, there are lambda_BT and lambda_AE. But there is only one lambda in the loss function (1).” -Thank you for spotting this typo. We fixed this in the revised version of the paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Skxizm1aaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your review and raising interesting questions about this work. (part 1)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1g2NhC5KQ&amp;noteId=Skxizm1aaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1489 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1489 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">“Is there any difference between the two discriminators/classifiers?” - The discriminator and classifier have completely identical architectures - a 3 layer MLP with 128 dimensional hidden layers and LeakyReLU activations (now clarified in the model architecture paragraph in Section 3.3). We used two different terms to describe them since the classifier is fit post-hoc and doesn’t adapt to the encoder representations in a min-max fashion while the discriminator does. Moreover, the classifier is fully trained on the final encoder representations, while the discriminator is “chasing” them without fully training after each and every update of the encoder representations. This is indeed a bit confusing, and we have clarified this in the paper. While a discriminator trained more thoroughly at each iteration might disentangle representations more, our goal was not to look at whether disentangled representations can result in better performance, but whether current training practices actually result in disentangled representations (see responses below as well).

“there should be enough signal from the discriminator to adapt the encoder in order to learn a more disentangled representation.” - This is a valid concern, but the experiments we ran suggest that this does not change the main observation. For instance, we also experimented with larger coefficients of adversarial training of 1.0 and 10.0 (as well as no adversarial training on the other end of the spectrum). While the attribute recovery accuracy drops a little at higher coefficients, it is still much higher than the discriminator accuracy during training. Also, models trained with high adversarial training coefficients have extremely high reconstruction and back-translation losses. Results are presented below, for better formatting please refer to the revised version of our paper.

        Coef        Disc(acc)  Clf(acc)
        0        &amp;	89.45% &amp;  93.8%
        0.001 &amp;	85.04% &amp;  92.6%
        0.01   &amp;	75.47% &amp;  91.3%
        0.03   &amp;	61.16% &amp;  93.5%
        0.1     &amp;	57.63% &amp;  94.5%
        1.0     &amp;	52.75% &amp;  86.1%
        10      &amp;	51.89% &amp;  85.2%

“On the other hand, this does not answer the question if a "true" disentangled representation would give better performance. The inferior performance from the adversarially learned models could be because of the "entangled" representations.” - We agree completely. Our point is not that disentangled representations would not lead to good performance, but simply that disentanglement doesn't happen in practice with the kind of adversarially trained models typically used for this problem. We have made changes to the writing to make our stance clearer.

“Request for ablation study on pooling and other architectural design choices.” - In addition to the averaged attribute embeddings, we also explored using a separate embedding for each attribute combination in the cross-product of all possible attribute values. We found this to have similar performance to our averaging method. We decided against concatenating embeddings because we use the attribute embedding as the first input token to the decoder, and using a concatenation would mean dividing the embedding size for each attribute value by the number of attributes, to maintain to overall embedding size. This wouldn’t scale as well to settings with many possible attributes. We settled on the attribute embedding averages because of its simplicity.

We have included a plot (Figure 1) that shows the evolution of attribute control (accuracy) and content preservation (BLEU) over the course of training as a function of the pooling kernel width. This demonstrates the latent space pooling operator’s ability to trade off self-BLEU and accuracy - larger kernel widths favor attribute control while smaller ones favor content preservation.

“As long as the "back-translation" gives expected result, it seems not necessary to have "meaningful" or hard "content-preserving" latent representations when the generator is powerful enough.”
We observed that operating without a DAE objective didn’t work since the model needs to be bootstrapped to be capable of producing outputs that are at least somewhat close to the original input before the back-translation process can take over. At the beginning of training, it is nearly impossible for the model to be able to recover the original input starting from a nearly random sequence of words. But it’s indeed true that later on the back-translation loss is enough: in practice, we in fact removed the DAE objective by progressively decreasing lambda_AE from 1 to 0 over the first 300,000 iterations (c.f. Appendix section), even though we didn’t observe a significant difference compared to simply fixing lambda_AE to 1.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1lXb9okiX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Impressive experiments, but hard to determine how much is methodologically new here</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1g2NhC5KQ&amp;noteId=H1lXb9okiX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1489 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1489 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes "style transfer" approaches for text rewriting that allow for controllable attributes. For example, given one piece of text (and the conditional attributes associated with the user who generated it, such as their age and gender), these attributes can be changed so as to generate equivalent text in a different style.

This is an interesting application, and somewhat different from "style transfer" approaches that I've seen elsewhere. That being said I'm not particularly expert in the use of such techniques for text data.

The architectural details provided in the paper are quite thin. Other than the starting point, which as I understand adapts machine translation techniques based on denoising autoencoders, the modifications used to apply the technique to the specific datasets used here were hard to follow: basically just a few sentences described at a high level. Maybe to somebody more familiar with these techniques will understand these modifications fully, but to me it was hard to follow whether something methodologically significant had been added to the model, or whether the technique was just a few straightforward modifications to an existing method to adapt it to the task. I'll defer to others for comments on this aspect.

Other than that the example results shown are quite compelling (both qualitatively and quantitatively), and the experiments are fairly detailed.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rylhcb16TX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for the review &amp; comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1g2NhC5KQ&amp;noteId=rylhcb16TX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1489 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1489 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">To make the architecture clearer, we updated the paper and added a paragraph, describing the architecture of the model in the “Implementation” section. That paragraph was previously in the appendix -- we hope inserting it into the main body makes the paper easier to follow. 

As for our additions to the model, the methodology we used is similar to previous approaches in unsupervised machine translation, but with two key differences.

First, our approach can handle multiple attributes, while previous approaches usually only consider two different domains (one for the positive reviews, and one for the negative reviews, for instance) and cannot be easily extended to multiple domains as they typically require one encoder and one decoder per domain. Our approach can handle multiple attributes at the same time, including categorical attributes (e.g. Table 9 in the Appendix).

Also, we introduced a pooling operator and we found it to be critical in our experiments. The problem we observed is that without it, the model has a tendency to converge to the “copy mode”, where it simply copy words one by one, without taking the attribute input into consideration. We included a plot in the ablation study (Figure 1) that shows the evolution of the attribute transfer accuracy and the content preservation over training, for different pooling layer configurations. We can see that without the pooling operator, the model directly converges to the “copy mode”, with a self-BLEU close to 90 after only a few epochs. A pooling operator with a window of size 8 not only alleviates this issue, but it also provides intermediate models during training with different trade-offs between content-preservation and attribute transfer.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>