<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Live Face De-Identification in Video | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Live Face De-Identification in Video" />
        <meta name="citation_author" content="Oran Gafni" />
        <meta name="citation_author" content="Lior Wolf" />
        <meta name="citation_author" content="Yaniv Taigman" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SyfdsjA9FX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Live Face De-Identification in Video" />
      <meta name="og:description" content="We propose a method for face de-identification that enables fully automatic video modification at high frame rates. The goal is to maximally decorrelate the identity, while having the perception..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SyfdsjA9FX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Live Face De-Identification in Video</a> <a class="note_content_pdf" href="/pdf?id=SyfdsjA9FX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=oran%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="oran@fb.com">Oran Gafni</a>, <a href="/profile?email=wolf%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="wolf@fb.com">Lior Wolf</a>, <a href="/profile?email=yaniv%40fb.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="yaniv@fb.com">Yaniv Taigman</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=SyfdsjA9FX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We propose a method for face de-identification that enables fully automatic video modification at high frame rates. The goal is to maximally decorrelate the identity, while having the perception (pose, illumination and expression) fixed. We achieve this by a novel feed forward encoder-decoder network architecture that is conditioned on the high-level representation of a person's facial image. The network is global, in the sense that it does not need to be retrained for a given video or for a given identity, and it creates natural-looking image sequences with little distortion in time. </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_r1l93JCn67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyfdsjA9FX&amp;noteId=r1l93JCn67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper637 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper637 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Byl6GY_qam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Well engineered solution, good results, somewhat limited novelty, could compare to prior art better</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyfdsjA9FX&amp;noteId=Byl6GY_qam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper637 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper637 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: 
In this paper the authors engineer a method for fully automatic de-identification, which can also be applied to videos by applying it separately frame-by-frame. The process utilizes an encoder-decoder architectures, which is trained using a combination of adversarial, reconstruction, and perceptual losses. The model also leverages features from a pre-trained face recognition model. Visually, the results look promising, but do occasionally contain some small artifacts or blurriness (albeit only noticeable when zoomed in). The de-identification process appears to work well, and with minimal change in the appearance of the face. Head pose and the majority of facial expression is also well preserved. Resulting images are evaluated using human evaluation and pretrained identity recognition models, which demonstrate quantitatively that de-identification is successful. Pixel-wise difference from the original image is also shown to be less compared to previous methods. Video samples look good and successive frames flow well with no visible flickering.

Pros:
-face images appear to be successfully de-identified, while face pose and expression is preserved
-produces smooth videos without flickering, even though applied per-frame
-paper is well written
-very good at including details needed for reproduction, such as hyperparameter settings
-contains ablation study

Cons:
-some faint blurriness still can be found in the resulting images 
-large number of hyperparamters that must be tuned
-only compares with a single alternative de-identification method from 4 years ago (no comparison against modern GAN based methods)
-ablation study has no quantitative metrics, so it is difficult to tell how much impact any individual component of the system had on the overall performance

Comments:
While the proposed system achieves its goal of de-identified videos, its novelty is somewhat limited. Performance is achieved mainly through clever application of many existing methods, rather than any singular new idea or technique. That being said, I think that this work does present some significant improvement over previous de-identification methods, but it needs to prove it better. In the related work the authors mention several alternative de-identification methods, but they only compare with a single method from 2014. I think the paper could be made much stronger if the authors could compare against more recent techniques, especially those that also utilize an adversarial component. 

The ablation study does a good job of demonstrating the impact of each component of the system visually. However, I think it could be improved if some quantitative metrics were included to measure how much improvement each component contributes to the overall performance of the system. 

Questions:
How sensitive are the results to the hyperparameter settings? Does it require a lot of tuning in order to get good results, or is it fairly robust to the settings?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkeRdXIjam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your constructive review. We have added new results to address it.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyfdsjA9FX&amp;noteId=rkeRdXIjam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper637 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper637 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your constructive review. Below we address your concerns one by one. Please let us know if these are appropriate.

Blurriness: this is mostly a result of limited resolution and we have just uploaded to the supplementary website (<a href="https://anonymous-deid-iclr2019submission.github.io/)" target="_blank" rel="nofollow">https://anonymous-deid-iclr2019submission.github.io/)</a> a new video obtained with a higher resolution model (256x256). In this model we replaced the decoder with six repetitions of an upscale block followed by a residual block. These results are sharper and contain less artifacts. In addition we present numerical results that show that the new model also performs strong de-identification.

Large number of hyperparameters: In practice, the only hyperparameter that needs to be carefully set during training is the distancing strength \lambda. All other parameters were either set to 1 (or 0.5 in case they are shared between the raw and masked generated output) or to their default value without any further tuning.

Network robustness vs. hyperparameter tuning:  The network is robust to small changes in parameter tuning, with an exception in the range of the distancing strength \lambda, for which any value in the range of [0,2*10^-6] provides natural looking results. With a value of zero a reconstruction is performed, and the value of 2*10^-6 provides maximal de-ID effect (within the range), which is why we use it in our experiments. This is illustrated in Fig. 7.

No comparison to current GAN methods: In the paper, we compared our work to the one of (Samarzija &amp; Ribaric, '14), since that was the only work to provide high-resolution results in color and varying poses. Following your comment, we have added a comparison of our method to the most recent de-identification work, that apply GANs as well. The results are now in the revised version (Fig. 5) and show a clear advantage to our method in terms of maintaining expression.

Quantitative ablation study results: we have added to the revised version a quantitative comparison between the different models shown in the ablation study. For each, we compare the mean pixel-level distance and the mean ID distance (as evaluated by the differences in the last layer of the VGGFace2 classifier), both in L1. The first should be low, while the second should be high. As can be seen in the results plot, the model we use (b) is very high in the ID distance axis, while considerably low in the pixel-level distance. The raw (unmasked) model (c) achieves an even higher ID distance, but this is anticipated, since it is not blended with the source image.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BkgW_nopn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a> This work presents an adversarial encoder-decoder network architecture that is conditioned on the high-level representation of a person’s facial image for face de-identification that enables fully automatic video modification. The effectiveness of the proposed method is verified qualitatively and quantitatively. Although the novelty of the method is not impressive, the proposed method seems to be useful for face-related applications and the experimental results are convincing to me.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyfdsjA9FX&amp;noteId=BkgW_nopn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper637 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper637 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value"> This work presents an encoder-decoder network architecture that is conditioned on the high-level representation of a person’s facial image for face de-identification that enables fully automatic video modification. The effectiveness of the proposed method is verified qualitatively and quantitatively. Although the novelty of the method is not impressive, the proposed method seems to be useful for face-related applications and the experimental results are convincing to me.

Pros:
- This method is simple, apparently effective and is a nice use of adversarial encoder-decoder for a practical task. The paper is written clearly and the English is fine.

Cons:
- My main concern with this paper is regarding the novelty. The authors seem to claim a novel GAN architecture by using an auto-encoder-based network architecture with a pre-trained face recognition network and multi-image perceptual loss. However, it is not clear to me what aspect of their GAN is particularly new.

- Missing experimental comparisons with state-of-the-arts. The most recent work that compared in the Experiment section is the work of Samarzija &amp; Ribaric (2014). Detailed experimental comparisons with more recent state-of-the-arts are needed to justify the superiority of the proposed method.

- Missing ablation study and more in-the-wild comparisons in the Experiment section. The proposed framework contains several modules, an ablation study is needed to verify the separate contribution of each component. Moreover, more in-the-wild qualitative and quantitative experiments on recent benchmarks with large facial variations (e.g., expression, occlusion, blur, etc.) are needed to verify the efficacy of the proposed method.

Additional comments:
- How did authors update each component and ensure stable yet fast convergence while optimising the whole GAN-based framework? 

- How did authors choose the value of \alpha in Eq. (2-4)?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1g-oTNJTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for the detailed review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyfdsjA9FX&amp;noteId=S1g-oTNJTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper637 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper637 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comments.

Novelty:
The main novel aspects are:
1. The concatenation of an independent face-recognition network descriptor into the latent space.
2. The use of a distancing loss to define the target image and the real-time control it enables over the perceived identity.
3. The partition of a multi-level perceptual loss into sub-domains (distancing the high-levels, while bringing the low/medium-levels closer).
4. Various masking techniques.

 These technical novelties translate to the following application novelties in the field of image and video translation:
1. Domain translation networks are commonly re-trained for every pair of domains (specifically in the case of face translation). In our case, the output domain is the same as the input domain.
2. Our network is trained once and can then be applied universally to any face, and in real-time,  without adaptation to the new face.
3. Previous work mainly relied on face-swapping for the task of de-identification (i.e. rely on other identities for this task), while our network depends solely on the identity of interest.
4. Previous works were not able to maintain the expression, pose, illumination conditions and handle occlusions over the source image/frame - hence our work is the first to address de-identification in video.
5. We handle varying backgrounds, considerable scene motion, and occlusions without difficulty. 

State-of-the-art comparisons:

For our comparison, we selected the literature work that presented results with the highest quality (Samarzija &amp; Ribaric, '14). More recent de-identification works (Jourabloo et al., '15; Wu et al., '18) present lower quality results (only black and white images, low resolution). Following the review, we tried to extract from their figures face images in order to perform this comparison, but the quality is too low to handle and the crops of the input faces are such that we would need to do special adjustments.

More generally, we detail the advantages of each work in Tab. 1. As can be seen, the literature methods, including the latest ones, are not-competitive (regardless of our improved quality, a larger diversity of output, much-reduced runtime) due to the fact that they do not preserve expression and pose.

Ablation study:

We present an ablation study in Fig. 7. The 6 ablation results presented were selected both due to their corresponding components importance and due to the results visibility in images. For example, result (c) emphasizes the importance of the masking component for handling occlusion and seamlessly blend between the source and generated image, and can be easily observed in images. On the other hand, we omitted the ablation on the edge-preserving losses since they do not result in lower quality output. However, in our experience, they are important for fast convergence.

More in-the-wild comparisons:

Our experiments were done both on images and videos. The images used in our experiments were specifically selected to provide challenging scenarios (e.g. the subset of images from the NIST Face recognition challenge known to be the most difficult, which include challenging illumination conditions and blurriness). The images use to compare with other methods also present challenging pose and expression.

For video samples, we selected challenging videos with a large amount of motion and sizable intra-video variation in the expression, pose, occlusion and illumination conditions. 

We provide a host of qualitative and quantitative experiments, all on real-world images and challenging videos. The qualitative results are presented as sample images and videos, while the quantitative results are presented in term of MOS, confusion matrices, automated identity ranking and pixel-level similarity vs. perceived identity distance. There are no controlled images or videos -- it is all “in the wild”. The specific “labeled faces in the wild” dataset is part of the training dataset and cannot be used for testing. However, the images we use, and especially the videos, are just as challenging if not more.

(continued below)</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJg16TVk67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>(the rest of our reply)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyfdsjA9FX&amp;noteId=rJg16TVk67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper637 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper637 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Ensuring stable training:

While in many GAN based contributions, the major concern is to balance the generator and the discriminator, in our framework, which behaves much more as a denoising autoencoder than a GAN, this balancing is not a major challenge.  However, there is a dissonance between the autoencoder reconstruction loss, which aspires to reconstruct the source, and the high-level perceptual loss, which distances the output from the input frame.

As stated in the training details (Sec. 4), the distance strength (\lambda) is trained gradually. This allows the autoencoder to first generate source-resembling outputs, then gradually focus on distancing the identity. The autoencoder does not regress into exact reconstruction, since the source image (input) is observed after a random distortion.

The reconstruction and edge-preserving losses provide for faster convergence, since they "direct" the autoencoder towards the source image (although full reconstruction is unwanted). The masking process aids in the fast convergence as well, since it alleviates the autoencoder from learning the embeddings of backgrounds, occlusions and other aspects that are unrelated to faces.

\alpha value:

\alpha is the parameter that determines the contribution to the loss of the masked output vs. the raw (unmasked) output. We set it to 0.5 in order to divide the error equally between the two. When we tried a value that creates a clear dominance to the masked output, which is the actual output we produce, we noticed that the mask was used to compensate for artifacts in the raw output. We therefore also make sure to have loss terms that directly consider the raw image.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_BJxLJU3chQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Non-trivial results on an interesting problem</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyfdsjA9FX&amp;noteId=BJxLJU3chQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper637 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper637 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The submission proposes a method for face de-identification. 
A network is trained to transform an input face image to simultaneously stay close to the low- and mid-level representations  while maximising the distance of the high-level representation of the input image in a face recognition network. Additionally an adversarial loss and reconstruction losses both in terms of the input image as well as its horizontal and vertical gradients are employed.
The network learns to transform faces to be maximally different in identity from a given reference image while preserving low-level features. Thus, at inference the network can de-identify face images from arbitrary new people given a single reference image of each person.

The method is evaluated in three different ways. 
a) It is shown that humans have difficulties discriminating between transformed and non-transformed videos.
b) In a fine-discrimination experiment human subjects are asked to re-identify faces from a given reference image among five dark-haired caucasian males. The subjects could re-identify the images without face de-identification but had substantial problems after the transformation was applied.
c) In a quantitative study it is shown that a state-of-the-art face recognition network cannot recognise faces of celebrities after the de-identification is applied while it recognise it fairly well before the transformation

I have some questions and remarks:

1.) How long are the videos used for evaluation a), what is the exact study protocol ?

2.) Are the evaluation networks trained on the same dataset as the loss network? If yes it is not surprising that the generated images can fool the networks as they represent the same data as the loss network and I would like to see the evaluation for networks not trained on the same dataset as the loss network. 

3.) The method is optimised to maximise confusion of the identity while preserving low and mid level features. This leads to good de-identification if the set of identities from which to retrieve is densely sampled in the space of low-level features. I.e. if in the set of identities from which to identify a person there exist many people with similar low-level features. However, there are many conceivable scenarios in which the number of possible identities from which to retrieve is far smaller. To de-identify in such scenarios, it is necessary to apply much stronger transformations to the face to obscure the identity of a person. It would be great if this dependency between the strength of the transformation that needs to be applied and the reference set of identities from which to retrieve the identity of a given person could be explicitly discussed in the submission.

4.) The emphasis on performance for video de-identification is somewhat misleading as the method does not seem to include any particular effort to explicitly improve video performance. It is great that the method also seems to work for video, but I cannot see strong evidence that it strongly outperforms pre-existing methods on video performance (they might also just work well out-of-the-box).

Nevertheless, I believe the submission addresses an interesting topic and shows non-trivial results. While I have some questions about the evaluation and minor concerns about the presentation I would overall recommend acceptance of the submission.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyeRH1Pea7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyfdsjA9FX&amp;noteId=SyeRH1Pea7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper637 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper637 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for your supportive review. To address your questions:

1. The non-celeb videos used for evaluation item (a) on your list, are of length 15-20 seconds each. A total of 9 non-celeb videos are used.

2. To make sure that we cover the intent of the question, we provide two answers.
I. We train on completely different datasets from the evaluation ones. As mentioned in the manuscript, training is done only on image based datasets, which are conventional datasets used for face recognition. No training is performed on video.
II. The VGGFace network and the two ArcFace networks are trained on different datasets, with an overlap in identities. The ArcFace network is considerably more accurate and employs a much larger training dataset (100,000 identities vs. 9,000), a different architecture, and different losses. Therefore, we believe that removing the persons that VGGFace was trained on from the ArcFace training dataset would not have changed the results in a significant way. 

3. Consider the experiment mentioned in your review as (b). Humans were asked to identify between a small group of five men of similar ethnicity and were not able to identify in this small-set scenario.
The question, as we understand it, is what would happen for persons of different ethnicity. 
As can be seen in supplementary video #7, distinctive ethnic features are changed by our system (eyes, nose, lips).
Identification based on skin-tone or hair color would still be possible. However, the literature of cross domain image translation has many solutions that change hair color (the well known blond to black hair example), and skin tone would probably behave similarly.

4. Our observation is that our method works without artifacts on video, which is not obvious. With regards to previous work: (1) Face replacement methods are discrete by design and would result in non-smooth videos with noticeable temporal artifacts. (2) As listed in Tab. 1, the previous work do not preserve pose, expression, gaze, and mouth position, which are crucial for video applications. For example, if a person is speaking in a video, the literature methods would not generate naturally looking mouth motion, and cannot generate a video where the lips are in synch with the audio.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>