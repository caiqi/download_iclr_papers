<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>D-GAN: Divergent generative adversarial network for positive unlabeled learning and counter-examples generation | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="D-GAN: Divergent generative adversarial network for positive unlabeled learning and counter-examples generation" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SJldZ2RqFX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="D-GAN: Divergent generative adversarial network for positive..." />
      <meta name="og:description" content="Positive Unlabeled learning task remains an interesting challenge in the context of image analysis. Recent approaches suggest to exploit the GANs abilities to answer this problem. In this paper, we..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SJldZ2RqFX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>D-GAN: Divergent generative adversarial network for positive unlabeled learning and counter-examples generation</a> <a class="note_content_pdf" href="/pdf?id=SJldZ2RqFX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019d-gan:,    &#10;title={D-GAN: Divergent generative adversarial network for positive unlabeled learning and counter-examples generation},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SJldZ2RqFX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Positive Unlabeled learning task remains an interesting challenge in the context of image analysis. Recent approaches suggest to exploit the GANs abilities to answer this problem. In this paper, we propose a new approach named Divergent-GAN (D-GAN). It keeps the light adversarial architecture of the PGAN method, with a better robustness counter the varying images complexity, while simultaneously allowing the same functionalities as the GenPU method, like the generation of relevant counter-examples. However, this is achieved without the need of prior knowledge, nor an onerous architecture and framework. Its functionning is based on the combination between the behaviour principles of Positive Unlabeled learning classification and the adversarial GAN training. Experimental results show that this divergent adversarial framework outperforms the state of the art PU learning in terms of prediction accuracy, training robustness, and its ability to work on both simple and complex real images. Combined with an additional generator, the proposed approach even allows to accomplish noisy labeled learning, and thus opening new application perspectives for GANs architectures.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Representation learning. Positive Unlabeled learning. Image classification</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A new two-stage positive unlabeled learning approach with GAN</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_B1lmrzQ937" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clear Rejection</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJldZ2RqFX&amp;noteId=B1lmrzQ937"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1188 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1188 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">[Summary]
PU learning is the problem of learning a binary classifier given labelled data from the positive class and unlabelled data from both the classes. The authors propose a new  GAN architecture in this paper called the Divergent Gan (DGAN) which they claim has the benefits of two previous GAN architectures proposed for PU learning: The GenPU method and the Positive-Gan architecture. The key-equation of the paper is (5) which essentially adds an additional loss term to the GAN objective to encourage the generator to generate samples from the negative class and not from the positive class. The proposed method is validated through experiments on CIFAR and MNIST.

[Pros]
1. The problem of PU learning is interesting.
2. The experimental results on CIFAR/MNIST suggest that some method that the authors coded worked at par with existing methods.

[Cons]
1. The quality of the writeup is quite bad and a large number of critical sentences are unclear. E.g.
a. [From Abstract] It keeps the light adversarial architecture of the PGAN method, with **a better robustness counter the varying images complexity**, while simultaneously allowing the same functionalities as the GenPU method, like the generation of relevant counter-examples.
b. Equation (3) and (4) which are unclear in defining R_{PN}(D, δ)
c. Equation (6) which says log[1 - D(Xp)] = Yp log[D(Xp)] + (1-Yp) log[1-D(Xp)] which does not make any sense.
d. The distinction between the true data distribution and the distribution hallucinated by the the generator is not maintained in the paper. In key places the authors mix one with the other such as the statement that supp(Pp (Xp )) ∩ supp(Pn (Xn )) → ∅
In short even after a careful reading it is not clear exactly what is the method that the authors are proposing.

2. Section 2.2 on noisy-label learning is only tangentially related to the paper and seems more like  a space filler.

3. The experimental results in Table 4 and Table 3 do not compare to GenPU. Although the authors claim several times that the GenPU method is *onerous*, it is not clear why GenPU is so much more onerous in comparison to other GAN based methods which all require careful hyper-parameter tuning and expensive training. Furthermore the reference PN method performs worse than other PU learning methods which does not make sense. Because of this I am not quite convinced by the experiments.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HkeOL7GeAX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Answers</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJldZ2RqFX&amp;noteId=HkeOL7GeAX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1188 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1188 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your constructive review,


1.
a. “a better robustness counter the varying images complexity” will be replaced by “a better adaptability to the images complexity”
According to the PGAN article, PGAN should not be used for simple tasks. D-GAN works on both simple and complex tasks: D-GAN is more adaptable to the images complexity.

b. l(D(X), δ)  will be replaced by l(D(X), y=δ) and “⇔” by “=”. 
δ is the label of positive samples: δ substitutes both contradictory labels “0” and “1” associated to Pp in the risk Rpu (equation 1). Unlabeled positive samples are separated from unlabeled negative ones with D trained with the risk Rpu.

c. Equation (6) will be replaced as follow.
The loss function Ld of D is defined as:
Ld = Rpu + Eg [ l(D(Xg),Yg=0) ],
with Rpu the PU risk (equation(1)), Yg=0 the label associated to the samples Xg generated by G; Xg = G(z), and Eg the expectation for samples Xg.
We recall Rpu = Eu [ l(D(Xu),Yu=1) ] + Ep [ l(D(Xp),Yp=0) ], with Yu=1 the label of unlabeled samples Xu with the expectation Eu, and Yp=0 the label of labeled positive samples Xp with the expectation Ep.
The loss function “l” used in the proposed D-GAN framework can be the binary cross-entropy “H” such that “l=-H”. So:
Ld = Eu [ -H(D(Xu),Yu=1) ] + Ep [ -H(D(Xp),Yp=0) ] + Eg [ -H(D(Xg),Yg=0) ].
The binary cross-entropy H is defined as below:
H(D(X),Y) = - Y log(D(X)) – (1-Y) log(1-D(X)), with Y represents the label associated to the D input samples X. Thus H(D(X),Y=1) = - log(D(X)) and H(D(X),Y=0) = - log(1-D(X)).
Finally, Ld can be developed as follow:
Ld = Eu [ log[D(Xu)] ] + Ep [ log[1-D(Xp)] ] + Eg [ log[1-D(Xg)] ].
This shows the incorporation of Rpu (equation(1)) inside the D-GAN loss function (equation(5)).
The role of G during the adversarial training is to generate samples considered by D as “1”. Only negative samples are considered as “1” by D thanks to the Rpu risk. This justifies intuitively the G convergence towards the negative samples distribution.

d. The sentence part "such that we have supp(Pp (Xp )) ∩ supp(Pn (Xn )) → ∅, with supp the support function of probability distributions." will be removed. 
We talked about D ability to distinguish positive samples distribution Pp from negative one Pn. If D does this distinction, then G converges towards Pn. If D fails to do this task, then G converges to the unlabeled samples distribution Pu as the PGAN.

2. We take into consideration your comment. This is not the main message of the article. Section 2.2 will be removed from the method part. 

3. 
“results in Table 4 and Table 3 do not compare to GenPU.” 
We do not compare our results to GenPU method for the challenging One vs. Rest task because: 
a. GenPU method is not reproducible. 
    - Code not provided.
    - Implementation details are missing in their article: Three hyper-parameters (lambda_P, lambda_N and lambda_U) are introduced in the GenPU article, but the values are not specified. They are important for the GenPU training with respect to their role inside the GenPU cost function (GenPU equation (3)): Instructions 8, 9 and 10 of the GenPU pseudo-code (GenPU Algorithm 1) apply them directly to the prior knowledge parameters πp and πn (=1-πp). That makes impossible the GenPU reproducibility. 

b. GenPU mode collapse issue does not enable to perform complex tasks as the One vs. Rest challenge.

c. GenPU is not valorized in their article as an interesting alternative for the standard PU context where we own relatively enough positive labeled samples: GenPU article does not present results with more than 100 positive labeled samples.

d. The goal of tables 3 and 4 is to compare methods which do not need prior knowledge.


“the authors claim several times that the GenPU method is *onerous*”
GenPU training computational cost cannot be quantified: GenPU is not reproducible and training epoch iterations needed to converge are not specified. If we consider that both standard GAN and GenPU architectures need the same number of training epochs to converge to the expected distribution, then training five models (GenPU) instead of two (D-GAN) is more computational demanding.
D-GAN does not add or modify hyper-parameters of GAN variants tested (GAN, DCGAN, WGAN-GP, LS-GAN). 


“the reference PN method performs worse than other PU learning methods which does not make sense.”
D-GAN performs better than PN on CIFAR-10 because: 
    - It learns relevant counter-examples distribution. RP article discusses the same behavior on CIFAR-10.

    - Generated images enable data augmentation. GANs latent linear interpolations result in semantic images interpolations outputs. Thus GANs learn generic representation. 
It is not observed on MNIST because data augmentation is difficult to produce on low-dimensional data.
PGAN score when πp=0 (as for PN) will be added in tables 3 and 4 to highlight this effect. 

This phenomenon is not straightforward, but these reasons clarify it.


We sincerely thank you for your review.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyxHcFfl0m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Furthermore,</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJldZ2RqFX&amp;noteId=SyxHcFfl0m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1188 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1188 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
* 
- Another novelty of the presented article is to highlight a critical Batch-Normalization effect on the discriminator (sections 2.3 and 3.1).

- D-GAN intuition can be expressed as follow:
        “- Show me what IS unlabeled AND NOT positive.” 
This is the task asked by D to G. Negative samples are both unlabeled and not positive. Consequently G learns to show the negative samples distribution to D.

This article presents an interesting contribution by merging GANs and PU learning areas in this way.

*

Your review helped us to clarify some formulations of the proposed method. You also highlighted that the article omitted some justifications concerning the experimental results. We apologize for not making the text clear enough. We will use shorter and concise sentences in the article. 

These previous answers to your respective points will contribute to improving the presentation clarity and strengthening the experiments.

We sincerely thank you for your review.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_BkxY2b-5hX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Problem and framework not well explained</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJldZ2RqFX&amp;noteId=BkxY2b-5hX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1188 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1188 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The motivation of the work is not clear but the novelty seems to be present.

The paper is very hard to follow as the problem description and intuition of the D-GAN is not clearly written.

Based on the experiments, the proposed method achieves marginal improvement in terms of F1 score but sometimes also slightly lower performance than other GAN based such as PGAN, so the impact of this work to solve positive unlabelled data problem is not evident. 

I am personally not as familiar with the PU problem and existing frameworks so my confidence in the assessment is low; my main experience is in the computer vision for autonomous driving and sparse coding.

But my feeling is this paper is marginally below the threshold of acceptance.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">1: The reviewer's evaluation is an educated guess</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SygDGpKthQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Too many issues</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJldZ2RqFX&amp;noteId=SygDGpKthQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1188 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1188 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposed another GAN-based PU learning method. The mathematics in this paper is not easy to follow, and there are many other critical issues.

*****

The clarity is really an issue. First of all, I cannot easily follow the meanings behind the equations. I guess the authors first came up with some concrete implementation and then formalize it into an algorithm. Given the current version of the paper, I am not sure whether this clarity of equations can be fixed without an additional round of review or not.

Moreover, the logic in the story line is unclear to me, especially the 3rd paragraph that seems to be mostly important in the introduction. There are two different binary classification problems, of separating the positive and negative classes, and of separating the given and generated data. I cannot see why the generated data can serve as negative data. This paragraph is discussing GenPU, PGAN and the proposed method, and consequently the motivation of the current paper does not make sense at least to me.

*****

The paper classified PU learning methods into two categories, one-stage methods and two-stage methods. This is interesting. However, before that, they should be classified into two categories, for censoring PU learning and for case-control PU learning. The former problem setting was proposed very early and formalized in "learning classifiers from only positive and unlabeled data", KDD 2008; the latter problem setting was proposed in "presence-only data and the EM algorithm", Biometrics 2009 and formalized in "analysis of learning from positive and unlabeled data", NIPS 2014. Surprisingly, none of these 3 papers was cited. By definition, GAN-based PU learning belongs to the latter problem setting while Rank Prune can only be applied to the former but was included as a baseline method.

The huge difference between these two settings and their connections to learning with noisy labels are known for long time. To be short, class-conditional noise model corrupts P(Y|X) and covers censoring PU, mutual contamination distribution framework corrupts P(X|Y) and covers case-control PU, and mathematically mutual contamination distribution framework is more general than class-conditional noise model and so is case-control PU than censoring PU. See "learning from corrupted binary labels via class-probability estimation", ICML 2015 for more information where the above theoretical result has been proven. An arXiv paper entitled "on the minimal supervision for training any binary classifier from only unlabeled data" has some experimental results showing that methods for class-conditional noise model cannot handle mutual contamination distributions. The situation is similar when applying censoring PU methods to case-control PU problem setting.

Furthermore, the class-prior probability pi is well-defined and easy to estimate in censoring PU, see "learning classifiers from only positive and unlabeled data" mentioned above. However, it is not well-defined in case-control PU due to an identifiability issue described in "presence-only data and the EM algorithm" mentioned above. Thus, the target to be estimated is defined as the maximal theta such that theta*P(X|Y)&lt;=P(X) following "estimating the class prior and posterior from noisy positives and unlabeled data", NIPS 2016. BTW, "mixture proportion estimation via kernel embedding of distributions" is SOTA in class-prior estimation; the previous NIPS paper was written earlier and accepted later.

In summary, as claimed in the paper and shown in Table 1 in the introduction, all discriminative PU methods and GenPU require to know pi for learning. This is true, but this is because they are designed for a more difficult problem setting---learning classifiers and estimating pi are both more difficult. Lacking some basic knowledge of PU learning is another big issue.

*****

The novelty is to be honest incremental and thus below the bar of ICLR. The significance is similarly poor, due to that the experiments mixed up methods for censoring PU and those for case-control PU. What is more, F1-score is a performance measure for information retrieval rather than binary classification. We all know GANs are pretty good at MNIST but not CIFAR-10. In fact, GenPU has a critical issue of mode collapse, and this is why GenPU reports 1-vs-1 rather than 5-vs-5 on MNIST. Even though, I still think GenPU makes much more sense than PGAN and D-GAN.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJlKFsUy3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Unclear definitions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJldZ2RqFX&amp;noteId=SJlKFsUy3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1188 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1188 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">This paper is studying the problem of PU learning which is an important and interesting problem, however I am having difficulty in reading the paper key definitions and equations are badly written. Please clarify the following:

a) Equation (6) says log[1 - D(Xp)] = Yp log[D(Xp)] + (1-Yp) log[1-D(Xp)] which does not make any sense. What are the authors saying here? 

b) Equation (3) defines R_{PN}(D, δ) in terms of l(D(X), δ) but l(D(X), δ) is not defined properly in equation (4). The left hand side of (4) has l(D(X), δ)  but δ vanishes on the right hand side of that equation. I have no idea what is going on here.

c) The authors frequently confuse the true data distribution and the distribution hallucinated by the the generator. For example consider the expressions that "supp(Pp (Xp )) ∩ supp(Pn (Xn )) → ∅ " Which distribution are the authors talking about? Is it an assumption on the true data distribution required for learning ? or this is a property of the generator's distribution. 

D) The experimental results in Table 4 and Table 3 do not compare to GenPU. Although the authors claim several times that the GenPU method is *onerous*, it is not clear why GenPU is so much more onerous in comparison to other GAN based methods which all require careful hyper-parameter tuning and expensive training. Furthermore the reference PN method performs significantly worse than other PU learning methods which does not make sense. The PN method should be much better or comparable to the performance of any PU method. Please clarify.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>