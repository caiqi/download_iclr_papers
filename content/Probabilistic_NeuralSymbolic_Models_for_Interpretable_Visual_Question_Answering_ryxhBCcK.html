<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Probabilistic Neural-Symbolic Models for Interpretable Visual Question Answering | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Probabilistic Neural-Symbolic Models for Interpretable Visual Question Answering" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ryxhB3CcK7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Probabilistic Neural-Symbolic Models for Interpretable Visual..." />
      <meta name="og:description" content="We propose a new class of probabilistic neural-symbolic models for visual question answering (VQA) that provide interpretable explanations of their decision making in the form of programs, given a..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ryxhB3CcK7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Probabilistic Neural-Symbolic Models for Interpretable Visual Question Answering</a> <a class="note_content_pdf" href="/pdf?id=ryxhB3CcK7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019probabilistic,    &#10;title={Probabilistic Neural-Symbolic Models for Interpretable Visual Question Answering},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=ryxhB3CcK7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We propose a new class of probabilistic neural-symbolic models for visual question answering (VQA) that provide interpretable explanations of their decision making in the form of programs, given a small annotated set of human programs. The key idea of our approach is to learn a rich latent space which effectively propagates program annotations from known questions to novel questions. We do this by formalizing prior work on VQA, called module networks (Andreas, 2016) as discrete, structured, latent variable models on the joint distribution over questions and answers given images, and devise a procedure to train the model effectively. Our results on a dataset of compositional questions about SHAPES (Andreas, 2016) show that our model generates more interpretable programs and obtains better accuracy on VQA in the low-data regime than prior work. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Neural-symbolic models, visual question answering, reasoning, interpretability, graphical models, variational inference</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A probabilistic neural symbolic model with a latent program space, for more interpretable question answering</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">10 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_ryxReMqxCX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Prior work on VAEs with discrete latent program space</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxhB3CcK7&amp;noteId=ryxReMqxCX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1578 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">This is a nice submission of approaching visual question answering using a probabilistic neural-symbolic model with discrete latent program space. However, there has been prior work on probabilistic neural-symbolic models with a latent program space (Yin et al., 2018), which seems to be one of the major contributions claimed in this submission (at least from the TL;DR line :)) I was wondering if the authors could explain the difference between V-NMN and Yin et al., which would better substantiate the novelty of this work compared with Yin et al.

Reference:
[1] Pengcheng Yin, Chunting Zhou, Junxian He, Graham Neubig. StructVAE: Tree-structured Latent Variable Models for Semi-supervised Semantic Parsing. ACL 2018.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HklnNrv53m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice paper, well written and through evaluation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxhB3CcK7&amp;noteId=HklnNrv53m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1578 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1578 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a discrete, structured latent variable model for visual question answering that involves compositional generalization and reasoning. In comparison to the existing approach, this paper well addressed the challenge of learning discrete latent variables in the presence of uncertainty. The results show a significant gain in performance as well as the capability of the model to generalize composition program to unseen data effectively. The qualitative analysis shows that the proposed model not only get the correct answer but also the correct behavior that leads to the answer.  </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJxb4jA567" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Urgent need for detail</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxhB3CcK7&amp;noteId=SJxb4jA567"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1578 Area Chair1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1578 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Can I please ask Reviewer 1 to, with due urgency, expand upon their review and/or comment upon those of the other reviewers, so as to proffer an appropriate defence of their recommendation in favour of the paper DURING the discussion period.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ryeoWKdlpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A bit more detail</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxhB3CcK7&amp;noteId=ryeoWKdlpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1578 Area Chair1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1578 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review. It is quite short, so it would be good to expand upon what you think makes this paper, perhaps in the form of a discussion with Reviewer 2, whose score is significantly different from yours.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJgdaqL5nQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Need improvement on the presentation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxhB3CcK7&amp;noteId=HJgdaqL5nQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1578 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 08 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1578 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=HJgdaqL5nQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a variational neural module networks (V-NMN), which compared to neural module networks (NMNs), is formed in a probabilistic aspect. The authors compare the performance of V-NMN and NMN on SHAPES dataset.

I find the technical part is hard to follow. To optimize the objective function, it involves many challenges. The authors described those challenges as well. It is not clear to me how those challenges are solved in section 2.1. I think that the presentation in section 2.1 needs to provide more details.

In the experiment, the authors only compare their work with NMNs without comparing it with other approaches for visual question answering. Besides accuracy, does V-NMN provide new applications that NMNs and other VQA models is not applicable because of the probabilistic formulation?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkevSwdl6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Please consider other reviews</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxhB3CcK7&amp;noteId=BkevSwdl6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1578 Area Chair1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1578 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Reviewer 2's review is very short. I do not see much substance or argument supporting the quite strict score (4) in favour of rejecting the paper. Regardless of the fact that it does not harmonise with the assessments provided by the other reviewers, it is not appropriate to make a recommendation of this nature without giving the authors a clear indication of what needs improving in the paper. If the true failing of this paper is that the technical part is hard to follow, is this due to poor presentation, or due to concerns with the actual applicability or scalability of the method proposed?

Please take a moment to read the other reviews, in particular Reviewer 3, as well as the author response(s), if and when they are made, and consider whether you can flesh out the concerns underlying your assessment in a way which the authors can respond to, rebut, or take into account when revising their paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkxggDsxam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Provide more details based on AC's comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxhB3CcK7&amp;noteId=SkxggDsxam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1578 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1578 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I provide more details for my reviews based on AC's comments.

First, I agree with the other two reviewers that the authors present the problem and their idea well and show V-NMN outperforms NMN in experiments. My concerns, however, are about the technical section 2.1, which I do not see other two reviewers comment on that.

I think that section 2.1 is hard to follow because the presentation is vague and lack details. There are several places pointing the readers to check other references or briefly summarizing the ideas where I think technical details should be provided. 

Technically, I have concerns about how the authors solve the challenges they mentioned in section 2.1. In the question coding stage, the goal is to learn an informative mapping from questions to programs, which based on my understanding, is q_\phi(z|x). As the authors mention, the prior distribution for p(z) is not a Gaussian distribution. How do the authors design the prior distribution p(z) and the variational distribution q_\phi(z|x), and how do the authors optimize the KL(q_\phi(z|x), p(z)) term? I agree with the authors that it is not an easy question. The presentation from the authors for this hard question is vague and lack technical details. Based on my understanding, I also think the solution is heuristic.

Second, in order to optimize q_\phi(z|x) in objective (1), the E_{z\sim q_\phi(z|x)}[\log p_{\phi_z}(a|z,i)] term has an effect as well. In the question coding stage, this term is totally removed. In order for the objective (1) to be a valid ELBO for log p(x, a|i), \beta need to be &gt;=1. As the authors pointed out, they need to relax \beta to &lt; 1 and it is no longer an ELBO. If the readers want to understand why it is good to relax \beta to &lt; 1, they need to check Alemi et al. (2018), which is the only information the authors provide.

In the module training stage, the goal is to learn the NMN for question answering with the objective (3) E_{z\sim q_\phi(z|x)}[\log p_{\phi_z}(a|z,i)]. After the authors obtain an estimate q_\phi(z|x) in question coding stage, it is possible to optimize this objective by sampling on q(z|x). Instead of using q(z|x), the authors presents "In practice, we take argmax decoded programs from q_\phi(z|x) to simplify training, but perform sampling during joint training in the next stage." Could the authors provide technical details for this "argmax decoded programs"?

In the joint train stage, the authors have an objective (4), which is different from objective (1) with a scalar \gamma. The authors presents the reason for that is because it is similar as Vedantam et al. (2018), which again totally points readers to check other references when the relaxation seems to have issue. For objective (4), is \beta &gt;=1 or &lt; 1? Why the objective (1) is hard to train but objective (4) is possible to train? How do the question coding stage and module training stage helps to solve the challenges for objective (1) so that objective (4) is easier to train?

Overall, my points for the technical section 2.1 is that at least the presentation needs to improve with more technical details so that readers can see how the authors solved the challenges they proposed. Also, I think the solution proposed by the authors is a heuristic way which is not clear how it solves those challenges. For other reviewers, do you think that section 2.1 need to provide more technical details as well?

For a heuristic method, if the authors show promising experimental results, I value this kind of work as well. My concerns are that the literature review is focused on restricted related works without a comprehensive introduction of VQA works, for example, 

Interpretable Visual Question Answering by Visual Grounding from Attention Supervision Mining (Zhang et al. 2018)
Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding (Yi et al. 2018).

The authors only compares V-NMN with NMN in the experiments. Could the authors answer why it is enough to only compare with NMN without comparing other VQA methods? Besides accuracy improvement, is there any other benefit by using V-NMN compared to NMN?

I am open to feedbacks and I will update my score if the authors can handle my concerns.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BygPBqabpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>As regards Section 2.1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxhB3CcK7&amp;noteId=BygPBqabpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1578 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1578 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I have found section 2.1 good enough though it is more descriptive than mathematically detailed. However, comments of Rev #2 are right so I can agree with them, maybe authors could add such details in the appendix or in the remaining half page depending on how long would be the discussion.
I still think that the document is worthy in this version. If the authors manage to add a good response to the Rev #2 comments, my current score will at least be confirmed if not increased.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hkeq6bpbaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxhB3CcK7&amp;noteId=Hkeq6bpbaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1578 Area Chair1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1578 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for providing an expanded review so quickly. I'm sure the authors will find this helpful for the purpose of this discussion period.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_S1x1C3I7i7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A good piece of work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxhB3CcK7&amp;noteId=S1x1C3I7i7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1578 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1578 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper presents a new approach for performing visual query answering. System responses are programs that can explain the truth value of the answer.
In the paper, both the problems of learning and inference are taken into account.
To answer queries, this system takes as input an image and a question, which is a set of word from a given vocabulary. Then the question is modeled by a plan (a series of operation that must be performed to answer the query).  Finally, the found answer with the plan are returned. To learn the parameters of the model, the examples are tuples composed by an image, a question, the answer, and the program.
Experiments performed on the SHAPES dataset show good performance compared to neural model networks by Johnson et al.

The paper is well written and clear. I have not found any specific problems in the paper, the quality is high and the approach seems to me to be new and worth studying.
The discussion on related work seems to be good, as well as the discussion on the results of the tests conducted.

On page 5, in equation (3) it seems to me that something is missing in J. Moreover, In Algorithm 1, in lines 4 and 9, the B after the arrow should be written in italic.

Overall, there are several typos that must be corrected. I suggest a double check of the English. For example:
- page 3, "as modeling *uncertaintly* should..."
- page 6, "Given this goal, we *consrtuct* a latent *varible* ..."
- page 8, in paragraph "Effect of optimizing the true ELBO", the word "that" is repeated twice in the 3rd row
- page 13, "for the" repeated twice in "Moving average baseline" paragraph. Also, in the last line of this paragraph, the sentence seems incomplete.



Pros
- The results are convincing
- The approach is clearly explained

Cons
- English must be checked</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>