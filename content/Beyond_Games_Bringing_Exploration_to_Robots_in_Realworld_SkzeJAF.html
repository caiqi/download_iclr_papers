<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Beyond Games: Bringing Exploration to Robots in Real-world | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Beyond Games: Bringing Exploration to Robots in Real-world" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SkzeJ3A9F7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Beyond Games: Bringing Exploration to Robots in Real-world" />
      <meta name="og:description" content="Exploration has been a long standing problem in both model-based and model-free learning methods for sensorimotor control. While there has been major advances over the years, most of these..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SkzeJ3A9F7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Beyond Games: Bringing Exploration to Robots in Real-world</a> <a class="note_content_pdf" href="/pdf?id=SkzeJ3A9F7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019beyond,    &#10;title={Beyond Games: Bringing Exploration to Robots in Real-world},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SkzeJ3A9F7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=SkzeJ3A9F7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Exploration has been a long standing problem in both model-based and model-free learning methods for sensorimotor control. While there has been major advances over the years, most of these successes have been demonstrated in either video games or simulation environments. This is primarily because the rewards (even the intrinsic ones) are non-differentiable since they are function of the environment (which is a black-box). In this paper, we focus on the policy optimization aspect of the intrinsic reward function. Specifically, by using a local approximation, we formulate intrinsic reward as a differentiable function so as to perform policy optimization using likelihood maximization -- much like supervised learning instead of reinforcement learning. This leads to a significantly sample efficient exploration policy. Our experiments clearly show that our approach outperforms both on-policy and off-policy optimization approaches like REINFORCE and DQN respectively. But most importantly, we are able to implement an exploration policy on a robot which learns to interact with objects completely from scratch just using data collected via the differentiable exploration module. See project videos at <a href="https://doubleblindICLR.github.io/robot-exploration/" target="_blank" rel="nofollow">https://doubleblindICLR.github.io/robot-exploration/</a></span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Exploration, curiosity, manipulation</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">15 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SklfCrWWRX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>[Author's Common Response] Added New Results; Will update Intro; Explaining Objective</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkzeJ3A9F7&amp;noteId=SklfCrWWRX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper955 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper955 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank reviewers for their time and useful feedback. It is always the authors' responsibility that the readers understand the context, the underlying technical approach, and the experiments. We believe we were short in all the three, but we also believe most of the content is there and all it needs is rewriting. On top of that, to convince reviewers we have also performed additional experiments including in simulation to demonstrate our baselines are the baselines reviewers are asking for (we just did not present them properly). Therefore, we urge both reviewers and AC to give us a second chance and consider the paper in light of the rebuttal.

Experiments -- REINFORCE baseline is the curiosity baseline [Pathak et al. 2017]
==========
First, we would handle the experimental concerns because we agree if we cannot demonstrate empirically that our approach works, then the paper should not be accepted. Reviewers are concerned we are comparing with DQN and REINFORCE but not with other exploration approaches. We would like to point out that both DQN and REINFORCE are not using external rewards and hence they are exploration approaches. Specifically, REINFORCE baseline is THE curiosity baseline and DQN is another optimization approach used for the same objective as curiosity baseline [Pathak et al. 2017]. One minor difference in the use of curiosity [Pathak et al. 2017] for our experiments is the use of pre-trained ImageNet feature. This feature is stationary which further helps the curiosity formulation as discussed in the follow-up (Burda et. al. 2018) [2].

In order to further demonstrate that our baselines are credible, we took the implementation and curves from recent curiosity paper (Burda et. al. 2018) [2]. We use the curiosity baseline provided in the paper and to demonstrate our objective function for exploration is meaningful, we add our objective to the original exploration objective. Note both these environments require long-term modeling and since using current approaches long-term models are hard to learn; we use our objective in conjunction with existing models. On both the environments, our objective seems to provide gain in efficiency. Hopefully, this confirms that the curiosity baselines used in the paper (referred to as REINFORCE) is a meaningful baseline. Please see results in Appendix Section-C of the updated paper.

Context
==========
We agree that some of our context placement was too strong, and we would be happy to soften it. But to put our paper in the right context. First, we agree that current Deep-RL approaches have made significant advances but as most RL experts would agree: sample efficiency still is a huge bottleneck. Again almost all RL experts would agree, this has been one of the bottlenecks towards bringing RL to robotics (without using expert demonstrations). 
One possible way is to train in simulation and then transfer to real-world robots; this might work but currently, in object manipulation, it is hard to transfer due to reality gap (contact dynamics are hard to simulate). As a research community, we should also investigate the alternative way: to train from scratch on the robot in real-world settings. This is where our paper comes in. Current RL-based exploration approaches (e.g., curiosity and visitation counts) use reward functions that are black-box in nature. Hence the gradients are not that meaningful which reduces the sample complexity. In our paper, we propose a new exploration objective that can be optimized via supervised learning (instead of reinforcement learning). We do not claim that this new objective has all properties of the previous one, and we elaborate on this below.

Technical Insights -- What does this new objective actually do?
===========
AR2 rightly asks the question: what does this new objective metric actually do? To answer this, and give intuition about the objective we have added a new section to the paper, see Appendix Section-D. We will move some part of it to the main paper in the final version. To summarize, due to the differentiable nature of our objective and presence of local constancy approximation, our objective does not behave identically to the original curiosity reward which is optimized by policy gradients (REINFORCE). The main difference in behavior is in case of interactions which have the same repetitive outcome: our method explicitly forces towards the action with different outcomes, while original curiosity objective penalizes the actions for repetitive outcomes. We further add pointers to the Experimental-Design in convex optimization literature which bears a close similarity to our setup. Please refer to Section-D for details.

We now answer other individual comments in the direct replies to reviewers.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ryeSpwrep7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Laudable goal, but paper not good enough</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkzeJ3A9F7&amp;noteId=ryeSpwrep7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper955 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper955 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Beyond Games: Bringing Exploration to Robots in Real-world
===========================================================

This paper tackles the laudable goal of making an algorithm for efficient exploration in "real-world" RL.
To do this, they augment the "curiosity" algorithm of Pathak et al with a differentiable approximation to the reward prediction model.
They motivate this algorithm through several intuitive arguments together with a series of experiments where the algorithm outperforms vanilla DQN/REINFORCE.


There are several things to like about this paper:

- The problem of making "real-world" practical algorithms for exploration is clearly one of the biggest outstanding problems in reinforcement learning.

- The authors have sucessfully gone from ideas, to algorithm, to real robot and their algorithm really seems to outperform the baselines.

- The authors clearly make an effort to survey a wide variety of recent papers in the field



However, there are several important places where this paper falls down:

- In a paper that posits a new, groundbreaking, real-world application of "exploration" there is remarkably little discussion of the key issues of "efficient exploration". Indeed, I don't think that this paper even presents a clear metric for how we can tell if something *is* a good method for exploration.
  + This is a huge shortcoming, since we know that it is possible to guarantee polynomial regret bounds for many settings (mostly tabular, but some with function approximation too... see UCRL2, PSRL and more)... there is no discussion of whether the proposed algorithm would also satisfy these bounds?
  + Of course, this is not a paper designed for "tabular MDPs", but we already have exploration algorithms like UCB / Thompson sampling that *are* widely used in online advertising... so why is this method not compared/contrasted to these approaches?

- There is very little *science* in this paper, beyond the experiments pitting "improved algorithm" vs DQN/REINFORCE, which nobody ever claimed would be a good approach to exploration! I don't think it's possible to assess if their algorithm (which I don't think has a clear name beyond "sample-efficient exploration formulation") performs better than the myriad of other exploration approaches listed. Although many intuitive arguments are presented, I did not find these convincing, and the overall narrative ends up being a little jumbled.

- A lot of the writing is generally imprecise, and alludes to claims/statements that make no sense to me:
  + "... most of these sucesses have been demonstrated in either video games or simulation environments. This is primarily becuase the rewards (even the intrinsic ones) are non-differentiable ..."
  + "Again these approaches have mostly been considered in context of external rewards and hence turn out to be sample inefficient"
I would suggest that each statement/claim is backed up by some material reasoning/statement/experiment unless extremely obvious - at the moment these are not!

- Nothing in this algorithm really seems specific to real-world... or at least nothing in the competing algorithms seems to preclude them from being run on a real-world robot... I think that the main issue is that if people want to iterate fast (or don't have a robot) they prefer to do things in simulation. If your point is really that findings from simulation don't translate to real robots, then I think that is really interesting, but I don't see any evidence for that in this paper.


Overall, it is clear that this is an interesting area to do work in.
The goal of making a practical algorithm for real-world exploration tasks is exciting.
However, in its current form, this paper falls well short of the level of science and insight I would expect for ICLR.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1xMn8bW0X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>[Authors' Response to R1] DQN/REINFORCE include exploration reward; Comparison to UCB</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkzeJ3A9F7&amp;noteId=B1xMn8bW0X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper955 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper955 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to first thank the reviewer for the comments. Please see the meta response in the combined reply for the common concerns among reviewers. We answer other individual concerns below:

[R1]: Only comparison to "DQN/REINFORCE, which nobody ever claimed would be a good approach to exploration!"
=&gt; We respectfully disagree with the reviewer as there seems to be some misunderstanding. Here, DQN/REINFORCE do not refer to the vanilla RL algorithm but are optimizing the curiosity exploration objective proposed in Pathak et.al. [2017]. Please see the combined response above for details. We hope that this answers the reviewer's major concern about the comparison to other exploration methods.


[R1]: Comparison to UCB / Thompson sampling; UCRL2, PSRL.
=&gt; As the reviewer agrees, most of these algorithms have been primarily studied in tabular cases and there have been some extensions to function approximators. These algorithms do not scale to the high dimensional image input (320x320x4) and a high dimensional action space (location, angle, push, grasp) used in our paper. 

Further, we respectfully believe there is a misunderstanding about the context in which the term "exploration" has been used in this paper. Our robotic agent explores the environment completely out of its own; using only its intrinsic reward and no external reward at all. UCB-like algorithms are parameter-space exploration and are successful in learning a policy in presence of external reward (even if sparse). One can not use UCB when there is no external reward as the value functions would collapse.

The algorithms UCRL2/PSRL integrate the prior over dynamics or reward function into learning a policy in a natural and efficient manner. These have been great advancement but not relevant to the problem setup or the goal of this paper because our agent's start completely from scratch without any prior, and has access to no external rewards.

As far as bounds are concerned, our algorithm draws a similarity to Experiment-Design literature from optimization. In a linear case and explicit policy, optimizing our objective is the same as finding a max-variance action which is an efficient way to discover the space satisfied by the model. While this is out of the scope of the paper, we have mentioned the pointers in Appendix Section-D.1 and elaborate more to spur discussion for future work to build upon.


[R1]: "how we can tell if something *is* a good method for exploration".
=&gt; It is difficult to measure exploration as an end in itself. There are several domain-specific metrics can be created that could act as a useful proxy to measure the quality of exploration. For instance, we measure how frequently the robot touches the object kept on the table purely out its exploration, and found out that this frequency increases to almost perfect as our training proceeds (please see the video on the website link provided). Another proxy to measure exploration is to see if a purely exploration-driven learning without using any external reward can help the agent obtain an external reward, as discussed in (Burda et. al. 2018) [2]. However, this latter proxy is only applicable to games-like environments. 

In this paper, we argue that the true measure of exploration is how well it is able to contribute to learning planning models later. Using the data collected by the exploration, we build models and report the success of those models for grasping, pushing etc. If the robot explored well during exploration, then it should have gathered good data for learning planning models for manipulation. This discussion is present in Section-5.1, 5.2 of paper. We will clarify it further.


[R1]: "Nothing ... seems specific to real-world... or at least nothing in the competing algorithms seems to preclude them from being run on a real-world robot…"
=&gt; The REINFORCE comparison is indeed existing curiosity-driven exploration from Pathak et.al. [2017] and being run on the real robot (please refer to meta combined response above for details)

We further provide a comparison for long-term horizon dependency tasks in video games. Please refer to the combined response for details.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1lHJGUk6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review: Clear reject</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkzeJ3A9F7&amp;noteId=S1lHJGUk6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper955 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper955 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:

This paper proposes a novel differentiable approximation to the curiosity reward by Pathak et al. that allows a learning agent to optimize a policy for greedy exploration directly by supervised learning, rather than RL.
The authors motivate this work with arguments about the sample-efficiency required by real robot learning, and demonstrate basic results using a real robot.

Comments:

The paper has serious style and tone issues that must be addressed before publication. The rest of my review will focus solely on the technical details.

The experimental details are lacking (learning rates? rollout lengths for REINFORCE? what are the inner and outer loops and what are their sizes? what are the plots measuring - extrinsic reward? intrinsic reward? what is "multi-step learning" in Table 2?).
Without these details, the results will be difficult to validate and reproduce independently.

The approach is compared only against very weak baselines. Why vanilla REINFORCE and not any of the modern policy-gradient algorithms (A3C, PPO, TRPO, DDPG, ...)? The ability to deal with this large action space is certainly impressive, but it is likely far too large for DQN or REINFORCE to work, so the comparison is questionable to begin with: you can make any algorithm fail if you give it an unnecessarily difficult interface. Did the authors try a smaller, more traditional action space?

The authors claim the sample-efficiency improvements by many existing exploration approaches are insignificant. In what way are the results in this paper more significant? Table 1 shows very minor improvements to a MPC planning task, Appendix Figure 1 shows barely any improvement over the baseline, and Appendix Figure 4 shows that learning from extrinsic rewards using REINFORCE seems to work just fine. Why use intrinsic rewards at all in this case? It appears that maybe some of the results look significant because the baselines are so weak.

The paper contains many factual errors and unsupported claims. For example:
- "the field of RL was born out of need to make our robots learn"
- "none of the recent advances have translated to success in the field of robotics" (see e.g. the proceedings of CoRL 2017 and 2018)
- "Building a good model will require enormous number of interactions" (see e.g. PILCO)
- "[our approach enables us to] for the first time ever, implement exploration on a real-world physical robot" (PILCO and many others)
- describing Pathak et al. curiosity as a "gaussian density model" in eq1; it's a deterministic forward model
- in sec3, "regress r^i_t to learn value estimates", this is probably meant to be the discounted sum of rewards
- also sec3, "[REINFORCE] gives no signal as to what action to take"; the signal has high variance but it works (see all policy gradient work)

These errors can be easily corrected. However, the contribution of the paper is based on a more serious error:
- sec3.1, "If the policy could be optimized using direct gradients, the rewarder could ... inform the agent to change its action space in the direction where forward prediction loss is high."
This is incorrect. The paper is based on using the gradient of the forward model to directly optimize the policy to produce higher prediction errors, as in Pathak et al.
But in order to make the prediction error differentiable, it makes the severe assumption that the next state x_{t+1} is constant and does not depend on a_t, which is false and invalidates the idea of optimizing actions for prediction error.
As a result, the gradient obtained does not actually move the policy toward higher prediction errors.

To understand what the author's approximation actually does, consider a perfect forward model. No matter what actions the policy produces, the prediction error is always zero, but the authors' gradient is not. So it can't be optimizing for higher prediction errors.
Instead of optimizing for high prediction errors as the authors claim, the policy is being optimized for state transitions that are maximally different from the observed state x_{t+1}.

This is an interesting objective to optimize. I can see how it could result in interesting exploration. But it's not what the authors say they're proposing.
It's much more like a count-based exploration strategy, which prefers visiting states that are maximally different from the states visited so far. It is much less like the prediction-error based curiosity of Pathak et al. that the authors are motivated by.
I would like to see focused analysis of this particular objective. For example, would this not result in the policy oscillating between different parts of the state space, since it's only optimizing for maximal difference to what it just saw, rather than long-term knowledge gain? This issue requires more discussion.

Finally, the approach is not really robot-specific despite the title and arguments in the paper. I recommend pursuing a more general investigation, because if this objective is truly as effective as the authors believe, then it should be applicable in a wide variety of domains (many of which are very easy to evaluate in: Atari, OpenAI Gym, DMLab, VizDoom, Mujoco, etc.).

Conclusion:

The paper proposes an interesting new objective, but it is motivated by a very naive approximation that completely changes the behavior of the exploration compared to what the authors want to approximate. The idea is novel and worth exploring, but the paper should be heavily rewritten to emphasize what the authors are actually doing with this new objective, and should include thorough analysis of its behavior, before I can recommend acceptance.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkgfN_-ZAX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>[1/2][Authors' Response to R3] Interpretation of the objective under the local approximation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkzeJ3A9F7&amp;noteId=SkgfN_-ZAX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper955 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper955 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to first thank the reviewer for the comments. These comments

[R3]: "The experimental details are lacking"
=&gt; We agree that these details are important for the reproducibility of work, and we will include them in supplementary -- we apologize that we missed adding them in the supplementary. That being said, we will make our code, environment, and models public upon release.

The learning rate for the model was 5e-4 and policy 1e-4 with an entropy loss coefficient of 1e-4. The optimizer used was Adam. We used standard DQN algorithm with prioritized experience replay [Schaul et. al. 2016] and epsilon-greedy to maximize the curiosity objective [Pathak et.al. 2017]. Q-value was trained with a learning rate of 1e-4. In epsilon-greedy, epsilon was initialized at 0.5 then annealed over training to 0.1.

[R3]: What are the plots measuring - extrinsic reward? intrinsic reward?
=&gt; The plots are measuring the how frequently the robotic arm touches the object which is a proxy for measuring how good the exploration is. Note that this reward was not used for training the policy, but just used to measure the quality of exploration at training. The robot was trained with an intrinsic reward only.

[R3]: "The approach is compared only against very weak baselines."
[R3]: "I recommend pursuing a more general investigation ..to evaluate in: Atari"
=&gt; Inspired by reviewer's suggestion, we have added a comparison on games in the supplementary using the state-of-the-art implementation proposed couple of months ago in (Burda et. al. 2018) [2]. Please see the combined response above for more details.

Our paper is just a first step towards explicitly using the structure of the prediction model in training the policy in a supervised manner. It relies on the model being good which is true for short-horizon tasks, but learning good long-term models is still an active research area [Finn et.al., 2017, Ebert et.al. 2017]. Hence, our approach might provide diminishing gains for long-horizon tasks. 

[R3]: "this large action space is certainly impressive, but it is likely far too large for DQN or REINFORCE"
=&gt; Our fundamental goal is to test exploration algorithms on real robotics setup where it is crucial to experiment in presence of large input space as well as large action space. We believe working with larger action space should be seen as a positive feature, and not something made to fail baselines. However, upon the reviewer's suggestion, we have added a comparison on games in the supplementary. Please see the combined response above for more details.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1xtwO-WR7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>[2/2][Authors' Response to R3] Interpretation of the objective under the local approximation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkzeJ3A9F7&amp;noteId=S1xtwO-WR7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper955 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper955 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[R3]: "Table 1 shows very minor improvements to a MPC planning task"
=&gt; As discussed in our combined response above, the baselines compared to in paper are same as the curiosity-driven exploration and our method achieves absolute 10% improvement in the mean accuracy for multi-object manipulation -- which we believe is a significant improvement.

[R3]: “Appendix Figure 4 shows that learning from extrinsic rewards using REINFORCE... the results look significant because the baselines are so weak."
=&gt; Please note that the external reward curve shown in Figure-4 is just to show that the REINFORCE baseline works in our setting. The reward, in that case, was touching the objects, while our single exploration policy learns diverse behavior without rewards learns to push, pull, grasp objects allowing us to collect useful data for training the planning models. A more detailed discussion on the usage of extrinsic and intrinsic rewards is presented in (Burda et. al. 2018) [2].
=&gt; We have now added comparisons to state-of-the-art off-the-shelf baselines on Atari games.

[R3]: "Building a good model require enormous number .." (see e.g. PILCO).
[R3]: " .. implement exploration on physical robot" (PILCO and many others)
=&gt; These are great advances and strong motivation for our work. We already discussed them in our method section (see Section-3.1, first para). Note that these are not reinforcement learning methods and based classical optimal control using learned models. However, those models are dynamics models in low-state-space of the agent learned using Gaussian processes and not from high dimensional raw image input. Even using deep networks, these methods have trouble generalizing to raw images as input (e.g. Deep PILCO [Gal et.al. 2016]) and rely on the learned model is quite good. Moreover, PILCO is not an exploration approach but learned for an end-task with well-defined cost objective.

[R3]: "To understand what the author's approximation actually does, consider a perfect forward model … is always zero.. maximally different from the observed state"
=&gt; If the prediction error is always zero, the action gradients will also be 0. The interesting scenario happens, when the error is low but not zero, in which objective encourages action that leads to states different from x_{t+1}. We have added a detailed clarification in Appendix Section-D.

Further, we re-emphasize that our local constancy approximation for x_{t+1} is only applied in a very local neighborhood of the executed action. It is only used for computing *local* gradients and nowhere else. This means when a completely different action a_t_new is applied at x_t, we will consider the new next state corresponding to a_t_new for computing gradients at a_t_new. One interesting consequence of our objective is that it has to be trained in an on-policy manner as off-policy violates the local-constancy approximation.

[R3]: "It's much more like a count-based exploration … much less like the prediction-error based"
=&gt; We agree with the reviewer that our is not exactly the same as Pathak et.al., and we did not claim this as well. However, we respectfully disagree that this objective is more-like a count-based strategy. This is so because our constant state approximation is only used in the small neighborhood of the executed action.

Thank you for bringing this point up. Inspired by this, we have added a detailed discussion in Appendix Section-D on understanding the behavior of objective.

[R3]: The paper has serious style and tone issues that must be addressed before publication.
=&gt; We will update and soften the introduction of the paper as discussed in the combined response above.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_HJlxrKt0nQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good engineering aspects worth appreciating</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkzeJ3A9F7&amp;noteId=HJlxrKt0nQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Aravind_Srinivas1" class="profile-link">Aravind Srinivas</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper955 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I read the spicy discussion below (very Reddit like). Though the anonymous comment highlighted some issues in the writing, I think the comment fails to see some details in this paper that are really worth appreciating. As someone who has worked a bit in visual continuous control, I find some details refreshingly fresh:

(1) Going for a very high dimensional discrete binned action space: This is a really good idea and might go on to become a standard in applying deep learning for robotics. Policies need more signal to learn better and faster and discrete has in general worked out better in deep learning. 

(2) Learning from high-resolution inputs: For some reason, applications of RL to robot control have tried to use 84x84 (or 64x64) inputs inspired from the original DQN architecture so far.  Learning from more high-resolution inputs makes more sense. 

(3) The rotation decision by augmenting the input: This is a nice idea taking the geometric aspect into consideration.

That said, it is possible that the DQN and REINFORCE baselines don't really work because of the high dimensional action space (especially REINFORCE). With a more engineered action space, they could. But this warrants more investigation in the future. 
 </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJlFlZOa2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Unsubstantiated claims, grandiose writing, poor experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkzeJ3A9F7&amp;noteId=HJlFlZOa2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 06 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper955 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I appreciate the intention of the authors to try curiosity-based learning on real-world tasks like robotic manipulation. It is definitely an important problem to work on and advances in it will help us build good motor primitives taking the visuomotor feedback loop into account. 

However, the readers here (people who know RL, DL) are not laymen on Hacker News / Reddit / casual readers of MIT Technology Review. So, we will not tolerate arbitrarily grandiose and incorrect statements made to portray your paper as a big advance. It is better to be grounded and stay true to the field as far as the writing on the paper is concerned.

(i) "There has been a lot of recent progress in the field of Reinforcement Learning (RL). However, most of the successful applications have been confined to the artificial world of video games (Mnih et al., 2015b) or simulations (Lillicrap et al., 2016)."  
   Not true. There have been successful applications for reducing power consumption in data centers, recommendations, advertisements, optimizing commercial applications for good user experience. Reinforcement Learning has existed even before Vlad Mnih invented DQN. There is a whole history of work on bandits and how to use them for online learning, optimization for ads, recommendations in e-commerce, etc. AutoML is a new upcoming application of reinforcement learning. This statement (the very first couple of sentences in the paper) clearly tells that the authors have no idea what they are talking about. 
(ii) "While the field of RL was born out of need to make our robots learn how to perform actions, none of the recent advances have translated to success in the field of robotics. Why is that?"
   This is again such an erroneous version of the field casually made with no evidence. Reinforcement Learning was studied from the perspective of behavior psychology initially. There is no evidence that it started only for making robots learn. One more evidence that the authors don't know what they say. Secondly, what classifies as success as far as robotics is concerned? As far as I know, there have been impressive demonstrations of reinforcement learning on real robots, such as OpenAI's Dactyl, Sergey Levine's Grasping work at Google, etc. So, are you saying that those are insignificant successes?
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJeWVWOphX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Continuation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkzeJ3A9F7&amp;noteId=BJeWVWOphX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 06 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper955 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">(iii) "In model-free Reinforcement Learning(RL) paradigm, the robot will try and try until it is able to stack objects and once it hits a successful instance, it is used as a positive signal (‘reward’) to promote these policy parameters. How does the robot try? Due to lack of any other signals from the environment, most-often robots use random-exploration policies (or random trajectories). It is clear that if the rewards are sparse, it may take millions of random “tries” before it hits any success. Clearly this approach is only scalable in video-games and not real-world robotics."
   Not correct. People have demonstrated that with parallel data collection with a farm of robots (Levine's work) / good sim2real transfer (OpenAI Dactyl), model-free reinforcement learning indeed scales even for sparse rewards. So, one more evidence that the authors don't understand the notion of 'scalable' but use it freely with no qualms. 
(iv) "Another possibility is to use model-driven approaches. Here, the robot will learn a model of the world from our millions of interactions and use the model to simulate and search. But what millions of interactions should be performed to build our models? Again due to lack of any external information, the most common approach is using random interactions to explore and build the world model (Agrawal et al., 2016; Levine et al., 2016; Pinto et al., 2016). Building a good model will require enormous number of interactions."
    The notion of model is completely incorrect here. None of the mentioned citations above actually build a world model. Agrawal learns inverse models, while Levine and Pinto learn a grasp success predictor. Agrawal's approach doesn't allow you to "simulate". It is a greedy one-step planner continuously executed in the real world till the provided goal is achieved. Levine uses cross-entropy method to adjust the actions for optimizing the output of the grasp success predictor. That's just basic policy optimization in a derivative-free manner and the whole pipeline is effectively a recurrent model-free approach (success predictor being the equivalent of a value function). The authors should clearly clarify that they are talking about models at the pixel level here, because there are lots of successes with reduced number of interactions by learning state-level models (Kurutach et al, Nagabandi et al, Chua et al).
(v) "it acts as reward shaping function which improves sample efficiency but not by a significant amount." - What will be significant enough for you? It is worth clarifying that to the readers. Random Network Distillation by OpenAI surpassed human level on Montezuma Revenge. Is that insignificant? Considering this paper was submitted even before that, I am curious if you consider all prior successes in exploration insignificant. Count based exploration could unlock upto 20+ rooms on Montezuma. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1eBUWdThm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Continuation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkzeJ3A9F7&amp;noteId=B1eBUWdThm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 06 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper955 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">(vi) "Another possibility is to use the inspiration from humans: humans even try to explore the world without the context of task. ...."
   Why is this "another possibility"? Burda et al's work falls into the same category as intrinsic rewards. They do an extensive evaluation of different intrinsic reward schemes. "Task agnostic" makes no sense here. When there are no extrinsic rewards provided,  the policy is inherently task-agnostic. 
(vii) "Our paper investigates exploration from an optimization viewpoint and asks a simple question: can we formulate curiosity reward as a differentiable function? We believe a differentiable reward function would enable to us to be sample efficient and for the first time ever, implement exploration on a real-world physical robot."
  Factually incorrect. Any reward function that is a DNN function of (s,a) is differentiable. For example, a value function built on a density model over states and actions. Or a simple model that takes in (s,a) and predicts a scalar. Curiosity reward being differentiable or not is hardly the issue. The point is you can make it differentiable function in multiple ways. 
   Secondly, this paper is NOT the first to show exploration on a real-world physical robot. Oudeyer's lab has a lot of work in this direction. Example: <a href="https://www.youtube.com/watch?v=NOLAwD4ZTW0" target="_blank" rel="nofollow">https://www.youtube.com/watch?v=NOLAwD4ZTW0</a>  - Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning": https://arxiv.org/abs/1708.02190 
(viii) "The standard way to optimize policy to maximize sequence of intrinsic rewards is to either use REINFORCE (i.e., on-policy way) or regress to learn value estimates (i.e., off-policy) as discussed in the previous section. Both these approaches do not make any use of the structure present in the design of rit. While these are unbiased estimators for training policy parameters with respect to rit, they suffer from high variance which is a known issue in reinforcement learning and an active area of research (Schulman et al., 2015). For instance, REINFORCE roughly amounts to saying that the agent should change the probability of the executed action in proportion to rewards received which fluctuates with the reward trajectories, leading to high variance. It gives no signal as to what action to take if the current action did not lead to a good reward."
    Not correct. There has been more recent work on using control variates to structure the critic to take into account the action executed and learn action-dependent baselines which could be estimated off-policy. They do not suffer from the variance issue as REINFORCE does. Secondly, Bengio's straight-through estimator doesn't suffer from the variance issue. 
(ix) The term differentiable exploration doesn't really mean anything. It doesn't solve the exploration issue any better than optimizing a policy on an intrinsic reward that is computed on environment transitions in different ways. The true exploration (which is intractable) is to maximize state visitation frequency. You resort to using straight through anyway. Calling straight through differentiable amounts to calling biased gradient estimators differentiable. Why invent new names for things that already exist? 
(x) "We perform simulation experiments to help us setup the right parameters and do extensive comparisons against REINFORCE and DQN" - These baselines are absolutely poor and as good as random policies. It is clearly not executed well. Makes me wonder the point of showing the comparisons. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1ghxZMR3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>[1/3][Author's Response] The Disconnect: Is 100yr training sample-efficient or scalable?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkzeJ3A9F7&amp;noteId=S1ghxZMR3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper955 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper955 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to first thank the commentator for the comments. These comments (and our reply below) definitely help us show the disconnect between two communities; the two spectrums of research directions. For example,

==&gt; Commentator: "good sim2real transfer (OpenAI Dactyl), model-free reinforcement learning indeed scales even for sparse rewards."
==&gt; Us: 100 years of experience for just manipulating one type of cube is completely unscalable. Tens-of-thousands of tasks each requiring tens of years of experience (even if experience requirements are diminishing) does not seem will get us far in AI.

==&gt; Commentator: "there have been impressive demonstrations of reinforcement learning on real robots, such as OpenAI's Dactyl, Sergey Levine's Grasping work at Google, etc."
==&gt; Us: Robotics is still missing its aha ImageNet moment (which computer vision had) and therefore a lot of results remain elusive and kind of non-replicable. That is why we still do not see every robotics person flocking to RL. Classical robotics can get some of these results with no training (<a href="https://www.youtube.com/watch?v=KVGn8tP9klI)" target="_blank" rel="nofollow">https://www.youtube.com/watch?v=KVGn8tP9klI)</a>

Which version is true? Probably both or probably none. This paper and our introduction hoped to tell the story from the other side. We want to emphasize our intention is not to grandify the claims but to highlight the disconnect between communities and we will be happy to soften the tone (especially in one place where we kind of agree with the commentator we should soften). Again these are changes in writing style in one paragraph or line and do not change the story or the experimental claims of the paper.

We believe research communities should be open (and respectful) enough to hear multiple perspectives and see how things will unfold in long-term. It is not healthy to say things like: "authors don't understand the notion of scalable" when things are not really as black/white. For the commentator, 100 years for one cube or billions of frames for one game seems scalable but for us, this highlights how sample-inefficient these algorithms are!

Finally, we highlight that commentator’s comments like action-dependent baselines "do not suffer from the variance issues" are not true and we highlight below the papers that clearly show these problems still exist.

We answer all individual comments below point-by-point. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkxL8bM027" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>[2/3][Author's Response] Does RL no longer have variance issues?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkzeJ3A9F7&amp;noteId=rkxL8bM027"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper955 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper955 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We first highlight technical and experimental comments because they are critical for evaluation of paper and then we will answer stylistic/philosophical questions:

(RE: viii) Commentator: "RL with action-dependent baselines does not suffer from the variance issue as REINFORCE does."
There have been great advances in RL optimization. However, the community still believes that the variance issue in RL is an open research area and it would be an over-claim to say that variance issue in RL no longer exists. For instance, in particular, see [Tucker et.al., ICLRW 2018] from Levine’s group which shows that "learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline" (quote from that paper). Therefore, we believe our comments regarding variance are correct.

(RE: ix) Here, differentiability is emphasized with respect to the actual (intrinsic) reward function agent is optimizing for. This is almost always is treated as a "scalar" quantity (black-box) in RL-based optimization (see Section-2 and Section-6). We do not argue that the use of straight-through estimator is being differentiable. Moreover, note that one would need to use straight through only in discrete action space, and directly back-prop in continuous cases.

(RE: x) It is not scalable to tune REINFORCE or DQN baselines on the real world. Hence, we tune them in the simulation. For instance, see Figure-4 in the appendix, which validates that the same REINFORCE baseline implementation works well in presence of hard-coded external reward, but not well during exploration. We used the same hyper-parameters for our method by only changing the optimization objective. So, to reiterate for both approaches parameters were optimized in simulation and then same parameters were used in real-world experiments!

(RE: vii) Our main argument is that intrinsic rewards have a lot of structure which gets lost if we learned to estimate it just as a "scalar" quantity, which is essentially the same as treating rewards as black-box. Policy gradients, value functions, Q-learning follow this regime which is differentiable, but the reward structure gets already lost in learning them. This is described in detail in Section-2.
The work of [Forestier, Mollard, Oudeyer, 2017] on real-robot is using low-dimensional fully-observable state-space information (e.g. exact location of objects etc.) and dynamic motion primitives to lower action complexity. This is in contrast to our setup, where our robotic agents learn to explore using high-dimensional raw images with large action-space beginning without any notions of objects or their existence. Our exploration learns the concept of an object directly from scratch rather than being hand-coded. Furthermore, the setting of Oudeyer is using the same object and does not try on a different set of objects and hence it is not clear how generalizable it is.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1ePezGAh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>[3/3][Author's Response] Does DeepRL for robotics work?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkzeJ3A9F7&amp;noteId=S1ePezGAh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper955 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper955 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We now answer philosophical/stylistic comments:

(RE: i) We highlighted "most" recent applications of RL being on games/simulation (we never used ONLY or ALL).  The applications pointed by the commentator (e.g., recommendations, advertisement etc.) are mostly that of the ‘multi-arm bandits’. However, multi-arm bandits are not the same as reinforcement learning and instead mostly deal with "stateless" problems where actions taken do not affect the next state. They are a much-restricted class of problems and have been used for advertisements in e-commerce. They are not applicable to the real-world robotics setting where an agent acts in a real manipulation environment.

(RE: ii) We will soften the point to say one of the major RL applications was supposed to be Robotics. That said, we still do not believe RL is as widely accepted as ConvNets in computer vision. Most robotics researchers still do not use robot learning or believe its results are as impressive as the commentator seems to suggest (robotics researchers are not flocking to RL). We believe there is long-way before RL can be applied to real-world robotics applications.

(RE: iii) As mentioned above, definitions of scalability can have a different perspective. 100-years of experience for a single task of manipulating cubes is unscalable for us. Consider tens-of-thousands of tasks each requiring years of experience does not seem the scalable way of doing things (even if experience requirements are diminishing). The parallel way of data-collection works only in primitive tasks like grasping and poking where success rates for random are high as well.

(RE: iv) Our point is indeed further emphasized by the commentator’s remark. There is very little to no work which builds actual accurate, long-term, non-greedy models from raw sensory observations in real robotics setting. We will clarify that such models should operate on raw sensory observations (e.g. pixels), and not state-space because estimating the exact state-space of the environment is a big research area in itself (i.e., system identification), and still an unsolved problem (i.e., estimating exact position of objects in a scene in computer vision).

(RE: vi) Most of the classical work on learning with intrinsic rewards has been to just augment an external reward, in which case the learned policy is specific to the task it is trained for. An "alternate" way here refers to that of developmental psychologist where intrinsic motivation itself (i.e., without any external reward or end-goal) is seen as the primary driver in the early stages of development to learn skills which are useful in future. [Burda et.al. 2018] is one such example where interesting behaviors emerge by only training for intrinsic rewards across 54 simulated environments.

(RE: v) Yes, Random Network Distillation (RND) paper showed up just 6 days ago on arXiv, much after the submission deadline. This paper also shows results on video games. While this paper shows great results on playing video games, but note that it uses "a total of 1.97 billion frames of experience" [quote from RND paper] for each experiment! This is not at all scalable to real-robotic settings, even in a robotic arm-farm setup.
Again we emphasize our argument is that these RL results are significant and great advances but just not scalable to learning with robots.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_rklLqv7q2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkzeJ3A9F7&amp;noteId=rklLqv7q2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper955 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper955 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents an interesting way to reformulate intrinsic curiosity as a differentiable function. The authors compare the differentiable function against using prediction error via REINFORCE and DQN, showing that their intrinsic curiosity method results in more interactions with unseen objects than the other two methods. For DQN this is to be expected, but it shows that backprop through this function is more efficient than reinforce in getting to unseen state spaces. I think this is an interesting method/proposal and is a somewhat novel reformulation of intrinsic error, but I do have some concerns in comparisons/claims. 

In the introduction, the authors say that the intrinsic curiosity method proposed by Pathak et al. is sample inefficient and isn’t tested in robots. However, to my understanding the REINFORCE baseline isn’t really equivalent (though it may be possible that it is, it was unclear how exactly the loss was formulated in the baseline, did include the other components from Pathak et al.?). If the claim is that this method is more efficient, I think it should have compared against that method directly. 

Moreover, I think the description of the experiments doesn’t provide enough information. For example, the method says that different learning rates were used for the min-max game to stabilize it, but doesn’t say what they were. 
Also, for the DQN baseline what were the parameters? Was there an epsilon greedy policy on top of the exploration reward? Was this annealed as in other work? Generally, I think more detail is needed throughout (even if it just refers to a more detailed appendix).

Overall, I think this work needs to be revised to include more details on hyperaparameters, details on the baselines, and describing differences between Pathak et al.’s method and the REINFORCE baseline. Moreover, feedback from other comments on this work should be addressed which reflect in more detail my comments below on opinionated claims (e.g., <a href="https://openreview.net/forum?id=SkzeJ3A9F7&amp;noteId=HJlFlZOa2X" target="_blank" rel="nofollow">https://openreview.net/forum?id=SkzeJ3A9F7&amp;noteId=HJlFlZOa2X</a> )


Comments/Thoughts:

+ I think in the introduction there are some statements that probably need citations. For example, “But the same formulation from an optimization viewpoint, it suffers from all the bad properties of extrinsic rewards. The reward is a function of environment behavior with respect to the performed action. Since the environment behavior function is unknown, it is treated as black-box and hence the gradients have to be computed using REINFORCE (Williams, 1992) which is quite sample inefficient.” —&gt; Why is this true? Is there a citation that can back this? Do you prove it later in the paper? 
+ “Yes, 54 environments but no real-world physical robots” —&gt; this and the intro seems like a blogpost at times. That can be fine (some would argue it’s a good thing), but there seem to be some opinions without citations/backing, I suggest trying to back up statements wherever possible and avoid opinions. For example in this statement, robots aren't a requirement for evaluating intrinsic motivation.
+ “Since the environment behavior function is unknown, it is treated as black-box and hence the gradients have to be computed using REINFORCE (Williams, 1992) which is quite sample inefficient.” —&gt; citation/backing? it might be nice to point to the experiment section here to back it (e.g., "As will be shown in Section X and in \citet{something}, REINFORCE can be quite sample inefficient")
+ “In practice, the existing on-policy algorithms, e.g., A3C (Mnih et al., 2016), PPO (Schulman et al., 2017) etc. are deployed off-the shelf -&gt; This is confusing, so is this using REINFORCE or PPO/A3C? what is this statement referring to?
+ “regress to rti to learn value estimates (i.e., off-policy) as discussed in the previous section” —&gt; regress to \sum r_t{I} for a value estimate?? Value is the expected return so not sure if this is a typo or i missed something earlier
+ What is the actual loss function used for the baseline? Is it the same as Pathak et al.?
+ What are the hyper parameters for DQN exploration? What are all the hyper parameters for any/all the algorithms? 
+ Was a variance-reducing baseline used in REINFORCE?
+ What is the variance representing in the graphs, std across several trials? Maybe I missed it, but how many trials represent this standard deviation?
+ “Hence, we train the forward predictor slightly faster than the policy by keeping higher learning rate to stabilize the learning process. “ —&gt; what were the learning rates?


Linguistic/Typos:

Also, some minor, but frequent, grammatical issues/typos that I’ve added below could be fixed. I would ask that the authors please have the submission proof-read for English style and grammar issues. There are many minor mistakes, some of which I’ve tried to point out below. 

+ “This leads to a significantly sample efficient exploration policy. “ —&gt; significantly more (?) sample efficient ?

“Why is that? To understand the reason behind sample inefficiency of curiosity or intrinsic rewards, notice how the intrinsic rewards are given by agent” —&gt; by the agent?

“Forward model fθF is trained to minimize its loss which amounts to minimizing rti with respect to θF” —&gt; the forward model

“However, policy is optimized to maximize the objective” —&gt; However, the policy

“We can also optimize  for policy parameters θP via differentiable loss function” —&gt; We can also optimize for (the) policy parameters \theta via (a) differentiable loss function?

“To optimize policy to maximize a discounted sum “ —&gt; To optimize the policy

“How good is Forward Prediction Model” —&gt; How good is the forward prediction model

There are several other spots, but basically another pass over the paper might be worth it to check for these sorts of issues. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1eCHvZb0m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>[Authors' Response to R2] Hyper-parameters and other details of experimental setup</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkzeJ3A9F7&amp;noteId=S1eCHvZb0m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper955 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper955 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to first thank the reviewer for the comments. Please see the meta response in the combined reply for the common concerns among reviewers. The reviewer found the formulation to be "interesting" and "novel" but has raised concerns about lack of details about the experiment hyper-parameters. We answer them below:

[R2]: REINFORCE baseline vs. Pathak et. al.? Is it the same as Pathak et al.?
=&gt; Please refer to the combined response to all reviewers above. It is indeed the same.

[R2]: "different learning rates .. but doesn't say what they were"
[R2]: "What were DQN parameters? Epsilon greedy"
[R2]: "include more details on hyper-parameters ..."
=&gt; Yes, in our case, we can simply train the forward model faster than policy to stabilize the training. The learning rate for the model was 5e-4 and policy 1e-4 with an entropy loss coefficient of 1e-4. The optimizer used was Adam. 

Yes, we used standard DQN algorithm with prioritized experience replay [Schaul et. al. 2016] and epsilon-greedy to maximize the curiosity objective [Pathak et.al. 2017]. Q-value was trained with a learning rate of 1e-4. In epsilon-greedy, epsilon was initialized at 0.5 then annealed over training to 0.1.

We agree that these details are important for the reproducibility of work, and we will include them in supplementary -- we apologize that we missed adding them in the supplementary. That being said, we will make our code, environment, and models public upon release.

[R2]: "Feedback from the other commentator should be addressed"
=&gt; We have answered both the factual and stylistic comments with proper citations in our reply.

[R2]: "... This is confusing, so is this using REINFORCE or PPO/A3C? .."
[R2]: "it might be nice to point to the experiment section here to back"
=&gt; With due respect, we believe reviewer has misunderstood the background here. REINFORCE is an operator that was proposed in (Williams, 1992), and then became the main ingredient of policy gradient algorithms. All policy gradient algorithms, whether PPO or A3C, use reinforce operator to update the policy. PPO, in addition to reward maximization, penalizes KL-divergence change while A3C relies on regularization from multiple workers, but both these algorithms use reinforce at the core of it.

[Schulman et.al. 2016] provides a nice discussion on how REINFORCE helps to optimize any black-box function in expectation using only samples from the function. More detailed and fundamentals of policy gradients are described in [Sutton &amp; Barto, 1998]. We implemented the policy-gradient algorithm as optimizing the curiosity objective in [Pathak et.al. 2017] as a baseline which we call "REINFORCE". We will update the legend of graphs to say [Pathak et.al. 2017] in future to avoid confusion.

[R2]: "regress to rti .. discussed in the previous section .. so not sure if this is a typo"
=&gt; Thanks, it should be indeed summed. However, note that in the previous section it was mentioned correctly. We will correct this typo.

[R2]: "Some minor, but frequent, grammatical issues/typos .."
=&gt; Thank you. We will fix all these writing typos and make sure there no other ones.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>