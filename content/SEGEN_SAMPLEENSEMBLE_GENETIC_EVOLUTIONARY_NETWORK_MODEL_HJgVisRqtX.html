<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>SEGEN: SAMPLE-ENSEMBLE GENETIC EVOLUTIONARY NETWORK MODEL | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="SEGEN: SAMPLE-ENSEMBLE GENETIC EVOLUTIONARY NETWORK MODEL" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HJgVisRqtX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="SEGEN: SAMPLE-ENSEMBLE GENETIC EVOLUTIONARY NETWORK MODEL" />
      <meta name="og:description" content="Deep learning, a rebranding of deep neural network research works, has achieved a remarkable success in recent years. With multiple hidden layers, deep learning models aim at computing the..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HJgVisRqtX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>SEGEN: SAMPLE-ENSEMBLE GENETIC EVOLUTIONARY NETWORK MODEL</a> <a class="note_content_pdf" href="/pdf?id=HJgVisRqtX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019segen:,    &#10;title={SEGEN: SAMPLE-ENSEMBLE GENETIC EVOLUTIONARY NETWORK MODEL},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HJgVisRqtX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Deep learning, a rebranding of deep neural network research works, has achieved a remarkable success in recent years. With multiple hidden layers, deep learning models aim at computing the hierarchical feature representations of the observational data. Meanwhile, due to its severe disadvantages in data consumption, computational resources, parameter tuning costs and the lack of result explainability, deep learning has also suffered from lots of criticism. In this paper, we will introduce a new representation learning model, namely “Sample-Ensemble Genetic Evolutionary Network” (SEGEN), which can serve as an alternative approach to deep learning models. Instead of building one single deep model, based on a set of sampled sub-instances, SEGEN adopts a genetic-evolutionary learning strategy to build a group of unit models generations by generations. The unit models incorporated in SEGEN can be either traditional machine learning models or the recent deep learning models with a much “narrower” and “shallower” architecture. The learning results of each instance at the final generation will be effectively combined from each unit model via diffusive propagation and ensemble learning strategies. From the computational perspective, SEGEN requires far less data, fewer computational resources and parameter tuning efforts, but has sound theoretic interpretability of the learning process and results. Extensive experiments have been done on several different real-world benchmark datasets, and the experimental results obtained by SEGEN have demonstrated its advantages over the state-of-the-art representation learning models.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Genetic Evolutionary Network, Deep Learning, Genetic Algorithm, Ensemble Learning, Representation Learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We introduce a new representation learning model, namely “Sample-Ensemble Genetic Evolutionary Network” (SEGEN), which can serve as an alternative approach to deep learning models.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HkgIko9h2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Evolutionary part is not clear</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJgVisRqtX&amp;noteId=HkgIko9h2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper613 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper613 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper introduces Sample-Ensemble Genetic Evolutionary Network, which adopts a genetic-evolutionary learning strategy to build a group of unit models. Explanation on the evolutionary network part is not enough. For example, there is no clear explanation on how chromosomes are defined. Also, detailed analysis on computational aspect is needed.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkeQ8n-caQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author Response to Reviewer 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJgVisRqtX&amp;noteId=SkeQ8n-caQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper613 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper613 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comments. Please find the response as follows. Hope we resolve your concerns and questions. Welcome to let us know if you have any other questions.

&lt;1&gt; We clarify that we introduce the model chromosomes as the variables of the models. You can refer to the last two sentences in section 4.2.1. as well as section 4.2.5. We also paste the sentences as follows.
4.2.1: Formally, the variables involved in each unit model, e.g., M_i^1, can be denoted as vector θ_i^1, which covers the weight and bias terms in the model (which will be treated as the model genes in the evolution to be introduced later) ).
4.2.5: For the k_th pair of parent unit model (M_i^1,M_j^1)k ∈ P^1, we can denote their genes as their variables θ_i^1,θ_j^1 respectively (since the differences among the unit models mainly lie in their variables), which are actually their chromosomes for crossover and mutation.

&lt;2&gt; We clarify that we introduce the computational analysis in Section 4.4, including its performance analysis, space and time cost analysis, as well as advantages analysis.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkxb8KF2hQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Using Subsampling + Genetic Algorithm for Network Embedding</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJgVisRqtX&amp;noteId=rkxb8KF2hQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper613 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper613 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes to subsample a large network into sub-networks, learn a network model (autoencoder) from each subgraph, perform crossover and mutation operations over the network parameters of different model pairs, and combine the latent representations following the ensemble idea.

The paper is clearly presented. Originality and significance is limited. Putting the three knowns components - subsampling, generation algorithm and ensembling together seems to be the main contribution of this paper. However, the ways of doing subsampling, performing the crossover and mutation operations and doing the ensembling are relatively straightforward ways of applying them. The fact that combining them to obtain better results is not a surprising result. And according to the experimental results, it is not clear how the gain in performance is resulted and to what extent each of the three components is contributing. For instance, I just guess the combination of subsampling + existing network embedding methods (LINE/DeepWalk/...) + ensembling may also give good results. Currently, the performance comparison is done with the original forms of LINE and DeepWalk. That makes the empirical results not very convincing to explain the key strengths of this work.

+ve:
`1. The paper is clearly presented.
2. The design is reasonable one.
3. A number of benchmark datasets are used for the evaluation.

-ve:
1. The originality and significance is limited.
2. The performance comparison should be done with references to more competitive candidates as explained above.
3. The nodes of different sub-networks are essentially projected to different embedding spaces. The validity and interpretation of performing the crossover operation on two different models (two different embedding spaces) will need more justifications.
4. The proposed methodology is not an end-to-end. The ensembling being evaluated is just simple addition.
5. The paper claims that "The unit learning model, genetic algorithm and ensemble learning can all provide the theoretic foundation for SEGEN, which will lead to sound theoretic explanation of both the learning result and the SEGEN model itself". Individually being sound does not imply that the way to combine them is sound. Currently, I cannot see the uniqueness of this particular combination.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJxPI_zcp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author Response to Reviewer 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJgVisRqtX&amp;noteId=SJxPI_zcp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper613 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper613 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Thank you for your comments. Please find the response as follows. Hope we resolve your concerns and questions.

&lt;1&gt; First of all, we need to re-clarify the contributions of this article.

&lt;1.1&gt; For the learning settings with extremely large-sized but small-numbered data instances (i.e., each data instance is large, but the total number of available data instance is small), training large and deep neural networks is an impossible mission. In this paper, we propose a solution to such a problem. 

&lt;1.2&gt; This paper doesn’t really like existing deep learning works focusing in stacking components together. It is not a good idea to interpret our contribution as “putting three known components together”. According to &lt;1.1&gt;, to solve the lack of data instance problem, we propose to divide the large graph into small-sized sub-graphs. To ensure the sub-graphs can capture the properties of the large graph, we use different sampling methods. Meanwhile, in the training process, to ensure the learning effectiveness, we also introduce a new learning framework, with both the gradient descent based algorithm with the genetic algorithm. As to the ensemble part, it not merely because we decompose the large graph into smaller graphs. The main reason is we have a group of small models, each one is trained on sub-graphs achieved by a sampling method, we need to integrate the outputs of these models together.

&lt;1.3&gt; The model learning part of the model proposed in this paper is based on both the gradient decent based algorithms (for each unit model), as well as the genetic algorithm (between different generations). This part should be notable to the reader and the reviewer.

&lt;2&gt; How the gain in performance is resulted? We clarify that with a small number of large data instance inputs, we cannot train effective deep models due to the lack of data instances. By decomposing the large graphs into smaller ones, we will be able to learn effective model variables.

&lt;3&gt; sampling+existing embedding model+ensemble should also be useful. The answer is yes, since our framework and our new learning algorithm is useful, replacing the auto-encoder based embedding algorithm with the other shallow or deep embedding models should also work fine. The reviewer is suggested to read the paper again to understand what we do, so as to understanding our contributions, instead of treating it is a combination of sampling+ensembling.

&lt;4&gt; Originality and significance is limited: This is the first paper to introduce the genetic evolutional neural network! Different from the existing deep learning model works, we propose a novel network model trainable with a small-set of extremely large data instances. We introduce a new model learning algorithm with both gradient decent based algorithms and the genetic algorithm. I assume the reviewer cannot find another paper with these two novelty and contributions.

&lt;5&gt; The baseline methods, LINE, DeepWalk, Node2Vec, and HPE are the state-of-the-art methods in network embedding introduced in recent years.

&lt;6&gt; Crossover on models. Crossover operation is to help learn a much better unit model actually. Based on the gradient descent algorithms, we will be able to learn good unit models. However, once the unit model achieving the local optima, it cannot be further improved any more. Genetic algorithm (including crossover, mutation, etc.) allows the models to jump out from the local optima and achieve better performance. The generated child models will be updated with gradient decent algorithm again to achieve the local optimas. Will this make the models worse? The answer is it is possible. However, in the proposed architecture, we will select top m models among the parent models and the newly generated child models. If the child models are bad, they will not be selected for the next generation. In other words, we can ensure the crossover will not degrade the learning performance of the unit models for the next generation. The readers and reviewers are suggested to refer to the recent article (<a href="https://arxiv.org/abs/1805.07500)" target="_blank" rel="nofollow">https://arxiv.org/abs/1805.07500)</a> to understand the advantages of incorporating genetic algorithm into the model optimization part.

&lt;7&gt; The method is not end-to-end. Since the genetic algorithm involves crossover and mutation, this part involves probabilities into the model, it is impossible to train the crossover and mutation operations with the existing error-backpropagation algorithm. In other words, training the method in an end-to-end is an impossible mission.

&lt;8&gt; GA + Ensemble together not sound. We clarify that we have provide the proof ready, but due to the limited space we remove many important proofs. We demonstrate that via GA and ensemble, we can achieve better performance. The reviewer is suggested to refer to Section 4 in the recent article (https://arxiv.org/abs/1805.07500) for more information. Especially Equation 14 in that article, it indicates that via generations the learning loss will be non-increasing. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SJlE9Qm32m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting topic but several issues with the paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJgVisRqtX&amp;noteId=SJlE9Qm32m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper613 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper613 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This manuscript introduces SEGEN, a model based on Evolutionary Computation for building deep models. Interestingly, the authors define deep models in a different way. Instead of stacking several hidden layers one after the other (as in traditional deep learning models), SEGEN uses the idea of generations in evolutionary models (Genetic Algorithms or GA) and puts the unit models in the successive generations into layers, i.e., “evolutionary layer”. Each layer then performs the validation, selection, crossover, and mutation operations, as in GA. Another interesting point of the proposed method is that the choice of unit models in SEGEN can be traditional machine learning or recent deep learning models.
The paper touches an interesting topic and proposes a sound method. However, there are several issues with the paper. There are several ungrounded and untested claims, as well as many unclear points in the method.
-	In page 5, Section 4.2.4, the authors introduce the loss function used to define the fitness for the evolutionary model. It is not clear why they use the difference between the latent representations of the autoencoders (z) from pairwise nodes to define the loss. There are no motivations or discussion for this. Two different representations of two nodes may both be good (e.g., in terms of classification of data), but they do not have to be necessarily identical. 
-	Given the loss defined in Section 4.2.4, it is not clear how the authors ran their model for MNIST and other datasets, for which they used CNN and MLP unit models. In CNN and MLP there is not latent representation z.
-	Based on the model descriptions in Section 4.2 (and its Subsections), the proposed method transfers the learned models in previous generations to the next ones. But there is no explanation if the new models are again fine-tuned on the data? For instance, take the autoencoders, for two different unit models, the cross-over operator defuses the variables (weights and bias) from the two selected models to create an offspring. There is no guarantee that the new autoencoder model works properly on the same dataset. As a naïve example, if there are correlated and redundant features in the data, different autoencoders may separately focus on one/some of these features. Defusing weights of the two autoencoders (built upon different aspects of the data) may most probably ruin the whole model. 
-	There are four claims in the paper on the advantages of the proposed model, compared to other deep learning algorithms. None of these claims are discussed in depth or at least illustrated experimentally. 
*** Less Data for Unit Model Learning. The authors could have reported the number of variables used in each model in the experiments. It is important to see with how many of a larger number of variables a traditional deep model can result in comparable results to SEGEN. 
*** Less Computational Resources. The model operates in several generations and in each generation, many unit models are built. It is not fair to say and not clear how it can occupy less space or time complexity than a regular GCNN or MLP.
*** Less Parameter Tuning. Again experiments could clarify this issue.
*** Sound Theoretic Explanation. The authors only refer to (Rudolph 1994) for the performance bounds of their model and claim that since they are using GA they are better than other deep learning models. However, performance bounds for GA models are very shallow and proximal. 
-	To calculate the computational complexity of the model, the authors analyzed the time for learning one unit model. However, in GA models, the complexity is calculated using the bounds on the number of times the fitness function is called since the fitness function is the most computationally intensive task (please see: Pelikan and Lobo 1999 ‘Parameterless Genetic Algorithm A Worst-case Time and Space Complexity Analysis’). 
-	One of the main fallacies of GAs and evolutionary algorithms is that they may lead to premature convergence. This is very common, especially at the presence of trap functions, such as non-convex functions that real-world problems deal with (please see: Goldberg et al. 1991 ‘Massive Multimodality, Deception, and Genetic Algorithms’). There are no discussions/experiments on how SEGEN may overcome the premature convergence, or even if it converges at all.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1x636G56Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author Response to Reviewer 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJgVisRqtX&amp;noteId=B1x636G56Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper613 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper613 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comments. Please find the response as follows. Hope we resolve your concerns and questions. Welcome to let us know if you have any other questions.

&lt;1&gt; Section 4.2.4 in Page 5 on the loss function. We clarify that for each node we can compute its latent feature representation z with the auto-encoder model. However, graph embedding is slightly different form other existing embedding problems, since the nodes are connected. Generally, in graph embedding, we may hope the learned representation features can capture the network structure: connected nodes will have closer representations. Therefore, given two nodes, v_i, v_j, if they are connected, i.e., s_ij = 1, then we may want to project them into close regions; if they are not connected, i.e., s_ij = 0, then we will not count the loss introduced by them, i.e., projecting them to any regions will not matter any more.

&lt;2&gt; The auto-encoder model as well as the z vectors is for the network embedding task only (auto-encoder as the base model, we further consider the graph connections). We use it as an example to introduce the overall SEGEN framework settings. The task and the unit models used in it can be changed to any other models, where the detailed loss function and the descriptions will be different. When it comes to CNN+MNIST or MLP+OtherDatasets, we will learn CNN unit models and MLP unit models instead, which will not contain the z vectors or the loss function in section 4.2.4. Instead, we will have some other loss functions on the CNN output, e.g., the cross-entropy on the CNN outputs compared against the true labels.

&lt;3&gt; New models are fine-tuned? We claim that the new models will be fine-tuned in the next generation on the dataset. Since in the new generation, the first step is to learn the unit models before involving them in the genetic evolutionary part. They will be trained on the training set. 

&lt;4&gt; We clarify that we have provide the proof ready, but due to the limited space we remove many important proofs. We demonstrate that via GA and ensemble, we can achieve better performance. The reviewer is suggested to refer to Section 4 in the recent article (<a href="https://arxiv.org/abs/1805.07500)" target="_blank" rel="nofollow">https://arxiv.org/abs/1805.07500)</a> for more information. Especially Equation 14 in that article, it indicates that via generations the learning loss will be non-increasing. 

&lt;5&gt; We clarify that our time and space cost analysis is not for one unit model, it is for the whole SEGEN model with multiple generations. The time complexiety provided before Section 4.4.3 contains K as the generation number, m as the population size.

&lt;6&gt; We clarify that for the SEGEN model introduced in this part, the fitness function computation is not the most computationally intensive task actually, since the unit model learning with gradient descent in Section 4.2.3 will be much time consuming. The time costs in learning the models may grow exponentially as the model size (I mean the input data size) increases. That is the reason we try to sample the sub-graphs in this paper instead to lower down the time cost compared against the existing deep models (it is also our main contribution and advantages). Fitness function computation is the most computationally intensive task in (Pelikan and Lobo 1999 mentioned in your comments), mainly compared against the mutation and cross operations. Here, the learning settings change to the deep learning model learning + evolutionary. Compared against model learning, GA based evolutionary time cost is not significant any more, not to mention the fitness function computation part.

&lt;7&gt; As to the convergence part, we clarify that we have provide the proof ready, but due to the limited space we remove many important proofs. The reviewer is also suggested to refer to Section 4 in the recent article (https://arxiv.org/abs/1805.07500) for more information. Especially Equation 14 in that article, it indicates that via generations the learning loss will be non-increasing, and it will converge generations by generations.


</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>