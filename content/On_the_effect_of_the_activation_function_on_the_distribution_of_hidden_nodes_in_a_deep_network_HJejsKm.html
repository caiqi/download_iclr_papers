<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>On the effect of the activation function on the distribution of hidden nodes in a deep network | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="On the effect of the activation function on the distribution of hidden nodes in a deep network" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HJej3s09Km" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="On the effect of the activation function on the distribution of..." />
      <meta name="og:description" content="We analyze the joint probability distribution on the lengths of the&#10;  vectors of hidden variables in different layers of a fully connected&#10;  deep network, when the weights and biases are chosen..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HJej3s09Km" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>On the effect of the activation function on the distribution of hidden nodes in a deep network</a> <a class="note_content_pdf" href="/pdf?id=HJej3s09Km" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019on,    &#10;title={On the effect of the activation function on the distribution of hidden nodes in a deep network},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HJej3s09Km},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We analyze the joint probability distribution on the lengths of the
vectors of hidden variables in different layers of a fully connected
deep network, when the weights and biases are chosen randomly according to
Gaussian distributions, and the input is binary-valued.  We show
that, if the activation function satisfies a minimal set of
assumptions, satisfied by all activation functions that we know that
are used in practice, then, as the width of the network gets large,
the ``length process'' converges in probability to a length map
that is determined as a simple function of the variances of the
random weights and biases, and the activation function.

We also show that this convergence may fail for activation functions 
that violate our assumptions.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">theory, length map, initialization</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We prove that, for activation functions satisfying some conditions, as a deep network gets wide, the lengths of the vectors of hidden variables converge to a length map.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BklpNmRoh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>an abstract analysis that does not aim to derive any conclusions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJej3s09Km&amp;noteId=BklpNmRoh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper744 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper744 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper performs an analysis of the length scale of activations for deep fully-connected neural networks with respect to the activation function in neural networks. The authors show that for a very large class of activation functions, the length process converges in probability.

I am listing my main concerns about this manuscript below.

1. The paper is poorly motivated and does not make an attempt to relate its results to observations in practice or the design of new techniques. It is an abstract analysis of the probability distribution of the activations.

2. Theorem 2, which is the main theoretical contribution of the paper, hinges on fixing the inputs of the neural network with weights sampled randomly from a Gaussian distribution. It is difficult to connect this with practice. This is not unreasonable and indeed common in mean-field analyses. However such analyses go further in their implications, e.g., <a href="https://arxiv.org/abs/1606.05340," target="_blank" rel="nofollow">https://arxiv.org/abs/1606.05340,</a> https://arxiv.org/abs/1806.05393 etc. This is my main concern about the paper, its lack of concrete implications despite the simplifying assumptions.

3. It would be very interesting if the analysis in this manuscript informs new activation functions or new initialization methods for training deep networks.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rygxqdckaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>motivation for our analysis</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJej3s09Km&amp;noteId=rygxqdckaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper744 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper744 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The length map studied in our paper, itself published in NIPS’16, has been repeatedly applied in a long series of papers published in NIPS, ICLR and ICML; we provide a list in the fourth paragraph of our paper.  It is therefore noteworthy that the logic supporting this length map published in [14] is not valid, and the claim made about it in that paper is incorrect.  This in turn motivates the question of what similar statement is correct. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1gSYsX52Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>In this paper, the authors studied how the activation function affects the behavior of randomized deep networks. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJej3s09Km&amp;noteId=S1gSYsX52Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper744 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper744 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">* summary
In this paper, the authors studied how the activation function affects the behavior of randomized deep networks. 
When the activation function is permissible and the weights of DNN are generated from the Gaussian distribution,
the output of each layer was related to the so-called length process. When the permissibility is violated,
the convergence property may not hold. Some numerical experiments confirm the theoretical findings. 


* comments
However, The randomized DNN is not clear whether theoretical results in this paper is related to the practical DNN. 
The authors showed intensive proofs of theorems.
I think that the relation between DNN in practice and the results in this paper should be pursued more. 

* The meaning of Theorem 10 is not clear. What does the theorem reveal about the ReLU function in the practical usage?

* In this paper, a limit theorem in terms of the dimension N is considered.
  However, the limit theorem in terms of the depth D is also important for the DNN.
  Some comments on that would be helpful for readers. 

* Is there any relation between the analysis in this paper and batch normalization or weight normalization? 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJlETOcJTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>motivation, and discussion of normalization</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJej3s09Km&amp;noteId=HJlETOcJTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper744 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper744 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Theorem 10 demonstrates a flaw in the logic provided in [14], motivating a new analysis.  As discussed in some earlier papers in this line of research, if sigma_w and sigma_b are chosen so that q_{ell} = q_{ell-1}, very deep networks can be trained.  

We agree that extending a rigorous analysis to concern batch normalization and weight normalization is an interesting direction for further research.  It seems that the most interesting effects would occur during training.  Framing this for tractable rigorous analysis looks like an interesting and important challenge.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HyeYsYKPh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Technically correct, not well-written</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJej3s09Km&amp;noteId=HyeYsYKPh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper744 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper744 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: the paper proves the convergence of empirical length map (length process) in NN to the length map for a permissible activation functions in a wide-network limit. The authors also show why the assumptions on the permissible functions can not be relaxed.

Quality: the paper seems to be technically correct. However, the authors do not discuss any consequence of their result. Why was it important to prove it? What does it tell us about the networks? While the proof may be of interest to the authors of [14] to correct their (possible) mistakes, I think the paper will go under the radar for most people and thus encourage the authors to heavily revise the paper.

Clarity: the writing is clear in general. The proofs sometimes jump over non-trivial things and explain easy steps, but that maybe subjective. The paper spends no effort explaining the contribution and its consequences.

Originality: the proven statements are novel and extend/fix the claims of [14]

Significance: as said above, I believe that in the current form the paper will have little to no impact. The importance of proving the main statement under more general conditions on activation functions is doubtful and the authors do not comment on that.

Minor comments:
* when introducing T{i,:,:} the &lt;&gt; notation is not clear. I could guess it from the later usage of the symbol, but these brackets can mean a lot of things, e.g. bracket mean (Section 2.1)
* it would be beneficial to define the main objects, wide-network limits, in a more formal way (Section 2.3)
* how the wide regime (large N) is interesting for studying deep NNs? [14] discusses that to some extent, but this should be explained here as well
* q_0 is never defined
* it's good practice to add numbers to all equations
* I believe the claim in the appendix of [14] was meant to be conditionally independent (see also the reviews of [14]). It's clear that preactivations should not be independent and, while technically interesting, spending a page of theorem 10 and on plots seems unnecessary. Even in the paper's example preactivations are uncorrelated in the limit of large N.
* I don't see the point of having experiments in this paper. The authors have already proven the fact. Also, it is not clear how to read the plots (no axes, little description) and come to the statements from page 9.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJxmsF5kp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>significance, and independence vs. near-independence</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJej3s09Km&amp;noteId=BJxmsF5kp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper744 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper744 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank this reviewer for the valuable detailed suggestions regarding presentation.  Thanks also for pointing out the need to define q_0 before the statement of Theorem 2;  q_0 = 1.

Regarding motivation, as we pointed out to Reviewer 1, the length map studied in our paper, itself published in NIPS’16, has been repeatedly applied in a long series of papers published in NIPS, ICLR and ICML; we provide a list in the fourth paragraph of our paper.  It is therefore noteworthy that the logic supporting this length map published in [14] is not valid, and the claim made about it in that paper is incorrect.  This then motivates the question of what similar statement is correct. 

We have spoken to two of the authors of [14] about our findings, and neither of them claimed that they meant “conditionally independent” when they wrote “independent”.  While, on a first reading, this claim in their paper struck us as highly implausible, we felt that it was necessary to prove our claim that their claim was incorrect.   The experiments illustrate the strength of some of the effects analyzed in our paper.

You are correct that, as N gets large, the dependence between pairs of preactivation values becomes weaker.  But then, when analyzing the next layer, there are competing effects: as N gets larger, the dependence between individual pairs of hidden nodes is approaching zero but the number of such interferences is approaching infinity, but the improved stability obtained by averaging more cases is improving.  A rigorous analysis must take account of all of these.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>