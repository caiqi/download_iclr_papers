<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Model Comparison for Semantic Grouping | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Model Comparison for Semantic Grouping" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HJMghjA9YX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Model Comparison for Semantic Grouping" />
      <meta name="og:description" content="We introduce a probabilistic framework for quantifying the semantic similarity between two groups of embeddings. We formulate this as a model comparison task in which we contrast a generative model..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HJMghjA9YX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Model Comparison for Semantic Grouping</a> <a class="note_content_pdf" href="/pdf?id=HJMghjA9YX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019model,    &#10;title={Model Comparison for Semantic Grouping},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HJMghjA9YX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We introduce a probabilistic framework for quantifying the semantic similarity between two groups of embeddings. We formulate this as a model comparison task in which we contrast a generative model that encodes similarity between the two groups versus one that does not. We illustrate how this framework can be used for the Semantic Text Similarity (STS) task using clear assumptions about how the embeddings of words are generated. We apply information criteria based model comparison to overcome the shortcomings of Bayesian model comparison, whilst still penalising model complexity. We achieve competitive results by applying the proposed framework with an appropriate choice of likelihood on the STS datasets.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">model comparison, semantic similarity, STS, von Mises-Fisher, information theoretic criteria</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Competitive alternative to sentence embeddings in the task of semantic similarity using model comparison</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HyI7safM6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea but somewhat incomplete study</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJMghjA9YX&amp;noteId=HyI7safM6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper680 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper680 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a Bayesian model comparison based approach for quantifying the semantic similarity between two groups of embeddings (e.g., two sentences). In particular, it proposes to use the difference between the probability that the two groups are from the same model and the probability that they are from different models.

While the approach looks interesting, I have a few concerns: 
-- Using the Bayesian model comparison framework seems to be an interesting idea. However, what are the advantages compared to widely used learned models (say, a learned CNN that takes as input two sentences and outputs the similarity score)? The latter can fit the ground-truth labels given by humans, while it's unclear the model comparison leads to good correlation with human judgments. Some discussion should be provided.
-- The von Mises-Fisher Likelihood is a very simplified model of actual text data. Have you considered using other models? In particular, more sophisticated ones may lead to better performance. 
-- Different information criteria can be plugged in. Are there comparisons? 
-- The experiments are just too simple and incomplete to make reasonable conclusions. For example, it seems compared to SIF there is not much advantage even in the online setting. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1e1AGj_pm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarifications and updated results (Gaussian likelihood function) [Part 1] </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJMghjA9YX&amp;noteId=S1e1AGj_pm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper680 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper680 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We want to thank the reviewer for the suggested directions on motivating this work. We will try to address the main concerns below and will deffer the details for the next version of the manuscript. 

"The paper proposes a Bayesian model comparison based approach for quantifying the semantic similarity between two groups of embeddings (e.g., two sentences)."

We would like to clarify a point here that didn't come across clearly enough in our submission. Unlike prior work [1, 2], we are not carrying out Bayesian model comparison - we cover this approach in depth since it is the most relevant prior work to our framework. We carefully review why Bayesian model comparison may not be well suited to this application due to the Bayes Factor's sensitivity to the prior [3, 4]. In order to overcome this we propose model comparison criteria that minimise KL divergence across a candidate set of models. This results in a penalized likelihood ratio test which gives competitive results. We will further clarify this difference in the next version of the manuscript.

We have carried out additional experiments using the Bayes Factor and Bayesian Information Criteria (BIC). The results given by these approaches under-perform significantly compared to the information theoretic based criteria. We have provided empirical [5] and theoretical [3, 4] justifications as to why these two techniques are not well suited for the STS task. We will also add the experimental evidence that both the Bayes Factor under a vague prior and BIC perform poorly on the STS task.

[1] P. Marshall et al. Bayesian evidence as a tool for comparing datasets. Physical Review D, 2006.
[2] Z. Ghahramani and K. Heller. Bayesian sets. In Advances in neural information processing systems, 2006.
[3] M. Bartlett.  A comment on D. V. Lindley’s statistical paradox. Biometrika, 1957
[4] H. Akaike et al. Likelihood of a model and information criteria. Journal of econometrics, 1981
[5] J. Dziak et al. Sensitivity and specificity of information criteria. The Methodology Center and Department of Statistics, The Pennsylvania State University, 2012.

"What are the advantages compared to widely used learned models (say, a learned CNN that takes as input two sentences and outputs the similarity score)?"

A supervised approach such as the one suggested by the reviewer would definitely be an interesting research direction. It is often argued that generative models, such as the one proposed, are less susceptible to over-fitting on small training sets than discriminative models. Discriminative models are likely to fit noise in small training sets, such as the STS data set, which has in the order of thousands of labeled pairs. For this reason, common competitive approaches in the domain mainly rely on either semi-supervised or unsupervised learning procedures.

Semi-supervised approaches (used in STS) do not use human labelled similarity pairs to train on, but instead train a supervised objective on a different task with plenty of data (such as aligned paraphrases) and then use the learned representations from these as sentence embeddings. The general focus of the STS task is in the unsupervised or low-resource setting.

It may be very costly to obtain a large enough labelled dataset for some of the supervised methods to be able to generalize in domain specific applications. This gives a practical motivation for the unsupervised approaches. We will discuss these comparisons and motivations in more detail in the updated version of the manuscript. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Bkx7YziO67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarifications and updated results (Gaussian likelihood function) [Part 2]</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJMghjA9YX&amp;noteId=Bkx7YziO67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper680 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper680 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">"The latter can fit the ground-truth labels given by humans, while it's unclear the model comparison leads to good correlation with human judgments. Some discussion should be provided."

STS provides a test set in order to evaluate how the methods correlate with human scores, which we have used to benchmark our proposed models. That is, performing well on the test set suggests there's a correlation between human judgment and the model's prediction of similarity score. We will clarify this in the new manuscript.

We will discuss the relation of our approach to the one presented in Equation (9) (Tversky's contrast model) of [6] - a work that analyses what a good similarity is from a cognitive science perspective.

[6] J.B. Tenenbaum, and T.L. Griffiths, 2001. Generalization, similarity, and Bayesian inference. Behavioral and brain sciences.

"Have you considered using other models? In particular, more sophisticated ones may lead to better performance."

The motivation for this paper is to introduce a framework in which different probabilistic models can be assessed on the STS task. This is done such that a practitioner, through specifying the likelihood function, can encode suitable assumptions and constraints that may be favourable to the application of interest. The primary goal of this work is not to find the most accurate model, however we hope that this framework could be a stepping stone in using more complex and accurate generative models of text to asses semantic similarity. In the next draft of the paper we will add two different likelihoods which allow for non-unit normed vectors, unlike the vMF distribution.

"The experiments are just too simple and incomplete to make reasonable conclusions."

We are unsure if the reviewer has concerns about the STS task in particular, or the variety of experiments ran. 

To address the former, we provide our argument for why we think STS is an adequate task to assess performance on. Our focus is on the setting where one has word level embeddings that contain semantic information about individual words, but no labelled corpus with examples of similar and dissimilar pairs (an unsupervised setting). Furthermore, we assume sentences arrive in an 'online' fashion meaning that we don't have access to the whole sentence corpus a priori. An example use-case like this is a chat-bot application.

To address the latter, we will extend our experiments by considering other word vectors usually used to assess performance on STS such as fasttext and word2vec. We will also include experimental results using other likelihoods and information criteria within our framework. Below we provide a preliminary set of results using a Gaussian likelihood with a diagonal covariance matrix.

+--------------+-------------+-----------+----------+----------+----------+----------+----------+
|                  |   Method | STS12  | STS13  | STS14  | STS15  | STS16| W. A.* |
+--------------+-------------+-----------+----------+----------+----------+----------+----------+
| fasttext   | Ours        | 0.6181 | 0.6630 | 0.6841 | 0.7476 | 0.7104| 0.6828|
|                  | SIF+PCA  | 0.6040 | 0.7229 | 0.6954 | 0.7566 | 0.7075| 0.6916|
+--------------+-------------+-----------+----------+----------+----------+----------+----------+
| glove       | Ours        | 0.6154 | 0.6676 | 0.6855 | 0.7478 | 0.7102| 0.6831|
|                  | SIF+PCA  | 0.5871 | 0.7029 | 0.6846 | 0.7288 | 0.6875| 0.6728|
+--------------+-------------+-----------+----------+----------+----------+----------+----------+
| word2vec| Ours       | 0.5820 | 0.6211 | 0.6556 | 0.7201 | 0.6530| 0.6480|
|                  | SIF+PCA  | 0.5611 | 0.6710 | 0.6686 | 0.7109 | 0.6671| 0.6512|
+--------------+-------------+-----------+----------+----------+----------+----------+----------+
* W.A. stands for weighted average

Would experiments along these lines address the simplicity concern of the reviewer?</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJlFVJbo3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting model, but would like to see some more motivation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJMghjA9YX&amp;noteId=rJlFVJbo3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper680 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper680 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose a probabilistic model for computing the sentence similarity between two sets of representations in an online fashion (that is, they do not need to see the entire dataset at once as SIF does when using PCA). They evaluate on the STS tasks and outperform competitive baselines like WMD, averaging embeddings, and SIF (without PCA), but they have worse performance that SIF + PCA.

The paper is clearly written and their model is carefully laid out along with their derivation. My concern with this paper however, is that I feel the paper lacks a motivation, was it derive an online similarity metric that outperforms SIF(without PCA)?

A few experimental questions/comments:

What happens to all methods when stop words are not removed? How far does performance fall? I think one reason it might fall (in addition to the reasons given in the paper) is that all vectors are set to have the same norm. For STS tasks, often the norms of these vectors are reduced during training which lessens their influence. What mechanism was used to identify the stop words and does removing these help the other methods (I know in the paper, stop words were removed in the baseline, did this unilaterally improve performance for these methods)?

Overall I do like the paper, however I do find the results to be lackluster. There are many papers on combining word embeddings trained in various ways that have much stronger numbers on STS, but these methods won't be effective with this type of similarity (namely because embeddings must have unit norm in their model). Therefore, I think the paper needs some more motivation and experimental evidence of its superiority over related methods like SIF+PCA in order for it to be accepted.

PROS
- Probabilistic model with clear design assumptions from which a similarity metric can be derived.
- Derived similarity metric doesn't require knowledge of the entire dataset (in comparison to SIF + PCA)

CONS
- Performance seems to be slightly better than SIF, WMD, and averaging word embeddings, but below that of SIF + PCA 
- Unclear motivation for the model, was it derive an online similarity metric that outperforms SIF(without PCA)?
- Requires the removal of stop words, but doesn't state how these were defined. Minor point, but tuning this could be enough to cause the improvement over related methods.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJeTGbYup7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarifications and updated results (Gaussian likelihood function)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJMghjA9YX&amp;noteId=SJeTGbYup7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper680 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper680 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank the reviewer for their in-depth feedback. Below, we present preliminary results, as well as clarifications on the conceptual questions that were posed.

"My concern with this paper however, is that I feel the paper lacks a motivation..."

The main focus of this paper is the introduction of a framework that allows for clear assumptions to be made about the distribution of word vectors in a sentence via a choice of likelihood and for these assumptions to be tested on the STS benchmark - any likelihood will fit into this general framework and produce a similarity measure. This allows practitioners to design likelihoods that encode suitable properties for their application.

The more practical motivation for this paper is that the online* setting is key for many real-world use-cases such as information retrieval for dialogue systems (i.e. chatbots) where new queries will arrive in an online fashion and methods like SIF+PCA will not be as applicable as they are in the STS task. Whilst the method is derived as an online method it can be used in applications that have offline components, and we have shown it remains competitive to offline methods such as SIF+PCA (see the next section of this response).

*We thank the reviewer for helpfully clarifying the definitions of an "online" and "offline" setting; we will include this definition in the next version of the manuscript.

"...namely because embeddings must have unit norm in their model."

The reviewer has helpfully pointed out an implicit assumption that we made - namely, we assumed that the magnitude of word embedding was noise rather than useful information. To test this assumption, we are running experiments with a multivariate Gaussian likelihood with diagonal covariance. This does not require unit norming the vectors and a set of preliminary results are presented below.

+--------------+-------------+-----------+----------+----------+----------+----------+----------+
|                  |   Method | STS12  | STS13  | STS14  | STS15  | STS16| W. A.* |
+--------------+-------------+-----------+----------+----------+----------+----------+----------+
| fasttext   | Ours        | 0.6181 | 0.6630 | 0.6841 | 0.7476 | 0.7104| 0.6828|
|                  | SIF+PCA  | 0.6040 | 0.7229 | 0.6954 | 0.7566 | 0.7075| 0.6916|
+--------------+-------------+-----------+----------+----------+----------+----------+----------+
| glove       | Ours        | 0.6154 | 0.6676 | 0.6855 | 0.7478 | 0.7102| 0.6831|
|                  | SIF+PCA  | 0.5871 | 0.7029 | 0.6846 | 0.7288 | 0.6875| 0.6728|
+--------------+-------------+-----------+----------+----------+----------+----------+----------+
| word2vec| Ours       | 0.5820 | 0.6211 | 0.6556 | 0.7201 | 0.6530| 0.6480|
|                  | SIF+PCA  | 0.5611 | 0.6710 | 0.6686 | 0.7109 | 0.6671| 0.6512|
+--------------+-------------+-----------+----------+----------+----------+----------+----------+
* W.A. stands for weighted average

"...I do find the results to be lackluster."

As we can see, the Gaussian distribution seems to be a better fit than the vMF one, matching SIF+PCA on the three word embeddings we tested on. We hope this addresses the concern of the reviewer that methods which depend on embedding magnitude won't be applicable with this framework. We will include a more thorough set of results in the next version of the manuscript.

"What mechanism was used to identify the stop words and does removing these help the other methods..."

We chose NLTK's standard set of stopwords in order to avoid over-fitting by selecting favourable ones - ``There is no universal list of stopwords in NLP research, however the NLTK module contains a list of stop words." [1]. We will explicitly state the stopwords used in the appendix of the paper in the next version. Popular models for text generation in NLP tend to remove a standard list of stopwords - as an example Latent Dirchlet Allocation [2]. The NLTK library which is a standard library for NLP recommends stopword removal for tasks involving semantic meaning and similarity.

[1] <a href="https://pythonspot.com/nltk-stop-words/" target="_blank" rel="nofollow">https://pythonspot.com/nltk-stop-words/</a>
[2] D. Blei et al. Latent Dirichlet Allocation. JMLR

"What happens to all methods when stop words are not removed?"

All methods experience a drastic increase in performance when stop-words are removed. Below we present a table for SIF+PCA (using GloVe vectors), with the rest of the methods being deferred to the next version of the manuscript.

+---------------------+-----------+-----------+----------+----------+----------+----------+
|    Method          | STS12  | STS13  | STS14  | STS15  | STS16  | W. A.  |
+---------------------+-----------+-----------+-----------+-----------+--------+----------+
| w/o stopwords| 0.5871 | 0.7029 | 0.6846 | 0.7288 | 0.6875 | 0.6728|
| w stopwords    | 0.5681 | 0.6844 | 0.6546 | 0.7166 | 0.6931 | 0.6552|
+---------------------+-----------+-----------+----------+----------+----------+----------+</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1gnfMKc3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting paper but lacking both context and comprehensive analyses</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJMghjA9YX&amp;noteId=S1gnfMKc3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper680 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper680 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Main contribution: devising and evaluating a theoretically-sound algorithm for quantifying the semantic similarity between two pieces of text (e.g., two sentences), given pre-trained word embeddings (glove).

Clarity:
The paper is generally well-written, but I would have liked to see more details regarding the motivation for the work, description of the prior work and discussion of the results. As an example, I could not understand what were the differences between the online and offline settings, with only a reference to the (Arora et al. 2016) paper that does not contain neither "online" nor "offline". The mathematical derivations are detailed, which is nice.

Originality:
The work looks original. It proposes a method for quantifying semantic similarity that does not rely on cosine similarity.

Significance:
I should start by saying I am not a great reviewer for this paper. I am not familiar with the STS dataset and don't have the mathematical background to fully understand the author's algorithm.
I like to see theoretical work in a field that desperately needs some, but overall I feel the paper could do a much better job at explaining the motivation behind the work, which is limited to "cosine similarity [...] is not backed by a solid theoretical foundation".
I am not convinced of the practicality of the algorithm either: the algorithm seems to improve slightly over the compared approaches (and it is unclear if the differences are significant), and only in some settings. The approach needs to remove stop-words, which is reminiscent of good old feature engineering. Finally, the paper claims better average time complexity than some other methods, but discussing whether the algorithm is faster for common ranges of d (the word embedding dimension) would also have been interesting.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">1: The reviewer's evaluation is an educated guess</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1eamKOupm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarifications and updated results (Gaussian likelihood function)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJMghjA9YX&amp;noteId=r1eamKOupm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper680 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper680 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank the reviewer for their comments. We have tried to address their concerns below.

"I could not understand what were the differences between the online and offline settings..."

We apologise for the lack of definition of these terms in the paper. This will be remedied in the next version. The difference between an online and offline setting is whether one has access to the entire dataset on which the methods will be evaluated at once. Information retrieval is an example setting in which one cannot perform the PCA on the query dataset as seen in (Arora et al. 2017), since new queries will arrive in an online fashion.

"...the paper could do a much better job at explaining the motivation behind the work..." 

Similarity measures are often not theoretically justified - for example, cosine similarity is preferred to dot product similarity based purely on empirical results. It is difficult for a practitioner to utilize word vectors efficiently if the underlying assumptions in the similarity measure are not well understood. Our framework addresses these issues by explicitly deriving the similarity through the likelihood of the chosen generative process, instead of empirically motivating the similarity measure. Via designing the likelihood the practitioner can encode suitable assumptions and constraints that may be favourable to the application of interest. Furthermore, this framework proposes a new research direction that could help understand semantic similarity, in which practitioners can study suitable distributions and see how these perform. We will elaborate on this further in the updated version of the manuscript.

The second motivation of this work is to derive a simple but competitive similarity measure that would perform well in online settings (as defined above). Online settings are both practical and key to use-cases that involve information retrieval in dialogue systems. For example, in a chat-bot application new queries will arrive in an online fashion and methods such as SIF+PCA will not perform as strongly as they do on STS. This is because the method itself (in this case the PCA part) was fitted on the reported test set, which will not be available a priori in online settings.

"I am not convinced of the practicality of the algorithm..." 

We were unsure of what the reviewer meant by practicality of the algorithm. The presented algorithm requires no more than 30 lines of code to implement once the derivatives for the chosen likelihood have been calculated. Furthermore, the derivatives can be computed automatically with frameworks such as autograd, TensorFlow, PyTorch, and others.

Below we provide the results when using a multivariate Gaussian distribution with diagonal covariance as a likelihood.

+--------------+---------------+-----------+-----------+----------+----------+----------+----------+
|                  |   Method   | STS12   | STS13  | STS14  | STS15  | STS16  | W. A.*|
+--------------+---------------+-----------+-----------+----------+----------+----------+----------+
| fasttext    | Ours          | 0.6181  | 0.6630 | 0.6841 | 0.7476 | 0.7104 | 0.6828|
|                  | SIF + PCA   | 0.6040 | 0.7229 | 0.6954 | 0.7566 | 0.7075 | 0.6916|
+--------------+---------------+-----------+-----------+----------+----------+----------+----------+
| glove       | Ours           | 0.6154 | 0.6676 | 0.6855 | 0.7478 | 0.7102 | 0.6831|
|                  | SIF + PCA   | 0.5871 | 0.7029 | 0.6846 | 0.7288 | 0.6875 | 0.6728|
+--------------+---------------+-----------+-----------+----------+----------+----------+----------+
| word2vec| Ours          | 0.5820  | 0.6211 | 0.6556 | 0.7201 | 0.6530 | 0.6480|
|                  | SIF + PCA  | 0.5611  | 0.6710 | 0.6686 | 0.7109 | 0.6671 | 0.6512|
+--------------+---------------+-----------+-----------+----------+----------+----------+----------+
* W. A. stands for weighted average

As can be seen from the table, the results for this likelihood are on par with the ones presented in (Arora et al. 2017).

"The approach needs to remove stop-words, which is reminiscent of good old feature engineering."

Popular models for text generation in NLP tend to remove a standard list of stopwords, for example Latent Dirchlet Allocation [1]. The NLTK library which is a standard library for NLP recommends stopword removal for tasks involving semantic meaning and similarity. That being said the aforementioned Gaussian likelihood model (using the AIC) seems more robust to stopword removal and is on par with SIF+PCA with or without them.

[1] D. Blei et al. Latent Dirichlet Allocation. Journal of Machine Learning Research

"...discussing whether the algorithm is faster for common ranges of d (the word embedding dimension)..."

We appreciate the suggestion of grounding the complexity analysis with values for N and D in the ranges experienced in the STS dataset. We will provide an analysis with the next version of the manuscript.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>