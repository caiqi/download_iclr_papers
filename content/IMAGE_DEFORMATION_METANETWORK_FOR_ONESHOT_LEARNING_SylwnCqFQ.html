<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>IMAGE DEFORMATION META-NETWORK FOR ONE-SHOT LEARNING | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="IMAGE DEFORMATION META-NETWORK FOR ONE-SHOT LEARNING" />
        <meta name="citation_author" content="Zitian Chen" />
        <meta name="citation_author" content="Yanwei Fu" />
        <meta name="citation_author" content="Yu-Xiong Wang" />
        <meta name="citation_author" content="Lin Ma" />
        <meta name="citation_author" content="Wei Liu" />
        <meta name="citation_author" content="Martial Hebert" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Sylw7nCqFQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="IMAGE DEFORMATION META-NETWORK FOR ONE-SHOT LEARNING" />
      <meta name="og:description" content="Humans can robustly learn novel visual concepts even when images undergo various deformations and loose certain information. Incorporating this ability to synthesize deformed instances of new..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Sylw7nCqFQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>IMAGE DEFORMATION META-NETWORK FOR ONE-SHOT LEARNING</a> <a class="note_content_pdf" href="/pdf?id=Sylw7nCqFQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=tankche2%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="tankche2@gmail.com">Zitian Chen</a>, <a href="/profile?email=yanweifu%40fudan.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="yanweifu@fudan.edu.cn">Yanwei Fu</a>, <a href="/profile?email=yuxiongw%40cs.cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="yuxiongw@cs.cmu.edu">Yu-Xiong Wang</a>, <a href="/profile?email=forest.linma%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="forest.linma@gmail.com">Lin Ma</a>, <a href="/profile?email=wl2223%40columbia.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="wl2223@columbia.edu">Wei Liu</a>, <a href="/profile?email=hebert%40ri.cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="hebert@ri.cmu.edu">Martial Hebert</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Humans can robustly learn novel visual concepts even when images undergo various deformations and loose certain information. Incorporating this ability to synthesize deformed instances of new concepts might help visual recognition systems perform better one-shot learning, i.e., learning concepts from one or few examples. Our key insight is that, while the deformed images might not be visually realistic, they still maintain critical semantic information and contribute significantly in formulating classifier decision boundaries. Inspired by the recent progress on meta-learning, we combine a meta-learner with an image deformation network that produces additional training examples, and optimize both models in an endto- end manner. The deformation network learns to synthesize images by fusing a pair of images—a probe image that keeps the visual content and a gallery image that diversifies the deformations. We demonstrate results on the widely used oneshot learning benchmarks (miniImageNet and ImageNet 1K challenge datasets), which significantly outperform the previous state-of-the-art approaches.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SJgGrJWa6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Sylw7nCqFQ&amp;noteId=SJgGrJWa6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1367 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1367 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Ske8wJak0X" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Sylw7nCqFQ&amp;noteId=Ske8wJak0X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper1367 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SkeCAtM327" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Strong results on some benchmarks but novelty and justification for method lacking</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Sylw7nCqFQ&amp;noteId=SkeCAtM327"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1367 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1367 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper addresses the problem of few-shot learning, specifically by incorporating a mechanism for generating additional training examples to supplement the given small number of true training examples in an episode. The example generation method takes as input a support set image and a gallery image (which can be any random image) and produces a new synthesized support example that is a linear weighted combination of the two input images (in terms of pixel patches) and is assumed to belong to the same class as the support set example input. The synthesis process is parameterized as an “image deformation network” whose weights must be learned. Additionally, there is an embedding network that takes as input the full support set (with true and synthesized examples) and is used in a prototypical-net fashion (Snell 2017) to classify the given query set. A set number of gallery images are created from the training base classes and this is the fixed set of gallery images that is used for both training and testing. The image deformation network and embedding network weights are trained end-to-end using the standard meta-learning loss on the query set given the support set. Experiments are conducted on the ImageNet-1K Challenge and Mini-Imagenet benchmarks.

Pros:
- Sizable improvement on ImageNet-1K benchmark relative to previous work.
- Detailed ablation study is done to display the benefits of choices made in model.

Cons:
- Idea is not that novel relative to all the recent work on learning to supplement training data for few-shot learning (Hariharan 2017, Wang 2018, Gao 2018, and Schwartz 2018). Additionally, it is not made clear why we expect the idea proposed in this paper to be better relative to previous ideas. For example, Delta-Encoder (Schwartz 2018) also supplements training data by working in image-space rather than in feature-space (as done in the other previous work mentioned). One big difference seems to be that for the Delta-Encoder work, the image generation process is not trained jointly with the classifier but this does not seem to have a big impact when we compare the performance between Delta-Encoder and proposed model on Mini-Imagenet benchmark.
- Benefit in Mini-Imagenet benchmark is not very large. Compared to Delta-Encoder (Schwartz 2018), the proposed model does worse for 1-shot case and intersects the confidence interval for 5-shot case.

Remarks:
- Could add citation for NIPS 2018 paper 'Low-shot Learning via Covariance-Preserving
Adversarial Augmentation Networks' (<a href="https://arxiv.org/pdf/1810.11730.pdf)" target="_blank" rel="nofollow">https://arxiv.org/pdf/1810.11730.pdf)</a>
- Delta-Encoder (Schwartz 2018) paper should be mentioned in related work and contrasted against.
- How many supplemented examples are used per class? I believe it is n_aug=8 but this could be stated more clearly.
Additionally, how many supplemented examples are used in the previous work that are being compared to? It would be useful to have this information and to make sure the comparison is consistent in that the number of supplemented examples are comparable across different models that synthesize examples.
- In algorithm 1, it seems that only CELoss is used to update parameters of embedding network but on page 6, it says "we found that using additional cross-entropy loss speeds up convergence and improves recognition performance than using prototypical loss solely"?
- At many points in the paper, 'prob image' is used instead of 'probe image'.
- Page 10: 'Our IDME-Net has different react...' =&gt; 'Our IDME-Net has different output...'
- Page 10: '...why our model work' =&gt; '...why our model works'

Gao et al. Low-shot Learning via Covariance-Preserving Adversarial Augmentation Networks. 2018.
Hariharan et al. Low-shot Visual Recognition by Shrinking and Hallucinating Features. 2017.
Schwartz et al. Delta-Encoder: an effective sample synthesis method for few-shot object recognition. 2018.
Snell et al. Prototypical Networks for Few-Shot Learning. 2017.
Wang et al. Low-Shot Learning from Imaginary Data. 2018.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJg7ZETFam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Difference and Comparison</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Sylw7nCqFQ&amp;noteId=BJg7ZETFam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1367 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018 (modified: 15 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1367 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Q1: Difference with Delta-Encoder and other works.
A1: Reviewer #1 states that “Delta-Encoder (Schwartz 2018) also supplements training data by working in image-space rather than in feature-space. ” However, “In all the experiments, images are represented by pre-computed feature vectors.” is clarified in Sec 3.1 in Delta-Encoder. In addition, Low-shot Learning via Covariance-Preserving Adversarial Augmentation Networks also works in feature space. There are three major differences: 1. other works “synthesize images in the feature domain while we can directly produce images in the image domain.” 2. As AnonReviewer1 kindly commented “One big difference seems to be that for the Delta-Encoder work, the image generation process is not trained jointly with the classifier”, we learn to deform images in a meta-learning formula to maximize one-shot recognition accuracy. 3.”our IDeMe-Net concentrates on learning to use the
complementarity and interaction among visual patches.” In contrast, other methods could not explore the relation between patches.

Q2: Compare the performance between Delta-Encoder and proposed models on miniImagenet benchmark.
A2: First, we want to highlight that we have a strong result on ImageNet1K as Reviewer#1 kindly commented. ImageNet1K is 5 times bigger than miniImageNet, so the result on ImageNet1K is more reliable and hard to overfit. Second, as you can see in Tab.4, our IDeMe-Net is more than 5% higher than our baseline(Prototype Classifier). Thus, it not only shows our efficiency but also proves that the performance of our baseline is the main cause(delta-encoder use Linear Classifier). 
Q3: Add citations for NIPS 2018 papers.
A3: Great suggestion! We will add citations in future versions. But when we submit our paper(September 27th), some NIPS papers are not released on arxiv and we only know the papers’ name. 

Q4: How many supplemented examples are used per class? How many supplemented examples are used in the previous works that are being compared to? 
A4: Yes, it is n_aug=8 as we mentioned that “n_aug is cross-validated as 8” in Sec6.1. So we have n*8 supplemented examples for each class in an n-shot setting. We show ablation study of Augment Number in Fig4.d. We will add comparison about the Augment Number to the previous works. 

Q5: CELoss and prototypical Loss.
A5: Sorry for the confusion. We will correct that sentence.

Q6: Typos.
A6: Thanks! We will fix these typos.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SJxKBigqhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice paper with exhaustive experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Sylw7nCqFQ&amp;noteId=SJxKBigqhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1367 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1367 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper is very well-written and is a nice read. 

The paper presents 1) a new approach of one-shot learning by training meta learning based deformation networks, 2) end-to-end meta learning framework and 3) a new way to synthesize images which boosts the performance a lot. It's a very well written paper with a exhaustive experimentation on various axis: data augmentation strategies (gaussian noise, flipping), comparison with competitive baselines in one-shot learning literature like matching network (Vinyals et al. 2016), ablations on the design of deformation network showcasing what makes this network work well. The ablations offer explanation behind the design choices. The papers also explains the network design and their approach very succinctly.

The experimentation section is very well done and high-quality. I also appreciated that the experiment setup used in the paper is the standard setup that other SOTA baselines have used which makes it easy to compare the results and contributions of this paper and also keeps the evaluation strategy consistent in literature. Overall the approach demonstrates great results on 5-shot classification learning over previous approaches.

I have following questions to the authors:
1. what is the training time for end-to-end model compared to previous SOTA baselines?
2. Can authors link to the design of ResNet-10 model for the sake of clarity even though it might feel obvious?
3. in Section 6.1 Setup section, authors set epsilon value to 2 for selecting gallery images. Can authors show ablation on epsilon values and why 2 works the best?
4. Can authors provide intuition into why the strategy proposed doesn't work as well on 1-shot classification? 

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">1: The reviewer's evaluation is an educated guess</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rke1MSpYT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Sylw7nCqFQ&amp;noteId=rke1MSpYT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1367 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018 (modified: 15 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1367 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Q1: What is the training time for end-to-end model compared to previous SOTA baselines?
A1: In practice, our training time is 4 times of prototypical network’s training time. 

Q2: ResNet-10 model.
A2: We use the same model as Hariharan et al. Low-shot Visual Recognition by Shrinking and Hallucinating Features. 2017. They have released the codes and models on <a href="https://github.com/facebookresearch/low-shot-shrink-hallucinate" target="_blank" rel="nofollow">https://github.com/facebookresearch/low-shot-shrink-hallucinate</a> .

Q3: Ablation on epsilon values and why 2 works the best?
A3: Great suggestion! We will add ablation study of epsilon values. On 1-shot miniImagenet, when epsilon=1,1.5,2,2.5,3,5,10, the performances are 57.65,57.67,57.71,57.53, 57.24, 56.59, 56.23 respectively.
The performance will increase when the epsilon values decrease. But there is a limited number of examples in Gallery so if epsilon values are too low we would not have enough gallery images.  

Q4: Can authors provide intuition into why the strategy proposed doesn't work as well on 1-shot classification? 
A4:  First, we want to highlight that we have a strong result on ImageNet1K. ImageNet1K is 5 times bigger than miniImageNet, so the result on ImageNet1K is more reliable and hard to overfit. We only slightly underperform Delta-Encoder on miniImagenet. Second, as you can see in Tab.4, our IDeMe-Net is more than 5% higher than our baseline(Prototype Classifier). Thus, it not only shows our efficiency but also proves that the performance of our baseline is the main cause(delta-encoder use Linear Classifier and generates 5*1024 new images). 


</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkx3tnTd2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting approach for one-shot learning in images</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Sylw7nCqFQ&amp;noteId=rkx3tnTd2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1367 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1367 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper is about a novel framework to address one-shot learning task by augmenting the support set with automatically generated images. The framework is composed by two networks: a deformation network which is responsible to generate synthetic images and an embedding networks which projects images into an embedding space used to perform the final classification. Compared to previous work, the idea is similar in spirit to recent work that synthesise new images (Wang et. al. CVPR 2018) with GANs, but exploit a simpler idea based on the fusion of two images directly into the image domain with weights generated by a network. Extensive experiments on the standard ImageNet1K and miniImageNet datasets are reported, with a comprehensive comparison of the state of the art and several ablation studies. The proposed method achieves better performance than the state of the art.

Strengths:
+ The paper is almost well written, with comprehensive related works and it is easy to read except for few details (see below).
+ The method is simple and is extensively studied. I appreciated the extensive ablation study and the discussion that follows it.

Weaknesses:
- The approach has some novelty in the method of generating new images and in the framework itself. Beside that, the idea of generating images was introduced previously (e.g. Wang et al. 2018) and the embedding network is a standard one-shot embedding with prototypical loss (Snell et al. 2017).
- The clarity on few details can confuse the reader and needs improvement on the use of the gallery.
- The comparison with the state of the art may be unfair due to the addition of the gallery images, which are proved in the ablation studies to be essential to the better performance of the proposed method. 

In particular:
- It is not clear to me how the gallery images are exactly used. Since the ablation study (sect 6.2) reported in (2) that the performance is worse using images from the support set and (3) the improved performance comes from the diversified images outside the support set, it may be that just the addition of the gallery images is the reason to have better performance. Also it is not clear to me if they are fixed all the time in advance during the meta training, if they change. It would be interesting to see an experiment where the gallery is available in the support set of the compared state of the art works and the baselines, and see if the performance is improved as well.

- Regarding the clarity, beside the use of the gallery, several small issues should be improved:
  * The paper use the term deformed images (until sect ~4) and then synthetic images for the generated images; 
  * Figure 2 is introduced early but explained late in the paper and it is not clear how the second and the third images should be interpreted; 
  * In the related work section, Wang et al. 2018 is said to generate imaginary images in contrast to realistic images of the proposed method not synthetic, however the images are clearly not real and are synthetically generated from the heuristic and the weight of the network. Moreover, the paper uses the term synthetic images later, so the comment is misleading.   

Minor issue:
- In the abstract, loose -&gt; lose

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HyxtYrpYTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Difference with prior works and other concerns</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Sylw7nCqFQ&amp;noteId=HyxtYrpYTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1367 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1367 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Q1: The idea of generating images was introduced previously (e.g. Wang et al. 2018).
A1: Even though there are previous works focusing on data augmentation, our work is quite different from their works. There are three major differences: 1. other works “synthesize images in the feature domain while we can directly produce images in the image domain.” 2. As AnonReviewer1 kindly commented “One big difference seems to be that for the Delta-Encoder work, the image generation process is not trained jointly with the classifier”, we learn to deform images in a meta-learning formula to maximize one-shot recognition accuracy. 3.”our IDeMe-Net concentrates on learning to use the complementarity and interaction among visual patches.” In contrast, other methods could not explore the relation between patches.


Q2:  The embedding network is a standard one-shot embedding with prototypical loss (Snell et al. 2017)
A2: We use prototypical classifier because we focus on our Deformation Network, not the Embedding Network. We want to make a fair comparison to prototypical network to prove that the performance gain comes from the efficient synthesized images. Also, our Deformation Network can combine with any other one-shot classifier (e.g. Matching Network). By replaced Prototypical Network with Matching Network, we reach 52.1,61.4, 69.6, 73.2, 75.0 top-5 accuracy on ImageNet1K when n = 1,2,5,10,20. 

Q3: The comparison with the state of the art may be unfair due to the addition of the gallery images, which are proved in the ablation studies to be essential to the better performance of the proposed method. 
A3: We want to highlight that we do not use any additional data compared to other methods. The gallery images are sampled from base classes and other methods train their network on the base classes. 

Q3: The gallery images is the reason to have better performance? An experiment when other methods have the gallery images?
A3: As discussed in Sec.6.2, when we use gallery images( Gallery Baseline) to replace synthesize images, it “perform worse than the prototypical classifier baseline, indicating the importance of meta-learning a deformation
strategy”.  When we provide gallery images to Prototypical Network, its performance drops 2% on miniImagenet 1-shot. 

Q4: Is gallery fixed all the time in advance during the meta training.
A4: Sorry for the confusion and we will make it clearer. In each training episode, we resample the Gallery.

Q5: Several small issues.
A5: Thanks! We will correct these issues.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>