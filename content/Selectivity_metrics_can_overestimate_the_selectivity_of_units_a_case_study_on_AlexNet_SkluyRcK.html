<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Selectivity metrics can overestimate the selectivity of units: a case study on AlexNet | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Selectivity metrics can overestimate the selectivity of units: a case study on AlexNet" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Skluy2RcK7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Selectivity metrics can overestimate the selectivity of units: a..." />
      <meta name="og:description" content="Various methods of measuring unit selectivity have been developed in order to understand the representations learned by neural networks.  Here we undertake a comparison of four such measures on the..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Skluy2RcK7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Selectivity metrics can overestimate the selectivity of units: a case study on AlexNet</a> <a class="note_content_pdf" href="/pdf?id=Skluy2RcK7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019selectivity,    &#10;title={Selectivity metrics can overestimate the selectivity of units: a case study on AlexNet},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Skluy2RcK7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Various methods of measuring unit selectivity have been developed in order to understand the representations learned by neural networks.  Here we undertake a comparison of four such measures on the well studied network AlexNet. In contrast to work on recurrent neural networks (RNNs), we fail to find any 100\% selective `localist units' in the hidden layers of AlexNet, and demonstrate that previous assessments of selectivity suggest a higher level of selectivity than is warranted, with the most selective units only responding most strongly to a small minority of images from within a category. No difference in selectivity was found between layers \layer{fc6} and \layer{fc7}, and \layer{fc8} was much more selective. Only the output \layer{prob} layer contained any localist units. We also  generated images that maximally activated individual units and found that under (5\%) of units in \layer{fc6} and \layer{conv5} produced images of interpretable objects that humans consistently labeled, whereas \layer{fc8} produced over 50\% interpretable images. We consider why different degrees of selectivity are observed with RNNs and AlexNet, and suggest visualizing activations with jitterplots, aside from being comparable to neuroscience techniques, are a good first step to assessing unit selectivity.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">AlexNet, neural networks, selectivity, localist, distributed, represenataion, precision, measures of selectivity, object detectors, single directions, network analysis</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Common selectivity metrics overestimate the selectivity of units, true object detectors and localist codes are extremely rare, but class selectivity does increase with depth. </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rkej1sdjhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Shows that single units are not perfectly class selective, a result that will be intuitive to many</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skluy2RcK7&amp;noteId=rkej1sdjhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1001 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1001 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: 

This paper explores different metrics to measure the ‘selectivity’ of single neurons for a class in deep neural networks. Using AlexNet as the model under study, the paper shows strengths and weaknesses of several recent methods in the literature. The paper conducts a psychophysics experiment to see if human subjects can reliably label images generated through activation maximization techniques. 

Major comments:

This paper undertakes a careful analysis of different ways of measuring single-unit selectivity for a class. The conclusions drawn are that no neurons exhibit true localist selectivity, and most have some more complex selectivity. Some of the specific examples make this point very nicely (for instance, a unit that responds very strongly to several custard apples and would appear to be a custard apple detector, except that it responds extremely weakly to other custard apples). This is a somewhat negative result that may be useful in advancing the field away from single neuron analyses, which may be misleading.

One worry is that the methods applied are looking for a very strong form of selectivity. In particular, even the output layer is judged to contain a low percentage of selective units according to the definitions in the paper. It may be worth considering slightly weakened versions of the metrics that allow for some errors. 

It would be useful to add discussion of the connections between these metrics and generalization performance. The class conditional selectivity metric, for instance, may not measure localist coding very directly, but it does correlate with important performance metrics like generalization performance. The discussion in Morcos 2018 suggests that high single unit selectivity is detrimental to generalization. Do these correlations persist using other metrics?

The psychophysics experiment with human subjects appears to have been done to a high standard, and yields the result that only the very highest layers of a network yield interpretable images. This is somewhat interesting but unlikely to be that surprising, as selectivity for objects in lower layers is not a claim made by many works. In these lower layers, selectivity for ‘object parts’ is a claim that has been made and could potentially be addressed by the data collected.

Overall this paper critically analyzes single unit selectivity measures, reaching the conclusion that tuning in modern deep networks is usually far more complex than strict localist coding. The significance of this conclusion may not be so high given that this conclusion is probably already the intuition of many.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1gQqFQq3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Surprising result and raises interesting questions.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skluy2RcK7&amp;noteId=B1gQqFQq3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1001 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1001 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary - This paper analyzes the selectivity of individual units in CNNs. The authors analyze existing techniques such as precision selectivity, class-conditional mean activity selection and localist sensitivity. These methods are analyzed in the context of AlexNet and ImageNet. The authors also use Activation Maximization (AM) techniques for visualizing single-unit representations in CNNs.


Paper strengths
- The authors have minutely examined each of the metrics and the underlying assumptions they make. 
    - Example - Number of images used for computing the precision threshold in Zhou et al., 2014; The wrongly stated range of CCMAS [0, 1]. Considering the second highest CCMAS class is a good way of handling multiple classes that activate a single unit.
- The results of this paper are surprising compared to existing work. The authors have made a surprising discovery and done a good job of both presenting it well and experimentally validating it. The paper raises interesting questions and this should inspire future work in understanding networks.
- Figure 2 is insightful - It compares the various different interpretations of selectivity for a single unit in fc6. It shows how the mean activating class and the maximally activating class can be semantically very different. It also shows that despite the high precision and CCMAS score, the unit cannot be labelled as a detector for the single concept "custard apple". More such results are presented in the Appendix (e.g. Fig A6)
- The human study in Section 3.3 is a good way to evaluate the generated AM images.


Paper weaknesses
- One of the major weaknesses of this paper is that it uses only ImageNet images to evaluate the units. As this is limited to 1000 classes, the authors cannot probe other visual concepts such as color, texture, materials for the units. As an example, Network dissection (Zhou et al., 2017) proposes a dataset called Broden which has many diverse sets of visual concepts labeled. This paper focuses only on one definition of selectivity - selecting objects. This should be made explicit and the authors have not done a good job of clarifying this assumption or showing that it exists.
- All of the analysis is limited to AlexNet. With modern architectures that use residual/skip connections, it is not clear how well this analysis will generalize. It is an open question if the authors work overfits to AlexNet.
- The jitterplots are hard to understand especially if there are many overlapping "dots" (samples). Since the y-axis values are not really meaningful anyway, using a histogram to see how many samples have a particular activation value is easier. A possible suggestion for Figure 2(a): split into two parts - 1) histogram of all samples; 2) histogram of the highest mean activating class.
- The organization of the paper could be improved. The sections in the paper are not well connected.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkgdcTkchX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Poorly written with unclear contributions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skluy2RcK7&amp;noteId=rkgdcTkchX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1001 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1001 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This is a paper with scattered potentially interesting ideas. But the execution is limited and the writing poor with critical details lacking.  A major limitation of the paper is that it is not clear what contribution it makes. Some of the analyses are indeed interesting but 1) these analyses are mostly descriptive and 2) they are limited to one particular (outdated) architecture. How would batch norm or residual connections or any of the developments that have happened since AlexNet affect these results?

As a side note, the references/comparisons between AlexNet and recurrent nets (see abstract, etc) are misleading. This is based on the claim that Bowers et al (2014) qualitatively different results but this is for entirely different domains (words). Indeed what could have made potentially the work more relevant would have been to show some kind of benchmarking between AlexNet and alternative architectures (possibly RNNs). As such the current study does not contribute much except for comparing different semi-arbitrary measures of selectivity for one specific (outdated) network architecture trained on a particular problem (ILSVRC).

****
Minor points:

The study is limited to correctly classified images as stated on page 3. This seems like a major confound in a study aimed at understanding the visual representations learned. It seems to me that the conclusions of the paper could be heavily biased because of this (when computing any measure based on inter and intraclass responses).

In general, this is a relatively poorly written paper which would be hard to reproduce. For instance, the image generation for activating units (assuming it is novel) could be interesting but it is not even described with sufficient details so as to reproduce the results.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>