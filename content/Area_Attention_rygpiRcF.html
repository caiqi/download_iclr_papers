<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Area Attention | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Area Attention" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rygp3iRcF7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Area Attention" />
      <meta name="og:description" content="Existing attention mechanisms, are mostly point-based in that a model is designed to attend to a single item in a collection of items (the memory). Intuitively, an area in the memory that may..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rygp3iRcF7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Area Attention</a> <a class="note_content_pdf" href="/pdf?id=rygp3iRcF7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019area,    &#10;title={Area Attention},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rygp3iRcF7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Existing attention mechanisms, are mostly point-based in that a model is designed to attend to a single item in a collection of items (the memory). Intuitively, an area in the memory that may contain multiple items can be worth attending to as well. Although Softmax, which is typically used for computing attention alignments, assigns non-zero probability for every item in memory, it tends to converge to a single item and cannot efficiently attend to a group of items that matter. We propose area attention: a way to attend to an area of the memory, where each area contains a group of items that are either spatially adjacent when the memory has a 2-dimensional structure, such as images, or temporally adjacent for 1-dimensional memory, such as natural language sentences. Importantly, the size of an area, i.e., the number of items in an area, can vary depending on the learned coherence of the adjacent items. Using an area of items, instead of a single, we hope attention mechanisms can better capture the nature of the task. Area attention can work along multi-head attention for attending multiple areas in the memory. We evaluate area attention on two tasks: character-level neural machine translation and image captioning, and improve upon strong (state-of-the-art) baselines in both cases. In addition to proposing the novel concept of area attention, we contribute an efficient way for computing it by leveraging the technique of summed area tables.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Deep Learning, attentional mechanisms, machine translation, captioning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">The paper presents a novel approach for attentional mechanisms that can benefit a range of tasks such as machine translation and image captioning.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">15 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SkxW5QBK3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Some important related studies are missing.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygp3iRcF7&amp;noteId=SkxW5QBK3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper751 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper751 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
I have several concerns about this paper.

[originality]
Some important related studies are missing.

# Related studies about the perspective of “area”.
The consecutive position in sequence is often referred to as “span” in NLP filed, which is identical to what the authors call “area” in this paper.
Then, the idea of utilizing spans currently becomes a very popular in NLP field. We can find several papers, 
e.g.,
Wenhui Wang, Baobao Chang, “Graph-based dependency parsing with bidirectional lstm”, ACL-2016.
Mitchell Stern, Jacob Andreas, Dan Klein, “A Minimal Span-Based Neural Constituency Parser”, ACL-2017.
Kenton Lee, Luheng He, Mike Lewis, Luke Zettlemoyer, “End-to-end Neural Coreference Resolution”, EMNLP-2017.
Nikita Kitaev, Dan Klein, “Constituency Parsing with a Self-Attentive Encoder”, ACL-2018.

Similarly, there are several related studies in image processing field,
e,g.,
Marco Pedersoli, Thomas Lucas, Cordelia Schmid, Jakob Verbeek, “Areas of Attention for image captioning”, ICCV-2017
Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo, “Image Captioning with Semantic Attention”, CVPR-2016.

# Related studies about the perspective of “structured attention”. 
Several papers about structured attention have already been proposed, 
e.g.,
Yoon Kim, Carl Denton, Luong Hoang, Alexander M. Rush. “Structured Attention Networks”, ICLR-2017.
Vlad Niculae, Mathieu Blondel. “A Regularized Framework for Sparse and Structured Neural Attention”, NIPS-2017.


I think the authors should explain the relations between their method and the methods proposed in the above listed papers.


[significance]
# Concern about experimental settings
The experimental setting for NMT looks unnormal in the community.
Currently, most of papers use sentences split in subword units rather than character units. I cannot find a reason to select the character units. I think the authors should report the effectiveness of the proposed method on the widely-used settings.


# computational cost
The authors should report the actual calculation speed by comparing with the baseline method and the proposed method.
In Sec. 2.2, the authors provided the computational cost. 
I feel that the cost of O(|M|A) is still enough large and that can unacceptably damage the actual calculation speed of the proposed method.



Overall, the proposed method itself seems to be novel and interesting.
However, in my opinion, writing and organization of this paper should be much improved as a conference paper. I feel like the current status of this paper is still ongoing to write.
Thus, it is a bit hard for me to strongly recommend this paper to be accepted. 



</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HygeYLjd27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Experiments are not convincing</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygp3iRcF7&amp;noteId=HygeYLjd27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper751 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper751 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">[Summary]
Paper “AREA ATTENTION” extends the current attention models from word level to “area level”, i.e., the combination of adjacent words. Specifically, every $r_i$ adjacent words are first merged into a new item; next a key and the value for this item is calculated based on Eqn.(3 or 7) and Eqn. (4), and then the conventional attention models are applied to these new items. The authors work on (char level) NMT and image captioning to verify the algorithm. 

[Details]
1.	In the abstract, “… Using an area of items, instead of a single, we hope attention mechanisms can better capture the nature of the task …”, can you provide an example to show why “an area of items” can “better capture the nature of the task”? In particular, you need to show why the conventional attention mechanism fails.
2.	In this new proposed framework, how should we define the query for each area including multiple items like words? For example, in Figure 1, what is the query for $n$-item areas where $n=1,2,3$.
3.	Two different kinds of keys are proposed in Eqn. (3) and Eqn. (7). Any comparison between them?
4.	I am not convinced by the experimental results.
(4a) On WMT’14 En-to-Fr and En-to-De, we know that “transformer_big” can achieve better results than the three settings shown in Table 1 &amp; 2. The results of using transformer_big are not reported. Besides, it is not necessary to use the “tiny” setting for En-to-{De, Fr} translation considering the data size.
(4b) It is widely adopted to use token-level neural machine translation. It is not convincing to work on char-level NMT only. Also, please provide the results using transformer_big setting.
(4c) There are no BLEU scores for the LSTM setting. Note that comment (4b) and (4c) are also pointed by anonymous readers.
(4d) It is really strange for me to “trained on COCO and tested on Flickr” (See the title of Table 4). It is not a common practice in image captioning literature. Even if in (Soricut et al., 2018), the authors report the results of training of COCO and test on COCO (the Table 5). Therefore, the results are not convincing. You should train on COCO and test on COCO too.
e.	What if we use different area size? I do not find the study in this paper.

[Pros &amp; Cons]
(+) A new attempt of the attention model that tries to build the attention beyond unigrams.
(-) Experiments are not convincing.
(-) The motivation is not strong.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1gTcC3xnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A few concerns</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygp3iRcF7&amp;noteId=H1gTcC3xnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper751 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">27 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper751 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">I prefer the idea of using some statistics (such as variances) of multiple items for attention. 
This direction may lead to better attention units for future works. 

I do not fully understand the argument, "Attention mechanisms are designed to focus on a single item in the entire memory". 
In my understanding, the attention formulation has no mathematical bias to focus on a single item. 
I have been working on the enterprise NMT for years, and observed many cases where the attention weights concentrate in a few (not a single), tokens. 
Do you have any comments? 

Could you show some concise examples that we really need to attend multiple (adjacent) items to boost the performance? 
For example, in char-based machine translation case, we can mimic the area attention with the wordpiece + token-wise NMT.  
For the image case, the adjacent area looks like a "super pixel". 

It is unfortunate to observe that the gains on BLEU and perplexity are limited. 
Since the authors do not provide any statistical tests, or a confidence interval of the scores, 
I cannot be sure these gains are truly significant. 
From my experiences +1.0 BLEU score is often insignificant in NMT experiments (BLEU variance is high in general). 

Summary
+ A new variant of attention, allowing attention to asses statistics of multiple items (such as variances) is interesting
- Claims are not so much convincing for the need of attending multiple adjacent items. 
- Gains in experiments are limited. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJlczK-Mcm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Some related work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygp3iRcF7&amp;noteId=BJlczK-Mcm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper751 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I find this is an interesting approach to attention that could be broadly applicable.  
I have been interested in those approaches for some time and am curious to see how it devellops.

Here is an reference that is relevant:  <a href="https://arxiv.org/pdf/1612.01033.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1612.01033.pdf</a> 
(Areas of Attention for image captioning) 

Cheers </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1lSNXPF57" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Will cite &amp; discuss the related work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygp3iRcF7&amp;noteId=H1lSNXPF57"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper751 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Oct 2018</span><span class="item">ICLR 2019 Conference Paper751 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for bringing up this previous work that is indeed relevant. The paper focused on image captioning and proposed two nice methods for attending to object regions on images, where both use a special network to infer regions to attend. In contrast, our method examines all possible areas with summed area table for fast computation. The basic form of area attention we proposed is parameter free. We also intend to propose area attention as a general mechanism beyond captioning tasks. We will cite and discuss the paper in the revision.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJxRSjwg9X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Some implementation or reproduction problem about this paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygp3iRcF7&amp;noteId=rJxRSjwg9X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Oct 2018</span><span class="item">ICLR 2019 Conference Paper751 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Interesting work, but the algorithm seems to be ambiguous. Hope you can release the code to verity the details.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Sygs7-Y-5X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Code</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygp3iRcF7&amp;noteId=Sygs7-Y-5X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper751 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper751 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your interest in reading the work. We will make the pseudo code more readable and release the source code that is written in TensorFlow. Our experiments were conducted based on the original Transformer implementation released in Tensor2Tensor (<a href="https://github.com/tensorflow/tensor2tensor)." target="_blank" rel="nofollow">https://github.com/tensorflow/tensor2tensor).</a></span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJxsDJ2jtQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Some concerns about this paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygp3iRcF7&amp;noteId=HJxsDJ2jtQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 19 Oct 2018)</span><span class="item">ICLR 2019 Conference Paper751 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, this idea is interesting, though need some explanations, can you explain some my concerns regarding the motivation and experiments?

1. How are you sure that "Although softmax (Equation 1) assigns non-zero probablities to every item in the memory, it quickly converges to the most probable item due to the exponential function used for calculating probabilities" ? Can you prove this claim mathematically ? If not, is there any research out there already confirm this? if so, please cite.
I am not going to prove you wrong, but I have seen many examples that softmax doesn't converge into 1 single item. Look at some ImageNet classification papers (resnet, densenet....), the real life softmax scores distribute a lot more evenly. I guess it depends on the data, not by the convergence of softmax function.

2. eqn 6 indicates ei has dimension 1xD, miu_i, sigma_i possibly also 1xD. So the term behind the Relu function will be also 1xD. Then the whole eqn 7 : W_d x relu(µi + σi + ei; θ) is a matrix dot product of DxD and 1xD, which is dimensionally incompatible?
Correct me if i'm wrong.

3. equation kri = Wdφ(µi + σi + ei; θ) looks weird. It is unusual to sum up the mean and variance together. variance is 2-degree term, should it be added to the mean (1-degree term)? Please justify.
To me, it makes more sense to sum the mean the standard deviation rather than the variance.

4. (Vaswani et al., 2017) experimented translation with BPE tokens, which already achieved more than this paper did with character-level experiments. What is the motivation to use character-level but not (at least) BPE or word-level translation?  Why not do BPE experiment to compare with Vaswani et al.

5. What happen if the elements in a particular area got reordered? Will the result after the attention be different or the same?

Thank you,
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1eyP2OZc7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarifications</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygp3iRcF7&amp;noteId=B1eyP2OZc7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper751 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper751 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for bringing up these questions. We briefly clarify them here and will address them further in the revision.

Re: Question #1
We agree that the peakiness of softmax depends on data and tasks. Our statement about softmax convergences is mostly empirical, from both previous work and our own observation. For example, the early attention work (<a href="https://arxiv.org/pdf/1409.0473.pdf)" target="_blank" rel="nofollow">https://arxiv.org/pdf/1409.0473.pdf)</a> by Bahdanau, Cho &amp; Bengio showed that it is often a single or very few items that are receiving most attention probability (see Figure 3 in the paper). Similar phenomena were reported in the Transformer work (https://arxiv.org/pdf/1706.03762.pdf) (In Figure 3, each row that uses color density to represent attention distribution is mostly white). In our own experiments, we found the entropy of the attention probability distribution tends to decrease rapidly, which indicates that the attention probability distribution is towards more deterministic rather than evenly distributed as the training proceeds. That said, we will clarify that our statement is empirical and cite the literature.

Re: Question #2
Thanks for pointing out the issue. There should be a transpose over relu(). Alternatively it should be relu(µi + σi + ei; θ) x W_d. We will correct this in the revision.

Re: Question #3
We explored both standard deviation and variance in the early experiments from which we did not see a noticeable difference. In φ, the sum of the three is projected before ReLU. That said, we agree it makes more sense to use standard deviation rather than variance, and we will run full experiments on standard deviation and report back.

Re: Question #4
Our motivation to study area attention on character-level instead of token-level comes from the fact that there are simply more areas to attend to on characters. Since many sentences only have a few tokens, it is intuitively less clear why attending to areas should help in that case. Having said that, area attention is a general framework that is applicable in all cases (e.g., image captioning as we presented), in the worst case performing similarly to plain attention. We will run token-level experiments as well and report back.

Re: Question #5
The overall attention distribution will be different, even though the representation for that specific area is the same. This is because area attention allows overlapping areas. The change in the order of items will cause these items to be picked up by different areas. For example, assume there is a sequence with six items: A, B, C, D, E, and F. Say Area 1 contains A, B and C; Area 2 contains C and D; and Area 3 contains D, E and F. Reordering C and D in Area 2 will not change Area 2’s representation. However, the reordering will leave Area 1 with A, B and D, and Area 3 with C, E and F.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryxUM6rPi7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Not convinced</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygp3iRcF7&amp;noteId=ryxUM6rPi7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Oct 2018</span><span class="item">ICLR 2019 Conference Paper751 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">#1
This is not convincing. The papers you mentioned only shows a figure of attention probability of an example in the dataset. Those were just for demonstration with a single example, not an overall observation of the entire dataset, so we cannot infer a single figure as a universal truth. I think they did not claim or proved that softmax score converge to a particular item in their papers either. They also didn't show any statistics supporting this claim.

In your experiments, there is no statistic data supporting "entropy of the attention probability distribution tends to decrease rapidly".

Anyway, there is nothing bad for attention softmax score to concentrate on a particular item, and that's what attention with softmax is used for. Attention acts as a word to word alignments, it should, inversely, attend on a particular item rather than spreading out equally.

#4 transformer is simply not suitable for character-level (24.65 BLEU vs 27.3 in the original paper using BPE). This is obvious because it is harder to attend on sequence of hundreds to thousands characters than to attend on less than 100 words. Your solution makes the character-level problem easier and it makes sense. But there should not be a problem to begin with because they use token-level (BPE) translation. A possible reason (other than BLEU) to prefer character-level is to construct UNK words or rare words, but BPE already did that and there is no analysis on this in the paper anyway.

Hope the results on token-level translation are better than original transformer, otherwise, it seems the paper just create a problem for the proposed model to work.

#5 agree that overall attention distribution will be different. But invariance within a region is important. For instance, what is the difference between "army" and "mary"? In such case, how can overlapping the characters with nearby words might be helpful to differentiate "army" and "mary"?

# 6 Why the paper didn't report LSTM on BLEU but perplexity? Perplexity is usually not a good indication of quality of translation rather than BLEU, the transformer paper also said that they sacrifice perplexity for better BLEU. I'm not 100% sure, but improvement in order of 0.0001 sounds not significant though.
Can you report BLEU of LSTM?

#7 This can be good for image captioning though. But I guess you miss Image Transformer paper (<a href="https://arxiv.org/abs/1802.05751)." target="_blank" rel="nofollow">https://arxiv.org/abs/1802.05751).</a> That is quite similar to this paper, should cite it.

#8 can you release the code?

Overall:
+ The motivation and problem (translation) is not convincing, proved or supported with statistical data to begin with. It is not shown that such characteristics of softmax (low entropy on convergence) is a problem for attention mechanism either.
+ For translation, unless token-level experiments work, purposefully using character-level task seems just to create a problem in which area attention has the advantage over normal attention.
+ Image caption task seems promising though. Perhaps it is more suitable and convincing to use this for vision than NLP.

</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1gjJpJ_oQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response on motivation, token-level performance and other points</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygp3iRcF7&amp;noteId=r1gjJpJ_oQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper751 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Oct 2018</span><span class="item">ICLR 2019 Conference Paper751 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks again for your further comments. Please see our responses here.

Re: #1
We did not intend to argue that focusing on a single item is always bad. Rather, area attention is simply to give the model the option to attend to a range of items that are structurally adjacent when needed. Please notice that area attention subsumes single-item attention rather than excluding it. For example, area attention with max_area_size=5 includes all the areas with size 1-5, where area_size=1 is effectively single-item attention. 

Again, we will clarify our point regarding attention convergence in the revision. To answer your question, we did observe that, for example, mean entropy started with 5.0 at the initial stage of training for some layers and then dropped to less than 1.8 within the first 20K iterations. That said, we want to clarify that attention convergence is not the issue we are tackling, which is what it should be doing as you pointed out. Rather, our main motivation with area attention is to give the model more options when attending to items.

Re: #4
We have run experiments on token-level translation, and found area attention outperformed the transformer baselines in most conditions as well. In particular, for the Transformer (Base Model) that you are questioning, area attention achieved BLEU scores: 28.17 (ende) 39.22 (enfr). All these BLEU scores are higher than what were previously reported in the Transformer paper (ende: 27.3 and enfr: 38.1). The performance gain is simply achieved by just using the parameter-free version of Area Attention. We will add a section and a table in the revision to report the token-level performance of area attention with Transformer.

Re: #5
Your example is assuming each item (character) is not carrying its positional information. For Transformer, this is not the case because each item encodes its position in the sequence (see Sec 3.5: Positional Encoding in the Transformer paper <a href="https://arxiv.org/pdf/1706.03762.pdf)." target="_blank" rel="nofollow">https://arxiv.org/pdf/1706.03762.pdf).</a> For LSTM, this is also less of an issue because attention is applied to the output of an LSTM layer which already captures order information to a certain extent.

That said, we see your point, and we could have used a sequence model to encode each area to directly capture the order of items in the area. However, one important advantage of area attention is that it is fast and takes constant time to compute (key, value) for each area due to the use of summed area table, and its basic form (Eq.3 &amp; 4) is parameter free.

Re: #6
We reported perplexity to see the relative trend when area attention is in use, which showed that area attention often improved over regular attention in LSTM. That said, we will report back the BLEU scores for LSTM. 

Re: #7
We will cite the Image Transformer paper. It uses regular multi-head attention and solves a different task on conditional image generation.

Re: #8
Yes. We will release the code as we implemented area attention directly based on the open source Tensor2Tensor library (https://github.com/tensorflow/tensor2tensor), where benchmark Transformer models and tasks are implemented, which guarantees a solid comparison with regular attention in Transformer.

Overall:
We clarify that our motivation with area attention is to give a model more options when attending to items. Area attention subsumes regular-attention rather than excluding it as explained earlier. Even the parameter-free version of area attention has outperformed regular attention on both character-level and token-level translation tasks, and image captioning tasks. We will add new experimental results to the revision.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJe3HncKoQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Wonderful</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygp3iRcF7&amp;noteId=BJe3HncKoQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">21 Oct 2018</span><span class="item">ICLR 2019 Conference Paper751 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the prompt response.

Now the motivation is clear. "Giving the model more options to attend to a range of items that are structurally adjacent when needed" is a nicer motivation rather than arguing the softmax convergence and focusing on a single item is bad.
You may want to make this point clear in the revision because the initial writing makes me feel the motivation is to tackle softmax convergence and the reviewers can possibly interpret like this too.

Now token-level translation also works. Good jobs.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1e0m61nom" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygp3iRcF7&amp;noteId=S1e0m61nom"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper751 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">23 Oct 2018</span><span class="item">ICLR 2019 Conference Paper751 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Yes. We will make this point clear in the revision. Thanks for these suggestions that strengthen the contribution of our paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_H1l0Ups3YQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>the notation for variance is usually \sigma^2, not \sigma</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygp3iRcF7&amp;noteId=H1l0Ups3YQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Sep 2018</span><span class="item">ICLR 2019 Conference Paper751 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I think in Eq. (5), it is more suitable by using \sigma^2, not \sigma, to denote the variance.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkges0dWqX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good suggestion</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygp3iRcF7&amp;noteId=Bkges0dWqX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper751 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper751 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Yes! Thanks for catching this. We will fix it in the revision.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>