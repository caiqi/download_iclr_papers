<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>HC-Net: Memory-based Incremental Dual-Network System for Continual learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="HC-Net: Memory-based Incremental Dual-Network System for Continual learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rJgz8sA5F7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="HC-Net: Memory-based Incremental Dual-Network System for Continual..." />
      <meta name="og:description" content="Training a neural network for a classification task typically assumes that the data to train are given from the beginning.&#10;  However, in the real world, additional data accumulate gradually and the..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rJgz8sA5F7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>HC-Net: Memory-based Incremental Dual-Network System for Continual learning</a> <a class="note_content_pdf" href="/pdf?id=rJgz8sA5F7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019hc-net:,    &#10;title={HC-Net: Memory-based Incremental Dual-Network System for Continual learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rJgz8sA5F7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Training a neural network for a classification task typically assumes that the data to train are given from the beginning.
However, in the real world, additional data accumulate gradually and the model requires additional training without accessing the old training data. This usually leads to the catastrophic forgetting problem which is inevitable for the traditional training methodology of neural networks.
In this paper, we propose a memory-based continual learning method that is able to learn additional tasks while retaining the performance of previously learned tasks.
Composed of two complementary networks, the Hippocampus-Net (H-Net) and the Cortex-Net (C-Net), our model estimates the index of the corresponding task for an input sample and utilizes a particular portion of itself with the estimated index.
The C-Net guarantees no degradation in the performance of the previously learned tasks and the H-Net shows high confidence in finding the origin of an input sample.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">continual learning, lifelong learning, catastrophic forgetting</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">In this paper, we propose a network which efficiently increases its complexity without degrading the performance of previous tasks inspired by the brain system of human being</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rye0nwomaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Not good enough</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJgz8sA5F7&amp;noteId=rye0nwomaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper152 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper152 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The work proposes a structure that mimics progressive nets. Maybe the main difference from progressive nets is that backwards connection from the new features to the old features in layer 2 are not 0 out. This could cause interference, however is solved by using the task ID to not evaluate those new features when going back to a previous task. I think this is a technical detail, that does not provide any explicit advantage or disadvantage over progressive nets. 

Employing GANs/VAE to predict task id also can be seen as not an ideal choice. In particular the GAN network will suffer from catastrophic forgetting, which is solved (if I understood correctly) by training the GAN with data from all tasks. Which makes one wonder, if we can affort to access data from all tasks to learn the GAN then why not the classification model too !?

I think an alternative might be something like the Forget Me Not Process published and used in the original work with EWC.

Unfortunately due to presence of these previous works, lack of more thorough comparison with other existing approaches, the work should not be accepted to ICLR.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ryg8dhcu2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>missing references and comparisons, unclear evaluation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJgz8sA5F7&amp;noteId=ryg8dhcu2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper152 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper152 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary
This paper proposes an extension of Progressive Networks [Rusu et al. NIPS 2016] (unfortunately, not cited) where the task id is not given at test time. This is inferred by a battery of classifiers trained on data produced by generative models trained on task specific data.
The authors argue strong connections to similar mechanisms in the human brain and demonstrate this method on a stream of 2 or 3 vision tasks. However, the interpretation of these results is dubious.

Novelty: given prior work on Progressive Nets and other methods using generative models for continual learning, novelty is limited.

Relevance: the motivation and aim of this work is certainly relevant for ICLR.

Clarity: the paper is overall clear, although it needs a bit of rewriting to improve fluency (see for instance sec. 3.4.1).

References: the authors should definitely cite Progressive Networks and their extension "Progress and Compress" (Schwarz ICML 2018), as their approach is an extension of the former with the only difference that the task id is inferred at test time by using a battery of binary classifiers.

Empirical validation: The empirical validation is limited because of:
a) lack of comparison to Progressive Nets, 
b) lack of simple baselines (e.g., how about replacing H-Net with an inference process like task_id = argmin_i=1..T loss(C-Net, task = i) ),
c) unclear interpretation of the provided results (how can the accuracy on MNIST be 100%? are the authors reporting training accuracy?)
d) very limited number of tasks considered (up to 3)

General comments
Major drawbacks of the proposed approach are: 1) training on new tasks can never improve performance on past tasks (unlike other methods like GEM (Lopez-Paz et al. NIPS 2017), 2) the number of parameters grow linearly with the number of tasks (an issue addressed by the Progress and Compress paper above), and 3) the overall approach is not efficient as it requires lots of data from each task in order to train the generative models.
Finally, I think all the connections and inspiration from how the human brain works should be toned down.  Statements like "the C-Net corresponds to the human cortex..." should be at the very least rephrased appropriately.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJg8d7lpiX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Not enough novelty and experiments are not very informative.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJgz8sA5F7&amp;noteId=rJg8d7lpiX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper152 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">24 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper152 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The key idea of this paper is to expand the network for training on new tasks which is termed as C-Net, and train an additional generative model which is used for predicting task id (which is called H-Net), and use the task id for selecting weights from the C-Net.

Pros:
1. It is relatively easy to understand the paper. 
2. The originality of this paper lies in the usage of generative model to predict task id (H-Net). To my knowledge has not been proposed before.
3. In contrast to previous works in multi-task learning, which assumes task id is available both during training and inference, this work tries to remove the need of task id during inference, which makes it closer to the general definition of continual learning.

Cons:
1. Expanding the network for new tasks is not a novel contribution of this paper, it has already been proposed in previous works on multi-task learning. Doing expansion on all of the layers does not qualify for a major contribution in my opinion.
2. The experimental comparison is not very fair in my opinion, 
     a. Comparing accuracy of C-Net to other methods is not very useful. Because this methods expands the network for every new task, while other methods (EWC, LwF) has limited to no expansion in the network. Given that the single network result is far from state of the art (table 3), I suppose model size could contribute to the accuracy boost.
     b. It is not explicitly stated in the paper whether the output neurons are shared between tasks or an individual set of output neurons are used for different tasks, but from the rest of the paper I suppose disjoint neurons are used. Then the comparison between EWC and this work is not fair because EWC shares the output neurons among tasks.
This is not to blame this paper for not making fair comparison, since given different assumptions between methods (availability of task id, shared output neurons etc.), it is usually difficult to fairly compare between continual learning methods.  This problem is raised in another submission <a href="https://openreview.net/forum?id=ByGVui0ctm." target="_blank" rel="nofollow">https://openreview.net/forum?id=ByGVui0ctm.</a> The point here is that the accuracy of C-Net is not a good measure of how good this method is.
3. I disagree with the point that MNIST and SVHN are similar, they have very different distributions and are very easy to tell apart with a model. One concern is that the generative H-Net may fail to work once the distributions of the tasks overlap to some extent. e.g. cifar10 vs cifar100.

As a conclusion, the key contribution of this work is using generative model to determine task id which removes the need for task id during inference. It is relatively insufficient for publication on ICLR.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>