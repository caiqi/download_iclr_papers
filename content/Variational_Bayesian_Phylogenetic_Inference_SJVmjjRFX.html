<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Variational Bayesian Phylogenetic Inference | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Variational Bayesian Phylogenetic Inference" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SJVmjjR9FX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Variational Bayesian Phylogenetic Inference" />
      <meta name="og:description" content="Bayesian phylogenetic inference is currently done via Markov chain Monte Carlo with simple mechanisms for proposing new states, which hinders exploration efficiency and often requires long runs to..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SJVmjjR9FX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Variational Bayesian Phylogenetic Inference</a> <a class="note_content_pdf" href="/pdf?id=SJVmjjR9FX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019variational,    &#10;title={Variational Bayesian Phylogenetic Inference},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SJVmjjR9FX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Bayesian phylogenetic inference is currently done via Markov chain Monte Carlo with simple mechanisms for proposing new states, which hinders exploration efficiency and often requires long runs to deliver accurate posterior estimates. In this paper we present an alternative approach: a variational framework for Bayesian phylogenetic analysis. We approximate the true posterior using an expressive graphical model for tree distributions, called a subsplit Bayesian network, together with appropriate branch length distributions. We train the variational approximation via stochastic gradient ascent and adopt multi-sample based gradient estimators for different latent variables separately to handle the composite latent space of phylogenetic models. We show that our structured variational approximations are flexible enough to provide comparable posterior estimation to MCMC, while requiring less computation due to a more efficient tree exploration mechanism enabled by variational inference. Moreover, the variational approximations can be readily used for further statistical analysis such as marginal likelihood estimation for model comparison via importance sampling. Experiments on both synthetic data and real data Bayesian phylogenetic inference problems demonstrate the effectiveness and efficiency of our methods.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Bayesian phylogenetic inference, Variational inference, Subsplit Bayesian networks</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">The first variational Bayes formulation of phylogenetic inference, a challenging inference problem over structures with intertwined discrete and continuous components</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SkgNStowpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A note to all reviewers</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJVmjjR9FX&amp;noteId=SkgNStowpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper611 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper611 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewers for their reviews and the time spent on the manuscript, and their encouragement on the novelty of our work. We want to emphasize that, in addition to using subsplit Bayesian networks for approximating phylogenetic tree posteriors, our structured parameterization of the branch length distributions is also novel which allows us to jointly learn the branch length distributions across tree topologies. In contrast, classical MCMC typically uses simple random perturbations which contributes to the low acceptance rate for large topological modifications.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BkgUn-Vwpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>New approximate inference approaches for phylogenetic trees</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJVmjjR9FX&amp;noteId=BkgUn-Vwpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper611 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper611 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper explores an approximate inference solution to the challenging problem of Bayesian inference of phylogenetic trees. Its leverages recently proposed subsplit Bayesian networks (SBNs) as a variational approximation over tree space and combines this with modern gradient estimators for VI. It is thorough in its evaluation of both methodological considerations and different datasets.

The main advantage would seem to be a large speedup over MCMC-based methods (Figure 4), which could be of significant value to the phylogenetics community. This point would benefit from more discussion. How do the number of iterations (reported in Figures 3&amp;4, which was done carefully) correspond to wallclock time? Can this new method scale to numbers of sites and sequences that were previously unfeasible?

The main technical contribution is the use of SBNs as variational approximations over tree-space, but it is difficult to follow their implementation and parameter sharing without the explanation of the original paper. Additionally, the issue of estimating the support of the subsplit CPTs needs more discussion. As the authors acknowledge, complete parameterizations of these models scale in a combinatorial way with “all possible parent-child subsplit pairs”, and they deal with this by shrinking the support up front with various heuristics. It seems that these support estimation approaches would be feasible when the data are strong but would become challenging to scale when the data are weak. Since VB is often concerned with the limited-data regime, more discussion of when support estimation is feasible and when it is difficult would clarify how widely applicable the method is.

Overall, this work is an interesting extension of variational Bayes to a tree-structured inference problem and is thorough in its evaluation. While it is a bit focused on classical inference for ICLR, it could be interesting both for the VI community and as a significant application advancement.

Other notes:
In table 1, is the point that all methods are basically the same with different variance? This is not clear from the text. What about the variational bounds?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rylqTcJF67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarification on technical issues (Part 2/2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJVmjjR9FX&amp;noteId=rylqTcJF67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper611 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper611 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">4) "In table 1, is the point that all methods are basically the same with different variance? This is not clear from the text. What about the variational bounds?"

Yes, you are right. All methods provide estimates for the same marginal likelihood, and better approximation would lead to smaller variance. The phylogenetic model is well defined with fixed structure, in contrast to generative models (e.g., VAE) where the generative network is trainable. In the paper we do not report the variational bounds since we want to compare to the stepping-stone (SS) algorithm on marginal likelihood estimation. These lower bounds are definitely different and improve as more particles and more flexible approximations are adopted, which we list below (averaged over 1000 runs).

In our revision we look forward to clarifying this point.

                                           Variational Lower Bounds
+---------+------------------+------------------+--------------------------+--------------------------+
|	      |  VIMCO(10)  |  VIMCO(20)  |   VIMCO(10)+PSP   |   VIMCO(20)+PSP   |
+---------+------------------+------------------+--------------------------+--------------------------+
|  DS1   |    -7108.91    |    -7108.77     |         -7108.73          |          -7108.61         |
+---------+------------------+------------------+--------------------------+--------------------------+
|  DS2   |   -26367.91   |    -26367.82   |         -26367.89       |          -26367.83       |
+---------+------------------+------------------+--------------------------+--------------------------+
|  DS3   |   -33735.36   |    -33735.26   |         -33735.29       |          -33735.24       |
+---------+------------------+------------------+--------------------------+--------------------------+
|  DS4   |   -13330.49   |    -13330.32   |         -13330.37       |          -13330.22       |
+---------+------------------+------------------+--------------------------+--------------------------+
|  DS5   |   -8215.85     |    -8215.56     |         -8215.64          |         -8215.36          |
+---------+------------------+------------------+--------------------------+--------------------------+
|  DS6   |   -6725.69     |    -6725.42     |         -6725.48          |         -6725.19          |
+---------+------------------+------------------+--------------------------+--------------------------+
|  DS7   |   -37332.91   |    -37332.65   |         -37332.72       |          -37332.49       |
+---------+------------------+------------------+--------------------------+--------------------------+
|  DS8   |   -8655.02     |    -8652.55     |         -8651.76          |         -8651.53          |
+---------+------------------+------------------+--------------------------+--------------------------+</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Bkl2HqJYTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarification on technical issues  (Part 1/2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJVmjjR9FX&amp;noteId=Bkl2HqJYTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper611 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper611 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your thoughtful review and valuable feedback. Below are the answers to your comments:

1) "The main advantage would seem to be a large speedup over MCMC-based methods (Figure 4), which could be of significant value to the phylogenetics community. This point would benefit from more discussion. How do the number of iterations (reported in Figures 3&amp;4, which was done carefully) correspond to wallclock time? Can this new method scale to numbers of sites and sequences that were previously unfeasible?"

We are glad that this reviewer appreciates the care with which we crafted the comparison in terms of number of likelihood evaluations. Our motivation in doing a comparison in terms of likelihood evaluations is because our current implementation is in Python, whereas while MrBayes is in C that has been optimized for many years. This is the first paper introducing the ideas and initial implementation of variational Bayes phylogenetic inference, and we think that this level of comparison is appropriate. We will soon begin developing a highly optimized implementation, for which we are planning a more applications-driven paper which will include a wallclock comparison.

Regarding large data sets, given that the learned SBNs can provide guided exploration in tree space and variational approaches naturally incorporate stochastic gradients, we believe it is much easier for VBPI to scale to datasets with large numbers of sequences and sites. However, we have not tried out our initial Python implementation on especially big data sets.

2) "The main technical contribution is the use of SBNs as variational approximations over tree-space, but it is difficult to follow their implementation and parameter sharing without the explanation of the original paper."

As explained in point 2 to reviewer 3, this is mainly due to the page limit of the conference. We will definitely add more detailed explanation in our revision if there is room after trimming proposed by Reviewer 2.

3) "Additionally, the issue of estimating the support of the subsplit CPTs needs more discussion. As the authors acknowledge, complete parameterizations of these models scale in a combinatorial way with ?all possible parent-child subsplit pairs?, and they deal with this by shrinking the support up front with various heuristics. It seems that these support estimation approaches would be feasible when the data are strong but would become challenging to scale when the data are weak. Since VB is often concerned with the limited-data regime, more discussion of when support estimation is feasible and when it is difficult would clarify how widely applicable the method is."

This is indeed an important point. We agree that when the data are weak, the posterior on subsplit pairs could have a large support.

However, the SBN approach actually has a strong natural advantage in the weak-data regime. When data is weak, the support of the posterior distribution on complete trees, as evaluated by classical MCMC approaches, is enormous. For example, if there is uncertainty in multiple different parts of the tree, the support on complete trees scales as the product of these local uncertainties.

The SBN parameterization alleviates this issue by factorizing the uncertainty into local structures. Thus, if the support of parent-child pairs is too large, then one should certainly not be trying to assign posterior support to each tree individually as in classical MCMC.

Regarding heuristics for support estimation, we show in section 4.2 that bootstrap-based support estimation is effective even for diffuse posteriors across four data sets (DS5, DS6, DS7, DS8). See below for the numbers of unique trees in the standard MCMC run samples for all data sets (which is an indicator of the diffusivity of the posteriors).

-------------------------------------------------------------------------------------------------------------------------
datasets                |     DS1        DS2        DS3         DS4         DS5          DS6         DS7        DS8
-------------------------------------------------------------------------------------------------------------------------
# sample trees     |    1228          7           43           828        33752      35407       1125      3067
-------------------------------------------------------------------------------------------------------------------------

We agree that a further discussion of the weak-data regime is important and we look forward to adding to the discussion in a revision.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rke2eXKkTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A nice approach to inferring phylogenetic trees</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJVmjjR9FX&amp;noteId=rke2eXKkTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper611 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper611 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a variational approach to Bayesian posterior inference in phylogenetic trees. The novel part of the approach (using subsplit Bayesian networks as a variational distribution) is intelligently combined with recent ideas from the approximate-inference literature (reweighted wake-sleep, VIMCO, reparameterization gradients, and multiple-sample ELBO estimators) to yield what seems to be an effective approach to a very hard inference problem.

My score would be higher were it not for two issues:
* The paper is 10 pages long, and I'm not convinced it needs to be. The reviewer guidelines (<a href="https://iclr.cc/Conferences/2019/Reviewer_Guidelines)" target="_blank" rel="nofollow">https://iclr.cc/Conferences/2019/Reviewer_Guidelines)</a> say that "the overall time to read a paper should be comparable to that of a typical 8-page conference paper. Reviewers may apply a higher reviewing standard to papers that substantially exceed this length." So I recommend trying to cut it down a bit during the revision phase.
* The empirical comparisons are all likelihood/ELBO-based. These metrics are important, but it would be nice to see some kind of qualitative summary of the inferences made by different methods—two methods can produce similar log-likelihoods or KL divergences but suggest different scientific conclusions.

One final comment: it's not clear to me that ICLR is the most relevant venue for this work, which is purely about Bayesian inference rather than deep learning. This isn't a huge deal—certainly there's plenty of variational inference at ICLR these days—but I suspect many ICLR attendees may tune out when they realize there aren't any neural nets in the paper.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkeE3OswTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for the suggestions and we need some clarifications on your part</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJVmjjR9FX&amp;noteId=rkeE3OswTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper611 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper611 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for your review and time. We would like to incorporate the suggestions into our revision and think we would benefit from some clarifications on your part.

1) "The paper is 10 pages long, and I'm not convinced it needs to be. The reviewer guidelines (<a href="https://iclr.cc/Conferences/2019/Reviewer_Guidelines)" target="_blank" rel="nofollow">https://iclr.cc/Conferences/2019/Reviewer_Guidelines)</a> say that "the overall time to read a paper should be comparable to that of a typical 8-page conference paper. Reviewers may apply a higher reviewing standard to papers that substantially exceed this length." So I recommend trying to cut it down a bit during the revision phase."

The main reason we took 10 pages for the paper is that phylogenetic inference is probably not well known to the machine learning community and much space is devoted to putting the phylogenetic models and experiments in context. We have tried to balance between being short and being a little bit long (but more self-contained) and thought the latter would eventually save the reviewers' time. However, we would like to cut down our paper as suggested and would appreciate it very much if the reviewer can point to us which parts of the paper that you find are redundant and can be made more brief.

2) "The empirical comparisons are all likelihood/ELBO-based. These metrics are important, but it would be nice to see some kind of qualitative summary of the inferences made by different methods?two methods can produce similar log-likelihoods or KL divergences but suggest different scientific conclusions."

First, we would like to make sure the reviewer is aware how the KL results show the SBN-based approximations to be very close in distribution on the discrete space of phylogenetic tree structures. We have emphasized in point 3 to reviewer 3, and realize that we should have been more clear on this point.

However, we are happy to incorporate any qualitative summaries the reviewer would like to suggest. We could certainly add, for example, tree shape summaries, but such a comparison would be significantly weaker than the current comparison on tree structures.

3) "One final comment: it's not clear to me that ICLR is the most relevant venue for this work, which is purely about Bayesian inference rather than deep learning. This isn't a huge deal?certainly there's plenty of variational inference at ICLR these days?but I suspect many ICLR attendees may tune out when they realize there aren't any neural nets in the paper."

We think ICLR is an excellent venue for this work because: (i) Representation learning on discrete/structured objects has received increasing attention from the machine learning community, and our work represents an important advance in variational inference on complex structured models. (ii) Our variational framework admits many extensions that can incorporate the approximating power of neural networks (e.g, using normalizing flow and deep networks for more flexible within-tree and between-tree approximations, as mentioned in the discussion section of our paper).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SJeInxZA3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A novel well executed paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJVmjjR9FX&amp;noteId=SJeInxZA3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper611 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper611 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper is well written, appears to be well executed, and the results look good. I am not particularly well informed about the area, but the work appears to be novel. MCMC for phylogenetic inference is hugely expensive, and anything we can do to reduce that cost would be beneficial (the computational expense is not given, or I've missed it, for the variational approach - presumably it is relatively small compared to MCMC?).

My main criticism is that I found the details of subsplit Bayesian networks difficult to follow. Googling them suggests they are a relatively new model, which has not been well studied or used (there are no citations of the paper that introduces them for example!). The paper would be stronger if it discussed these in more detail - how close can they come to approximating the models usually used in phylogenetic analyses? Often the inferred phylogeny is itself of interest - how similar are the trees inferred here to those found from MrBayes?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">1: The reviewer's evaluation is an educated guess</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HklnmwjDaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Computational expense and approximation performance compared to MrBayes were reported.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJVmjjR9FX&amp;noteId=HklnmwjDaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper611 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper611 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review and feedback. We address your specific questions and comments below:

1) "the computational expense is not given, or I've missed it, for the variational approach - presumably it is relatively small compared to MCMC?"

We present the computational expense for the variational approach in terms of the number of likelihood evaluations, and compare to MCMC. We direct the reviewer to Figure 4 in section 4.2, where we show the KL divergence to the ground truth as a function of the number of iterations of different methods (including MCMC via MrBayes). For a fair comparison, the number of iterations for MCMC is mapped to the number of iterations of variational methods that take the same number of likelihood evaluations.

2) "My main criticism is that I found the details of subsplit Bayesian networks difficult to follow. Googling them suggests they are a relatively new model, which has not been well studied or used (there are no citations of the paper that introduces them for example!)."

SBNs are indeed a new model. The relatively short discussion of subsplit Bayesian networks (SBNs) is mainly due to the page limit of the conference, but we would like to present a more detailed discussion of SBNs if there is room in our revision. For a more detailed discussion, we refer the reviewer to the original paper [1] that introduced SBNs, which has been accepted to NIPS this year.

3) "The paper would be stronger if it discussed these in more detail - how close can they come to approximating the models usually used in phylogenetic analyses? Often the inferred phylogeny is itself of interest - how similar are the trees inferred here to those found from MrBayes?"

First, we would like to ensure that our means of evaluating the SBN approximation is clear. We compute KL divergence over the discrete collection of phylogenetic tree structures, from the SBN distribution to the ground truth distribution on phylogenetic tree models obtained from extremely long MCMC runs using MrBayes. In order to get a low KL divergence to this ground truth, it is not enough to have similar trees: one must find practically the same set of trees as MrBayes, with nearly identical probability weights.

Based on the low KL divergence reported in [1] and our experiments, SBNs can indeed provide accurate approximations to the phylogenetic posteriors inferred from real data (see Table 1 in [1] and section 4.2 in our paper.). Therefore, we believe SBN-based phylogenetic inference represents an important advance in this field, especially on structural learning of phylogenies.


Reference
[1] C. Zhang and FA. Matsen. Generalizing tree probability estimation via Bayesian networks. arXiv preprint arXiv:1805.07834, 2018</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>