<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Knowledge Distillation from Few Samples | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Knowledge Distillation from Few Samples" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HkgDTiCctQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Knowledge Distillation from Few Samples" />
      <meta name="og:description" content="Current knowledge distillation methods require full training data to distill knowledge from a large " teacher"="" network="" to="" a="" compact="" "student"="" by="" matching="" certain="" statistics="" between="" "teacher"..."="" />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HkgDTiCctQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Knowledge Distillation from Few Samples</a> <a class="note_content_pdf" href="/pdf?id=HkgDTiCctQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019knowledge,    &#10;title={Knowledge Distillation from Few Samples},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HkgDTiCctQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Current knowledge distillation methods require full training data to distill knowledge from a large "teacher" network to a compact "student" network by matching certain statistics between "teacher" and "student" such as softmax outputs and feature responses. This is not only time-consuming but also inconsistent with human cognition in which children can learn knowledge from adults with few examples. This paper proposes a novel and simple method for knowledge distillation from few samples. Taking the assumption that both "teacher" and "student" have the same  feature map sizes at each corresponding block, we add a 1x1 conv-layer at the end of each block in the student network, and align the block-level outputs between "teacher" and "student" by estimating the parameters of the added layer with limited samples. We prove that the added layer can be absorbed into the previous conv-layer so that no extra parameters and computation are introduced. Experiments show that the proposed method can recover a student network's top-1 accuracy on ImageNet from 0.2% to 62.7% with just 1000 samples in a few minutes, and is effective on various ways for constructing student networks.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">knowledge distillation, few-sample learning, network compression</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">This paper proposes a novel and simple method for knowledge distillation from few samples.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">10 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_S1lp283D6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>absorb convolution?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgDTiCctQ&amp;noteId=S1lp283D6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper808 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">What does it mean with absorb 1x1 conv layer to previous conv layer?
I think absorb is too ambiguous for a word
Copy weights to previous conv layer?
Stack trained 1x1 conv on top of previous layer but below ReLU?

Need better explainability, some figure to visualize would help

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1e5MapjhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A new formulation of knowledge distillation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgDTiCctQ&amp;noteId=B1e5MapjhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper808 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper808 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a framework for few-sample knowledge distillation of convolution neural networks. The basic idea is to fit the output of the student network and that of the teacher network layer-wisely. Such a regression problem is parameterized by a 1x1 point-wise convolution per layer (i.e. minimizing the fitting objective over the parameters of 1x1 convolutions). The author claims such an approach, called FSKD, is much more sample-efficient than previous works on knowledge distillation. Besides, it is also fast to finish the alignment procedure as the number of parameters is smaller than that in previous works. The sample efficiency is confirmed in the experiments on CIFAR-10, CIFAR-100 and ImageNet with various pruning techniques. In particular, FSKD outperforms the FitNet and fine-tuning by non-trivial margins if only small amount of samples are provided (e.g. 100).

Here are some comments:

1. What exactly does “absorb” mean? Is it formally defined in the paper?

2. “we do not optimize this loss all together using SGD due to that too much hyper-parameters need tuning in SGD”. I don’t understand (1) why does SGD require “too much” hyper-parameters tuning and (2) if not SGD, what algorithm do you use?  

3. According to the illustration in 3.3, the algorithm looks like a coordinate decent that optimizing L over one Q_j at a time, with the rest fixed. However, the sentence “until we reach the last block in the student-net” means the algorithm only runs one iteration, which I suspect might not be sufficient to converge.

4. It is also confusing to use the notation SGD+FSKD v.s. FitNet+FSKD, as it seems SGD and FitNet are referring to the same type of terminology. However, SGD is an algorithm, while FitNet is an approach for neural network distillation. 

5. While I understand the training of student network with FSKD should be faster because the 1x1 convolution has fewer parameters to optimize, why is it also sample-efficient? 

6. I assume the Top-1 accuracies of teacher networks in Figure 4 are the same as table 2 and 3, i.e. 93.38% and 72.08% for CIFAR-10 and CIFAR-100 respectively. Then the student networks have much worse performance (~85% for CIFAR-10 and ~48% for CIFAR-100) than the teachers. So does it mean FSKD is not good for redesigned student networks?

7. While most of the experiments are on CIFAR10 and CIFAR100, the abstract and conclusion only mention the results of ImageNet. Why?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HkxK78ZK27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Surprisingly good model distillation given few samples and non-iterative solution, but practical implications are unclear</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgDTiCctQ&amp;noteId=HkxK78ZK27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper808 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper808 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Model distillation can be tricky and in my own experience can take a lot of samples (albeit unlabeled, so cheaper and more readily available), as well as time to train. This simple trick seems to be doing quite well at training students quickly with few samples. However, it departs from most student-teacher training that find its primary purpose by actually outperforming students trained from scratch (on the full dataset without time constraints). This trick does not outperform this baseline, so its emphasis is entirely on quick and cheap. However, it's unclear to me how often that is actually necessary and I don't think the paper makes a compelling case in this regard. I am borderline on this work and could probably be swayed either way.

Strengths:
- It's a very simple and fast technique. As I will cover in a later bullet point (under weaknesses), the paper does not make it clear why this type of model distillation is that useful (since it doesn't improve the student model over full fine-tuning, unlike most student-teacher work). However, the reason why I do see some potential for this paper is because there might be a use case in quickly being able to adapt a pretrained network. It is very common to start from a pretrained model and then attach a new loss and fine-tune. Under this paradigm, it is harder to make architectural adjustments, since you are starting from a finite set of pretrained models made available by other folks (or accept the cost of re-training one yourself). However, it is unclear how careful one needs to treat the pretrained model if more fine-tuning is going to occur. If for instance you could just remove layers, drop some channels, glue it all together, and then that model would still be reasonable as a pretrained model since the fine-tuning stage could tidy everything up, then this method would not be useful in this situation.
- The fact that least squares solvers can be used at each stage, without the need for a final end-to-end fine-tune is interesting.
- It is good that the paper demonstrates improvements coupled with three separate compression techniques (Li et al., Liu et al., Guo et al.).
- The paper is technically thorough.
- It's good that the method is evaluated on different styles of networks (VGG, ResNet, DenseNet).

Weaknesses:
- Limited application because it only makes the distillation faster and cheaper. The primary goal of student-teacher training in literature is to outperform a student trained from scratch by the wisdom of the teacher. It ties into this notion that networks are grossly over-parameterized, but perhaps that is where the training magic comes from. Student-teacher training acknowledges this and tries to find a way to benefit from the over-parameterized training and still end up with a small model. I think the same motivation is used for work in  low-rank decomposition and many other network compression methods. However, in Table 1 the "full fine-tune" model is actually the clear winner and presented almost as an upper bound here, so the only benefit this paper presents is quick and cheap model distillation, not better models. Because of this, I think this paper needs to spend more time making a case for why this is so important.
- Since this technique doesn't outperform full fine-tuning, the goal of this work is much more focused on pure model compression. This could put emphasis on reducing model size, RAM usage reduction, or FLOPS reduction. The paper focuses on the last one, which is an important one as it correlates fairly well with power (the biggest constraint in most on-device scenarios). However, it would be great if the paper gave a broader comparison with compression technique that may have slightly different focus, such as low-rank decomposition. Size and memory usage could be included as columns in tables like 1, along with a few of these methods. 
- Does it work for aggressive compression? The paper presents mostly modest reductions (30-50%). I thin even if accuracy takes a hit, it could still work to various degrees. From what I can see, the biggest reduction is in Table 4, but FSKD is used throughout this table, so there is no comparison for aggressive compression with other techniques. 
- The method requires appropriate blocks to line up. If you completely re-design a network, it is not as straightforward as regular student-teacher training. Even the zero-student method requires the same number of channels at certain block ends and it is unclear from the experiments how robust this is. Actually, a bit more analysis into the zero student would be great. For instance, it's very interesting how you randomly initialize (let's say 3x3) kernels, and then the final kernels are actually just linear combinations of these - so, will they look random or will they look fairly good? What if this was done at the initial layer where we can visualize the filters, will they look smooth or not?

Other comments:
- A comparison with "Deep Mutual Learning" might be relevant (Zhang et al.). I think there are also some papers on gradually adjusting neural network architectures (by adding/dropping layers/channels) that are not addressed but seem relevant. I didn't read this recently, but perhaps "Gradual DropIn of Layers to Train Very Deep Neural Networks" could be relevant. There is at least one more like this that I've seen that I can't seem to find now.
- It could be more clear in the tables exactly what cited method is. For instance, in Table 1, does "Fine-tuning" (without FitNet/FSKD) correspond to the work of Li et al. (2016)? I think this should be made more clear, for instance by including a citation in the table for the correct row. Right now, at a glance, it would seem that these results are only comparing against prior work when it compares to FitNet, but as I read further, I understood that's not the case.
- The paper could use a visual aid for explaining pruning/slimming/decoupling.

Minor comments:
- page 4, "due to that too much hyper-parameters"
- page 4, "each of the M term" -&gt; "terms"
- page 6, methods like FitNet provides" -&gt; "provide"</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1x7gjxLnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarification on non-square Q</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgDTiCctQ&amp;noteId=H1x7gjxLnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper808 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018</span><span class="item">ICLR 2019 Conference Paper808 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I'm a bit confused by the fact that in sec 3.3, Q is said to be square (satisfy c4). Why is this always satisfied here, because I thought the whole point was that the student might have fewer channels than the teacher. I guess there are a couple of different situations to consider (such as reducing channels, or reducing layers). In the former, the way I understood the training was as following: We start with after the first layer (let's say teacher output has 128 channels and student 64). We construct a Q with shape (64, 128, 1, 1) and and assume the first layer in the student is the same as the teacher. We solve it and absorb Q into this weight for the student. Note, at this point if the student also had 128 channels output, there would be no need to do solve for a Q. Next, we look at the activations of the teacher after the second layer (let's say it's still 128). This is where we run into the first issue I'm not sure how to address, since the weights of the teacher layer is no longer compatible with the student, so we cannot use it anymore. We would have to first absorb the inverse of the previous Q into this weight, to get us back to 128 channels going into that layers. I guess that's when you don't copy weights from the teacher and instead initialize randomly (zero student).

Anyway, a bit more clarity on when you can re-use original teacher weights and when you have to randomly initialize - as well as when Q is square and when Q is non-square. Thanks!</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJgfkfRUhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Three detailed cases how Q is defined</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgDTiCctQ&amp;noteId=BJgfkfRUhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper808 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018</span><span class="item">ICLR 2019 Conference Paper808 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your comments!

First we clarify the initialization problem. In section 3.3, we conduct two sets of experiments. The first set obtains student-net from compressing teacher-net, including filter pruning, network slimming, and network decoupling. The second set fully redesigned the student-net with a different structure from the teacher-net and random initialized the parameters (i.e., zero net in the paper). 
For the first set of experiments, the student-net already has an initialization from original teacher-net's weights. 
For the second set of experiment, we start the student-net with random weights, then use SGD to initialize the student-net using few samples before adopting our FSKD. 

Second, Q is required to be squared in condition-4 (c4) due to two reasons. 
(1) Q must be squared to make current layer and next layer connectable. 
(2) If Q is not squared, it will decrease the compression effect after absorbing Q into previous layer. 
Let's give an example to explain that. 
Suppose current conv-layer is 64*64*k^2 (64 channels in and 64 out, k^2 is the spatial kernel size), and next layer is 64*128*k^2 (64 channels in and 128 out). 
If we set Q being 64*128*1^2, after absorbing, the current layer will be 64*128*k^2, which can't connect to next layer (with size 64*128*k^2).
Besides, it also increases the parameter number and computing cost quite a lot for current layer. 

Third, we list the 3 cases how Q is defined. 
(1) For the fully redesigned student network (zero net), we ensure that the corresponding block-level output channels are matched between teacher and student.  If the block output channel number is n, then Q is a matrix with size n*n. 
(2) For the network decoupling case, the regular convolution and depthwise separable block has the same number of output channels, it is also straightforward to define the size of Q. 
(3) For the  pruning and slimming cases, there are two different sub-cases. 
3a) When there are multi-layers within an alignment block, the student-net may either have less layers or smaller intermediate channels in the block comparing to the teacher-net, but still keep both teacher and student have the same number of output channels for that corresponding block. For instance, the teacher-net has a block with 2 conv-layers (64*64*k^2 followed by 64*128*k^2), the student-net may just have less conv-layers (1 here) in the block as (64*128*k^2). Or the student-net may have smaller number of internal channels in the block as (64*32*k^2, 32*128*k^2), here 32 is the number of  output channels for the first layer and number of input channel for the second layer.  For both examples, Q should be 128*128, so that it could be absorbed into previous conv-layer. 

3b) When we do per-layer alignment, suppose the layer of the teacher-net is 64*128*k^2. After pruning, we have the corresponding layer in student-net as 64*64*k^2. The #channels between teacher and student are different here. We split the 128 channels of teacher-net into two parts, pruned 64 channels and unpruned 64 channels. We make a linear regression between the student-net and the unpruned 64 channels of teacher-net, so that we defined the matrix Q with size 64*64.  For the first layer, the alignment matrix Q is an identical matrix since the unpruned part of teacher-net is copied to the student-net. However, when moving to next layer, for teacher-net, the input information will come from all the 128 channels of previous conv-layer. But for the student-net, the input information will come from only the 64 channels of previous conv-layer.  There is obvious information loss, so that we estimate Q to alleviate this information loss. 
    
We will clarify this in our revision with text explanation and figure illustrations. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SkgVZoJ7h7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A practical method</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgDTiCctQ&amp;noteId=SkgVZoJ7h7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper808 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper808 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this paper, an efficient re-training algorithm for neural networks is proposed. The essence is like Hinton's distillation, but in addition to use the output of the last layer, the outputs of intermediate layers are also used. The core idea is to add 1x1 convolutions to the end of each layer and train them by fixing other parameters. Since the number of parameters to train is small, it performs well with the small number of samples such as 500 samples. 

The proposed method named FKSD is simple yet achieves good performance. Also, it performs well with a few samples, which is desirable in terms of time complexity. 

The downside of this paper is that there is no clear explanation of why the FKSD method goes well. For me, adding 1x1 convolution after the original convolution and fitting the kernel of the 1x1 conv instead of the original kernel looks a kind of reparametrization trick. Of course, learning 1x1 conv is easier than learning original conv because of a few parameters. However, it also restricts the representation power so we cannot say which one is always better. Do you have any hypothesis of why 1x1 conv works so well?



Minor:

The operator * in (1) is undefined.

What does the boldface in tables of the experiments mean? I was confused because, in Table 1, the accuracy achieved by FKSD is in bold but is not the highest one.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SkgvhosGhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>About "Q is degarded to the matrix form"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgDTiCctQ&amp;noteId=SkgvhosGhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Oct 2018</span><span class="item">ICLR 2019 Conference Paper808 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I don't know "Q is degarded to the matrix form". Could you tell what is the specific operation here? Any other references?

Thanks！</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJg7ZjdQh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarification on Q</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgDTiCctQ&amp;noteId=BJg7ZjdQh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper808 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018</span><span class="item">ICLR 2019 Conference Paper808 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your comment!

We use Q to denote the parameters of the added 1x1 conv-layer, the size is nxmx1x1, where n is the input channel number, m is the output channel number, 1x1 is the kernel size. The 4D tensor is then degraded to a matrix with size nxm. Q acts as a linear combination of the input and output channels. For more information about the 1x1 convolution, please refer to [1]. 
To be more specific, suppose Q_{ij} is the element of the matrix  Q at i-th row and j-th column, it reflects the combination coefficient between input channel i and output channel j.  This is how we represent Q as a matrix.

[1] Min Lin, Qiang Chen, and Shuicheng Yan. "Network in network." arXiv preprint arXiv:1312.4400 (2013).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJlpXbFb27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Question about the optimize of loss</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgDTiCctQ&amp;noteId=rJlpXbFb27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">27 Oct 2018</span><span class="item">ICLR 2019 Conference Paper808 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I'm trying to reproduce your results in Sec. 4, but had a question about the the optimize of loss in Sec. 3.3 Algorithm 1: 
1. not use SGD optimize the loss, instead by Algorithm 1, but how does it work well, I can't understand here. What more can you describe?.



Thanks!</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJgOAKO7nQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarification on the optimization process</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkgDTiCctQ&amp;noteId=HJgOAKO7nQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper808 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018</span><span class="item">ICLR 2019 Conference Paper808 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your comment! 

Yes, our problem can be optimized using SGD with loss function defined in Eq(2) . However, we did not use it to report results in the paper. Instead, we estimate the 1x1 conv-layer parameter Q by solving the least squared problem layer by layer sequentially. 

To be more specific, given randomly selected few-samples, we forward the data to the alignment point of the first block in both student network and teacher network, and obtain the feature map responses at this point. Suppose the teacher network response is X^t, and the student-net response is X^s, we obtain Q using X^s and X^t with Eq(1). Then based on our Theorem-1, we absorb the 1x1 conv defined by Q into previous conv-layer. After that, we move to the alignment point of the next block, and repeat this procedure until we reach to the final alignment point. This simple solution works well since the alignment point is before non-linear activation function. Figure-3 shows the block-level correlation before and after alignment between teacher and student, which also demonstrate this effectiveness of this linear approximation. 

We use this procedure instead of the SGD based optimization due to the following reasons. 

(1)  We in fact implement the SGD based solution on the filter-pruning and slimming experiments. But we did not find noticeable results difference between these two solutions. 

(2)  SGD requires tuning several hyper parameters, while our simple solution is hyper-parameter free. We find it is relatively difficult to tune SGD based solution on the network decoupling case due to multi-branch network structures. There are no advantages on time budget over the proposed simple solution.

(3)  Our experiments demonstrates the proposed simple solution works pretty well for the cases, and we also have some figures which illustrates steady accuracy improvement during block-by-block alignment. And we will include that in the revision. 

We will also make our source code available in the near future.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>