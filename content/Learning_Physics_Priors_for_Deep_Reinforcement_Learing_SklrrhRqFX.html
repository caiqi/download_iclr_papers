<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learning Physics Priors for Deep Reinforcement Learing | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learning Physics Priors for Deep Reinforcement Learing" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SklrrhRqFX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learning Physics Priors for Deep Reinforcement Learing" />
      <meta name="og:description" content="While model-based deep reinforcement learning (RL) holds great promise for sample efficiency and generalization, learning an accurate dynamics model is challenging and often requires substantial..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SklrrhRqFX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning Physics Priors for Deep Reinforcement Learing</a> <a class="note_content_pdf" href="/pdf?id=SklrrhRqFX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019learning,    &#10;title={Learning Physics Priors for Deep Reinforcement Learing},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SklrrhRqFX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=SklrrhRqFX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">While model-based deep reinforcement learning (RL) holds great promise for sample efficiency and generalization, learning an accurate dynamics model is challenging and often requires substantial interactions with the environment. Further, a wide variety of domains have dynamics that share common foundations like the laws of physics, which are rarely exploited by these algorithms. Humans often acquire such physics priors that allow us to easily adapt to the dynamics of any environment. In this work, we propose an approach to learn such physics priors and incorporate them into an RL agent. Our method involves pre-training a frame predictor on raw videos and then using it to initialize the dynamics prediction model on a target task. Our prediction model, SpatialNet, is designed to implicitly capture localized physical phenomena and interactions.  We show the value of incorporating this prior through empirical experiments on two different domains – a newly created PhysWorld and games from the Atari benchmark, outperforming competitive approaches and demonstrating effective transfer learning.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Model-Based Reinforcement Learning, Intuitive Physics</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a new approach to pre-train a physics prior from raw videos and incorporate it into an RL framework that allows for better learning and efficient generalization.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Skgq5LTxR7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Added Requested Baselines, Atari Results, Writing Clarifications</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SklrrhRqFX&amp;noteId=Skgq5LTxR7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1538 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1538 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank all the reviewers for the helpful feedback. We have addressed all the comments below as replies to individual reviews. We have also made the following modifications to the revised version of the paper:

a) We have included four additional baselines as suggested by the reviewers including ConvLSTM, imagination augmented agents  (I2A), and two versions of our IPA model: (1) ISP: initialization of a model with weights learned with SpatialNet on the PhysWorld environment -- where we use the convolutional encoding of SpatialNet as input into the policy network, and  (2) JISP: jointly optimizing future frame prediction + environment reward (Section 5.1, Table 2).

b) We emphasize that our experiments are on a stochastic version of Atari, which is more challenging than the benchmark used by previous work [Machado et al., 2017]. We have added results for the entire suite of Atari games (along with standard deviations across runs with different random seeds) in the appendix.

c) We have also added clarifications to the writing suggested by the reviewers, including ego-dynamics (Section 5), descriptions and details of the new baselines (Section 5.1), as well as analysis on their performance compared to our method.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJex19xJam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting Idea, Unclear Writing</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SklrrhRqFX&amp;noteId=SJex19xJam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1538 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1538 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">A method for learning physics priors is proposed for faster learning and better transfer learning. The key idea in learning physics priors using spatial net, which is similar to a convolutional LSTM model for making predictions. Authors propose to improve the sample efficiency of Deep RL algorithms, by augmenting PPO’s state input with 3 future frames predicted by the physics prior model. 

Authors show that using Spatial-Net leads to better prediction of the future as compared to previous methods on simple simulated physics environment and can be incorporated to improve performance on ATARI games. 

(a) I am a bit unclear on how Spatial-Net is trained along with the policy in the IPA architecture. In section 5.1 it is mentioned that, “We train both SpatialNet and the policy simultaneously and use Proximal Policy optimization (PPO) as our model free algorithm”, however earlier in Section 3 it is mentioned that first the agent is pre-trained with prediction and then the pre-trained model is used with the RL algorithm. Can the authors clarify the training procedure? Is it the case that the Spatial-Net is first pre-trained with some data and then fine-tuned along with the environment rewards? Do the policy-net and the frame prediction net share any parameters? 

(b) Is the comparison in Table 2/Figure 5 fair in terms of number of frames seen by the agent? Let a PPO agent see N frames? How many frames does the IPA agent say (both for training spatial Net + Policy). 

(c) How about baselines, where instead of augmenting PPO with any additional frames, the Policy is initialized with weights learned by Spatial Net? Other baseline is to jointly optimize for future frame prediction + environment reward (in this case atleast some parameters between the spatial net and the policy net will be shared), but without augmenting the input state with future predicted frames? 

The Spatial net architecture is similar to convolutional LSTM — and I therefore don’t think that is a significantly novel technical contribution. The application of spatial net to augment frames in the state is although novel in my best knowledge. The above questions will help me understand the experiments better. Right now the method is slightly unclear to me and the results on ATARI (figure 11) are a bit underwhelming. Also, why did the authors chose the specific ATARI games that they reported results on — why not other games too? </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkgxeUagCX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SklrrhRqFX&amp;noteId=SkgxeUagCX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1538 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1538 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the comments. We provide individual responses below. 

(a) In our IPA model, the policy net and frame prediction network do not share any parameters. SpatialNet is fine-tuned to more accurately predict future frames on the new environment while the policy net is optimized for control performance. Policy gradients are not back-propagated to SpatialNet. We have added this clarification in the paper (Section 3.2). Both PPO and IPA agents see identical number of frames, we train the frame prediction network on the frames used to train the policy. We have updated the description in Section 3.2 to make this clearer.

(b) All the tested approaches see exactly the same number of frames on the control environment. SpatialNet sees extra frames from the PhysVideos data, but these are offline, from a different domain and contain objects of different shapes, colors, dynamics from the target control environments (PhysWorld and Atari). The PhysVideo frames are only used to train SpatialNet for dynamics prediction, and not for any policy learning.

(c) Thank you for the suggestions. We have added comparisons with the two baselines: (1) ISP: initialization of a model with weights learned with SpatialNet on the PhysWorld environment -- where we use the convolutional encoding of SpatialNet (z_t) as input into the policy network, and  (2) JISP: jointly optimizing future frame prediction + environment reward (see Section 5.1, Table 2).  We find that initializing with the weights from SpatialNet performs about the same as normal PPO, likely due to much of the initially learned priors being corrupted with reward updates. As for the second baseline, we find that joint training does provide a benefit in performance over PPO, but not as large as IPA (except on PhysForage).

(d)  We first emphasize that our experiments are on a stochastic version of Atari, which is more challenging than the benchmark used by previous work [Machado et al., 2017]. Further, we did perform experiments on all Atari games but didn’t present them all due to lack of space. We have added results for the entire suite of Atari games in the appendix in this revised version. We included the Atari results since they are a standard benchmark to compare with previous approaches -- not all games require an understanding of physical dynamics, which explains the cases where our method does not improve upon PPO. We specifically created PhysWorld and performed empirical studies to test our approach on environments that rely more on understanding basic physics like velocity, collision laws, etc.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1euYXIq2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Comparison with closely related method is necessary</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SklrrhRqFX&amp;noteId=H1euYXIq2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1538 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1538 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary
This paper propose to learning a dynamics model with future prediction in video and using it for reinforcement learning.
The dynamics models is a variants of convolution LSTM and it is trained mean squared error in the future frame.
The way of using dynamics model for reinforcement learning is similar to Weber et al., 2017, where K step prediction of the dynamics model is uses as an augmented input of the policy.

Strength
Training dynamic model to understand physic and using it for reinforcement learning is an interesting problem that worth exploring. This paper tackles this problem and demonstrated experimental setting based on physics games. 

Weakness
The part for understanding dynamics model is very close to existing convolutional LSTM model (Xingjian et al., 2015), which is a popular baseline in video modelling community and how pretrained dynamics model is used for reinforcement learning is similar to Weber et al., 2017, but this paper does not provide comparison to any of these two baseline. 
Since the difference with these existing method is subtle, clear comparison with these method and difference in characteristic is essential to show the novelty of the paper. 

Overall comment
This paper address the interesting problem of understanding dynamics for solving reinforcement learning, but the suggested method is not novel and comparison with existing close methods are not performed.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1gsaHTlR7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SklrrhRqFX&amp;noteId=S1gsaHTlR7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1538 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1538 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the helpful feedback. We have added relevant comparisons to both the imagination augmented agents architecture (I2A) (Table 3) and the ConvLSTM model (Table 1) as baselines. Below, we provide a comparison of our method with each baseline along with empirical results.

ConvLSTM: SpatialNet differs from ConvLSTM in two main ways that allow it to maintain dynamics information more accurately: 1)  the grid states are updated through convolutions instead of LSTM updates (which blur dynamics over time), and (2)  SpatialNet also has a input copy mechanism that add the current state to the output of the spatialnet encoding -- this allows the encoding to focus better on the dynamics.

We trained a ConvLSTM following the specifications in (Xingjian et al., 2015) and also performed some hyperparameter tuning. We find that the ConvLSTM architecture allows for similar 1 step future frame prediction as SpatialNet (our model) -- see Table 1. However, we find that ConvLSTM  is unable to maintain dynamics information over longer horizons and achieves significantly worse multi step future frame prediction (Table 1 and Figure 3). ConvLSTM also does not generalize well to new datasets with smaller and faster objects (Figure 8). SpatialNet, on the other hand, has a much simpler mechanism for capturing state transitions and is able to effectively model physics and generalize better.

I2A: I2A encodes a global context summary of future frames which is fed into a policy while IPA stacks future frames, allowing convolutional filters to encode local dynamics of different objects.

We trained the I2A model following the specifications in [Weber et al., 2017] and also performed some hyperparameter tuning, where SpatialNet is used as a future frame predictor.
In our experiments, we find that I2A performs significantly worse than IPA (our approach) and performs on par with PPO on the PhysWorld environments. By feeding stacked future frames in IPA, we allow convolutions to locally extract information about each individual object to predict its dynamics in the future. In contrast, I2A’s structure only allows global encoding of the future states of objects that makes it difficult for policy to infer the future dynamics of objects and their interactions.

References:
[Weber et al., 2017] Imagination-Augmented Agents for Deep Reinforcement Learning
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1ljcqfW37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting Direction</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SklrrhRqFX&amp;noteId=H1ljcqfW37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1538 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">27 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1538 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Quality: The paper proposed a new method to learn some physics prior in an environment along with a new SpatialNetwork Architecture. Instead of learning a specific dynamics model, they propose to learn a dynamics model that is action-free, purely learning the extrinsic dynamics.  They formulate this problem as a video prediction problem. A series of experiments are conducted on PhysWorld (a new physics based simulator) and a subset of Atari games.
Clarity: The writing is good.
Originality: This work is original as most of the model-based RL works are focusing on learning one environment instead of common rules of physics.
significance of this work: This work propose an interesting direction to pursue.

cons:
1. In Figure 4, the authors show that a pretrained model can learn faster than random initialization. However, it is hard to ablate the factor that causes this effect.  Does the dynamics predictor learn the physics priors or is it just because it learn the visual prior of the shape of the objects, etc? 
2. The baseline for atari games is quite limited. First of all, 3 out of 5  atari games  in the original PPO paper show that ACER performs better than PPO. (asteroid, breakout, DemonAttack). I think it is better to make some improvement upon state-of-the-art methods.
3. All the experiments are shown with only 3 random seeds, without error bar in the main paper. Although the reward plots are shown in Figure 11. 
4. 5 out of 10 atari games are similar to PPO (according to Figure 11). It's hard to be conclusive when half of the experiments are positive and the rest are not. 
5. Lack of discussion about ego-dynamics. There are physics priors for both the environment and the controller. Usually the controller/agent  requires an action to predict its dynamics. Then why should we omit the ego-dynamics and only model the outer world. 
6. Physics prior usually happen in physical environment. The proposed method works well in the physworld environments. But is there some task that are more realistic than atari games that can leverage the power of physics priors more? It's good that this method works in some atari games. But isn't learning the dynamics of atari games a bit off the topic? 
7. The transfer learning experiments should contain a baseline -- maml/reptile. Since you are learning physics prior, it is fair to add meta-learning baselines for comparison.

I think the direction is interesting and the effort is made well. But the experiments are less convincing than the abstract/introduction.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryl_iHpl07" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SklrrhRqFX&amp;noteId=ryl_iHpl07"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1538 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1538 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank Reviewer 1 for the helpful feedback. We provide answers to individual questions below.

(1) We note that the environments in PhysWorld contain objects of different color and shapes than the objects in PhysVideos, the video dataset used for pre-training SpatialNet. As a result, the pretrained model’s notion of shape or color does not have any transferability to the new task, only its knowledge of physical dynamics does.

(2) When comparing average performance across all the Atari benchmark games, PPO is the state of the art approach, competitive with ACER and better than A2C [Schulman et al., 2017]. 

(3) We have included standard deviation values for rewards in in PhysWorld and Atari environments (Table 2,3,6) . Following prior work [Schulman et al., 2017], we use 3 seeds for all our experiments. We also ran extra experiments for the Atari games (5 seeds) and observed similar mean and standard deviation performance (Tables 3 and 6)

(4) We have added in results for the entire suite of Atari games in Appendix A.3 (Table 6). Across all the 49 games, IPA outperforms PPO in 31 games. Not all Atari games require an understanding of physical dynamics, which explains why IPA does not improve upon PPO in those games. We specifically use PhysWorld for this purpose -- to test our approach on environments that rely more on understanding basics physics like velocity, collision laws, etc.

(5) Thank you for this suggestion -- we have added a discussion about ego-dynamics in the paper. Since our approach is to learn physics priors that transfer well to new environments, we don’t learn ego-dynamics, which require the action space of the agent to be input to the model -- this is usually task-specific. The dynamics of the world minus the ego-dynamics is more general and transfers well to new environments. See our comparison for transfer with a “model+policy transfer” baseline in Table 4.

(6) We agree that achieving performance gains on many Atari games are limited by factors other physics such as exploration or reflexive action, which we note maybe the reason we do not achieve universal improvement across all Atari games. However, we believe certain Atari games, such as Asteroids, do benefit from predicting the dynamics of moving rocks, etc., and we do observe substantial gains in such environments. We specifically created PhysWorld and performed empirical studies to test our approach on environments that rely more on understanding various aspects of basic physics like velocity, collision laws, etc.

(7) Our transfer learning experiments (Table 4) test the generalization of a policy from a single source environment to a single target environment. In this scenario, techniques like MAML are not directly applicable since they require meta-learning over multiple different environments to find good initialization points for the policy parameters. We also note that methods like PPO do perform better than approaches like MAML on tasks like the Sonic Benchmark [Nichol et al., 2018].


References:
[Machado et al., 2017] Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents 
[Schulman et al., 2017] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp; Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.
[Nichol et al., 2018] Nichol, A., Pfau, V., Hesse, C., Klimov, O., &amp; Schulman, J. (2018). Gotta Learn Fast: A New Benchmark for Generalization in RL. arXiv preprint arXiv:1804.03720.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>