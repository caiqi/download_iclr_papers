<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Unsupervised Image to Sequence Translation with Canvas-Drawer Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Unsupervised Image to Sequence Translation with Canvas-Drawer Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ByeLBj0qFQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Unsupervised Image to Sequence Translation with Canvas-Drawer Networks" />
      <meta name="og:description" content="Encoding images as a series of high-level constructs, such as brush strokes or discrete shapes, can often be key to both human and machine understanding. In many cases, however, data is only..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ByeLBj0qFQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Unsupervised Image to Sequence Translation with Canvas-Drawer Networks</a> <a class="note_content_pdf" href="/pdf?id=ByeLBj0qFQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019unsupervised,    &#10;title={Unsupervised Image to Sequence Translation with Canvas-Drawer Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=ByeLBj0qFQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=ByeLBj0qFQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Encoding images as a series of high-level constructs, such as brush strokes or discrete shapes, can often be key to both human and machine understanding. In many cases, however, data is only available in pixel form. We present a method for generating images directly in a high-level domain (e.g.  brush strokes), without the need for real pairwise data.  Specifically, we train a ”canvas” network to imitate the mapping of high-level constructs to pixels, followed by a high-level ”drawing” network which is optimized through this mapping towards solving a desired image recreation or translation task.  We successfully discover sequential vector representations of symbols,  large sketches,  and 3D objects,  utilizing only pixel data.  We display applications of our method in image segmentation, and present several ablation studies comparing various configurations.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">image, translation, unsupervised, model-based</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Recreate images as interpretable high-level sequences without the need for paired data.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_r1gzGnYyCX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Revision: New experiments and small changes</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByeLBj0qFQ&amp;noteId=r1gzGnYyCX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper89 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper89 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have updated the paper with an experiment comparing our method to a standard RL algorithm (PPO) on the MNIST recreation task. We show improved performance and training speeds, even when taking account the training time of the canvas network. In addition, we have written a more detailed experimental setup in the Appendix, and updated our Limitations section to more clearly state the current boundaries of our method.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJlNNSUKaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Results reproduction</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByeLBj0qFQ&amp;noteId=rJlNNSUKaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper89 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks a lot for the nice work! 
Found the paper very interesting and promising for tasks I have in hands and therefore was considering to reproduce the results on the dataset given in the paper, as well as my own dataset. However, didn't find in the paper enough data to reproduce it, including detailed network structure, regularizations, optimizers used to train. 
It would be very helpful for reproducing the results sharing the implementation of any of configurations, specified in the paper. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1gbxsKyAQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Experimental details added</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByeLBj0qFQ&amp;noteId=H1gbxsKyAQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper89 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper89 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hey, appreciate the interest! We’ve updated the paper with a new revision, including an appendix that goes over the experimental setup in extended detail.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1x0DGZc2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByeLBj0qFQ&amp;noteId=S1x0DGZc2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper89 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper89 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents an unsupervised method for generating images in a high-level domain (brush strokes and geometric primitives). The proposed system is comprised of two neural networks: the drawer D and a forward model C of an external renderer R. The latter is trained on the rollouts produced by sending random actions to R. The forward model is then freezed and used to train D, i.e., the network that repeatedly interacts (sends commands) with the C to produce a desired image. Since everything is differentiable, D can be optimized via regular gradient descent.

Pros:
+ The paper is written clearly and relatively easy to read
+ The idea to replace the non-differentiable renderer with a differentiable approximation makes sense. Pure RL setups (i.e., in [Ganin et al., 2018]) are quite sample inefficient and hard to train due to high variance of REINFORCE.
+ The proposed method is tested both in 2D (drawings and floor plans) and 3D (prisms) domains and yields relatively good results.

Cons:
- The datasets used in the paper are quite simplistic. I’m wondering how hard it is to train a forward model for more complex data. At the very least, I would want to see how the method handles the 3D experiment when the view is not axis-aligned and there is more variety in the number of primitives and their types.
- The performance of the method especially on drawings and floor plans is not excellent. The drawer tends to reproduce line art with small disjoint strokes - very different from how humans would accomplish this task. A similar observation can be made for floor plans (the system outputs small pixel-like boxes that tile bigger rooms). This suggests that recovered commands do not really correspond to the higher-level structure of the input. Unlike in RL approaches, injection of prior knowledge about the data (e.g., the floor plan should be reproduced using the minimum possible number of rectangles) seems problematic within the proposed framework.
- It’s unclear how to use the approach for non-continuous actions (e.g., choosing types of primitives in 3D or different instruments in music).
- It seems the method may suffer from significant exploration problems in more complex settings. Consider an image of a rectangle that the system should reproduce. Say, it initially outputs a box that doesn’t intersect with the target one. The gradient of l^2-distance between those two images in the pixel space is non-zero but it is zero w.r.t. the action taken since no small change of the action parameters would make the generated box intersect with the target (assuming that the target is far from the model’s output) so l^2 will stay the same.
- I would love to see some comparison (preferably quantitative - speed of training, quality of reconstructions, etc.) to an RL system. So far, in the paper, there is only one figure showing a couple of images produced by such system.

Notes/questions:
* Section 2, paragraph 2: The systems by Xie et al. and Ganin et al. are very distinct. The former models the appearance of a single stroke while the latter is more similar to the present paper and synthesizes the entire image using strokes of a predefined appearance.
* Section 3.3, paragraph 3: “pixel-wise maximum” - it seems to be a fairly restrictive setup which only works when the model increases intensity of pixels.
* Section 3.4: This is a straightforward idea and is not novel (e.g., already used in some demonstrations of the method in [Ganin et al., 2018])
* Section 4.2, paragraph 2: During training, do you use all the patches or randomly sample them? Is your loss computed per patch or for the entire image?

In summary, the paper presents an interesting idea but the execution needs some improvement (especially, in terms of evaluation) before the paper is ready to be accepted to the conference.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BygofiKyRm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>New experiments + Addressed points</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByeLBj0qFQ&amp;noteId=BygofiKyRm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper89 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper89 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, many thanks for the detailed review! We have conducted additional experiments, and address some points below:

“The datasets used in the paper are quite simplistic”: Regarding the 3D experiment, it is true that the experiment is simplistic, however that result is mostly a proof-of-concept that differentiable canvas methods can be extended to higher dimensions if setup correctly. The problem of unsupervised translation through a simulator is still relatively unexplored, and compared to current state-of-the-art methods like SPIRAL that operate over MNIST and Omniglot, we go further and show previously unseen results on detailed sketches.

“The performance of the method especially on drawings … reproduce line art with small disjoint strokes”. This is a fair point, and it is an issue that is inherent to the problem at hand, which we mention in the Limitations section.. Since we optimize for pixelwise accuracy, the drawer network prefers to use small, accurate strokes rather than a few long strokes with higher potential for error. We address this issue in our paper via the introduction of a hierarchical network, and qualitatively the smoothness improves (Figure 9). Potential future improvements to this include utilizing GAN-type loss to encourage natural looking strokes, which we suggest in the Discussion.

“It’s unclear how to use the approach for non-continuous actions”: This is another fair point, and we have revised our Limitations section to include it. Since discrete actions are non-differentiable by nature, integrating them into the canvas-drawer setup would require a non-trivial amount of modifications, which could be considered in future work.

“It seems the method may suffer from significant exploration problems”. It is true that exploration is an inherent issue in optimizing through a simulator, as we see in many RL contexts, and the example you give certainly has a possibility of occurring. In our experiments, however, we found that a proper initialization of the actions can mitigate this issue, as we are able to learn a reasonable drawing policy in the floor-plan rectangle experiment. The Con compared to RL methods is that certain off-policy RL methods can utilize hand-engineered exploration policies, whereas we are limited to actions close to the output of our network -- we have updated our Limitations section to include this statement.

“I would love to see some comparison … to an RL system”. We have run additional experiments, with the same experimental conditions, except using an off-the-box RL system (PPO) to produce the actions. We quantitatively compare pixelwise performance and training times -- Figure 7 contains this updated graph.

Notes/Questions
Section 2, paragraph 2: We have cleared up the distinction between these two methods.
Section 3.3: It is true that the pixel-wise maximum only works when strokes add to the intensity, and we actually encountered this limitation in our colored experiments. To fix the issue, we introduce an alpha layer in the canvas network, and compute the next state via a weighted average -- we have added details in the Appendix regarding this setup.
Section 3.4: Our desired point in this section was that our method is extendable to high-resolution images via position independence, a technique not seen yet in the context of unsupervised image-to-sequence translation. We have updated the section to clarify this.
Section 4.2, paragraph 2: We use all patches, but losses are computed per batch. We have revised the experimental details in the Appendix to mention this.

While there are certainly inherent limitations to our method, we believe our contributions are significant as the idea of a differentiable renderer in the context of sequence generation is relatively unexplored. We have conducted additional quantitative experiments, and show that our method outperforms alternative (RL) in terms of accuracy and sample efficiency.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ByxIZdYF3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Simple and working idea, insufficient evaluations</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByeLBj0qFQ&amp;noteId=ByxIZdYF3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper89 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper89 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This is an interesting paper with simple idea and good results. I like the fact that the authors adopt simple autoencoder-like models instead of GANs or RL.
Following are a couple questions that I am concerned about:
1. Is the ordering of strokes important at all? I suspect that a drawer model that outputs 10 strokes in one pass could perform the same. It might be unnecessary to learn an RNN in this context. Can the authors comment on this?
2. Quantitative evaluations are not well-presented. In Table 1 and Table 2, it is better to normalize pixel wise loss so that the readers could understand the actual error on each pixel.
3. Section 4.3 and 4.4 do not have any quantitative evaluations.
4. How does this system compare with other works, like GANs or RL? Quantitative comparisons are preferred.

The limitation of the proposed approach is also clear: first it is limited to one kind of curves (like a primitive shape in graphics); second it does not learn when to stop, which is already mentioned in the discussion.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1xJrit1RX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>New experiments + Clarifications</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByeLBj0qFQ&amp;noteId=H1xJrit1RX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper89 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper89 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, thanks for the comments! We have conducted additional experiments and address some concerns below:

“Is the ordering of strokes important at all … might be unnecessary to learn an RNN”: This point is discussed in the MNIST/Omniglot and Sketch experimental sections of the paper. We show qualitative and quantitative comparisons of an LSTM network and a non-LSTM network, and the LSTM network outperforms the non-LSTM on a regular basis.

“Quantitative evaluations are not well-presented. In Table 1 and Table 2, it is better to normalize pixel wise loss”: This is a good design change, and we have updated the tables to show normalized L2 loss. In addition, we have performed a new comparison to an off-the-box RL method and show the training curves in a graph on Figure 7.

“Section 4.3 and 4.4 do not have any quantitative evaluations”: In this research, our ablation studies and comparisons are conducted mainly on the MNIST/Omniglot and Sketch experiments, as they are more traditional experimental settings. 4.3 and 4.4 are new problem domains that we developed for this paper, and we can best measure how well our method performs through a qualitative rather than quantitative comparison, as these settings are unconsidered in previous work.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJgte88Knm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The evaluation is weak to show its usefulness, despite a nice idea in the underexplored subject</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByeLBj0qFQ&amp;noteId=HJgte88Knm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper89 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper89 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">To generate a sequence of high-level visual elements for recreation or translation of images, the authors propose differentiable "canvas" networks and "drawer" networks based on convolutional neural networks. One of the main ideas is the replacement of the "canvas" networks instead of non-differentiable "renderer" to end-to-end train the whole model with mean-squared error loss. It seems to be a novel approach to optimize drawing actions. It is reasonable to use separate networks to approximate the behavior of renderer and to fix the parameters of the "canvas" networks to maintain the pretrained rendering capability.

Integrating the high-level visual constructs for recreation or translation of images is to eliminate or attenuate visual artifacts and blurriness, as mentioned in the introduction of the paper. Qualitative comparison with the other state-of-the-art methods is shown in Figure 6f; however, it fails to show significant improvement over them. Quantitative results do not include in the comparison, but only for the ablation study to determine the proposing method. Although the paper proposes an interesting approach to enhance an image generation task, the provided evidence is weak to support the argument, which should be useful for their criteria.

Moreover, experimental details fall short to ensure the validity of experiments. How do you split the dataset as train/val/test? Are the reporting figures (L2 loss) from test results? How are the statistics of the datasets you used?

In Related Work, the authors describe "reinforcement learning methods can be unstable and often depend on large amounts of training samples." Many RL methods use various techniques to stabilize the learning, and this argument alone cannot be the grounding that the supervised approach is better than RL. Unsupervised learning also needs a large amount of data. What is the point of this paragraph (the second paragraph in Related Work)?


Quality: 
  Figure 1-3 are taking too much space, which might lead to exceeding 8 pages. 

Clarity:
  The experimental procedure is not clear. Please clarify the issues mentioned above. It is not hinder to understand the content; however, the writing can be improved by proof-reading and correcting a few grammatical errors.

Originality and significance:
  Using the differentiable "canvas" networks to avoid non-differentiable "renderer" is a novel approach as far as I know. 

Pros:
  Differentiable drawing networks are underexplored in our community.

Cons:
  It failed to show the excellency over pixel-wise generation methods and limited to simple visual elements, line drawings or box generations. This work does not explore "brush strokes" in paintings.


Minor comments:

- In Related Work, the inline citation should be "Simhon &amp; Dudek (2004)" instead of "(Simhon &amp; Dudek, 2004)", and this may apply to the others.

- In Figure 2, the Hint should be x_n, the current state, or target image X for regeneration (X' for translation)?

- In 4.1, a typo, "Out state consists of" to "Our state consists of".</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyxZKiFJCm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>New experiments + Clarification on target comparisons</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByeLBj0qFQ&amp;noteId=SyxZKiFJCm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper89 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper89 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, thanks for the response and comments! Some quick clarifications and fixes:

“Integrating the high-level visual constructs for recreation or translation of images is to eliminate or attenuate visual artifacts and blurriness …. It failed to show the excellency over pixel-wise generation methods”: 
The key aspect of our work is to operate in a space where pixel-based representations fail. In real world use cases, such as engineering, design, and art: pixels are not the general representation that professionals use, and it is hard to edit the results in pixel format. In these fields, 2D and 3D objects are represented as high-level representations, for example, vector-based 2D engineering drawings and 3D solid modeling. While we can compare pixel-wise performance and visual artifacts as a useful metric, recreation of images as sequences is an inherently different task than a pixel-based recreation.

“Quantitative results do not include in the comparison”: We’ve conducted additional experiments, directly comparing with an off-the-shelf reinforcement learning algorithm (PPO), showcasing our improvements in terms of accuracy and training time.

“experimental details fall short to ensure the validity of experiments”: We’ve addressed this in the latest revision, with a section in the Appendix containing the detailed experimental details.

“In Related Work, the authors describe "reinforcement learning methods can be unstable and often depend on large amounts of training samples”: We understand that such a claim can seem ungrounded, and we’ve addressed this in the latest revision by conducting experiments comparing our method vs. a typical RL algorithm on the MNIST example -- see Fig 7.

While our method has clear limitations, we believe our contributions are significant as this work is a step into the field of self-learned image-to-sequence translation, which remains relatively unexplored.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>