<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>DeepTwist: Learning Model Compression via Occasional Weight Distortion | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="DeepTwist: Learning Model Compression via Occasional Weight Distortion" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HJzLdjR9FX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="DeepTwist: Learning Model Compression via Occasional Weight Distortion" />
      <meta name="og:description" content="Model compression has been introduced to reduce the required hardware resources while maintaining the model accuracy. Lots of techniques for model compression, such as pruning, quantization, and..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HJzLdjR9FX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>DeepTwist: Learning Model Compression via Occasional Weight Distortion</a> <a class="note_content_pdf" href="/pdf?id=HJzLdjR9FX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019deeptwist:,    &#10;title={DeepTwist: Learning Model Compression via Occasional Weight Distortion},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HJzLdjR9FX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Model compression has been introduced to reduce the required hardware resources while maintaining the model accuracy. Lots of techniques for model compression, such as pruning, quantization, and low-rank approximation, have been suggested along with different inference implementation characteristics. Adopting model compression is, however, still challenging because the design complexity of model compression is rapidly increasing due to additional hyper-parameters and computation overhead in order to achieve a high compression ratio. In this paper, we propose a simple and efficient model compression framework called DeepTwist which distorts weights in an occasional manner without modifying the underlying training algorithms. The ideas of designing weight distortion functions are intuitive and straightforward given formats of compressed weights. We show that our proposed framework improves compression rate significantly for pruning, quantization, and low-rank approximation techniques while the efforts of additional retraining and/or hyper-parameter search are highly reduced. Regularization effects of DeepTwist are also reported.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">deep learning, model compression, pruning, quantization, SVD, regularization, framework</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a unified model compression framework for performing a variety of model compression techniques.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">13 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HJlozZE9pX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Contributions of this paper (To respond to some reviewers' comments)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJzLdjR9FX&amp;noteId=HJlozZE9pX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper359 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper359 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">To address some concerns of the reviewers, let us summarize some major contributions of this paper.

1. Introducing distortion step
--&gt; Enables 'Exploration vs. Exploitation' to search for a particular local minimum dedicated to model compression. It also reduces the amount of compression computation overhead significantly. Finding a good distortion step is more important than designing complicated distortion functions.
2. Removing lots of hyper-parameters required to set sophisticated prior information. Our Algorithms 1,2, and 3 have the basic compression format, but they are enough to gain very high compression rates (while recent papers introduce more and more hyper-parameters) which have been only possible with stronger prior information, assumptions, and various heuristic approaches.
3. Moreover, we do not modify underlying training algorithms unlike recent papers with heavy 'compression-aware' ideas along with significantly increased training time, which is a general concern on model compression.
4. As a result, model compression researchers can focus on the fundamental representation format after compression with only basic prior information model on the parameters, without a lot of heuristic and hand-crafted hyper-parameters for each dedicated compression technique (that's why Algorithm 1,2, and 3 can be a lot simpler than other papers due to the concept of 'distortion step' and DeepTwist can be a general framework for model compression)
5. We have shown that our local minimum exploration method can lead to a good regularization effect (much improved accuracy especially for RNN)
6. We could achieve a stable retraining procedure with low-rank approximation (with surprisingly high learning rate)</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJe0Kq69h7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A simple repeated compress and fine-tune method.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJzLdjR9FX&amp;noteId=HJe0Kq69h7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper359 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper359 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper does not really propose a new way of compressing the model weights, but rather a way of applying existing weight compression techniques. Specifically, the proposed solution is to repeatedly apply weight compression and fine-tuning over the entire training process. Unlike the existing work, weight compression is applied as a form of weight distortion, i.e. the model has the full degree of freedom during fine-tuning (to recover potential compression errors). 

Pros:

- The proposed method is shown to work with existing methods like weight pruning, low-rank compression and quantization.


Cons:

- The idea is a simple extension of existing work.
- In Table 4, it is hard to compare DeepTwist with the other methods because activation quantization is not used.
  </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1l7nBc4pQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJzLdjR9FX&amp;noteId=H1l7nBc4pQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper359 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper359 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the review.

While the weight formats after model compression follow well known ones, our model compression method is significantly different from the existing ones. Let us discuss some parts of reasons.

- Training models after compression in order to recover accuracy is as important (if not more) as compressing weights. We have found that occasional distortions (not compressing weights for every mini-batch like previous techniques), relatively large learning rate, and training batches in full-precision (unlike previous ones which store compressed weights during entire training) would be the key to recovering or even increasing the accuracy.
- Exploring large search space in much wider area is suggested in this paper through large distortion step and large learning rate (note that many compression-aware techniques perform compression at every batch has distortion step of “1” while much smaller learning rate for retraining that normal training is chosen). As we discussed in the paper, investigating various local minima is crucial for good model compression.
- Our pruning method is fundamentally different from the previous ones because we do not incorporate a masking layer. While previous pruning ideas keep zero weights during training, we do not have any zero weights at any moment except at the weight distortion step.
- Our low-rank approximation is also unique one since 1) we do not alter the structure for training even after performing SVD, 2) very high learning rate associated with transient accuracy loss is allowed for DeepTwist, and 3) we change SV spectrum continuously while the previous ones perform SVD only once (in practice, retraining low-rank approximated model has been considered to be very difficult, if not impossible). 
- Even though our pruning method is even simpler compared to the previous ones, compression rate is significantly better or very close to the one based on sophisticated Bayesian inference model.
- Low-rank approximation results on PTB (Figure 2) shows even higher compression rate compared with weight pruning (Table 3), which is surprising to us because pruning has been known to show much higher compression ratio compared with SVD (fine-grain vs. coarse-grain or structured).
- Quantization is performed also in a very different way. Unlike previous ones, we do not consider quatization during 
 training. “Do not perform quantization at every batch, but instead recover accuracy through full-precision training, high learning rate, and occasional quantization” is the key message.
- Overall, our occasional compression is a significant one since we can greatly reduce amount of computation overhead from compression.

If our technique is a simple extension from the previous ones, we could not obtain such impressive results with high compression rate and improved accuracy. We believe that our paper suggests a wide view on how model compression should be performed.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJlhGPM9hm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The significance of the proposed method is limited</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJzLdjR9FX&amp;noteId=HJlhGPM9hm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper359 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper359 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">A model compression framework, DeepTwist, was proposed which makes the weights zero if they are small in magnitude. They used different model compression techniques in this framework to show the effectiveness of the proposed method. 

This paper proposes a framework intending to use fewer hardware resources without compromising the model accuracy. However, when the weights are set to zero the weight matrix became sparser but still requires the whole weight matrix to be used by the computing resources, as removing some of the weights based on the sorting will not remove a node, only removes some of the connection with that node. Therefore, it is not clear how the proposed framework is helping the model compression techniques.  
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJe2kdKNTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJzLdjR9FX&amp;noteId=HJe2kdKNTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper359 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper359 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the review.

First, we want to mention that DeepTwist is proposed not only for weight pruning, but also for other compression techniques, such as quantization and low-rank approximation, as we discussed in Section 4.2 and 4.3

After weight pruning is performed and zero weights are removed, we usually obtain a sparse matrix to represent non-zero weights. There are lots of existing sparse matrix computation libraries to support SpMV (sparse matrix-vector multiplication) and so on. If a matrix is highly sparse, then we would reduce memory footprint and amount of computations (for example, we can skip zero weights during computation) significantly.
There have been extensive studies of efficient hardware implementation after weight pruning, and we want you to refer to the paper “EIE: efficient inference engine on compressed deep neural network” or “Deep compression: compressing deep neural networks with pruning, trained quatization and Huffman coding.”
In this paper, we have not discussed particular sparse matrix implementation methods which are not our focus in this paper.

We would greatly appreciate if you can reconsider your decision based on our comments and other methods we also discussed (i.e., quantization and low-rank approximation).
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJgUwKHDhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>limited novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJzLdjR9FX&amp;noteId=rJgUwKHDhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper359 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper359 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=rJgUwKHDhX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposed a general framework, DeepTwist, for model compression. The so-called weight distortion procedure is added into the training every several epochs. Three applications are shown to demonstrate the usage of the proposed approach.

Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent. See <a href="http://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/08-prox-grad-scribed.pdf" target="_blank" rel="nofollow">http://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/08-prox-grad-scribed.pdf</a> for a reference.

Specifically, the proposed framework can be easily reformulated as a loss function plus a regularizer for proximal gradient. Using gradient descent (GD), there will be two steps: (1) finding a new solution using GD, and (2) project the new solution using proximal function. Now in deep learning, since SGD is used for optimization, several steps are need to locate reasonable solutions, i.e. the Distortion Step in the framework. Then proximal function can be applied directly after Distortion Step to project the solutions. In this way, we can easily see that the proposed framework is a stochastic version of proximal gradient descent. Since SGD is used for training, several minibatches are needed to achieve a relatively stable solution for projection using the proximal function, which is exactly the proposed framework in Fig. 1.

PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works.  </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1gCk1tNpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJzLdjR9FX&amp;noteId=B1gCk1tNpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper359 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper359 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the review.

While formulating a proximal function for model compression might be an interesting idea (if search space is highly limited) as the reviewer suggested, we believe that our proposed method is fundamentally different from proximal gradient descent approaches due to the following reasons:

1) Proximal gradient descent is meant to solve a convex optimization problem while our aim is to solve a non-convex problem in which each local minimum exhibits vastly different test accuracy after compression. Jumping to another local minimum from a certain minimum would not be easily achieved by convex optimization methods.
2) Finding a particular flat minimum is the key to obtaining good model compression (and good generalization as well). Such an exploration, however, cannot be obtained by a proximal function since we need to investigate lots of different local minima with different amount of flatness in loss surface.
3) While proximal gradient descent can be useful to find a certain local minimum close to the starting point given a convex constraint, wide exploration (associated with possibly transient accuracy loss in the initial training as shown in Figure 2.(b)) is necessary to escape from a point with sharp loss surface.  Investigating many different local minima would be only available with large learning rate (as we have chosen for our experiments) and/or large amount of weight distortion.
4) Our effort to introduce optimal distortion step size and learning rate for a given compression problems is connected to exploration, not exploitation (which potentially supported by proximal functions where convergence matters).

Even though proximal gradient descent selects step size only considering convergence, Figure 1 can lead to the results such as Figure 2(b) which cannot be obtained if only local exploitation is employed.

Finding a flat minimum has been known to be a difficult work as shown in the paper “On large-batch training for deep learning: generalization gap and sharp minima”, ICLR 2016. We firmly believe that our search space exploration method based on optimal distortion step size and amount of weight distortion enable us to produce better local minima well-suited to various model compression techniques.
In short, unfortunately, we have failed to understand how you could connect our technique to proximal functions and proximal gradient descent.
We strongly hope that you reconsider your decision.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rygI3jtN6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for the reply, but I will keep the same rate.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJzLdjR9FX&amp;noteId=rygI3jtN6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper359 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper359 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Below are my answers:

1. Proximal gradient descent can be used for nonconvex optimization. Just google it and you will find many papers on this topic.

2. Let me clarify my review.

The proposed framework can be easily reformulated as a loss function plus a regularizer for proximal gradient. Using gradient descent (GD), there will be two steps: (1) finding a new solution using GD, and (2) project the new solution using proximal function. 

Now in deep learning, since SGD is used for optimization, several steps are need to locate reasonable solutions, i.e. the Distortion Step in the framework. Then proximal function can be applied directly after Distortion Step to project the solutions. 

In this way, we can easily see that the proposed framework is a stochastic version of proximal gradient descent.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJedNc5VaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Why low novelty?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJzLdjR9FX&amp;noteId=BJedNc5VaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper359 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper359 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you so much for rapid response and kind clarification.

We see your point and agree that our technique can follow the form of proximal function.
But overall, we are wondering why the fact that DeepTwist can be formulated as a proximal function should be considered as limited novelty.
Do you have concerns on the numbers of compression rate or accuracy we have shown in the paper?
Or is there a similar work already published previously?
We would greatly appreciate if you can explain why our work shows low novelty if our technique can be explained as proximal gradient descent.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkgGUdiV6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>GD vs SGD</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJzLdjR9FX&amp;noteId=SkgGUdiV6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper359 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper359 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The reason I think this work is of limited novelty is due to the *stochastic* version of proximal gradient descent. This is just another way of solving the optimization problem, which is straightforward.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1emOH0BaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Performing model compression based on proximal gradient descent could be of great novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJzLdjR9FX&amp;noteId=B1emOH0BaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper359 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018 (modified: 14 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper359 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">As we discuss in the paper, previous model compressions have been proposed in a heuristic way while each model compression technique has been developed separately and independently. If there is a straightforward and general ‘optimization’ method which can support different model compression techniques, we believe it should be considered to be of high novelty as the first attempt to solve various model compression issues using a general framework, such as ‘stochastic’ proximal gradient descent or DeepTwist. Since we present such a general optimization method for model compression for the first time associated with state-of-the-art compression rate, your suggestion that our technique can be explained with proximal gradient descent could be a strong claim that our technique solves critical problems in model compression (i.e., heuristic method dedicated to each technique with lots of additional hyper-parameters which cannot be formulated by using proximal function forms). 

1. It has been challenging to find a good prior probability distribution on weights for a particular model compression format. Hence, instead of searching for a regularization form, lots of heuristic ways to obtain prior information on weights have been developed. For example, weight pruning is mainly performed by their magnitude (due to practical computation issues) instead of Hessian computation which can lead to higher compression rate. Our work has shown that a systematic way to perform ‘optimization’ for model compression without such heuristics is possible.

2. All the numbers of accuracy in our paper are obtained after weight distortion, not during normal training with full-precision weights. Our work is the first one that the accuracy after model compression can be achieved by a general framework (such as proximal function), unlike previous model compression which cannot be achieved by a simple and straightforward optimization (instead, all previous works modify training algorithms significantly, like adopting masking layers, temporal full-precision weights for quantization, feedforward and backward paths with different weights, and so on).

3. To present more details, for low-rank approximation, retraining after SVD to enhance accuracy has been a great challenge, hence no proximal functions have existed in the literature. Masking layers which have been essential for pruning also cannot be accommodated by using proximal functions. Our work is the first one in which masking layers are not necessary.

4. We are the first one that even if model compression is optimized by proximal functions, the accuracy still convergences under the constraints for model compression. To the best of our knowledge, there is no any previous papers showing that model compression can be done with a proximal function (or its variants).

In the future, it would be of great importance to further develop and analyze model compression techniques using the knowledge on proximal gradient descent. We believe your suggestion is precious since it is a strong support that our technique is not only useful for model compression but also useful for suggesting a new regularization method based on various model compression formats.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1leNn2YpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>proximal gradient descent is *my* understanding/hypothesis</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJzLdjR9FX&amp;noteId=r1leNn2YpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper359 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper359 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">First of all, we need to be clear that interpreting the proposed approach using proximal gradient descent is *my* understanding/hypothesis, NOT THE CONTRIBUTION OF THIS SUBMISSION AT ALL, even though the authors agreed with it. If the proposed approach was formulated in a proximal gradient descent way and justified by theoretical or empirical evidence, I might raise my rate because this would be very useful to explain why the approach worked in different problems. Unfortunately, the paper in current shape does not meet it.

Secondly, Reviewer 2 has similar questions on the importance of the approach.

In summary, I will keep my rate unchanged at this point.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJgFV7N9a7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJzLdjR9FX&amp;noteId=SJgFV7N9a7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper359 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper359 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the response and your patience.

We absolutely acknowledge that such interpreting is your understanding and we have no any intention to claim anything regarding proximal gradient approximation.

But please understand that because the first comment discusses proximal gradient descent as your major concern, we could not avoid connecting your understanding to the contributions of this paper.
Also please refer to our response above (newly attached) regarding the list of contributions.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>