<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learning Factorized Multimodal Representations | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learning Factorized Multimodal Representations" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rygqqsA9KX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learning Factorized Multimodal Representations" />
      <meta name="og:description" content="Learning multimodal representations is a fundamentally complex research problem due to the presence of multiple heterogeneous sources of information. Although the presence of multiple modalities..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rygqqsA9KX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning Factorized Multimodal Representations</a> <a class="note_content_pdf" href="/pdf?id=rygqqsA9KX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 15 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019learning,    &#10;title={Learning Factorized Multimodal Representations},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rygqqsA9KX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=rygqqsA9KX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Learning multimodal representations is a fundamentally complex research problem due to the presence of multiple heterogeneous sources of information. Although the presence of multiple modalities provides additional valuable information, there are two key challenges to address when learning from multimodal data: 1) models must learn the complex intra-modal and cross-modal interactions for prediction and 2) models must be robust to unexpected missing or noisy modalities during testing. In this paper, we propose to optimize for a joint generative-discriminative objective across multimodal data and labels. We introduce a model that factorizes representations into two sets of independent factors: multimodal discriminative and modality-specific generative factors. Multimodal discriminative factors are shared across all modalities and contain joint multimodal features required for discriminative tasks such as sentiment prediction. Modality-specific generative factors are unique for each modality and contain the information required for generating data. Experimental results show that our model is able to learn meaningful multimodal representations that achieve state-of-the-art or competitive performance on six multimodal datasets. Our model demonstrates flexible generative capabilities by conditioning on independent factors and can reconstruct missing modalities without significantly impacting performance. Lastly, we interpret our factorized representations to understand the interactions that influence multimodal learning.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">multimodal learning, representation learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a model to learn factorized multimodal representations that are discriminative, generative, and interpretable.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_ryeAL1v927" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Multimodal Joint Generative Discriminative Factorization for disentangled representations with good performance and practical application (noise robustness)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygqqsA9KX&amp;noteId=ryeAL1v927"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper556 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper556 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents 'Multimodal Factorization model' that factorizes representations into shared multimodal discriminative factors and modality specific generative factors. This work applies 'Wassertein Auto-Encoders' by Tolstikhin et al (with proofs that this setup works in the multimodal case) for handling factorized joint distributions over the multimodal space. Can this method be considered as a generalization of the wasserstein autoencoder based method with a broader application? - the authors should discuss this more broadly in the paper.

Pros:
- There has been many recent work in the area of disentangling joint representations for improving generative auto-encoding architectures using VAEs, GANs, WAE and some variants of these. This work falls in this category with many interesting experiments showing SOTA generation and discrimination results on several tasks.
- This work is practical due to its robustness to noisy and/or missing data for one or more of the modalities in a multimodal machine learning classification (or generation) problem. Application of this technique for continuous multimodal time series data modeling and prediction for high accuracy requirement applications is very promising.
- The methods seems to be easily portable to other tasks. The authors say that they will make the code available to other researchers.

Cons:
- Some more comparison to other disentangling approaches such as beta-VAE, InfoGAN and partitioned VAE methods would have been useful for understanding the advantages and disadvantages of this techniques. (The authors do add a note about comparison with partitioned VAE method in the Appendix)
- For generation and classification tasks, the authors have chosen the tasks for digit recognition and sentiment analysis - I wonder if the results would hold for other types of multimodal tasks.

Overall the paper is very well-written with many experiments to support the claims.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1gemYFqp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>thank you for your feedback! </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygqqsA9KX&amp;noteId=H1gemYFqp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper556 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper556 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your positive comments and suggestions for improvement. We address your comments and questions below.

[Regarding generalization of the Wasserstein autoencoder] Inspired by Wasserstein autoencoders, we generalize the definition of Wasserstein distance from marginal to joint distributions. This generalization enables us to perform structured prediction for multimodal learning. We believe the generalization can not only be used in multimodal learning but can also be applied in other domains. Will include this discussion in the revised manuscript.

[Comparisons to additional baselines] Thank you for suggesting these additional baselines. For the beta-VAE model, we set the choice of the prior matching discrepancy as the KL-divergence and set beta large enough to encourage disentanglement of the latent variables. We train a beta-VAE to model multimodal data using the same factorization as proposed in our model (i.e. modality-specific generative factors and a multimodal discriminative factor). To provide a fair comparison to our discriminative model, we fine tune by training a classifier on top of the multimodal discriminative factor Z_y to the label. We perform experiments on 4 multimodal datasets. These experimental results have been added to section I of the appendix. MFM outperforms beta-VAE across these datasets and metrics:
- 9.5% improvement on CMU-MOSI
- 25.1% improvement on ICT-MMMO
- 14.1% improvement on YouTube
- 35.9% improvement on MOUD

We have also provided additional experimental results that compare our model with partitioned VAE (Hsu and Glass, 2018) in section H of the appendix. In summary, we obtain:
- 3.9% improvement on CMU-MOSI
- 4.9% improvement on ICT-MMMO
- 3.1% improvement on YouTube
- 24.4% improvement on MOUD

[Other multimodal tasks] We obtain improved results for 3 types of multimodal tasks: multimodal sentiment analysis (CMU-MOSI, ICT-MMMO, YouTube, MOUD datasets), emotion recognition (IEMOCAP dataset) and personality traits prediction (POM dataset). These results are presented in Table 1 (see page 5). Our multimodal factorization model can also be combined with many of the existing discriminative models. In future work we will explore applications of multimodal factorization to various additional tasks, such as Video QA, Image retrieval, etc. We will also release the code so that our model can be adapted to other multimodal tasks.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_r1ldcd85nQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygqqsA9KX&amp;noteId=r1ldcd85nQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper556 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper556 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Multimodality learning is an important topic in multimedia and human computer interaction.  How to efficiently leverage the additional information cross multimodality is the key to the task. Authors proposed the Bayesian latent variable model to factorize the multimodality representation into multimodal discriminative factors and modality-specific generating factors, which is interesting. Approximate inference is also proposed to learn this model via a generalised mean-field assumption. 

The technical quality of the paper is sound and significant, The problem to solve in this paper is also well motivated and important.  In general, this is a well-written paper, 

I have a few minor questions which requires authors for further elaboration.
1. If I understand it correctly, in the current work, the feature Xs are continuous. Does the approach apply to categorical or binary features?

2. In equation(4), MMD is used. How to solve the computation complexity problem since the complexity of MMD is O(n^2)?  It is true that the batch size should be small?  How to select the hyper-parameters of kernels?

 </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rklUotF96Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>thank you for your feedback!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygqqsA9KX&amp;noteId=rklUotF96Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper556 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper556 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your positive comments and suggestions for improvement. We address your comments and questions below.

[Categorical or binary features] Yes, the encoders and decoders can be flexibly chosen for continuous or categorical features (e.g. for text, we can use discrete word tokens, represented as one-hot vectors, and train an embedding layer for task-specific word embeddings.) To model categorical or binary features, we can also choose the suitable distance metric designed for them, for example, Jaccard distance.

[MMD computational complexity] In this work, we adopt the unbiased MMD estimator which has a computational complexity of O(n^2). The complexity can be reduced if we choose a block, linear, or incomplete MMD estimator [1]. For example, if we use a linear MMD estimator, the computation time required is O(n), and in the batch setting, it is O(b) where b is the batch size.

[Batchsize] We found that a batchsize of 32, 64 or 128 works well in our experiments. 

[Hyperparameters] As suggested by WAE, we first set our RBF kernel bandwidth to be sqrt(d), where d is the dimension of latent variables in WAE. Using cross-validation, we also found that setting the bandwidth to 1.0 works well across the datasets we considered.

[1] Makoto Yamada, Denny Wu, Yao-Hung Hubert Tsai, Ichiro Takeuchi, Ruslan Salakhutdinov, Kenji Fukumizu, 2018. Post Selection Inference with Incomplete Maximum Mean Discrepancy Estimator, <a href="https://arxiv.org/pdf/1802.06226.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1802.06226.pdf</a></span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SklvlX672X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice Work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygqqsA9KX&amp;noteId=SklvlX672X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper556 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper556 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors splitted the features of multimodal representations to "common" (multimodal discriminative) and "specific" (modality-specific generative) factors. In this framework, their MFM can capture more detailed features. 

Pros:
(*) Learning the feature representations from two perspectives. 

(*) Even missing one modality, MFM can still achieve acceptable performance. 

(*) Using mutual information and gradient-based method to interpret their method. 

Cons:
(*) The work has some similarity to Hsu &amp; Glass (2018), but the comparison between this work is only on CMU-MOSI.

(*) In Table. 3, it shows that language is the most informative feature for prediction. However, in Table. 2, it can be seen that if audio is missing, the result it the worse compared to the other two cases. It seems the interpretation is not convincing to me. Can you give us more explanation about this phenomenon? 

Comments:
(*) The details of SVHN-MNIST experiment are missing. Appendix B gave some information about models but specified the targeted datasets.

(*) The appendix is not clear, e.g. In Appendix B, it is said "subsection 3.3" but there is no section 3.3.  


</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1eAxcFc67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>thank you for your feedback!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygqqsA9KX&amp;noteId=H1eAxcFc67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper556 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper556 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your positive comments and suggestions for improvement. We address your comments and questions below.

[Comparison with  Hsu &amp; Glass (2018)] We have performed additional experiments between our model and Hsu &amp; Glass (2018) on 3 more multimodal datasets. Our proposed MFM model consistently outperforms the model proposed in Hsu &amp; Glass (2018). In terms of sentiment classification accuracy, we obtain:
- 3.9% improvement on CMU-MOSI
- 4.9% improvement on ICT-MMMO
- 3.1% improvement on YouTube
- 24.4% improvement on MOUD
For more details, please refer to full results in appendix H. We would like to emphasize that our present work started in February, and was performed independently and in parallel to the arXiv submission by Hsu &amp; Glass (2018) in July. 

We also highlight a few key differences: 1) we use an MMD prior matching discrepancy derived from an extension of WAE from marginal to joint distributions over multimodal data and labels, 2) we train a single network architecture to learn factorized representations, 3) we use a generative-discriminative objective function, and 4) we perform experiments over 6 large-scale multimodal datasets across 3 multimodal tasks. For more details, please refer to appendix section H.

[Language and audio] There seems to be some misunderstanding regarding Table 2: we note that having a missing audio modality does not lead to worse prediction performance. Instead, label prediction performance (Y prediction) suffers the most when the language modality is missing. 
Results in terms of binary classification accuracy (see Table 2): 
- Language missing: 62.0%
- Audio missing: 74.3%
- Visual missing: 74.6%
- All present: 78.1%
Similar results hold for other metrics as well. Discriminative performance is most affected when the language modality is missing, which is consistent with prior work which indicates that language is most informative in multimodal setting (Zadeh et al., 2017). On the other hand, sentiment prediction is more robust to missing acoustic and visual features.

[SVHN-MNIST experiments] We will release the code along with the appropriate dataset preprocessing details to avoid any ambiguity. Some details are provided in subsection 3.1, multimodal image datasets: Specifically, SVHN and MNIST are images with different styles but the same labels (digits 0 âˆ¼ 9). We randomly pair 100,000 SVHN and MNIST images that have the same label, creating a multimodal dataset which we call SVHN+MNIST. 80,000 pairs are used for training and the rest for testing. To show that MFM is able to learn improved multimodal representations, we provided both classification and generation results on SVHN+MNIST in Figure 2. We use convolution layers to learn the latent codes from images and deconvolution layers to generate images from the latent codes. We have updated the paper with more details (appendix section E).

[Appendix] We apologize for the typo. The baselines models referred to are in subsection 3.2, paragraph 3 (prediction on multimodal time series datasets). We have updated the paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>