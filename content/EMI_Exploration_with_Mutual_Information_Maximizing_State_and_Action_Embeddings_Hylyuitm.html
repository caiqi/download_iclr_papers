<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>EMI: Exploration with Mutual Information Maximizing State and Action Embeddings | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="EMI: Exploration with Mutual Information Maximizing State and Action Embeddings" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Hylyui09tm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="EMI: Exploration with Mutual Information Maximizing State and..." />
      <meta name="og:description" content="Policy optimization struggles when the reward feedback signal is very sparse and essentially becomes a random search algorithm until the agent stumbles upon a rewarding or the goal state. Recent..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Hylyui09tm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>EMI: Exploration with Mutual Information Maximizing State and Action Embeddings</a> <a class="note_content_pdf" href="/pdf?id=Hylyui09tm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019emi:,    &#10;title={EMI: Exploration with Mutual Information Maximizing State and Action Embeddings},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Hylyui09tm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=Hylyui09tm" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Policy optimization struggles when the reward feedback signal is very sparse and essentially becomes a random search algorithm until the agent stumbles upon a rewarding or the goal state. Recent works utilize intrinsic motivation to guide the exploration via generative models, predictive forward models, or more ad-hoc measures of surprise. We propose EMI, which is an exploration method that constructs embedding representation of states and actions that does not rely on generative decoding of the full observation but extracts predictive signals that can be used to guide exploration based on forward prediction in the representation space. Our experiments show the state of the art performance on challenging locomotion task with continuous control and on image-based exploration tasks with discrete actions on Atari.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">reinforcement learning, exploration, representation learning</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">10 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SJgZ10d5a7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting paper about intrinsic rewards for exploration, via embeddings which improve mutual information</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hylyui09tm&amp;noteId=SJgZ10d5a7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper319 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper319 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=SJgZ10d5a7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes an approach for exploration via reward bonuses based on a form of surprise. The surprise factor is based on the next state of a particular transition, and the error in the embedding space to satisfy a linear dynamics formulation. The embedding space of the states and actions are optimized to increase the mutual-information in predicting next state, and current action - encouraging meaningful embeddings with more training, and hence gradual fading away of the extrinsic rewards.

The paper is mostly well-written, and the idea is interesting. The experimental results do show that the proposed reward augmentation leads to better performing policies, but the claims in the experimental section need to be less strong ("outperforms the baseline by a large margin" - Figure 4 - overlapping error bars; "state of the art" - Figure 5 - again, error bars, and no improvement in some domains.) But overall, I think the paper can be accepted as it is an interesting approach.

Below are some comments that I hope the authors address in their rebuttal, followed by some possible typos in the current draft.

- Theorem 1 content placement: The organization here is rather unclear. Currently, you present Theorem 1, and then talk about using "JSD instead of MI". Maybe this is a last minute mistake. In either case, it is strongly suggested that the section be reworked to be clearer.

- JSD is upper bounded by ln(2); your bounds (7) and (8) would change consequently too.

- Training regime employed:
  3 epochs-512 minibatch -&gt; assuming distinct minibatches are sampled, (512x3) samples used
  Collected (5k*500; sparsehalfcheetah)/(50*500; swimmercatcher)/(100k*4500; atari)
  Is this the sample usage for training? Axis labels for all plots are missing -- specifically scale of x-axis.
  Commenting on the sample complexity -- especially as the embedding network seems easy-to-train (or insufficiently trained), would be good; optimizing a lower-bound insufficiently leads one to doubt if the bound is meaningful at all. Is the huge batch of samples mostly used in TRPO/RL part of the infrastructure?

- Discussing extrinsic rewards: the pros. vs. cons of the two reward formulations, why both are used etc. would be useful.

- Embedding dimension: d=8 in Gravitar and Solaris, but performance is less significant (no significance) in these domains. Is this due to insufficient training?

- RL method: Including details about form of TRPO used in appendix would be good (vine/single-path). Further if entropy regularization is used, how does the exploration interplay work.

- A.2 is an interesting section. A linear dynamics model being effective in MuJoCo tasks seems plausible. But an Atari example is definitely more interesting. Therefore this section can be clearer - specifically distinction between residual error and sample error.

Typos:
- Appendix \lambda parameters unclear
- Appendix step-size information contradictory.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJeLMTzgCm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 4</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hylyui09tm&amp;noteId=SJeLMTzgCm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper319 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper319 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Reviewer,

We would like to thank you for the time and effort spent providing the feedback. We address the questions below and also updated corresponding response in the pdf submission.

1. “Theorem 1 organization”: We reorganized the structure around Theorem 1 in the paper.

2. “JSD is upper bounded by ln(2)”: In f-GAN [1], they actually derived the lower bound of D_{JSD} = D_{KL} (P||M)+D_{KL} (Q||M), instead of D_{JSD} =1/2(D_{KL} (P||M)+D_{KL} (Q||M)). Thus our D_{JSD} formulation coherent with [1] is upper-bounded by log(4). As there was a typo in the upper-bound of D_{JSD}, we fixed it from log(2) to log(4).

3. “Sample usage for training and sample complexity”: At each iteration in the atari environment, 100k (state, action, reward) pairs are sampled as a batch. The number 4500 is the max path length, which means that the length of each episode should not exceed 4500. (When the 4500th state is reached, the game is reset even though the agent does not reach the terminal state.) Thus, if we assume that every episode is reset at 4500th state then we have about 100k / 4500 = 22 episodes in the batch at each iteration. After the batch of 100k samples is collected, we split the batch into minibatches of size 512 and train our network. (We repeat this minibatch training 3 times because epoch is 3.)
For the plot axes, the x-axis is iteration and the y-axis is the mean reward. We clarified the axes in the figure captions.
For the sample complexity, the huge batch is used to train both TRPO policy network and EMI embedding network at approximately the equal amount. It is not the case that the embedding network underfits.

4. “Pros. vs. cons of the two reward formulations”: We propose two different intrinsic reward functions to show that EMI can be utilized with diverse intrinsic reward functions and is not constrained to the certain intrinsic reward function. As RL agents behave differently with different intrinsic reward functions in various environments, we can use EMI with appropriate intrinsic reward functions depending on environments. (Example: 1) Diversity reward gives good performance in SparseHalfCheetah (Mujoco) where the agent should go as far from the origin. 2) Prediction error reward works well in Montezuma’s Revenge (Atari) where the agent can receive high prediction error by entering different rooms.)

5. “Performance is less significant in Gravitar and Solaris with d=8”: We used higher dimension embedding for Gravitar and Solaris because their environment dynamics are much challenging to predict with d=2 linear embeddings. Games like Freeway and Montezuma’s Revenge give discrete actions related to 4-way direction moves in 2D space. However, Gravitar and Solaris have different kinds of action dynamics and consist of visually more complex states. These factors make the performance of Gravitar and Solaris to be less significant compared to other games.

6. “RL method: details about TRPO and entropy regularization”: We added the details about TRPO in appendix. (We use Single path method for TRPO.) The entropy regularization is a technique that is widely used for policy-based RL algorithms. We think that it would be possible to combine these two as entropy regularization seeks to diversify sampled actions and EMI seeks to diversify sampled states.

7. “Section A.2 can be clearer”: We polished Appendix 2 in terms of the distinction between residual errors and sample errors (error terms).

8. “Claims to be less strong”: As RL problems exhibit high variance on the return especially on environments with sparse rewards, we do not agree that one method is not better than the other just because of the overlapping error bars. Our experiments are all performed in sparse reward setting and the return plots in Figure 4 and a subset of Figure 5 show clear advantage of our method over other exploration baselines. Having said that, we moderated the claim a bit.

9. “Typos: lambda, step size”: Thank you for pointing this out. We modified Equation 9 to make it coherent with lambdas in Appendix. TRPO step size is hyper-parameter related to KL divergence in TRPO algorithm which uses conjugate gradients to update policy network.

[1]  Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. In Advances in Neural Information Processing Systems, pp. 271–279, 2016.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1ly5VWY2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review of EMI</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hylyui09tm&amp;noteId=S1ly5VWY2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper319 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper319 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This is a very interesting paper about a novel approach to exploration in agents with state and action representations, making heavy use of recent progress in the use of deep learning for estimating and maximizing mutual information, as well as introducing an approach to model the latent space dynamics with a linear models with sparse errors.

A closely related work which is not mentioned is the work of Thomas et al 2017 arXiv:1708.01289 where they also maximize mutual information between distributed representations of actions (policies, actually) and of distributed representations of changes in the state (as the result of applying the policy).

The phrase 'functionally similar states' is used several times and would require a bit of explanation.

I would also like to see more motivations for the two different reward functions r_e and r_d, and why one should be computed before the update while the other should be computed after.

Regarding the experiments, and this is probably the weakest part of this paper, I would have expected to see comparisons against several of the numerous exploration methods which have been proposed in the past and are discussed in the paper (with many negative comments about their weakness, but no empirical support provided). Only one (EX2) was compared. The comparison with TRPO is without exploration (if I understand well, but should be stated clearly).  It's also not clear how these results compare to the best reported results on these games (whether or not exploration is used).
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Sygal6Me0Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hylyui09tm&amp;noteId=Sygal6Me0Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper319 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper319 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Reviewer,

We would like to thank you for the time and effort spent providing the feedback. We address the questions below and also updated corresponding response in the pdf submission.

1. “Mention the work of Thomas et al 2017”: Thank you for spotting this. We added the work to the related works section.

2. “Motivations for the two different reward functions r_e and r_d, and updating order”: We propose two different intrinsic reward functions to show that EMI can be utilized with diverse intrinsic reward functions and is not constrained to the particular form of intrinsic reward function. As RL agents behave differently with different intrinsic reward functions in various environments, we can use EMI with appropriate intrinsic reward functions depending on environments. (Example: 1) Diversity reward gives good performance in SparseHalfCheetah (Mujuco) where the task is to go as far away from the origin. 2) Prediction error reward works well in Montezuma’s Revenge (Atari) where the agent can receive high prediction error by entering different rooms.)

3. “Comparisons against several of the numerous exploration methods”: We had experiments for both EX2 [1] and ICM [2] on Atari experiments. But for locomotion tasks, we only compared against EX2. This was because ICM was mainly designed for discrete actions. To this end, we extended ICM to accommodate continuous actions (by replacing the cross entropy loss for categorical policy with L2 loss for continuous policy) ran it on continuous locomotion tasks to as another baseline in the experiments section. Also, we added Table 1 to further compare the final performance of EMI against other exploration methods.

[1] Justin Fu, John Co-Reyes, and Sergey Levine. Ex2: Exploration with exemplar models for deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2577–2587, 2017.

[2] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International Conference on Machine Learning, volume 2017, 2017.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1gd_ctS37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Question</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hylyui09tm&amp;noteId=S1gd_ctS37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018</span><span class="item">ICLR 2019 Conference Paper319 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I enjoyed your paper, and I have a question.

At the beginning of page 6, you mentioned that 'the relative diversity term should be computed after the representations are updated ... while the prediction error term should be computed before the update.'

Could you explain the reason why the prediction error term should be computed before the update in detail? 
For example, what if we calculate the prediction error term after the update, too?

I think readers may wonder the clear reason for this. :)</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1xxOpzeR7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Public Comment 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hylyui09tm&amp;noteId=r1xxOpzeR7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper319 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper319 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for your thoughtful comments. We address your questions below.

The relative diversity term is computed after the update because we desire to give high intrinsic rewards to states that are far away from common states in learned embedding space. However, the prediction error term should be computed before the update because we desire to give high intrinsic rewards to states that give more surprise (high prediction error). If we compute the prediction error term after the update, then novel states would not give high intrinsic reward because the update reduces prediction error already.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SkxSVV7AjQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review of EMI</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hylyui09tm&amp;noteId=SkxSVV7AjQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper319 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper319 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper introduces actions as a co-predictor of next-states and the predicted (from current and next state) in the context of (model-based) RL. In addition they incorporate the idea of using a JSD-based objective do prediction (as the Deep InfoMax paper), which is novel to RL. The enforce a linear structure between current / next states and actions with an additional sparse nonlinear term computed from both current states and actions. From this, they are able to quantify the amount of novelty in the representation space as a measure of exploration, which can be used as an intrinsic reward.

I found the paper to be very well-written and easy to understand. The prediction part is similar to that used in CPC structurally, except they include the action in two different prediction tasks and they have some built-in intrinsic rewards, which is good.

I had some issues with the motivations of some of the loss functions. 
- The JSD-based objective makes sense, but I don't think it's correct to call it an "approximation" to the KL (this is only true where the log-ratio of the joint and the product of marginals is small). Rather, it would be better to describe this choice as simply using a different measure between the joint and marginals.
- It seems like the best motivation for having linear relations is you can do multiple predictions using the same state / action encodings.
- For measuring exploration (11) couldn't one just use the predictor models T? How does the output of T (perhaps correctly normalized with the marginals) correlate with (11)?

Other notes:
Page 2:
Figure 1 is awfully confusing. Could this be clarified a little bit? I’m not sure what the small dots or their colors are supposed to represent.

Could diversity also be added by adding a prior to the state representations (as is done in Deep InfoMax)?

Why were the vision experiments stopped at 500 x 100k (500 million) frames?  I can’t validate the SOTA claims, but it seems like the model is still improving: are there’s further experiments?

An ablation study would be nice comparing the different hyper parameters (intrinsic rewards, diversity, etc).</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJlNCnMxA7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hylyui09tm&amp;noteId=SJlNCnMxA7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper319 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper319 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Reviewer,

We would like to thank you for the time and effort spent providing the feedback. We address the questions below and also updated corresponding response in the pdf submission.

1. “JSD as an "approximation" to the KL”: We agree that it would be better not to state JSD as “approximation” to the KL. We modified our statement to call it “different measure” in section 4.1.

2. “Using predictor models with output of T”: This is a valid suggestion, as by the mutual information maximizing objective, the network T is encouraged to output smaller values for novel samples. We ran experiments in SparseHalfCheetah environment, and were able to confirm some degree of exploration effect of the intrinsic rewards derived by the outputs of the network T. Although the results are not comparable to our proposed method (average return of 75 vs. 200 after 1,000 iterations in SparseHalfCheetah), it will be an interesting future research direction.

3. “Figure 1 is awfully confusing”: Sorry for the confusion. We updated Figure 1 and hope it clarifies the big picture.

4. “Adding a prior to the state representations”: Regularizing the distribution of state embeddings instead causes the optimization process to be much more unstable. This is because the distribution of states is much more likely to be skewed than the distribution of actions, especially during the initial stage of optimization, so the Gaussian approximation of the distribution of state embeddings in the KL regularization term becomes much less accurate in contrast to the distribution of actions. We added this statement to section 4.2.

5. “Vision experiments stopped at 50 frames”: We stopped at 50 million frames to perform fair comparisons to other baseline methods. TRPO-based exploration methods such as EX2 [1], SimHash [2] stopped at 50 million frames. 

6.” An ablation study would be nice”: We added ablation study in Appendix. In figure 7, We ablated on each loss terms of Equation 9 to show the impact of each term. In figure 8, we tested on different intrinsic reward hyper-parameters to show the robustness of our method.

[1] Justin Fu, John Co-Reyes, and Sergey Levine. Ex2: Exploration with exemplar models for deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2577–2587, 2017.

[2] Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman, Filip DeTurck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep reinforcement learning. In Advances in Neural Information Processing Systems, pp. 2753–2762, 2017.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HkxsYJGlcQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting paper.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hylyui09tm&amp;noteId=HkxsYJGlcQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Oct 2018</span><span class="item">ICLR 2019 Conference Paper319 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">You combine 1. an idea for learning representations and 2 . an idea for factorizing the dynamics into a linear term plus a sparse irreducible error term.

It would be informative to ablate these. As it stands we have no information as to which was important. 

In addition you should not claim these results are state of the art, there are much better results on montezuma's revenge for example.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1xhrazgAQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Public Comment 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hylyui09tm&amp;noteId=S1xhrazgAQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper319 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper319 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for your thoughtful comments. We address your questions below.

We added the ablation study section in Appendix. In figure 7, We ablated on each loss terms of Equation 9 to show the impact of each term. Also, we agree that there are other great results in Atari domains, but in our paper, we claim SOTA among TRPO-based exploration methods.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>