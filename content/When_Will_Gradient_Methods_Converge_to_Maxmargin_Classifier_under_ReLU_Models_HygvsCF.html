<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>When Will Gradient Methods Converge to Max-margin Classifier under ReLU Models? | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="When Will Gradient Methods Converge to Max-margin Classifier under ReLU Models?" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Hygv0sC5F7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="When Will Gradient Methods Converge to Max-margin Classifier under..." />
      <meta name="og:description" content="We study the implicit bias of gradient descent methods in solving a binary classification problem over a linearly separable dataset. The classifier is described by a nonlinear ReLU model and the..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Hygv0sC5F7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>When Will Gradient Methods Converge to Max-margin Classifier under ReLU Models?</a> <a class="note_content_pdf" href="/pdf?id=Hygv0sC5F7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019when,    &#10;title={When Will Gradient Methods Converge to Max-margin Classifier under ReLU Models?},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Hygv0sC5F7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We study the implicit bias of gradient descent methods in solving a binary classification problem over a linearly separable dataset. The classifier is described by a nonlinear ReLU model and the objective function adopts the exponential loss function. We first characterize the landscape of the loss function and show that there can exist spurious asymptotic local minima besides asymptotic global minima. We then show that gradient descent (GD) can converge to either a global or a local max-margin direction, or may diverge from the desired max-margin direction in a general context. For stochastic gradient descent (SGD), we show that it converges in expectation to either the global or the local max-margin direction if SGD converges. We further explore the implicit bias of these algorithms in learning a multi-neuron network under certain stationary conditions, and show that the learned classifier maximizes the margins of each sample pattern partition under the ReLU activation.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">gradient method, max-margin, ReLU model</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We study the implicit bias of gradient methods in solving a binary classification problem with nonlinear ReLU models.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SJg80NOj2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A theoretical paper with very stringent assumptions.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hygv0sC5F7&amp;noteId=SJg80NOj2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper903 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper903 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper considers the binary classification problem with exponential loss and ReLu activation function (single neuron). The authors characterize the asymptotic loss landscape by three different types of critical points. They prove that gradient descent (GD) will result in four different regions and provide convergence rates for GD to converge to an asymptotic global minimum, asymptotic local minimum and local minimum under certain assumptions. The authors also provide convergence results for stochastic gradient descent (SGD) and provide extensions to leaky ReLu activation and multi-neuron networks. The paper is well written and the results are mostly clearly presented. This paper mostly follows the line of research by Soudry et al. (2017, 2018), while it has its own merit due to the ReLu activation function considered. However, there are many strong assumptions that are not carefully verified and I really have concerns about the contribution of this paper since they simplify their analysis and results merely by imposing stringent conditions. In particular, I have the following major comments about the paper:

1.	In the definition of max-margin direction, why you use \argmin_{w} max_{i} (w^{\top}x_i)? It seems to me that the definition should be \argmax_{w} min_{i} (w^{\top}x_i). This definition keeps appearing in multiple places in the main paper. 
2.	In the proof of Theorem 3.2, I am confused by the argument of the case that \hat w^{+} is not in the linearly separable region. More clarification is needed to make the proof rigorous.
3.	In the analysis of Theorem 3.3 and 3.4, the authors make a very stringent assumption that the iterate w_t staying in linear separable region for all t&gt;\mathcal{T}. This assumption seems too strong, which should be verified rather than imposed in analysis of SGD. Note that even the example shown in Proposition 2 is still very restrictive (you require all the positive examples or negative examples are very close to one another).
4.	Furthermore, in the analysis of SGD, the authors did not specify the assumption that \hat w^{+} lies in the linear separable region, which is also required in this theorem and also very strong. Given such strong assumptions, the analytic results seem to be trivial and it is hard to evaluate the authors’ contribution.  
5.	For the convergence results of SGD, the current rate is derived on the distance between \|E[w_t] - \hat{w}\|^2. Can you provide similar results for mean square error (E\| w_t - \hat{w} \|^2)? 
6.	In multi-neuron case, the authors again make very strong assumptions that all the neurons have unchanging activation status. This is not easily achievable without careful characterization or other rigorous assumptions. Under such strong assumptions, the extension to multi-neuron again seems not very meaningful.

Other minor comments:
1.	The references are not correctly cited. For instance, please correct the use of parenthesis in “… which is different from that in (Soudry et al., 2017, Corollary 8)” and “… hold for various other types of gradient-based algorithms Gunasekar et al. (2018)”.
2.	The sentence “…, which the nature of convergence is different from …” does not read well. Should it be “where” or “of which”?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HkgRSG1t3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Importance of ReLU networks and max-margin used in this paper are unclear.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hygv0sC5F7&amp;noteId=HkgRSG1t3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper903 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper903 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Recently, the implicit bias where gradient descent converges the max-margin classifier was shown for linear models without an explicit regularization.
This paper tries to extend this result to ReLU network, which is more challenging because of the non-convexity.
Moreover, a similar property of stochastic gradient descent is also discussed.

The implicit bias is a key property to ensure the superior performance of over-parameterized models, hence this line of research is also important.
However, I think there are several concerns as summarized below.

1. I'm not sure about the significance of the ReLU model (P) considered in the paper.
Indeed, the problem (P) is challenging, but an obtained model is linear defined by $w$.
Therefore, an advantage of this model over linear models is unclear.

Moreover, since the max-margin in this paper is defined by using part of dataset and it is different from the conventional max-margin, the generalization guarantees are not ensured by the margin theory.
Therefore, I cannot figure out the importance of an implicit bias in this setting (, which ensures the convergence to this modified max-margin solution).
In addition, the definition of the max-margin seems to be incorrect: argmin max -&gt; argmax min.

2. Proposition 1 (variance bound) gives a bound on the sum of norms of stochastic gradients.
However, I think this bound is obvious because stochastic gradients of the ReLU model (P) are uniformly bounded by the ReLU activation.
Combining this boundedness and decreasing learning rates, the bound in Proposition 1 can be obtained immediately.
Moreover, the validity of an assumption on $w_t$ made in the proposition should be discussed.

3. Lemma F.2 is key to show the main theorem, but I wonder whether this lemma is correct.
I think the third equation in the proof seems to be incorrect.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkx8w0vw3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hygv0sC5F7&amp;noteId=rkx8w0vw3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper903 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper903 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies ReLU model, or equivalently, one-layer-one-neuron model, for the classification problem. This paper shows if the data is linearly separable, gradient descent may converge to either a global minimum or a sub-optimal local minimum, or diverges. This paper further studies the implicit bias induced by GD and SGD and shows if they converge, they can have a maximum margin solution. 

Comments:
1. Using ReLU model for linearly separable data doesn't make sense to me. When ReLU is used, I expect some more complicated separable condition. 
2. This paper only studies one-layer-one-neuron model, which is a very restricted setting. It's hard to see how this result can be generalized to the multiple-neuron case.
3. The analysis follows closely with previous work in studying the implicit bias for linear models.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>