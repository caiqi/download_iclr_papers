<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1l6qiR5F7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Ordered Neurons: Integrating Tree Structures into Recurrent Neural..." />
      <meta name="og:description" content="Recurrent neural network (RNN) models are widely used for processing sequential data governed by a latent tree structure. Previous work shows that RNN models (especially Long Short-Term Memory..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1l6qiR5F7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks</a> <a class="note_content_pdf" href="/pdf?id=B1l6qiR5F7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019ordered,    &#10;title={Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=B1l6qiR5F7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=B1l6qiR5F7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Recurrent neural network (RNN) models are widely used for processing sequential data governed by a latent tree structure. Previous work shows that RNN models (especially Long Short-Term Memory (LSTM) based models) could learn to exploit the underlying tree structure. However, its performance consistently lags behind that of tree-based models. This work proposes a new inductive bias Ordered Neurons, which enforces an order of updating frequencies between hidden state neurons. We show that the ordered neurons could explicitly integrate the latent tree structure into recurrent models. To this end, we propose a new RNN unit: ON-LSTM, which achieve good performances on four different tasks: language modeling, unsupervised parsing, targeted syntactic evaluation, and logical inference.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Deep Learning, Natural Language Processing, Recurrent Neural Networks, Language Modeling</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We introduce a new inductive bias that integrates tree structures in recurrent neural networks.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Bygp1Apv2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A very interesting new proposal, thoroughly explored, with at least middling good results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1l6qiR5F7&amp;noteId=Bygp1Apv2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper574 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper574 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Quality
 - Pro:
   o This paper was in general a quality effort. It had a thorough bibliography of both older and recent relevant research contributions
   o Providing useful, well done experimental results on four tasks was also a sign of this good thoroughness
 - Con: none observed

Clarity
 - Pro:
   o The paper was generally well-written and clear. Results were clearly presented.
 - Con:
   o Notwithstanding the half page of explanation of the intuition behind the new ON-LSTM update rules (top of p.5), it wasn't really enough for my old brain to get a good sense of what was going on – though I'm sure younger, smarter people will have made more sense of it. :) It would really help to try to provide more intuition and understanding here. Things that would probably really help include a worked example and diagrams.
   o There were minor English/copyediting problems, but nothing that interfered with understanding. E.g., "monotonously" on p.4 should be "monotonically" (twice).

Originality
 - Pro
   o This was REALLY NEAT! This paper had a real, clear, different idea that appeared interesting and promising. That puts it into the top half of accepted papers right there.
   o The basic idea of the different update time scales, done flexibly, controlled by the master forget/input gates seemed original, flexible, and good.
 - Con: Nothing really observed; there are clearly a bunch of slightly related ideas, well referenced in this paper.

Significance
 - Pro
   o If this idea pans out well, it would be a really interesting new structural prior to add to the somewhat impoverished vocabulary of successful techniques for building deep learning systems.
   o Has an original, promising approach. That has the opportunity for impact and significance.
 - Con:
   o The results so far are interesting, and in places promising, but not so clearly good that this idea doesn't need further evaluation of its usefulness.
   o All the results presented are on small datasets (Penn Treebank WSJ (1 million words) size or smaller). What are the prospects on bigger datasets?  It looks like in principle this shouldn't be a big obstacle – except for not having a highly tuned CuDNN implementation, it looks like this should basically be fairly efficient like an LSTM and not hard to scale like, e.g., an RNNG.

Other comments:
 - Some of the wording on page 1 seemed strange to me. Natural language has a linear overt form as spoken and (hence) written. It's really not that the sequential form is just how people conventionally "present" it. That is, it's not akin to a chemical compound which is really 3 dimensional but commonly "presented" by chemists in a convenient sequential notation.
 - p.2 2nd paragraph: Don't RNNs "explicitly impose a chain structure" not "implicitly"?!?
 - I wasn't sure I was sold on the name "Ordered Neurons". I'm not sure I have the perfect answer here, but it feels more like "multi-timescale units" is what is going on.
 - The LM results look good.
 - Because of all the different datasets, etc. it was a little hard to call the grammar induction results, but they at least look competently strong.
 - The stronger results on long dependencies in targeted syntactic evaluation look promising, but maybe you need a bigger hidden size so you can also do as well on short dependencies?
 - The logical inference results were promising – they seem to suggest that you capture some but not all of the value of explicit tree structure (a TreeLSTM) on a task like this.
 - The tree structures in Appendix A look promisingly good.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">9: Top 15% of accepted papers, strong accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyeNaVYxCm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for your review and kind comments. We have made some modifications to the paper based on some of your feedbacks.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1l6qiR5F7&amp;noteId=SyeNaVYxCm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper574 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper574 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Regarding the “hidden size” for targeted syntactic evaluation.
For fair comparison, we kept the number of parameters of our model comparable with the baseline. We also tried bigger hidden sizes and more layers. We observe that increasing the capacity doesn’t improve the performance on the task. This maybe due to overfitting. Since the training set and test set don’t share the same data distribution, overfitting the training set doesn’t necessarily provide better results on the test set.

Regarding the “logical inference results”. 
Understanding the causes of that gap and investigating to what extent we can fill that gap is an important future direction. The advantages of the TreeLSTM over the ON-LSTM are 1) TreeLSTMs have access to the true structure; 2) TreeLSTMs reuse weights across the compositional processes related to non-terminal nodes, while the ON-LSTMs don’t have shared weights across different levels in the tree. For this task, the test set contains data with deeper tree structures than those seen during training. The weight sharing feature in TreeLSTMs may be beneficial in order to achieve better generalization.

Regarding the wording in the introduction. 
Thank you for the valuable suggestions. We have reworded the introduction to make it clear that the overtly sequential form is an essential characteristic for natural language, not just a conventional presentation format. In addition, we have changed the sentence to say that RNN explicitly imposes a chain structure.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BkgiNwT7h7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Solid contribution</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1l6qiR5F7&amp;noteId=BkgiNwT7h7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper574 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper574 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a new RNN unit: ON-LSTM. The idea is to explicitly integrates the latent tree structure into recurrent models. Experiments are conducted to evaluate performances on four different tasks: language modeling, unsupervised parsing, targeted syntactic evaluation, and logical inference. Good results on unsupervised parsing show that the model learns something close to human judgments of the sentence parses.

The paper is clearly written, and the experiments seem planned well.
The language modeling results are not state-of-the-art, but the unsupervised parsing results of layer 1 are quite impressive. The analyses are reasonable.

Overall, the paper seems worthy of being accepted.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Skxnyrtx0m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for the review and comments.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1l6qiR5F7&amp;noteId=Skxnyrtx0m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper574 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper574 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">It is true that our language modeling results are not state-of-the-art. The ordered neurons primarily focuses on inducing the latent structure of sequential data. We wanted to demonstrate that the model is able to give good parsing results in the acceptable range in terms of perplexity. One of the future research direction is trying to improve current SOTA models with the ordered neurons.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1ewsJDcjm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Promising approach for adding a hierarchical inductive bias to LSTMs</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1l6qiR5F7&amp;noteId=H1ewsJDcjm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper574 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">22 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper574 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Language is hierarchically structured: smaller units (e.g., noun phrases) are nested within larger units (e.g., clauses). This is a strict hierarchy: when a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. While the different units of an LSTM can learn to track information at different time scales, the standard architecture does not impose this sort of strict hierarchy. This paper proposes to add this constraint to the system by ordering the units; a vector of "master" input and forget gates ensures that when a given unit is reset all of the units that follow it in the ordering are also reset.

Strengths:
* The paper introduces an elegant way of adding a hierarchical inductive bias; the intuition behind this idea is explained clearly.
* The evaluation tasks are very sensible. It's good that the model is shown to obtain good perplexity and slightly improve over an LSTM baseline; it's not the state of the art, but that's not the point of the paper (in fact, I would emphasize that even more than the authors do). The unsupervised parse evaluation (Table 2) is the heart of the paper, in my opinion (and should probably be emphasized more) -- the results from the second layer are quite impressive.
* The (mildly) better performance than LSTMs on long-distance dependencies, and (mildly) worse performance on local dependencies, in the Marvin &amp; Linzen dataset, is interesting (and merits additional analysis).

Weaknesses:
* The discussion of the motivation for unsupervised structure induction in the introduction is somewhat confused. I am not sure that neural networks with latent syntactic structures can really address the seemingly very fundamental question mentioned in the first paragraph (whether syntax is related to "an underlying mechanism of human cognition") - I would suggest eliminating this part. At the same time, the authors might want to add another motivation for studying architectures that discover latent structure (as opposed to being given that structure) - this setting corresponds more closely to human language acquisition, where children aren't given annotated parse trees.
* The authors discuss hierarchy in terms of syntactic structure alone, but it would seem to me that the hierarchy that the LSTM is inducing could just as well include topic shifts, speech acts and others, especially if the network is trained across sentences.
* There is limited analysis of the model. Why does the second layer show better unsupervised parsing performance than the third layer? (Could this be related to syntactic vs. semantic/discourse units I mention in the previous bullet?) Why is the model better at ADJP boundaries than NP boundaries? It would have been more useful to report less experiments but analyze the results of each experiment in greater depth.
* In this vein, I am not sure it's useful to include WSJ10 in Table 2, which is busy as it is. These sentences are clearly too easy, as the right branching baseline shows, and require additional POS tagging.
* I found it difficult to read Figure A.2: could you help us understand what we should take away from it? 
* It is not entirely clear why the model needs both unit-specific forget/input gates and the "master" forget/input gates, and there is no discussion of this issue. Have you tried using only the "master" gates?

Minor notes:
* RNNGs are described as having an explicit bias to model syntactic structure; this is an arguably confusing use of the word "bias", in that the architecture has a hard constraint enforcing syntactic structures (bias implies a soft constraint).
* There are some language issues: agreement errors (e.g. "have" in the sentence that starts with "Developing" in the introduction), typos ("A order should exist", "co-occurance"), determiner issues ("values in [the] master forget gate", "when the overlap exists") - I would suggest going through and copy editing the paper.
* "cummax" seems like a better choice of name for cumulative maximum than "cumax".
* It may be helpful to remind the reader of the update equation for c_t in a standard LSTM.
* Did the language model have 1150 units in each layer or in total? Why did you use exactly three layers? Did you try one, two and four?
* It's not clear if the results in Table 2 reflect the best seed out of five (as the title of the column "max" indicates) or the average (as the caption says).
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkgp-SFxRQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for the review and comments. We have made some modifications to the paper based on some of your feedback.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1l6qiR5F7&amp;noteId=Bkgp-SFxRQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper574 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper574 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Regarding “The discussion of the motivation for unsupervised structure induction in the introduction is somewhat confused”. 
Thanks for this suggestion. We have modified our introduction accordingly. 

Regarding “the author discuss hierarchy in terms of syntactic structure alone”. 
It’s possible that the learned hierarchy also reflects topic and other structures. However, to our knowledge, it maybe hard to quantitatively measure whether the model captures such semantic level structures. We will further study the relationship between the induced structure and semantic units.

Regarding “Why does the second layer show better unsupervised parsing performance than the third layer?”. 
Our hypothesis is that the first and last layer focus on low level or short term information, while the middle also include longer term information. Similar results can be found in [1].
[1]Blevins, Terra, Omer Levy, and Luke Zettlemoyer. "Deep RNNs Encode Soft Hierarchical Syntax." arXiv preprint arXiv:1805.04218 (2018).

Regarding “Have you tried using only the "master" gates?“
We did try this. We found that for better language modelling performance, we still needed to use the unit specific gates. The unsupervised parsing capability of a master-gate-only model was similar.

Regarding “Did the language model have 1150 units in each layer or in total? Why did you use exactly three layers? Did you try one, two and four?”
The main reason for these choices was to compare with the AWD-LSTM model, so we followed the hyperparameters used there as closely as possible. The first and second layer uses 1150 units, the last layer has 400 units. Having tried the one and two layer settings, we find that the hyperparameters do not result in similar parsing performance. 


Regarding “It's not clear if the results in Table 2 reflect the best seed out of five”
The “max” column provides the best result among different random seeds, while the “µ (σ)” column provides the mean and variance.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>