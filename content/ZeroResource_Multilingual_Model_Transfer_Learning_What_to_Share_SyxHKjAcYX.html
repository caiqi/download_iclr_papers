<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Zero-Resource Multilingual Model Transfer: Learning What to Share | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Zero-Resource Multilingual Model Transfer: Learning What to Share" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SyxHKjAcYX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Zero-Resource Multilingual Model Transfer: Learning What to Share" />
      <meta name="og:description" content="Modern natural language processing and understanding applications have enjoyed a great boost utilizing neural networks models. However, this is not the case for most languages especially..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SyxHKjAcYX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Zero-Resource Multilingual Model Transfer: Learning What to Share</a> <a class="note_content_pdf" href="/pdf?id=SyxHKjAcYX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 13 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019zero-resource,    &#10;title={Zero-Resource Multilingual Model Transfer: Learning What to Share},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SyxHKjAcYX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=SyxHKjAcYX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Modern natural language processing and understanding applications have enjoyed a great boost utilizing neural networks models. However, this is not the case for most languages especially low-resource ones with insufficient annotated training data. Cross-lingual transfer learning methods improve the performance on a low-resource target language by leveraging labeled data from other (source) languages, typically with the help of cross-lingual resources such as parallel corpora. In this work, we propose a zero-resource multilingual transfer learning model that can utilize training data in multiple source languages, while not requiring target language training data nor cross-lingual supervision. Unlike most existing methods that only rely on language-invariant features for cross-lingual transfer, our approach utilizes both language-invariant and language-specific features in a coherent way. Our model leverages adversarial networks to learn language-invariant features and mixture-of-experts models to dynamically exploit the relation between the target language and each individual source language. This enables our model to learn effectively what to share between various languages in the multilingual setup. It results in significant performance gains over prior art, as shown in an extensive set of experiments over multiple text classification and sequence tagging tasks including a large-scale real-world industry dataset.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">cross-lingual transfer learning, multilingual transfer learning, zero-resource model transfer, adversarial training, mixture of experts, multilingual natural language understanding</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A zero-resource multilingual transfer learning model that requires neither target language training data nor cross-lingual resources.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_S1eFzjDlAX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reviewers: Please consider author response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxHKjAcYX&amp;noteId=S1eFzjDlAX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper440 Area Chair1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper440 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Reviewers,

The authors have posted a response, and I'd appreciate if you could take a look at it and see if it has addressed any of your concerns. Thank you!</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJe0M38JpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A missing citation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxHKjAcYX&amp;noteId=SJe0M38JpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Jiateng_Xie1" class="profile-link">Jiateng Xie</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper440 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear authors, we would like to point out that a citation to our recent paper at EMNLP '18 that does cross-lingual NER under the same setting as Section 4.2 and achieves slightly better results (at the time of this comment) is missing. Link: <a href="https://arxiv.org/abs/1808.09861" target="_blank" rel="nofollow">https://arxiv.org/abs/1808.09861</a>
Thank you!</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Byl5skjDTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to your comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxHKjAcYX&amp;noteId=Byl5skjDTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper440 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper440 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for bringing this paper to our attention, and we added the citation and comparison in the latest update.
Our results are similar with the ones obtained in this paper when it uses similar unsupervised embeddings (BWET (adv.) + self-att. in Table 1), despite that we did not use the CRF decoding layer.
Furthermore, our method is a general multilingual model transfer approach and we experimented on multiple tasks in addition to NER.
Please excuse of negligence of this paper since this paper was made available on arXiv only a few weeks before the submission deadline while the official EMNLP proceedings actually came after it.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJxtmM89n7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxHKjAcYX&amp;noteId=rJxtmM89n7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper440 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper440 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a multilingual NLP model which performs very well on a target language with any leveraging labeled data. The authors evaluated their framework on there different tasks: slot filling, named entity recognition and text classification. Overall, the results look very promising.
- Strengthens:
+ The proposed idea is novel.
+ The results are very good for all three tasks.
- Weaknesses:
+ The authors claimed that their model knows what to share. However, they did not provide any evidence proving this hypothesis. Only the experimental results are not enough.
+ The paper also lacks an analysis to show to some extent what the model learned, e.g. the attention weights or the value of the gate. Is there any correlation between the similarity among languages (source and target) and the attention weights.
- What are not clear:
+ It is not clear to me what exactly has been done with the CharCNN embeddings in Section 4.2? How did the authors train the embeddings (only with the source languages or also with the target language)? It seems to me that the proposed model did not work well in this case. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByeWEkiP6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to the reviewer's comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxHKjAcYX&amp;noteId=ByeWEkiP6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper440 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper440 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comments and feedback.

Regarding the weaknesses:
- We showed detailed ablation analysis (Section 4.1.2) that closely matched our hypothesis that the MAN-MoE model learns what features to share across different languages dynamically. In particular, when transferring to the less similar language Chinese, the model with MAN removed performs significantly worse, indicating language-invariant features are important for this case. On the other hand, when transferring to German or Spanish, where the target language is more similar to a subset of source languages, we observe that the model with MoE removed performs much worse, illustrating the importance of private features in such cases. The fact that MAN-MoE outperforms both MAN-only and MoE-only in both cases show that our model is able to learn what's important for sharing in every scenario.

- We added the visualization of gate outputs to the paper in the latest update. One complication is that the expert gate makes decision at the token level instead of at the sentence level. That means the weights vary from token to token even in the same input sentence. It is hence harder to visualize. We did some aggregation to see if there were any insights on the language or sentence level.

For clarification:
The CharCNN produces a character-level word embedding for each word, and is concatenated to the pretrained word embeddings. The CharCNN is randomly initialized and is updated end to end together with the rest of the model, and is not separately trained. Therefore, it is trained on the target language during the MAN training, since the adversarial training part uses monolingual data from all languages including the target language.
We clarified this in the paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_r1lBbvaSnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Limited novelty, nice write-up, impressive results, little/no analysis</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxHKjAcYX&amp;noteId=r1lBbvaSnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper440 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper440 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">My main reservation with this paper is the limited novelty. The approach seems to be a rather direct application of a subset of the sluice network architecture in [0] - which has been available on ArXiV since 2017 - with MUSE pre-trained embeddings. In particular, I don’t think the claim that the authors “propose the first zero-resource multilingual transfer learning model” is necessary - and I think it is way too strong a claim. Training an LSTM on English data with MUSE/vecmap embeddings is pretty standard by now, and this does not require any target language training data or cross-lingual supervision either. See zero-shot scenarios in [1-2], for example.

Apart from that, I think the write-up is nice, the approach makes a lot of sense, and results are impressive. I would have liked to see a bit more analysis. In particular, the fact that you learn gate values, makes it easy to analyze/visualize what and how your networks learn to share. 

I think there’s a few baselines in between BWE and MAN, e.g., simple adversarial training and adversarial training with GradNorm [3], that would put your results in perspective. Finally, I would like to encourage the authors to run experiments with actual low-resource languages: A literature on cross-lingual transfer experimenting with German, Spanish, and Japanese, could end up being very heavily biased. For tasks with data in more languages, consider, for example, POS tagging [4], morphological analysis [5], or machine translation [6]. 

[0] <a href="https://arxiv.org/abs/1705.08142" target="_blank" rel="nofollow">https://arxiv.org/abs/1705.08142</a>
[1] http://aclweb.org/anthology/P18-1074	
[2] http://aclweb.org/anthology/P18-2063
[3] https://arxiv.org/abs/1711.02257
[4] http://universaldependencies.org/
[5] http://unimorph.org/
[6] http://christos-c.com/bible/</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJxG1kov6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to the reviewer's comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxHKjAcYX&amp;noteId=BJxG1kov6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper440 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper440 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comments and feedback.

Our method is similar to the Sluice Network (and other work on multi-task learning) in that we both try to learn what subspaces of features/parameter to share between tasks (in our case between languages) but it is also different in several fundamental ways.
Most importantly, they use a global \alpha matrix to determine how to share information between various tasks statically on a task level, where two different samples from the same task would be treated in the same way. In contrast, our MoE model dynamically decide what to share on a token-level for any given input sample, where the model is able to dynamically use distinct expert mixtures for two samples from the same language.
In addition, their method focuses on the multi-task learning setting, while ours deals with the multi-source transfer learning case.
Finally, the shared features in our model is learned by language-adversarial training, which is absent in Sluice Network.

We agree that it is not necessary to emphasize our method being the first zero-resource CLTL method, and we removed the claim in the latest update.

We added the visualization of gate outputs to the paper in the latest update. One complication is that the expert gate makes decision at the token level instead of at the sentence level. That means the weights vary from token to token even in the same input sentence. It is hence harder to visualize. We did some aggregation to see if there were any insights on the language or sentence level.

For other adversarial training baselines, we experimented with alternative adversarial training methods, such as the WGAN-WeightClip and the WGAN-GP training, but found the result to be similar with standard MAN. We did not further explore more possibilities for the MAN training part, but those can be readily adopted by our model if certain training technique is found helpful.

We reached out to the authors of Xie et al. 2018 for their NER data on the low-resource language, Uyghur, but they were unable to share the data due to license issue. We will explore other possibilities of adding more experiments on low-resource languages.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HkeghLbf2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice idea but not very novel, extensive evaluation setup with positive resutls, unawareness of some important previos work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxHKjAcYX&amp;noteId=HkeghLbf2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper440 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">27 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper440 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper describes a model for cross lingual transfer with no target language information. This is a well written paper that makes a number of contributions:

1. It provides an interesting discussion of transfer form multiple source languages into a target language. This is a timely problem and the paper points out that adversarial networks may be too limiting in this setup.

2. It provides a modeling approach that deals with the limitations of adversarial networks as mentioned in (1).

3. It demonstrates the value of the proposed approach through an extensive experimental setup.

At the same time, I see two major limitations to the paper:

1. While the proposed approach is valid, it is not very original, at least in my subjective eyes. The authors integrate a classifier that combines the private, language-specific features so that not only features that are shared between all the involved languages can be used in the classification process. While this is a reasonable idea that works well in practice, IMO it is quite straight forward and builds on ideas that have been recently been proposed in many other works.

2. The authors claim that: "To our best knowledge, this work is the first to propose
an unsupervised CLTL framework without depending on any cross-lingual resource"

This is, unfortunately, not true. I refer the authors to the paper:

Deep Pivot-Based Modeling for Cross-language Cross-domain Transfer with Minimal Guidance. Yftah Ziser and Roi Reichart. EMNLP 2018.

In their lazy setup, the EMNLP authors do exactly that. They address the more complicated cross-language, cross-domain setup, but their model can be easily employed within a single domain. Their experiments even use the multilingual sentiment dataset used in the current paper. The model in the EMNLP paper shows to outperform adversarial networks, so it can be competitive here as well.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BklQiRqvpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to the reviewer's comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxHKjAcYX&amp;noteId=BklQiRqvpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper440 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper440 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comments and feedback.

Regarding the limitations:
1: We believe the originality of our model lies in the fact that, unlike previous work, it is able to coherently utilize both the shared and the private features when transferring from multiple sources. In particular, our model dynamically determines what knowledge to share to the target language on a token-level basis. In addition, the usage of the mixture-of-experts model in a transfer learning setting is also novel to our knowledge.

2: In the Ziser and Reichart paper, the authors used the multilingual embeddings from Smith et al. (<a href="https://github.com/Babylonpartners/fastText_multilingual)," target="_blank" rel="nofollow">https://github.com/Babylonpartners/fastText_multilingual),</a> which according the following github description, is not unsupervised: "Of the 89 languages provided by Facebook, 78 are supported by the Google Translate API. We first obtained the 10,000 most common words in the English fastText vocabulary, and then use the API to translate these words into the 78 languages available. We split this vocabulary in two, assigning the first 5000 words to the training dictionary, and the second 5000 to the test dictionary."
In addition, we would like to ask the reviewer to kindly consider the fact that EMNLP 2018 proceedings were not available until the end of October, which was one month after the ICLR submission deadline. 
Nevertheless, we agree with the reviewer that it is not necessary to emphasize our method being the first zero-resource CLTL method, and we softened the claim in the latest update.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>