<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Variadic Learning by Bayesian Nonparametric Deep Embedding | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Variadic Learning by Bayesian Nonparametric Deep Embedding" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SJf6BhAqK7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Variadic Learning by Bayesian Nonparametric Deep Embedding" />
      <meta name="og:description" content="Learning at small or large scales of data is addressed by two strong but divided frontiers: few-shot learning and standard supervised learning. Few-shot learning focuses on sample efficiency at..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SJf6BhAqK7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Variadic Learning by Bayesian Nonparametric Deep Embedding</a> <a class="note_content_pdf" href="/pdf?id=SJf6BhAqK7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019variadic,    &#10;title={Variadic Learning by Bayesian Nonparametric Deep Embedding},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SJf6BhAqK7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Learning at small or large scales of data is addressed by two strong but divided frontiers: few-shot learning and standard supervised learning. Few-shot learning focuses on sample efficiency at small scale, while supervised learning focuses on accuracy at large scale. Ideally they could be reconciled for effective learning at any number of data points (shot) and number of classes (way).  To span the full spectrum of shot and way, we frame the variadic learning regime of learning from any number of inputs.  We approach variadic learning by meta-learning a novel multi-modal clustering model that connects bayesian nonparametrics and deep metric learning. Our bayesian nonparametric deep embedding (BANDE) method is optimized end-to-end with a single objective, and adaptively adjusts capacity to learn from variable amounts of supervision. BANDE achieves a) state-of-the-art results on semi-supervised classification of Omniglot and mini-ImageNet, b)impressive 75% classification accuracy on the 1692-way, 10-shot classification task of Omniglot while only training for 5-way 1-shot classification, c)94.37% accuracy on CIFAR-10 by episodic optimization, comparable to state-of-the-art supervised learning techniques, and d) strong unsupervised clustering performance, with the ability to discover character classes given no character supervision.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">meta-learning, metric learning, bayesian nonparametrics, few-shot learning, deep learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We address any-shot, any-way learning with multi-modal prototypes by connecting bayesian nonparametrics and deep metric learning</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_S1xCeCI9nm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Hard to read and relies on unjustified, shifting assumptions </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJf6BhAqK7&amp;noteId=S1xCeCI9nm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1587 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1587 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Review Summary
--------------
While the focus on variadic learning is interesting, I think the present version of the paper needs far more presentational polish as well as algorithmic improvements before it is ready for ICLR. I think there is the potential for some neat ideas here and I hope the authors prepare stronger versions in the future. However, the current version is unfortunately not comprehensible or reproducible.

Paper Summary
-------------

The paper investigates developing an effective ML method for the "variadic" regime, where the method might be required to perform learning from few or many examples (shots) and few or many classes (ways). The term "variadic" comes from use in computer science for functions that can a flexible number of arguments. There may also be unlabeled data available in the few shot case, creating semi-supervised learning opportunities.

The specific method proposed is called BANDE: Bayesian Nonparametric Deep Embedding. The idea is that each data point's feature vector x_i is transformed into an embedding vector h(x_i) using a neural network, and then clustering occurs in the embedding space via a single-pass of the DP-means algorithm (Kulis &amp; Jordan 2012). Each cluster is assumed to correspond to one "class" in the eventual classification problem, though each class might have multiple clusters (and thus be multi-modal).  

Learning occurs in an episodic manner. After each episode (single-pass of DP-means), each point in a query set is embedded to its feature vector, then fed into each cluster's Gaussian likelihoods to produce a normalized cluster-assignment-probability vector that sums to one. This vector is then fed into a cross-entropy loss, where the true class's nearest cluster (largest probability value) is taken to be the true cluster. This loss is used to perform gradient updates of the embedding neural network.

There is also a "cumulative" version of the method called BANDE-C. This version keeps track of cluster means from previous episodes and allows new episodes to be initialized with these.

Experiments examine the proposed approach across image categorization tasks on Omniglot, mini-ImageNet, and CIFAR datasets.


Strengths
---------
* I like that many clusters are used for each true class label, which is better than rigid one-to-one assumptions.


Limitations
-----------
* Can only be used for classification, not regression
* The DP-means procedure does not account for the cluster-specific variance information that is used at other steps of the algorithm


Significance and Originality
----------------------------
To me, the method appears original. Any method that could really succeed across various variadic settings would be significant.



Presentation Concerns
---------------------

I have serious concerns about the presentation quality of this paper. Each section needs careful reorganization as well as rewording.

## P1: Algo. 1 contains numerous omissions that make it as written not correct.

* the number of clusters count variable "n" is not updated anywhere. As writting this algo can only update one extra cluster beyond the original n.
* the variable "c" is unbound in the else clause. You need a line that clarifies that c = argmin_{c in 1 ... n} d_ic

Would be careful about saying that "a single pass is sufficient"... you have *chosen* to do only one pass. When doing k-means, we could also make this choice. Certainly the DP-means objective could keep improving with multiple passes.

## P2: Many figures and tables lack appropriate captions/labels

Table 1: What metric is reported? Accuracy percentage? Not obvious from title/caption. Should also make very clear here how much labeled data was used.

Table 2: What metric is reported? Accuracy percentage? Not obvious from title/caption. Should also make how many labeled and unlabeled examples were used easier to find.

## P3: Descriptions of episodic learning and overall algorithm clarity

Readers unfamiliar with episodic learning are not helped with the limited coverage provided here in 3.1 and 3.2. When exactly is the "support" set used and the "query" set used? How do unlabeled points get used (both support and query appear fully labeled)? What is n? What is k? What is T? Why are some points in Q denoted with apostrophes but not others? Providing a more formal step-by-step description (perhaps with pseudocode) will be crucial.

In Sec. 3.2, the paragraph that starts with "The loss is defined" is very hard to read and parse. I suggest adding math to formally define the loss with equations. What parameters are being optimized? Which ones are fixed?

Additionally, in Sec. 3.2: "computed in the same way as standard prototypical networks"... what is the procedure exactly? If your method relies on a procedure, you should specify it in this paper and not make readers guess or lookup a procedure elsewhere.


## P4: Many steps of the algorithm are not detailed

The paper claims to set \lambda using a technique from another paper, but does not summarize this technique. This makes things nearly impossible to reproduce. Please add such details in the appendix.

Major Technical Concerns
------------------------

## Alg. 1 concerns: Requires two (not one) passes and mixes hard and soft assingments and different variance assumptions awkwardly

The BANDE algorithm (Alg. 1) has some unjustified properties. Hard assignment decisions which assume vanishing variances are used to find a closest cluster, but then later soft assignments with non-zero variances are used. This is a bit heuristic and lacks justification... why not use soft assignment throughout? The DP means procedure is derived from a specific objective function that assumes hard assignment. Seems weird to use it for convenience and then discard instead of coming up with the small fix that would make soft assignment consistent throughout.

Furthermore, The authors claim it is a one pass algorithm, but in fact as written in Alg. 1 it seems to require two passes: the first pass keeps an original set of cluster centers fixed and then creates new centers whenever an example's distance to the closest center exceeds \lambda. But then, the *soft* assignment step that updates "z" requires again the distance from each point to all centers be computed, which requires another pass (since some new clusters may exist which did not when the point was first visited). While the new soft values will be close to zero, they will not be *exactly* zero, and thus they matter. 

## Unclear if/how cluster-specific variance parameters learned

From the text on top of page 4, it seems that the paper assumes that there exist cluster-specific variances \sigma_c. However, these are not mentioned elsewhere, only a general (not cluster-specific) label variance \sigma and fixed unlabeled variance sigma_u are used.

## Experiments lack comparison to internal baselines

The paper doesn't evaluate sensitivity to key fixed hyperparameters (e.g. \alpha, \lambda) or compare variants of their approach (with and without soft clustering step, with and without multimodality via DP-means). It is difficult to tell which design choices of the method are most crucial.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1eAoVj_6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarity, Reproducibility, and Details to Resolve Technical Concerns</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJf6BhAqK7&amp;noteId=B1eAoVj_6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1587 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018 (modified: 15 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1587 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for their detailed feedback, in particular the attention to the technical aspects of the clustering steps and probabilistic interpretations in our work, and the comments on clarity and accessibility for audiences less familiar with meta-learning and few-shot learning. We agree with the reviewer on the importance of multi-modal clustering as "better than rigid, one-to-one assumptions" of prior work, which we show by experiment on alphabet recognition in section 4.3 and improved semi-supervised few-shot classification in section 4.1. We likewise agree that methods that "really succeed across various variadic settings would be significant" which is why we propose it in this work and investigate it by experiment in section 4.2.

Regarding concerns of clarity and reproducibility, we are incorporating the feedback of the reviews into a revision to be posted during the rebuttal period and will release code after decision (omitted here only to preserve anonymity). Our comprehensive code release will cover our model, experimental evaluation and training settings, all few-shot baselines (including prototypical networks, semi-supervised prototypical networks, and our variadic extensions of MAML and few-shot graph networks), and datasets. This will help safeguard reproducibility for future work and serve as a reference implementation of the variadic setting.

We now clarify our method and experiments to address the reviewer's technical concerns.

hard/soft assignments and probabilistic interpretation: We thank the reviewer for their theoretical precision. We are in full agreement, and wish to point out that we identify and experiment with fully hard (sec. 3.4) and fully soft variants of our method (appendix A4) for this reason of probabilistic justification. We choose the hard-soft hybrid for our main results, as mentioned in the paper, because it is marginally more accurate in experiments. We appreciate the feedback on this point, and are revising the text to make these variants more clear.

number of passes/clustering steps: We will clarify our language to use the term “clustering iteration” instead of passes/clustering steps. In the fully hard model, an iteration corresponds to the assignment of all labeled and unlabeled points to clusters, and then an update of the means of all clusters. In the fully soft model, an iteration corresponds to computing soft assignments for all points, and then updating the means. In the hard-soft hybrid, we use the “hard” step to compute a set of cluster means, and then perform a "soft" clustering step in order to update these cluster means.

cluster-specific variances: \sigma and \sigma_u are learned and are shared across all labeled and all unlabeled clusters respectively. \sigma_c was a typo for \sigma as it is the variance of class clusters. The only exception to learning these variances, as noted, is Section 4.3 where they are fixed.

internal baselines/ablations: We agree with the reviewer on this list of ablations/internal baselines, so much so that we have already experimented with them in the development of the method: the selected multi-modal clustering with hard-soft assignment was best. For exposition we chose to focus on the hard-soft variant as our method and compare to competing works like Ren et al. and Finn et al., but for completeness we will include these ablation experiments in our revision to the text (to be posted during the rebuttal period).

“our method can only be used for classification, and not regression”: While true, this weakness holds for prior prototypical methods too by Snell et al. and Ren et al. so our work is no more and no less limited in this regard.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJxdpNodTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Incorporation of Presentation Feedback (Thanks!)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJf6BhAqK7&amp;noteId=HJxdpNodTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1587 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018 (modified: 15 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1587 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We now turn to the reviewer's thorough feedback on presentation.

P1: numbers of iterations. In principle, BANDE can be iterated multiple times, as in DP-means. However, in our experiments we found accuracy does not improve with more iterations. We will modify the text to make this more clear. We will also correct the algorithm description to appropriately update n and c with the required two lines (thank you for catching this).

P2: Tables 1 and 2 captions and details. The metric is indeed accuracy percentage, as is standard for these benchmarks, which we are clarifying in our revision (to be posted during the rebuttal period). We appreciate that the semi-supervised setting of Ren et al. has a number of details, which is why we placed the paragraph on semi-supervised episode composition under table 2, and we will incorporate more of this text into the caption to make it easier to find.

P3: episodic learning. We would like to thank the reviewer for commenting on the clarity of our work for readers who do not specialize in meta-learning and few-shot learning. We tried to follow the standard summary in this field (see Ren et al., Finn et al.). While fuller tutorial coverage of few-shot learning would be the most clear, we are constrained by the page limit when explaining the existing settings and our contributions of multi-modality and any-shot/any-way generalization in new settings. We are clarifying few-shot details in captions and the main text in the revision.

P4: setting λ. The technique for setting λ is our own, which we summarize at the end of 3.2: "We estimate ρ as the variance in the labeled cluster means within an episode, while α is treated as a hyperparameter." The algebraic expression of λ in terms of ρ, α is what we borrow from Kulis et al., and we are rewording this for clarity in the revision.

"computed in the same way as standard prototypical networks" (section 3.2). This is explained in the last paragraph of 3.1, so we remove it here to avoid redundancy and potential confusion.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_S1xc8-Uchm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Novelty is unclear</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJf6BhAqK7&amp;noteId=S1xc8-Uchm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1587 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1587 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a meta-learning method that utilizes unlabeled examples along with labeled examples. The technique proposed is very similar to the one by (Ren et al. 2018), only differing in the choice of a different clustering algorithm (Kulis and Jordan, 2012) instead of soft k-means as used by Ren et al. 

I feel the contrast to Ren et al, is not provided to the degree it should be. The Appendix paragraph A4 is not sufficient in terms of explaining why this method is conceptually different or significantly better than the related approach. It is hard for me to certify the merits of their work, including explaining the experimental results.

I also do not understand the significance of "multi-model clustering" in this context. Also, by their definition of "variadic", how is this more variadic than Ren et al. or Snell et al.?

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyltxBsuTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Contrast with Ren et al., Significance of Multi-modal (Many-to-One) Clustering, and Variadic Setting</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJf6BhAqK7&amp;noteId=SyltxBsuTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1587 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018 (modified: 15 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1587 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for raising three key points of our work: (1) clustering algorithm choices and our difference with Ren et al., (2) our technical contribution of extending prototypical methods to multi-modal representation for handling more complicated data distributions, and (3) our empirical contribution of proposing and thoroughly investigating the variadic setting of any-shot/any-way generalization.

&gt; the contrast to Ren et al, is not provided to the degree it should be
&gt; only differing in the choice of a different clustering algorithm

The difference in choice of clustering is crucial:
- our method is capable of adaptive, multi-modal clustering unlike the fixed, uni-modal clustering of Ren et al. and Snell et al. This gives an improvement of +3 points accuracy on the standard few-shot benchmark of 5-way, 5-shot mini-ImageNet classification (Table 2), extends prototypical nets to problems without any labeled data (see next bullet point), and for more diverse classes like alphabets our accuracy is ~25 points higher.
- our method handles labeled data by the same clustering rule unlike the heuristics of Ren et al. for unlabeled data, making inference in our method possible for zero labeled examples (of any kind, including meta-data as in zero-shot learning) whereas Ren et al. and Snell et al. are undefined in this setting. Section 4.3 shows high quality clustering without labels (Table 3), and 10-25 point improvements on prior work for learning more diverse classes like alphabets instead of single characters (Table 4), underlining the importance of multiple modes.
- We shed further light on the choice of clustering with the lens of probabilistic interpretation: we derive an approximate interpretation of Ren et al. (Appendix A4), which lacked theoretical justification, while explaining the direct interpretation of the hard variant of our own method (Section 3.4).

&gt; significance of "multi-model clustering" 

Multi-modality is a key and distinguishing property of our method that is necessary for the quality of our results. Please refer to figure 1 for a schematic of the difference among Snell et. al, Ren et al., and BANDE (ours): note that having multiple modes lets BANDE more accurately cluster the labeled and unlabeled data alike. Among these methods, only BANDE can adjust its capacity to model simple, compact classes with a single mode while simultaneously modeling diverse, complicated classes with multiple modes. We achieve higher accuracy than Ren et al. for semi-supervised few-shot learning (Table 2). Furthermore, Table 4 in particular highlights the needs for multi-modal representation: a full alphabet is not uni-modal in the learned embedding, unlike a single character, and here we show major (10-25) point gains over the prototypical nets of Snell et al. and Ren et al. that assume each class has a uni-modal data distribution.

&gt; by their definition of "variadic", how is this more variadic than Ren et al. or Snell et al.?

Snell et al., Ren et al., and our method do indeed generalize better across shot and way as we show (Figure 2). Our first contribution is in evaluating this generalization at all in our novel experiments: we cover extreme way at 1692 Omniglot classes (Figure 3), extreme shot at zero labeled examples for clustering (Table 3) and at scaling episodic optimization to the supervised learning regime of 50k labeled examples on CIFAR-10 and CIFAR-100. Existing work was restricted to the few-shot settings of Section 4.1 with training/testing on the same way and shot.

BANDE (ours) is more variadic than Ren et al. and Snell et al. in 1. handling the case of purely unlabeled data and 2. handling more diverse data with complicated class distributions such as alphabet classes instead of character classes (section 4.3). We forecast that meta-learning, as it scales to more diverse data distributions, will encounter more tasks like our alphabet recognition experiments in the variety and even hierarchy of classes, where our adaptive, multi-modal clustering helps significantly (Table 4). While we expect further progress to improve on Ren et al., Snell et al., and our own method, the main point here is to encourage this kind of shot/way generalization to reconcile the distant poles of small-scale and large-scale learning.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJeDOoul2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A work lacking clarity</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJf6BhAqK7&amp;noteId=rJeDOoul2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1587 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">26 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1587 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This work proposes a learning method based on deep subspace clustering. The method is formulated by identifying a deep data embedding, where clustering is performed in the latent space by a revised version of k-means, inspired by the work [1]. In this way, the proposed method can adapt to account for uni-modal distributions. The authors propose some variations of the framework based on soft cluster assignments, and on cumulative learning of the cluster means.
The method is tested on several scenarios and datasets, showing promising results in prediction accuracy.

The idea presented in this work is reasonable and rather intuitive. However, the paper presentation is often unnecessarily convoluted, and fails in clarifying the key points about the proposed methodology. The paper makes often use of abstract terms and jargon, which sensibly reduce the manuscript clarity and readability. For this reason, in my opinion, it is very difficult to appreciate the contribution of this work, from both methodological and applicative point of view. 

Related to this latter point, the use of the term “Bayesian nonparametric” is inappropriate. It is completely unclear in which sense the proposed framework is Bayesian, as it doesn’t present any element related to parameters inference, uncertainty estimation, … Even the fact that the method uses an algorithm illustrated in [1] doesn’t justifies this terminology, as the clustering procedure used here only corresponds to the limit case of a Dirichlet Process Gibbs Sampler when the covariance parameters goes to zero. Moreover, the original procedure requires the iteration until convergence, while it is here applied with a single pass only. The procedure is also known to be sensitive to the order by which the data is provided, and this point is not addressed in this work. 

Finally, the novelty of the proposed contribution is questionable. To my understanding, it may consist in the use of embedding methods based on the approach provided in [1]. However, for the reasons illustrated above, this is not clear. There is also a substantial amount of literature on deep subspace embeddings that proposes very similar methodologies to the one of this paper (e.g. [2-5]).  For this reason, the paper would largely benefit from further clarifications and comparison with respect to these methods.  





[1] Kulis and Jordan,  Revisiting k-means: New Algorithms via Bayesian Nonparametrics, ICML 2012

[2] Xie, Junyuan, Ross Girshick, and Ali Farhadi. "Unsupervised deep embedding for clustering analysis." International conference on machine learning. 2016.
[3] Ji, Pan, et al. "Deep subspace clustering networks." Advances in Neural Information Processing Systems. 2017.
[4] Jiang, Zhuxi, et al. "Variational deep embedding: An unsupervised and generative approach to clustering." IJCAI 2017
[5] Kodirov, Elyor, Tao Xiang, and Shaogang Gong. "Semantic autoencoder for zero-shot learning. CVPR 2017.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJxN_rsOT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Relation/Contrast to Deep Subspace Embedding, Novelty, and Breadth of Results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJf6BhAqK7&amp;noteId=BJxN_rsOT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1587 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018 (modified: 15 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1587 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt; proposes a learning method based on deep subspace clustering
&gt; substantial amount of literature on deep subspace embeddings that proposes very similar methodologies to the one of this paper (e.g. [2-5])

We thank the reviewer for bringing up deep subspace embedding. While our work and these are generally related by metric learning, they are quite separate in approach and purpose. Ours is a meta-learning approach for multi-modal representation (that is, having an adaptive number of centroids per class) of labeled and unlabeled data, it is optimized for classification tasks, and it is evaluated by generalization to new data and tasks. The cited [2-5] address unsupervised clustering, have fixed numbers of clusters, and are evaluated by clustering metrics on the same data they are optimized on.

Most significantly, these works *do not consider generalization*: the clustering methods are optimized on the data that is to be clustered and do not experiment on held-out tasks/classes as in meta-learning settings like ours. Only [5] can incorporate labeled data, and in their experiments they train and test on the same classes, without generalization, on a tiny synthetic dataset and the Oxford flowers dataset of 17 classes and &lt;1000 images.

[2, 3, 4, 5] learn and evaluate unsupervised and zero-shot clustering models on the same train/test data with the same classes without generalization experiments. [2] cannot incorporate labeled data, requires pre-training, and shows results on the toy datasets of MNIST and STL-10. [3] cannot incorporate labeled data and is only evaluated on the simple face and object datasets Yale B, ORL, and COIL. [4] addresses generative modeling and unsupervised clustering for problems, not few-shot learning and classification, and its experiments are restricted to small-scale datasets with 10 or fewer clusters. [5] focuses on zero-shot learning with a linear auto-encoder on off-the-shelf features, and its "supervised clustering" section has only a 3-class synthetic dataset and a 17-class dataset of flower images where the clustering is optimized for the same 17 flower species it is evaluated on.

&gt; novelty of the proposed contribution is questionable

Here is a brief summary of our key, novel contributions:

technical novelty: our method is capable of adaptive, multi-modal clustering unlike the fixed, uni-modal clustering of Ren et al. and Snell et al. by our reconciliation of DP-means from Kulis et al. with end-to-end learning (section 3.2).

empirical novelty: we propose and thoroughly investigate our "variadic" setting of any-shot/any-way generalization (section 4.2), find that several popular methods degrade in this setting (MAML, Reptile, few-shot graph nets), show that it is possible to learn a large-scale classifier (1692-way character recognition) from small-scale episodic optimization (5-way 1-shot tasks), show that episodic optimization of a prototypical method rivals the accuracy from large-scale SGD optimization of a strong fully-parametric baseline optimized by SGD on CIFAR-10/100, and evaluate few-shot learning of alphabets instead of characters to examine accuracy on more complex data distributions.

theoretical novelty: We shed further light on prototypical network methods with the lens of probabilistic interpretation. We derive an approximate interpretation of Ren et al. (Appendix A4), which lacked theoretical justification, and explain the direct interpretation of the hard variant of our own method (Section 3.4).

&gt; method is tested on several scenarios and datasets, showing promising results in prediction accuracy

We thank the reviewer for commenting on our breadth of evaluation and promising results. To reinforce this point, we note that our experiments cover several problem statements: few-shot fully-supervised/semi-supervised classification (Section 4.1, Tables 1 &amp; 2), our proposed variadic setting of any-shot/any-way generalization (Section 4.2), purely unsupervised clustering (Section 4.3, table 3) and transfer learning from super-class training to sub-class recognition (Section 4.3, table 4). We approach each of these problems by meta-learning through episodic optimization of classification tasks, and these experiments focus on generalization to new tasks (of held-out classes, different settings of shot and way, or discovery of sub-classes from super-class training). 
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJgo9Hiuam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Bayesian Nonparametric Name, Terminology, and Clustering Details</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJf6BhAqK7&amp;noteId=rJgo9Hiuam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1587 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018 (modified: 15 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1587 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt; use of the term “Bayesian nonparametric” is inappropriate

The DP-means clustering method of Kulis et al., which our work adapts to end-to-end optimization for metric learning, is derived through bayesian nonparametric infinite mixture modeling in the limit of zero variance. The existence of the method, and others that share this mathematical framework (Broderick et al. 2013, Roychowdhury et al. 2013, Wang &amp; Zhu 2015), are due to bayesian nonparametrics and identify as such in their titles and text. Not acknowledging this connection could obscure the origin and properties of the method. Does the reviewer have an alternate term in mind?

&gt; paper makes often use of abstract terms and jargon

Could the reviewer please be more precise on this point? We have made our best effort to follow the standard terminology for meta-learning and few-shot learning (Vinyals et al. Finn et al., Snell et al, Ren et al.), but would appreciate knowing specifically where this is confusing, so that it can be more clear for a broader audience.

&gt; procedure is also known to be sensitive to the order by which the data is provided, and this point is not addressed in this work. 

While it is true that the clustering is dependent on the order of the data, we simply have not found the method to be sensitive to this in practice, although we can include this result in the revision. We note that this dependence is likewise mentioned in Kulis et al. 2012 but they make no mention of it impacting the quality of their results.

&gt; proposed method can adapt to account for uni-modal distributions

Our method critically allows for *multi-modality* in the data distribution for both labeled and unlabeled data, adaptively choosing the number of clusters, unlike the prior work by Snell et al. and Ren et al. that assume fixed numbers of clusters, as do [2, 3, 4, 5] cited in the review. This is explained in Section 3.2 and shown to be crucial for diverse classes like alphabets in Section 4.3 Table 4.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>