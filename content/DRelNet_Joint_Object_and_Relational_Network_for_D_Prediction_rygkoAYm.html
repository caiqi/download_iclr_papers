<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>3D-RelNet: Joint Object and Relational Network for 3D Prediction | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="3D-RelNet: Joint Object and Relational Network for 3D Prediction" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rygk9oA9Ym" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="3D-RelNet: Joint Object and Relational Network for 3D Prediction" />
      <meta name="og:description" content="We propose an approach to predict the 3D shape and pose for the objects present in a scene. Existing learning based methods that pursue this goal make independent predictions per object, and do not..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rygk9oA9Ym" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>3D-RelNet: Joint Object and Relational Network for 3D Prediction</a> <a class="note_content_pdf" href="/pdf?id=rygk9oA9Ym" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous20193d-relnet:,    &#10;title={3D-RelNet: Joint Object and Relational Network for 3D Prediction},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rygk9oA9Ym},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=rygk9oA9Ym" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We propose an approach to predict the 3D shape and pose for the objects present in a scene. Existing learning based methods that pursue this goal make independent predictions per object, and do not leverage the relationships amongst them. We argue that reasoning about these relationships is crucial, and present an approach to incorporate these in a 3D prediction framework. In addition to independent per-object predictions, we predict pairwise relations in the form of relative 3D pose, and demonstrate that these can be easily incorporated to improve object level estimates. We report performance across different datasets (SUNCG, NYUv2), and show that our approach significantly improves over independent prediction approaches while also outperforming alternate implicit reasoning methods.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">3D Reconstruction, 3D Scene Understanding, Relative Prediction</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We reason about relative spatial relationships between the objects in a scene to produce better 3D predictions</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SJxi1JwiTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Overall Response and Clarifications</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygk9oA9Ym&amp;noteId=SJxi1JwiTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper497 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper497 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank the reviewers for their helpful comments and valuable feedback. We first address the common questions raised by the reviewers and then talk about reviewer specific queries. Major changes to the pdf post reviews are colored in *blue* to enhance readability. While AR1 gives this work the lowest rating among reviewers, we would like to highlight that some of the raised concerns/issues are incorrect (for eg, no quantitative results on real images -- we have performance number on NYUv2 which is a real-world indoor image dataset), and we hope that they would reconsider their rating in this light.

A common concern among reviewers seems to be - “Is our work significant enough?” 
We believe it is; in terms of both -- empirical results and technical novelty.

[Empirical Results]: In computer vision, significant advances have been made by making progress on benchmarks. Our paper represents a simple yet effective approach which advances state of the art significantly on NYUv2 (and SUNCG as well). We believe the computer vision community deserves to know the new benchmark performance so it can build upon this.

[Technical Novelty]: As AR2 points out, our work like several others in 3D scene modeling and other areas leverages the use of pairwise relations as an inductive bias, and the reviewers feel our contribution is not significant in this light. We would first like to emphasize that the questions of whether an inductive bias is useful, and how its inclusion is operationalized are both extremely important, and research along the latter is no less useful despite evidence for the former -- object detection methods have over the years refined how object proposals can be used, investigating how geometry can be used as an inductive bias for learning based 3D reconstruction is a popular recent area, etc.

In addition to arguing for the need of modeling relationships (which we agree has been previously explored), our contribution in this work also relates to investigating how this inductive bias of leveraging pairwise relationships should be incorporated, and we propose an approach different from previous implicit (e.g. IN, RN) or prior-based methods (e.g. existing work in vision/graphics).


The previous vision/graphics approaches rely on image-agnostic category-level priors, we instead propose to use image-dependent pairwise prediction.


Unlike implicit methods, our ‘relations’ pertain to specific variables of interest i.e. relative pose and are combined during learning with the unary estimates. 


We note that both these modeling insights: i) image-dependent pairwise prediction, and ii) using relative pose as a pairwise relation, are contributions that future work can draw upon, and we do not feel that these proposed innovations have been previously explored.

[Additional Experiments on Real Data]:
AR1 incorrectly comments that “Quantitative result on a dataset of real images is missing” - we do report quantitative results on the NYU dataset (see Table 1) in the setting with known GT boxes. AR3 additionally suggested evaluating our approach in detection setting on a real dataset to further strengthen the empirical validation of the approach. We did so on the NYUv2 dataset, and obtained an AP of 8.5 for our method (compared to Factored3D: 5.3, GCN: 6.5, InteractionNet: 7.6). The lower absolute numbers indicate that the difficulty of the full task, but we note that the relative performance trends are similar to SUNCG (in fact, our relative improvements over baselines are larger). We have included the full table and more details in the paper.

[Details on Setup]:
AR1 and AR3 found some aspects of the setup unclear. We emphasize that the experimental setup is exactly the same as followed in the previous work we compare to (Factored3D). The task of predicting 3D for all objects are considered in two evaluation settings: -
[Sec 4.2] Known GT 2D bounding box - the input is an RGB image and the 2D bounding boxes for all objects are input, and we estimate the 3D shape and pose for all given objects.
[Sec 4.3] Detection setting - only the 2D image is input, and we perform detection as well as 3D estimation for all detected objects (the bounding boxes come from a pretrained object-detector).

Note that in addition to the above input, we do not use any other annotation at inference. We clarify the other specific questions is the individual responses to reviewers and will make the setup clearer in the main text.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Sye4stm16X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The authors propose an end-to-end trainable model which leveraged pair-wise relationships between objects to predict objects 3D shape and pose given a single 2D image.  The proposed method outperforms independent prediction approaches on two publicly available datasets.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygk9oA9Ym&amp;noteId=Sye4stm16X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper497 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper497 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper is well-written with a few figures to illustrate the ideas and components of the proposed method. However, one of the main components in the proposed method is based on Tulsiani et al. CVPR'18. The remaining components of the proposed method are not very new. Hence, I am not very sure whether the novelty of the paper is significant. Nevertheless, the performance of the proposed method is fairly good outperforming all baseline methods. 
I also have a few questions:
1. How did you get the instance boxes, union boxes, and binary masks in testing?
2. What are the training and inference time? </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1ld8lvop7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygk9oA9Ym&amp;noteId=B1ld8lvop7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper497 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper497 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We are grateful for your feedback. We hope that the above discussion assuaged the reviewer’s concerns regarding novelty and some unclear details. We briefly address the two questions regarding the setup:

During testing, in the setting with known GT boxes (Sec 4.2), we assume that the 2D instance boxes are given. In the detection setting, the 2D instance boxes are the result of the learned detector. Given the (detected or known) instance boxes, the union boxes and binary masks can be easily computed - the union box is just the larger box containing both instance boxes, and the mask highlights these instance boxes in the union box.
Training and Testing  Inference Time on a single GPU (Maxwell Titan X)
1. Train time: 65 hrs
2. Test time: 0.55s per image
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1lDjH9p3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Incremental technical novelty, issues with experiments and results, unclear presentation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygk9oA9Ym&amp;noteId=H1lDjH9p3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper497 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper497 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">&lt;Summary&gt;: This paper presented a method for incorporating binary relationship between objects (relative location, rotation and scale) into single object 3d prediction. It is built on top of previously published work of [a] and used same network architecture and loss as of [a] and only added the binary relations between objects for object 3d estimation. The results are shown on SUNCG synthetic dataset and only *4 image* instances of NYUv2 dataset which is very small for a computer vision task.

[a] Shubham Tulsiani, Saurabh Gupta, David Fouhey, Alexei A Efros, and Jitendra Malik. Factoring shape, pose, and layout from the 2d image of a 3d scene. In CVPR, 2018.

&lt;Pros&gt;: The paper tackles a problem of obvious interest to computer vision research community. It shows better results compared to previous similar work of [a] without considering binary relation between objects.

&lt;Cons&gt;:

*Technical details are missing:

The set of known and unknown variables are not clear throughout the paper:
-The extrinsic camera parameters are known or estimated by the method?
-The intrinsic camera parameters are known or estimated by the method?
-What are the properties of ground truth bounding boxes in 2D camera frame and 3D space?
-What is the coordinate of translation? is it in camera coordinate or world coordinate?
-What are the variations of camera poses in training and testing for synthetic dataset and how are the samples generated? Are the train/test images generated or are rendered images from previously published work of [b] used?

[b] Yinda Zhang, Shuran Song, Ersin Yumer, Manolis Savva, Joon-Young Lee, Hailin Jin, and Thomas Funkhouser. Physically-based rendering for indoor scene understanding using convolutional neural networks. In CVPR, 2017.

*The proposed method is trained on synthetic dataset of SUNCG and their object relations have biases from scene creators. While using binary relation between objects increase the recall in prediction it can also make the predictions bias to the most dominant relations and decrease the precision of detection in rare cases in synthetic dataset. Also, such bias can decrease prediction precision in images of real scenes.
 
*One of the main issues in this paper is that the result of fully automated pipeline versus having ground-truth annotation at test time are mixed up. For example, in the teaser figure (Figure 1-b), does the proposed method use ground truth bounding boxes or not? It is mentioned in figure caption: “(b) Output: An example result of our method that takes as input the 2D image and generates the 3D layout.”. Is the input only 2D image or 2D image + ground truth object bounding boxes?
In order to make sure that reader understands each qualitative result, there should be a column showing the “Input” to the pipeline (Not “Image”). For example, in Figure 3 and Figure 4, the image overlaid with input ground-truth bounding boxes should be shown as input to the algorithm. 


*The experiments and results does not convey the effectiveness of the proposed approach. There are major issues with the quality of the experiments and results. Here are several examples:

- Missing baseline: Comparison with the CRF-based baseline is missing. This statement is not convincing in the introduction: “One classical approach is to use graphical models such as CRFs. However, these classical approaches have usually provided little improvements over object-based approaches.” For a fair comparison with prior works, reporting results on a CRF-based baseline using similar unary predictions is necessary. 

-The experimental results are heavily based on ground truth boxes for the objects, but it is not clear how/where the ground truth boxes are given at the test time and which part is actually predicted.

-If the ground truth boxes are given at the test time, it means that the ground truth binary relations between objects are given and it makes the problem trivial.

-It is not clear what is the ground truth box in experimental setup. Is it amodal object box or the ground truth box contains only the visible part of the object? 

-The qualitative results shown in Figure 4 have full objects in voxel space with predicted rotation, scale and translation. In the qualitative result of Figure 3 and Figure 5 the voxel prediction is shown as final output. Why the result of full object in voxel space with predicted (rotation, scale and translation) is not shown in Figure 3 and Figure 5 and why it is shown in Figure 4?


*Very limited results on real images:

-Quantitative result on a dataset of real images is missing. The results on synthetic datasets is not a good proxy for the actual performance of the algorithm in real use cases and applications.

- The paper only shows few results of NYUv2 on known ground truth boxes. The errors in object detection can be propagated to the 3D estimation therefore these qualitative results are not representative of the actual qualitative performance of the proposed algorithm. Several randomly selected qualitative results on a dataset of real images “without ground-truth boxes” are needed for evaluating the performance of the proposed method on real images. 

-Reporting variation in all parameters of scale, rotation and translation is necessary in order to find the difficulty of the problem. For example, what is the distribution of object scale in different object categories. What is the error of scale prediction of we use mean object scale for each object category for all object instance at test set?


*Unclear statements and presentation:

- It is mentioned in the paper: “While the incorporation of unary and relative predictions can be expressed via linear constraints in the case of translation and scale, a similar closed form update does not apply for rotation because of the framing as a classification task and non-linearity of the manifold.”

-Is it necessary for the relative rotation to be formulated to classification task? 

-If not the comparison of modeling relative rotation via linear constraints is missing.

- In some of the tables and figures the “know ground-truth boxes/detection setting” are in bold face and in some cases are not. This should be consistent throughout the paper.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1lY_kDsT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygk9oA9Ym&amp;noteId=B1lY_kDsT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper497 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper497 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the comments. Below we address specific concerns regarding the work.

[Missing details]
We point out relevant sections in the original submission where we already had mentioned the details. 

- Translation in camera vs. world frame: In Section 3.1, we outline the parameterization of the 3D pose in terms of translation, rotation, and scale in the camera frame.
- Rendered images and pose variations: As we mentioned in Section 4.1 (Experimental Setup). We use the rendered images provided by Zhang et al.[1] Their work generated these images with a variety of different camera poses. We randomly partition this set of images into three splits from different houses - 70% (train), 10% (val) and 20% (test).
- Extrinsic vs. Intrinsic camera parameters: We estimate the pose of objects in the camera frame we can say that the extrinsics are Identity. Also, we make no assumptions about Intrinsics of the camera. This is outlined in Section 3.1 (page 3).

- Properties of ground truth boxes in 2D: In Section 4.2, we explain that we only input the ground truth boxes in 2D. We are unclear on what the reviewer means by “properties” of bounding boxes in 3D space but would be happy to clarify if the reviewer can elaborate. 

- What part is predicted at test time: In Section 4.2, the ground-truth 2D boxes and the image are given as input and the method predicts the 3D pose and shape of each object. In Section 4.3, the image is given as input and the method first detects the objects and then predicts the 3D pose of each object. We predict the voxel of the object for the experiments on the SUNCG dataset.

- No voxel output in Figure 4: As we mention in the Figure 4 caption - "We use the ground-truth meshes for visualization due to lack of variability in shape annotations for learning." (We have also described this in section 4.1, NYUv2 setup)
 
- "object relations have biases from scene creators" (in SUNCG): We appreciate the concerns about the biases in the synthetic data, but note that we do show results on the real images from the NYUv2 dataset.

Unclear presentation: We thank the reviewer for these suggestions. They will surely help us improve the quality of the paper.
- Set of knowns vs. Unknowns: We will clarify this in the paper. Your suggestion of using "Input" in the figures as opposed to "Image" is a great one and will be incorporated.
- Figure 1 (b): The input to the method is the 2D image and the associated ground-truth object bounding boxes. The output is the 3D pose and shape for each object.
- Amodal box vs. full box: We use 2D bounding boxes as is standard in the object detection literature. These boxes are *not* amodal.
- Relative rotation as classification, and (possible) missing comparison to linear baseline: As quaternion algebra is non-commutative the quaternion "vector" space is not linear. Thus, we formulate relative rotation as a classification problem, and hence there is no linear baseline. Also, note that previous works have also modeled rotation as a classification problem. 

Incorrect statements by the reviewer
- Ground truth boxes are given implies relations are given: This is incorrect. The boxes are in 2D while the relations we use and predict in the paper are in 3D coordinates.
- "Quantitative result on a dataset of real images is missing": Please see Table 1 that shows quantitative results on the NYUv2 dataset.
- "*4 image* instances of NYUv2 dataset": This statement is a mis-characterization of our work. We only visualize results of a few images but note that Table 1 reports evaluations using *all* (654)  test set images on NYUv2. This is explained in Section 4.1.

Missing baseline
- CRF baseline: While this is an excellent suggestion, there are many practical reasons why it is not straightforward for us to do this. CRF based methods typically rely on class-specific pairwise potentials, and while these can be designed for specific cases (e.g. chair-table), there are numerous non-trivial design decisions that make this not scalable for generic classes e.g. what is the form of potential function, do all (or only nearby) object pairs have an edge between them, do all classes pairs have potential functions etc. However, if the reviewer has any specific suggestions regarding a CRF baseline that can generically handle 3D pose across classes, we would be happy to compare. We would also like to point out the GCN baseline, which also does message passing, can be considered as an implicitly learned CRF.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJgJHwjq2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A scene parsing model that takes both objects and their relations into account</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygk9oA9Ym&amp;noteId=BJgJHwjq2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper497 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper497 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposed a 3D scene parsing that takes both objects and their relations into account, extending the Factor3D model proposed by Tulsiani et al 18. Results are demonstrated on both synthetic and real datasets.

The paper is in general well written and clear. The approach is new, the results are good, the experiments are complete. However, I am still lukewarm about the paper and cannot champion it. I feel the paper interesting but not exciting, and it’s unclear what we can really learn from it. 

Approach-wise, the idea of using pair-wise relationship as an inductive bias is getting popular. This paper demonstrated that it can be used for scene parsing, too, within a neural net. This is good to know, but not surprising given what have been demonstrated in the extensive literature in the computer graphics and vision community. In particular, the authors should discuss many related papers from Pat Hanrahan’s group and Song-Chun Zhu’s group (see some examples below). Apart from that, this paper doesn’t have an obvious technical innovation that can inspire future work. This is different from Factor3D, which is the first voxel-based semantic scene parsing model from a single color image, with modern neural architecture.

The results are good, but are on either synthetic data, or using ground truth bounding boxes. Requiring ground truth boxes greatly restricts the usage of these models. Would that be possible to include results under the detection setting on NYU-D or Matterport 3D? The authors claimed that the gain of 6 points is significant; however, a simple interaction net achieves a gain of 5 points, so the technical contribution of the proposed model is not too impressive.

In general, I’m on the border but leaning slightly toward rejection, because this paper is very similar to Tulsiani et al, and the proposed innovation has been explored in various forms in other papers.

A minor issue:
-	In fig 5. The object colors are not matched for GT and Factor3D and ours.

Related work
Holistic 3D Scene Parsing and Reconstruction from a Single RGB Image. ECCV’18.
Configurable 3D Scene Synthesis and 2D Image Rendering with Per-pixel Ground Truth Using Stochastic Grammars. IJCV’18.
Characterizing Structural Relationships in Scenes Using Graph Kernels. SIGGRAPH’11.
Example-based Synthesis of 3D Object Arrangements. SIGGRAPH Asia’12.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkxzbxPsT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to  AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygk9oA9Ym&amp;noteId=rkxzbxPsT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper497 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper497 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the comments and additional references - we will include these in related work. The reviewer raised concerns regarding the lessons learned from the paper in the context of previous pairwise relation modeling work in vision/graphics, suggested additional experiments in detection setting on real data, and pointed out a relatively small improvement over a particular baseline. Regarding the last concern, we note that apart from mAP scores we show significant improvements on errors in translations and scales as compared to all other baselines and that the difference between interaction net and our approach on these errors is significant.

We hope that the discussion and results presented above in the reply to all reviewers addressed the first two concerns mentioned. We would again like to emphasize that we agree with the reviewer that previous work has examined similar inductive biases but would point out that our contributions regarding how these biases should be incorporated are novel, and would be useful for future attempts.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>