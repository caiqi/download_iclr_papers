<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Deep Generative Models for learning Coherent Latent Representations from Multi-Modal Data | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Deep Generative Models for learning Coherent Latent Representations from Multi-Modal Data" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rJl8FoRcY7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Deep Generative Models for learning Coherent Latent Representations..." />
      <meta name="og:description" content="The application of multi-modal generative models by means of a Variational Auto Encoder (VAE) is an upcoming research topic for sensor fusion and bi-directional modality exchange.&#10;  This contribution..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rJl8FoRcY7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Deep Generative Models for learning Coherent Latent Representations from Multi-Modal Data</a> <a class="note_content_pdf" href="/pdf?id=rJl8FoRcY7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 08 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019deep,    &#10;title={Deep Generative Models for learning Coherent Latent Representations from Multi-Modal Data},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rJl8FoRcY7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=rJl8FoRcY7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">The application of multi-modal generative models by means of a Variational Auto Encoder (VAE) is an upcoming research topic for sensor fusion and bi-directional modality exchange.
This contribution gives insights into the learned joint latent representation and shows that expressiveness and coherence are decisive properties for multi-modal datasets.
Furthermore, we propose a multi-modal VAE derived from the full joint marginal log-likelihood that is able to learn the most meaningful representation for ambiguous observations.
Since the properties of multi-modal sensor setups are essential for our approach but hardly available, we also propose a technique to generate correlated datasets from uni-modal ones.
</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Multi-Modal Deep Generative Models, Sensor Fusion, Data Generation, VAE</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Deriving a general formulation of a multi-modal VAE from the joint marginal log-likelihood.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Hyxzcy50nX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Disjointed paper on an interesting topic</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJl8FoRcY7&amp;noteId=Hyxzcy50nX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper445 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper445 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper introduces a new VAE model (JMVAE) for multi-modal data with a
shared latent representation. An method is also introduced to synthetically
created bi-modal datasets with correlated latent representations.

The writing was a little awkward to follow at times, and I'm still not
sure what Ι am suppose to take away from the figures plotting the latent
representation. The evaluation is fairly qualitative and it's difficult to
understand what we achieving from using JMVAE.

I'm not clear what the contribution of this work provides, as there is already
plenty done on learning multi-modal representations.

One weakness with this work is all the examples are fairly toy
problems. The article motivates the work as combining raw multi-modal
sensor datasets, but no real tasks are shown.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BklFYf9ZTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Explaining takeaway message and applicability of toyish dataset</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJl8FoRcY7&amp;noteId=BklFYf9ZTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper445 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper445 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We’d like to thank the reviewer for their thorough review and helpful suggestions. We will improve our work further but also want to start a discussion to clarify the significance of our contribution.

From our point of view, current multi-modal VAE approaches do not explicitly care or investigate the coherence of the latent space in multi-modal cases. Therefore, exploiting multi-modal VAEs as e.g. feature extractor may be questionable by means of the manifold theory. It happens for SOTA VAEs, that the encoder networks for example in a bi-modal case, i.e. q_\phi_a(z_a|a), q_\phi_b(z_b|b), and q_\phi_{a,b}(z_{a,b}|a,b), may project the observations into different latent spaces (z_a != z_b != z_{a,b}). However, in our work, the VAE’s objective has been consequently derived from the joint marginal log-likelihood which also disproves statements in the related papers, which state that this approach is not applicable. As a result, our model shows coherence in the latent spaces (z_a = z_b = z_{a,b}) regarding the sampled latent space as well as the ELBO for each sample which is, from our point of view, a significant and unique finding in the field of multi-modal generative models. However, the takeaway message from the figures showing the latent representations is as follows: If the latent spaces (z_a, z_b, z_{a,b}) found by the various encoders (q_\phi_a(z_a|a), q_\phi_b(z_b|b), and q_\phi_{a,b}(z_{a,b}|a,b)) are the same, z_a and z_b should be sub-spaces of z_{a,b}. This should result in a congruence of samples and in a similarity of the ELBO per sample between the subspaces. Furthermore, the trained latent space should serve as a much better representation, i.e. feature extractor for subsequent models, which is out of scope for this contribution.

To explain the application of the “toyish” dataset, our main objective is not to show that our model performs best on complex multi-modal data, but to show the unique property of our proposed model that it learns a correct coherent latent space between modalities. We found that current available multi-modal datasets do not satisfy our needs to show the significance of our M²VAE approach. As stated in our contribution, multi-modal data for e.g. sensor fusion should show a correlation where it is fuseable. Available datasets, i.e. as listed in [1], may hold this requirement, but may be too complex in nature to qualitatively prove the distinction between SOTA VAEs and our proposed M²VAE. Therefore, we proposed a technique to correlate different datasets (MNITS and fashion-MNIST in our case) by superimposing the latent space of distinctively trained CVAEs. The simultaneous sampling from the various CVAE decoders then produces a correlated dataset, that allows us to show the benefits of our M²VAE approach.

However, we would be again very thankful if the reviewer could point out non-toyish datasets which satisfy the needs for sensor-fusion as well as the necessary complexity to improve the visibility of our work.

[1] Suzuki, M., Nakayama, K., &amp; Matsuo, Y. (2017). Joint multimodal learning with deep generative models, 1–12. Retrieved from <a href="https://arxiv.org/abs/1611.01891" target="_blank" rel="nofollow">https://arxiv.org/abs/1611.01891</a></span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ryxgAFHq27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting  topic; potential technical error.  </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJl8FoRcY7&amp;noteId=ryxgAFHq27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper445 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper445 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a multi-modal VAE with a variational bound derived from chain rule. 

Pros:
It is an interesting and important research direction. 
The presentation is in general clear. 

Cons:
1. The re-visit of JMVAE seems not precise. The JMVAE should bound the joint p(a, b) not log p(a|b)p(b|a).
2. Due to the potential misunderstanding of JMVAE, the paper uses the JMVAE bound for log p(a|b) + log p(b|a) in equation (5), which seems wrong. 
Equation (4) &amp;(5) itself seems confusing alone. It says L_m = log p(a,b) in (4) then L_m = log p(a|b) + log p(b|a) in (5).
3. If I am not mistaken the error above, the proposed bound is in fact wrong. 
4. Assume that the method is correct, with a massive amount of beta:s, I doubt the method would be very sensitive to beta tuning. The experiments just presented some examples of different betas. Quantitive evaluation of beta and performance is needed. 
5. To generate multi-modal data, other methods such as VAE-CCA or JMVAE are able to that as well. It is not unique to the proposed method. 
6. The experiments are very toyish. The multi-modal data were generated. The method should be evaluated with a real-world benchmarking multi-modal dataset. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rylXgI9-am" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to reviewers cons</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJl8FoRcY7&amp;noteId=rylXgI9-am"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper445 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper445 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for his/her work. We hope our responses below may help clarify the scope of our work and its significance.

1. The original JMVAE proposed by Suzuki et al. [1] does, in fact, optimize the bound of the VI log(p(a|b))+log(p(b|a)) as derived in the appendix A of their work [1]. Further, they derive the ELBO for the VI by substituting the reconstruction terms by the ELBO of the joint multi-modal VAE.

2. Given 1., we can substitute log p(a|b) + log p(b|a) with L_m. But in fact, there is a typo in eq. (4) &amp; (5) which definitely causes confusion. The left-hand-side (LHS) term should be L_m², denoting the proposed VAE, and not L_m (which denotes the VAE by Suzuki et al.).

3. As explained in 2. and by correcting the typo in the LHS term, the bounds are correct. We apologize for this confusion and upload a corrected version of the paper. (Origin of the issue: we used the superscript symbol 2 which was dropped by the LaTeX compiler)

4. In fact, beta tuning is pretty robust as shown by Higgins et al. [2]. One only has to care about the proper ratios between the size of the observable and latent dimensions as briefly explained by us: “... our main concern is the balance between the input (D) and the latent space (Dz) using a constant normalized factor βnorm=βD/Dz.”

5. As we do not know if this statement relates to our proposed M²VAE (section 3) or the multi-modal data generation via the CVAE (section 4.1), we try to answer this concern two-fold: 5.a relates to the approach of our proposed M²VAE model while 5.b relates to the correlated data generation via the CVAE.

5.a. From our point of view, current VAE approaches do not explicitly care or investigate the coherence of the latent space in multi-modal cases. Thus, the encoder networks for example in a bi-modal case, i.e. q_\phi_a(z_a|a), q_\phi_b(z_b|b), and q_\phi_{a,b}(z_{a,b}|a,b), may project the observations into different latent spaces (z_a != z_b != z_{a,b}). This happens for the tVAE and JMVAE-Zero, as qualitatively shown in our results, due to the fact that these approaches do not maintain a fully analytical way for deriving the VAE’s objective from the joint marginal log-likelihood (JMLL). In our work, the VAE’s objective has been consequently derived from the JMLL which also disproves statements in related papers, which state that this approach is not applicable. As a result, our model shows coherence in the latent spaces (z_a = z_b = z_{a,b}) regarding the sampled latent space as well as the ELBO for each sample which is, from our point of view, a significant and unique finding in the field of multi-modal generative models. Furthermore, the trained latent space should serve as a much better representation, i.e. feature extractor for subsequent models, which is out of scope for this contribution.

5.b. We found that current available multi-modal datasets do not satisfy our needs to show the significance of our M²VAE approach. As stated in our contribution, multi-modal data for e.g. sensor fusion should show a correlation. Available datasets [3] may hold this requirement, but may be too complex in nature to qualitatively prove the distinction between SOTA VAEs and our proposed M²VAE. Therefore, we proposed a technique to correlate different datasets (MNITS and fashion-MNIST in our case) by superimposing the latent space of distinctively trained CVAEs. The simultaneous sampling from the various CVAE decoders then produces a correlated dataset, that allows us to show the benefits of our approach.
If one would just feed for instance MNIST and f-MNIST into a multi-modal VAE, even by keeping the labels the same, the VAE would train an orthogonal distribution per label. This shows that if one feeds uncorrelated data into any VAE, the VAE will obtain the uncorrelatedness as shown in Figure 2 Bottom-Left. One can see that per label, almost all distributions remain orthogonal to each other, which proves our assumption. However, sampling from that latent space will again result in uncorrelated data.
On the other hand, a multi-modal VAE is able to obtain the correlation of the proposed sampled dataset e-MNIST, which suites our needs for further evaluations in this contribution. 
However, we would be very thankful if the reviewer could point out the literature to the named models (VAE-CCA or JMVAE) which investigate similar nature in VAEs.

6. As stated in 5.a and 5.b our main objective is to show that our model learns a correct coherent latent space between modalities. However, we would be again very thankful if the reviewer could point out datasets which satisfy the needs for sensor-fusion as well as the necessary complexity to improve the visibility of our work.

[1] Suzuki, M., et al.  (2017). Joint multimodal learning with deep generative models
[2] Higgins, I., et al. (2016). Early Visual Concept Learning with Unsupervised Deep Learning.
[3] <a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research" target="_blank" rel="nofollow">https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research</a></span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJxI6Mqd2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Unclear about the novelty of the objective. Clarity could be improved.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJl8FoRcY7&amp;noteId=rJxI6Mqd2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper445 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper445 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
This paper proposes an objective, M^2VAE, for multi-modal VAEs, which is supposed to learn a more meaningful latent space representation. To summarize my understanding of the proposed objective, in the bi-modal case, it combines both objectives of TELBO [1] and JMVAE-kl [2] with some hyperparameters to learn the uni-modal encoders. The terms of Eqns 7,8, and 9 are equivalent to TELBO and Eqns 9 and 10 are JMVAE-kl. It would be very beneficial for the readers if you could more clearly contrast your objective with the related work given how similar they are. 

Given these similarities between objectives, its unclear why JMVAE-Zero was chosen over JMVAE-kl as a baseline. Furthermore, the reasoning for the improvement of the ELBO of M^2VAE over the baselines in Section 5.3 is unclear, given the similarities between the objectives. 

The qualitative figures throughout the paper are hard to interpret. By looking at Fig 4., I cannot tell which latent space is best. 
“one can see from Fig. 4 that the most coherent latent space distribution was learned by the proposed M^2VAE” 
What is meant by ‘coherent latent space’? 

This paper was hard to follow and there are a number of typos throughout the paper. For instance, the labels within Fig 4 and the caption contradict themselves. If the clarity and quality of the writing could be improved then perhaps the contributions may become more evident.  

[1] R. Vedantam, I. Fischer, J. Huang, and K. Murphy. Generative Models of Visually GroundedImagination. ArXiv e-prints, May 2017.
[2] M. Suzuki, K. Nakayama, and Y. Matsuo. Improving Bi-directional Generation betweenDifferent Modalities with Variational Autoencoders. ArXiv e-prints, January 2018

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1gTGBq-67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Add takeaway message and JMVAE-Zero vs. JMVAE-kl</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJl8FoRcY7&amp;noteId=S1gTGBq-67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper445 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper445 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We’d like to thank the reviewer for their thorough review and helpful suggestions. We will improve our work further but also want to start a discussion to clarify the significance of our contribution.

From our point of view, current VAE approaches do not explicitly care or investigate the coherence of the latent space in multi-modal cases. Thus, the encoder networks for example in a bi-modal case, i.e. q_\phi_a(z_a|a), q_\phi_b(z_b|b), and q_\phi_{a,b}(z_{a,b}|a,b), may project the observations into different latent spaces (z_a != z_b != z_{a,b}). This happens for the tVAE and JMVAE-Zero, as qualitatively shown in our results, due to the fact that these approaches do not maintain a fully analytical way for deriving the VAE’s objective from the joint marginal log-likelihood (JMLL). In our work, the VAE’s (namely M²VAE) objective has been consequently derived from the JMLL and therefore also disproves statements in related papers, which state that this approach is not applicable. As a result, our model shows coherence in the latent spaces (z_a = z_b = z_{a,b}) regarding the sampled latent space as well as the ELBO for each sample which is, from our point of view, a significant and unique finding in the field of multi-modal generative models. Furthermore, the trained latent space should serve as a much better representation, i.e. feature extractor for subsequent models, which is out of scope for this contribution.

Furthermore, we choose the JMVAE-Zero over JMVAE-kl for the following reasons: JMVAE-Zero is a special case of the JMVAE-kl which is equivalent of using the ELBO derived from the joint marginal log-likelihood (JMLL) p(a,b). In general, the JMVAE-kl derives the ELBO from the variation of information log(p(a|b))+log(p(b|a)). For the sake of consistency, we chose only approaches which derive the objective from the JMLL.

We apologize for the possibly unstructured Figure 4 and try to rephrase the takeaway message as follows: If the latent spaces (z_a, z_b, z_{a,b}) found by the various encoders (q_\phi_a(z_a|a), q_\phi_b(z_b|b), and q_\phi_{a,b}(z_{a,b}|a,b)) are the same, z_a and z_b should be sub-spaces of z_{a,b}. This should result in a congruence of samples and in a similarity of the ELBO per sample between the subspaces. These statements only hold for our proposed M²VAE as shown in the numerous examples in Figure 4. However, we will de-clutter Figure 4 and upload a new version with this answer.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hyx27qIj6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJl8FoRcY7&amp;noteId=Hyx27qIj6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper445 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper445 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">"in a bi-modal case, i.e. q_\phi_a(z_a|a), q_\phi_b(z_b|b), and q_\phi_{a,b}(z_{a,b}|a,b), may project the observations into different latent spaces (z_a != z_b != z_{a,b})"

The fact that z_a != z_b != z_{a,b} should be expected if a and b provide different information. I don't see the problem with this. 

"This happens for the tVAE and JMVAE-Zero, as qualitatively shown in our results, due to the fact that these approaches do not maintain a fully analytical way for deriving the VAE’s objective from the joint marginal log-likelihood (JMLL)"

The model learned by tVAE is derived from the JMLL. q(z|a) and q(z|b) are learned separately from the model. 

"For the sake of consistency, we chose only approaches which derive the objective from the JMLL."

Didn't you say that tVAE and JMVAE-Zero were not derived from the JMLL, yet you compared to them? So you could also compare to JMVAE-kl given how similar it is to your model. Right?
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyggX2426X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Answers to the comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJl8FoRcY7&amp;noteId=SyggX2426X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper445 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper445 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the discussion and want to explain the following concerns as follows:

First of all, we like to give some definitions to avoid confusion:
A: Is the first dataset (e.g., MNIST) from which we can draw a sample "a" (e.g., a picture showing the letter "0")
B: Is the second dataset (e.g., fashion-MNIST) from which we can draw a sample "b" (e.g., a picture showing a "t-shirt")
AB: Is the entangled dataset (c.f. Figure 2 "proposed e-MNIST") from which we can draw a joint sample "a,b"
z_a: Is the projection of sample "a" using q_{\phi_a} into the latent space
z_b: vice versa
z_{a,b}: Is the projection of the joint sample "a,b" using q_{\phi_{a,b}} into the latent space

----------------------------------------------------------------------------------------------------------------------------------------

----------------- Comment by the reviewer -----------------
"The fact that z_a != z_b != z_{a,b} should be expected if a and b provide different information. I don't see the problem with this."

----------------- Answer by the authors -----------------
This statement is absolutely correct and holds if two datasets A and B don't show any correlation. This would also mean that both datasets have a different generative model. On the other hand, if the datasets share some information (for instants in the m-MNIST in Figure 2 where "0"s occur together with "t-shirt"s, "7"s together with "heel"s, etc.) they might also share the same generative model and therefore come frome the same latent representation. 

Referring to Figure 7, we trained q_{\phi_{a}}, q_{\phi_{b}}, and q_{\phi_{a,b}} for the JMVAE-Zero, tVAE, and the proposed M²VAE on the AB dataset. Since we ensure a strong correlation not only per label but also per sample in the e-MNIST dataset, all q_{\phi_*} should project any sample "a", "b", or "a,b" drawn from AB onto the same latent representation, resulting in z_a = z_b = z_{a,b}. If this is not the case, the VAE was not able to find the common generative model.

A disjoint latent space example can be seen in the latent space of the JMVAE-Zero (top of Figure 7), where the projections of q_{\phi_{a}} and q_{\phi_{b}} are entirely disjoint. Furthermore, the projections from q_{\phi_{a,b}} share a vague similarity with q_{\phi_{a}}. However, sampling from that confused latent space via p_{\theta_{a}} and p_{\theta_{b}} results in bad reconstruction.

On the other hand, the tVAE finds a more congruent latent space projection via q_{\phi_{a}}, q_{\phi_{b}}, and q_{\phi_{a,b}}. But it also shows no clear seperation between labels (green, violet, and pink are strongly confused).

Finally, our model finds the most coherent and congruent latent space projection of AB by its' encoders q_{\phi_{a}}, q_{\phi_{b}}, and q_{\phi_{a,b}}. Furthermore, they show the clearest separability and best reconstructability.

----------------------------------------------------------------------------------------------------------------------------------------

----------------- Comment by the reviewer -----------------
"The model learned by tVAE is derived from the JMLL. q(z|a) and q(z|b) are learned separately from the model."

----------------- Answer by the authors -----------------
That is correct. But the separate learning of q(z|a) and q(z|b) is what we consider as a "not fully analytical derivation". The training of q(z|a) and q(z|b) has no impact on shaping the latent space and also not on the joint encoder q(z|a,b).

----------------------------------------------------------------------------------------------------------------------------------------

----------------- Comment by the reviewer -----------------
"Didn't you say that tVAE and JMVAE-Zero were not derived from the JMLL, yet you compared to them? So you could also compare to JMVAE-kl given how similar it is to your model. Right?"

----------------- Answer by the authors -----------------
We apologize for confusing the reviewer with too implicit information. We consider the M²VAE as "fully derived" from the JMLL while tVAE and JMVAE-Zero are just "derived" utilizing the JMLL as a starting point. 

However, you are right with the fact that the JMVAE-kl is indeed similar to ours it is, therefore, worth comparing it against the other approaches. We will include the evaluations in our future publications but also want to forecast that the missing uni-modal reconstruction terms in the loss function result in noticeable confusion if uni-modal observations are incomplete (as in our MoG example).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>