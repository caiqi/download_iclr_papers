<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>PeerNets: Exploiting Peer Wisdom Against Adversarial Attacks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="PeerNets: Exploiting Peer Wisdom Against Adversarial Attacks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Sk4jFoA9K7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="PeerNets: Exploiting Peer Wisdom Against Adversarial Attacks" />
      <meta name="og:description" content="Deep learning systems have become ubiquitous in many aspects of our lives. Unfortunately, it has been shown that such systems are vulnerable to adversarial attacks, making them prone to potential..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Sk4jFoA9K7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>PeerNets: Exploiting Peer Wisdom Against Adversarial Attacks</a> <a class="note_content_pdf" href="/pdf?id=Sk4jFoA9K7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019peernets:,    &#10;title={PeerNets: Exploiting Peer Wisdom Against Adversarial Attacks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Sk4jFoA9K7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=Sk4jFoA9K7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Deep learning systems have become ubiquitous in many aspects of our lives. Unfortunately, it has been shown that such systems are vulnerable to adversarial attacks, making them prone to potential unlawful uses. 
Designing deep neural networks that are robust to adversarial attacks is a fundamental step in making such systems safer and deployable in a broader variety of applications (e.g. autonomous driving), but more importantly is a necessary step to design novel and more advanced architectures built on new computational paradigms rather than marginally building on the existing ones.
In this paper we introduce PeerNets, a novel family of convolutional networks alternating classical Euclidean convolutions with graph convolutions to harness information from a graph of peer samples. This results in a form of non-local forward propagation in the model, where latent features are conditioned on the global structure induced by the graph, that is up to 3 times more robust to a variety of white- and black-box adversarial attacks compared to conventional architectures with almost no drop in accuracy.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">10 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_B1lhSkeq67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The defense appears to be causing gradient masking</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Sk4jFoA9K7&amp;noteId=B1lhSkeq67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper479 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">It appears this paper is causing gradient masking. Looking at Figure 5, FGSM is a more effective attack than PGD at eps=0.1, which is highly suspicious and is listed as one of the tests for identifying gradient masking in Athalye et al. 2018.

The authors may wish to try black-box attacks (e.g., SPSA by Uesato et al at ICML'18) to see if this is happening.

Further, this paper claims ~15% robustness at eps=0.1 (25/255) on CIFAR-10, and a non-zero robustness at eps=0.2 (50/255). No prior work has been able to achieve greater than ~10% accuracy at eps=0.06 (16/255), see Madry et al. 2018.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Byg8yHtZRX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Additional evaluation suggests the defense does not cause gradient masking</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Sk4jFoA9K7&amp;noteId=Byg8yHtZRX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper479 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper479 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comments.

Based on your concerns, we below provide additional evaluation to show that our approach does not cause gradient masking. We also add the new results to the supplementary material of our paper.

1. It appears this paper is causing gradient masking. Looking at Figure 5, FGSM is a more effective attack than PGD at eps=0.1, which is highly suspicious and is listed as one of the tests for identifying gradient masking in Athalye et al. 2018.
---
Regarding your concern to the Figure 5, where FGSM seems more effective than PGD at eps=0.1. We attribute this to the selection of the hyperparameters of the FGSM and PGD attack. The number of iterations per each value of epsilon for the PGD attack was set to 40. We have tried to increase this parameters to 100 iterations and evaluate on a small subset of the test set. This causes the PGD to find a slightly better perturbations in many cases, and slightly improves the attack on PeerNet. The overall comparison PeerNet vs. CNN baseline however does not change significantly, and we therefore choose to leave the default configuration of 40 iterations, as it is much more computationally feasible for the full test set of 10000 samples.

2. The authors may wish to try black-box attacks (e.g., SPSA by Uesato et al at ICML'18) to see if this is happening.
---
Further, to support our claim, we have taken this repository implementing black box attacks, including SPSA mentioned by the reviewer <a href="https://github.com/sunblaze-ucb/blackbox-attacks" target="_blank" rel="nofollow">https://github.com/sunblaze-ucb/blackbox-attacks</a> .
We have used their code for cifar-10 dataset and performed evaluation on a subset of 100 test samples. Using untargetted both single-step and iterative query black-box attack computed using finite difference method, compared to CW-loss based white-box attack.
The code reports "fraction of targtets achieved" for both white-box (whitebox_succ) and black-box (blackbox_succ) attacks, which is the percentage of samples that the attack was successful on.
We have left the default attack configuration from the repository, using epsilon = 8.0.
Single step:
ResNet-32 CNN (eps=8.0): whitebox_succ = 82% | blackbox_succ = 82%
PeerNet (eps=8.0):       whitebox_succ = 34% | blackbox_succ = 14%

For the iterative attack, we have tried different values of epsilon for CNN and PeerNet to allow fair comparison of the attacks
Iterative:
ResNet-32 CNN (eps=0.5): whitebox_succ = 41%  | blackbox_succ = 41%
ResNet-32 CNN (eps=8.0): whitebox_succ = 100% | blackbox_succ = 100%
PeerNet (eps=8.0):       whitebox_succ = 45%  | blackbox_succ = 28%
PeerNet (eps=12.0):      whitebox_succ = 63%  | blackbox_succ = 37%

From the results above, we verify that our method does not cause gradient masking. Athalye et al. 2018 mentions that iterative attacks should be stronger than single step, which in the results above holds. Moreover, it states that black-box attacks should be strict subset of white-box attacks, which is confirmed as well.

3. Further, this paper claims ~15% robustness at eps=0.1 (25/255) on CIFAR-10, and a non-zero robustness at eps=0.2 (50/255). No prior work has been able to achieve greater than ~10% accuracy at eps=0.06 (16/255), see Madry et al. 2018.
---
We are not aware of any proof that would set the theoretical bound of ~10% accuracy at eps=0.06 on CIFAR-10. We attribute the significant gap to the superiority of our approach. The significant margin can be potentially partially reduced by tuning hyperparameters of the attack methods. Nevertheless, from our observations, PeerNet will still stay very robust.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HkgMKRcth7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting work introducing graph neural nets as regularization, with practical limitations</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Sk4jFoA9K7&amp;noteId=HkgMKRcth7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper479 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper479 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper presents a an interesting novel approach to train neural networks with so called peer regularization which aims to provide robustness to adversarial attacks. The idea is to add a graph neural network to a spatial CNN. A graph is defined over similar training samples which are found using a Monte Carlo approximation.

The regularization using graphs reminds me of recent work at ICML on semi-supervised learning (Kamnitsas et al. (2018) Semi-supervised learning via compact latent space clustering) which is using a graph to approximate cluster density which acts as a regularizer for training on labelled data.

The main problem I see with these approaches is that they rely on sufficiently large batch sizes which could be (currently) problematic for many real-world applications. Memory and computation limitations are mentioned, but not sufficently discussed. It would be good to add further details on practical limitations.

Experiments are limited to benchmark data using MNIST, CIFAR-10, CIFAR-100. Comprehensive evaluation has been carried out with insightful experiments and good comparison to state-of-the-art. Both white- and black-box adversarial attacks are explored with promising results for the proposed approach.

However, it is difficult to draw conclusions for real-world problems of larger scale. The authors state that proposed framework can be added to any baseline model, but miss to clearly mention the limitations. It is stated that future work will aim at scaling PeerNets to benchmarks like ImageNet, but it is unclear how this could be done. Is there any hope this could be applied to problems like 3D imaging data or videos?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkecy4F-0Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Scalability of our method</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Sk4jFoA9K7&amp;noteId=Bkecy4F-0Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper479 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper479 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the insightful comments. 

1. It is stated that future work will aim at scaling PeerNets to benchmarks like ImageNet, but it is unclear how this could be done. Is there any hope this could be applied to problems like 3D imaging data or videos?
---
Regarding the concerns on method scalability. The current bottleneck of our approach is processing of all feature maps pixel-wise. We see potential of scaling our approach by operating on superpixels, or NxN patches, instead of processing all pixels individually.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJxejaXmhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Analysis and experimental comparisons are lacking</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Sk4jFoA9K7&amp;noteId=rJxejaXmhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper479 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper479 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose a defense against adversarial examples, that is inspired by "non local means filtering". The underlying assumption seems to be that, at feature level, adversarial examples manifest as IID noise in feature maps, which can be "filtered away" by using features from other images. While this assumption seems plausible,  no analysis has been done to verify it in a systematic way. Some examples of verifying this are:

1. How does varying the number of nearest neighbors change the network behavior?
2. At test time, a fixed number of images are used for denoising - how does the choice of these images change accuracy or adversarial robustness?
3. Does just simple filtering of the feature map, say, by local averaging, perform equally well? 
4. When do things start to break down? I imagine randomly replacing feature map values (i.e. with very poor nearest neighbors) will cause robustness and accuracy to go down - was this tested?

Based on the paper of Athalye et. al., really the only method worth comparing to for adversarial defense, is adversarial training. It is hard to judge absolute adversarial robustness performance without a baseline of adversarial training.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HklyONYZAX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Requested analysis and comparison</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Sk4jFoA9K7&amp;noteId=HklyONYZAX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper479 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper479 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the valuable remarks.
 
We have tested most of the concerns in points 1. - 4. during our experiments. We however could not provide full-extent analysis due to the limited length of the paper. Let us respond to each of the points separately below.

1. How does varying the number of nearest neighbors change the network behavior?
---
We observe that using small k (~5) doesn't always provide enough information to perform the denoising and the network is therefore less robust against adversarial examples.
On the other hand, having k too high (~20) yields too much regularization and the network original performance decreases more significantly.
In our experiments, we have found k=10 to be a reasonable compromise.

2. At test time, a fixed number of images are used for denoising - how does the choice of these images change accuracy or adversarial robustness?
---
We refer the reviewer to the Section 3 and Section 4.1. of our paper where this is addressed in detail.

3. Does just simple filtering of the feature map, say, by local averaging, perform equally well? 
---
It does not. We have tried simple smoothing of the feature maps and it not only does not make the network robust against adversarial attacks, but also regularizes the original network too much which results in significant loss in classification accuracy. Moreover, local averaging uses the information from the corrupted image itself to filter the feature map, which could even further amplify the noise.

4. When do things start to break down? I imagine randomly replacing feature map values (i.e. with very poor nearest neighbors) will cause robustness and accuracy to go down - was this tested?
---
This is of-course true. Obviously, selecting very poor nearest neighbors will definitely break the method as the newly created feature map will not express the original information anymore. In our paper, we even reason that the adversary often tries to fool the KNN algorithm directly, as we mention at the end of Section 4.3.2.
Moreover, we believe that our results show when do things start to "break down". We explicitly mention that an unbounded attack will always fool the network. Also, our figures in the main text and tables in the supplementary material show that with increasing magnitude of the perturbation, things start to "break down". 

5. Based on the paper of Athalye et. al., really the only method worth comparing to for adversarial defense, is adversarial training. It is hard to judge absolute adversarial robustness performance without a baseline of adversarial training.
---
We provide an evaluation below as well as add an additional section with the results in the supplementary material.

We have compared our approach to adversarial training method using the code provided by Madry etal. <a href="https://github.com/MadryLab/cifar10_challenge." target="_blank" rel="nofollow">https://github.com/MadryLab/cifar10_challenge.</a>
The ResNet-32 baseline model provided in Tensorflow repository (the same we use as CNN baseline in our paper) was trained using the script provided in the cifar10_challenge repository above.
We have used two training configurations producing two baseline models1 - the default one provided by the repository (ResNet-32 CNN A) and then the same one as in our paper (ResNet-32 CNN B).
PeerNet was trained traditionally without adversarial training. 
The attack was left as defined by the repository by Madry etal.

ResNet-32 CNN A: original_acc = 78.86% | adversarial_acc = 45.47%
ResNet-32 CNN B: original_acc = 75.59% | adversarial_acc = 42.53% 
PeerNet:         original_acc = 77.44% | adversarial_acc = 64.76%

Results show superiority of PeerNet on this benchmark. PeerNet was trained without considering any specific attacks and still outperforms ResNet-32 CNN, which was adversarially trained using this specific attack, by margin of 20%.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1lwBrml27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Comparison with Madry et al. 2017</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Sk4jFoA9K7&amp;noteId=H1lwBrml27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">26 Oct 2018</span><span class="item">ICLR 2019 Conference Paper479 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Can you compare with adversarial training? Also, did you vary the PGD iterations and verify that the accuracy does not change?

Further, have you considered the CW attack, gradient-free attacks and adaptive versions?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HygvKNtThX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Additional comparison</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Sk4jFoA9K7&amp;noteId=HygvKNtThX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper479 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018</span><span class="item">ICLR 2019 Conference Paper479 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comments.

Can you compare with adversarial training? 
---

1. We tried ResNet-32 with adversarial training. The resulting combination is not efficient, as also reported by Dezfooli et al. 

2. We used the default parameters suggested by foolbox. During PGD iterations, we have observed that the update in perturbation strength (increase or decrease) is diminishing into orders of 1-e3 over the iterations and we therefore did not expect that increasing the  number of iterations would give us any noticeable improvement. 

Further, have you considered the CW attack, gradient-free attacks and adaptive versions?
---

3. In our work, we mainly focused on gradient-based white box attacks and compared primarily against universal adversarial perturbations by Dezfooli etal. and also against some of the classic gradient-based attacks (gradient descent, FGSM, PGD). We have not tried gradient-free attacks. 

Following your suggestion, we evaluated out method on CW L2 attack, which is newly implemented in foolbox. To provide a fair comparison, we used the same settings for PeerNets and CNN baseline. The results are reported below:

CNN baseline: L2 error = 49.86 | Linf error = 4.327 | fooling rate = 100%
PeerNets: L2 error = 365.59 | Linf error = 27.43 | fooling rate = 93.76%

This clearly shows that PeerNets are more robust to CW L2 attack (the perturbation required to achieve a similar fooling rate on PeerNet has to be nearly an order of magnitude stronger). 

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1x6i6fgnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Neither MagNet nor BRELU are effective</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Sk4jFoA9K7&amp;noteId=H1x6i6fgnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">26 Oct 2018</span><span class="item">ICLR 2019 Conference Paper479 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">For what it's worth, both of these defenses you compare against are ineffective ( <a href="https://arxiv.org/abs/1711.08478" target="_blank" rel="nofollow">https://arxiv.org/abs/1711.08478</a> ).</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkxmCEYpnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Baseline comparisons</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Sk4jFoA9K7&amp;noteId=BkxmCEYpnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper479 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018</span><span class="item">ICLR 2019 Conference Paper479 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for suggesting the reference, which we will include in the revision. Considering the paper by Athalye et al. (<a href="https://arxiv.org/abs/1802.00420)," target="_blank" rel="nofollow">https://arxiv.org/abs/1802.00420),</a> most of the recently proposed defenses are ineffective. We actually refer to this fact in Section 4.3.2, where the 2nd paragraph points out that the way we train our model should make it more robust compared to the baselines, which, by the nature of their use, are prone to the problem of obfuscated gradients.

We however needed to select some baselines to compare to, and chose the methods that are, in some sense, similar to our approach. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>