<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>On the Sensitivity of Adversarial Robustness to Input Data Distributions | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="On the Sensitivity of Adversarial Robustness to Input Data Distributions" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=S1xNEhR9KX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="On the Sensitivity of Adversarial Robustness to Input Data..." />
      <meta name="og:description" content="Neural networks are vulnerable to small adversarial perturbations. Existing literature largely focused on understanding and mitigating the vulnerability of learned models. In this paper, we..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_S1xNEhR9KX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>On the Sensitivity of Adversarial Robustness to Input Data Distributions</a> <a class="note_content_pdf" href="/pdf?id=S1xNEhR9KX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019on,    &#10;title={On the Sensitivity of Adversarial Robustness to Input Data Distributions},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=S1xNEhR9KX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=S1xNEhR9KX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Neural networks are vulnerable to small adversarial perturbations. Existing literature largely focused on understanding and mitigating the vulnerability of learned models. In this paper, we demonstrate an intriguing phenomenon about adversarial training, arguably the most popular robust training method in the literature: Adversarial robustness, unlike clean accuracy, is sensitive to the input data distribution. Even a semantics-preserving transformations on the input data distribution can cause a significantly different robustness for the adversarial trained model that is both trained and evaluated on the new distribution. Our discovery of such sensitivity on data distribution is based on a study which disentangles the behaviors of clean accuracy and robust accuracy of the Bayes classifier. Further empirical investigation on MNIST and CIFAR10 confirms our finding that numerous MNIST and CIFAR10 variants achieve comparable clean accuracies in various different neural nets under standard training but significantly different robustness under adversarial training. This counter-intuitive phenomenon indicates that input data distribution alone can affect the adversarial robustness of trained neural networks, not necessarily the tasks themselves. Lastly, we discuss the practical implications on evaluating adversarial robustness, and make initial attempts to understand this complex phenomenon.
</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">adversarial robustness, PGD training, adversarial perturbation, input data distribution</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Robustness performance of PGD trained models are sensitive to semantics-preserving transformation of image datasets, which implies the trickiness of evaluation of robust learning algorithms in practice.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_ByeXyX2j67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to all reviewers</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xNEhR9KX&amp;noteId=ByeXyX2j67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1443 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1443 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewers for their time and efforts. We especially appreciate that all reviewers find that the problem being investigated is interesting. 
We summarize our main contributions to address common concerns in this post, and provide more details in the responses to each reviewer.

To avoid clutter, we use the following abbreviated phrases:
"sensitivity" means "the sensitivity of adversarial robustness to input data distributions".
"clean accuracy" means "prediction accuracy of the standardly trained model on natural examples".
"robust accuracy" means "prediction accuracy of the adversarially trained model on adversarial examples".

Our main contribution is the discovery that the robust accuracy of adversarially trained models is very sensitive to input data distributions, which is previously unnoticed in the literature and in sharp contrast to the steady clean accuracy in the standard learning setting. In theory, we show regular Bayes error's invariance and robust error's sensitivity to input distribution shift. We also show that if the data is uniformly distributed in a unit cube, then the perfect decision boundary cannot be robust. On the other hand, for the binary MNIST dataset, we found a provably robust classifier, using the algorithm by Kolter and Wong (2017), which guarantees 97% robust accuracy under 0.3 \ell_infty perturbation. Such contrast motivates us to design systematic experiments (smoothed MNIST and saturated CIFAR in the paper) to investigate the dependence of adversarial robustness on the input data distributions, which empirically demonstrates the sensitivity. 

We admit (also in the paper) that we don't have a definitive explanation or remedy for such sensitivity. Section 5 is only an initial attempt, and we do not consider it as the main contribution of the paper. We examined some natural hypotheses. We found highly correlated factors, but they don't fully explain the phenomenon, which suggests the absence of obvious solutions. We report these results not to make conclusive claims, but to hope to inspire future research in this direction. 

We believe that solely by itself our discovery of the sensitivity is a significant contribution, and it has an important implication in both practice and theory. In practice, our finding raises questions on how to properly evaluate the adversarial robustness of different learning algorithms. Benchmarking adversarial robustness on only a few datasets may not be reliable due to such sensitivity. In theory, our finding opens a new angle for understanding the cause of "lack of robustness". More specifically, Schmidt et al. (2018) show that different data distributions could have drastically different properties of adversarially robust generalization. Our finding indicates that gradual semantics-preserving transformations of data distribution can also cause large changes to datasets' achievable robustness. Tsipras et al. (2018) hypothesize the existence of the intrinsic tradeoff between clean accuracy and adversarial robustness. Our work complements this result, by showing different levels of tradeoffs for different input data distributions.

In summary, we would like to emphasize that the main contribution of this paper is discovering and firmly demonstrating the existence of the sensitivity both in theory and in experiments, which we believe solely by itself is important in practice and understanding the phenomenon of adversarial examples. Although we don't have a definitive explanation or remedy for it, our paper is a starting point for future lines of research around this topic.
We have improved the introduction of the paper to make this message more direct. Please see the latest updated version.


Kolter, J. Z. and Wong, E. (2017). Provable defenses against adversarial examples via the convex outer adversarial polytope. arXiv preprint arXiv:1711.00851.

Schmidt, L., Santurkar, S., Tsipras, D., Talwar, K., and M Ë›adry, A. (2018). Adversarially robust generalization requires more data. arXiv preprint arXiv:1804.11285.

Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and Madry, A. (2018). There is no free lunch in adversarial robustness (but there are unexpected benefits). arXiv preprint arXiv:1805.12152.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Bkxs4_XJaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xNEhR9KX&amp;noteId=Bkxs4_XJaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1443 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1443 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">A nice paper that clarifies the difference between the clean accuracy (accuracy of models on non-perturbed examples) and the robust accuracy (accuracy of models on adversarially perturbed examples) and it shows that changing the marginal distribution of the input data P(x) while preserving its semantic P(y|x) fixed affects the robustness of the model. Therefore, testing the robustness of the model should be performed in a careful manner. Comprehensive experiments were performed to show that changing the distribution of the MINST (smoothing) and CIFAR (saturation) data could lead to a significant difference in robust accuracy while the clean accuracy is almost steady. In addition, a set of experiments were performed in an attempt to search for the criteria required for choosing a proper dataset for testing adversarial attack to measure the robustness. 

Although Iâ€™m not expert in the field of adversarial attack but the paper is very nice to read and easy to follow (I have not checked the proof of the theorems though). 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJgnyglkaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>investigation as to the origin of lack of robustness of classifiers to perturbations of the input data</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xNEhR9KX&amp;noteId=HJgnyglkaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1443 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1443 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper is interesting and topical: robustness to adversarial input presentation (or shifts in training data itself, even those of the nature described by the authors 'semantic-lossless' shifts). Adversarial inputs are investigated under l-inf bounded perturbations, while multiclass classification on images is the target problem considered. The theoretical parts of the paper, assigning lack of adversarial robustness to the shape of the input distribution (Section 2) is the strongest part of the paper, adding some simple and important insights. Unfortunately, the empirical part of the paper is weakened by an over-reliance of (custom perturbations of ) the popular MNIST and CIFAR10 datasets (which are themselves based on larger sets). Furthermore, the basic conclusion as to causes and remedies of lack of robustness is not evident, and it is not evident that it has been sufficiently investigated. Shape yes, differences in perturbable volume not (how does that concur with Section 2?), and inter-class distance also not. Are we to base these conclusions on 2 perturbed datasets? How are readers to synthesize the final conclusion that robustness is a 'complex interaction of tasks and data', other than what they would already expect? In short, a valiant effort, and a good direction, but one that needs more work.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1ey4m3jTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xNEhR9KX&amp;noteId=S1ey4m3jTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1443 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1443 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the effort in reviewing the paper and also finding that the topic is interesting.

--------
&gt;&gt; "The paper is interesting and topical: robustness to adversarial input presentation (or shifts in training data itself, even those of the nature described by the authors 'semantic-lossless' shifts)"

We don't quite understand the phrase "adversarial input presentation" in the review. Just to avoid possible misunderstanding, let us first clarify our finding in this paper: the sensitivity of adversarial robustness to the input data distribution D. Assume that R is the adversarial robustness of the method of adversarial training on D. Now consider a semantic-lossless shift of D, say D'. Again we train a model on the training data sampled from D', and test its adversarial robustness R' on the test set again sampled from D' too. We find that R' can be significantly different from R. We have improved the introduction to make this point more clear.

So, in our terminology, there is no adversarial input presentation. D' is just a common 'semantic-lossless' shift of D in the paper, without being against adversarial training.


--------
&gt;&gt; "Unfortunately, the empirical part of the paper is weakened by an over-reliance of (custom perturbations of ) the popular MNIST and CIFAR10 datasets (which are themselves based on larger sets). Furthermore, the basic conclusion as to causes and remedies of lack of robustness is not evident, and it is not evident that it has been sufficiently investigated. Shape yes, differences in perturbable volume not (how does that concur with Section 2?), and inter-class distance also not. Are we to base these conclusions on 2 perturbed datasets? "

We have two sets of experiments in this paper, one in Section 3 and one in Section 5. We are not sure which one this review corresponds to.

As mentioned above, the purpose of Section 3 is to demonstrate the existence of the sensitivity. The transformations (semantic-lossless shifts) in this section are common and are not designed to overthrow the adversarial robustness. We believe our experimental results in this section is sufficiently significant to prove the existence of such sensitivity. Furthermore, the gamma correction in Section 4 is a common transformation in image processing, which also demonstrates the sensitivity.
 
On the other hand, we agree with the reviewer that results in Section 5 are not evident enough to conclude about the causes and remedies. In fact, it is exactly because of this that we only make conservative claims in this section. As mentioned above, Section 5 is an initial attempt, and we do not consider it as the main contribution of this paper. Although we don't have a definitive answer for the problem, we believe that our findings in this paper should be noticed by the adversarial example community and it is already sufficiently significant and important for a publication.

Lastly, let us also clarify our statements on perturbable volume and inter-class distance in Section 5. We found that they are both correlated with robust accuracy. However, when we examine whether they are decisive factors for robustness, counterexamples exist for both of them. We therefore made inconclusive statements highlighting the complexity of the problem. We faithfully report our investigations in this section, and we hope that they can inspire future research around this topic. 

In case our responses above do not address your concerns, it would be nice if you can clarify them further. 


--------
&gt;&gt; "How are readers to synthesize the final conclusion that robustness is a 'complex interaction of tasks and data', other than what they would already expect?" 

We do not intend to make this a "final conclusion". By "complex interaction of tasks and data", we only want to make a remark on the sample complexity difference between binarized MNIST and binarized CIFAR10. Our intention is to truthfully report to the readers that although binarization largely affects robustness, it does not decide every aspect of it. We have updated our paper to avoid these confusions.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rke7hBji2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A super interesting paper discussing the impact of data distribution on adversarial robustness of trained neural networks</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xNEhR9KX&amp;noteId=rke7hBji2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1443 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1443 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper provides several theoretical and practical insights on the impact of data distribution to adversarial robustness of trained networks. The paper reads well and provides analysis on two datasets MNIST and CIFAR10. I particularly like the result demonstrating that a lossless transformation on the data distribution could significantly impact the robustness of an adversarial trained models. The idea of using smoothness and saturation to bridge the gap between the MNIST and CIFAR10 datasets was also very interesting. One thing that is not clear from the paper is how one could use the findings from this paper and put it into practice. In other words, it would help if the authors could provide some insights on how to improve a model robustness w.r.t the changes in the data distribution. The authors did an attempt toward this in section 5, but that seems to only cover three factors that do not cause the difference in robustness.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkeFL72oTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xNEhR9KX&amp;noteId=SkeFL72oTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1443 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1443 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for your time spent, interests and kind evaluation on our work.

With regard to your comments on "put findings into practice" and "how to improve a model robustness w.r.t the changes in the data distribution", we believe these are very meaningful future research directions. We covered a few aspects, but other aspects are out of the scope of our current paper. In particular, Section 4 demonstrates the issues of robustness evaluation caused by the sensitivity. Section 5 intends to inspire future research by excluding the possibilities that the sensitivity can be explained by obvious reasons, or be resolved in trivial ways, which implies that understanding the causes or finding the remedies is non-trivial future directions.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>