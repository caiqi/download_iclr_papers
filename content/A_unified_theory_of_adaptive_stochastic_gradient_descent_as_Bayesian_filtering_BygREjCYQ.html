<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>A unified theory of adaptive stochastic gradient descent as Bayesian filtering | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="A unified theory of adaptive stochastic gradient descent as Bayesian filtering" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BygREjC9YQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="A unified theory of adaptive stochastic gradient descent as..." />
      <meta name="og:description" content="We formulate stochastic gradient descent (SGD) as a novel factorised Bayesian filtering problem, in which each parameter is inferred separately, conditioned on the corresopnding backpropagated..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BygREjC9YQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A unified theory of adaptive stochastic gradient descent as Bayesian filtering</a> <a class="note_content_pdf" href="/pdf?id=BygREjC9YQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019a,    &#10;title={A unified theory of adaptive stochastic gradient descent as Bayesian filtering},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BygREjC9YQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=BygREjC9YQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We formulate stochastic gradient descent (SGD) as a novel factorised Bayesian filtering problem, in which each parameter is inferred separately, conditioned on the corresopnding backpropagated gradient.  Inference in this setting naturally gives rise to BRMSprop and BAdam: Bayesian variants of RMSprop and Adam.  Remarkably, the Bayesian approach recovers many features of state-of-the-art adaptive SGD methods, including amongst others root-mean-square normalization, Nesterov acceleration and AdamW.  As such, the Bayesian approach provides one explanation for the empirical effectiveness of state-of-the-art adaptive SGD algorithms.  Empirically comparing BRMSprop and BAdam with naive RMSprop and Adam on MNIST, we find that Bayesian methods have the potential to considerably reduce test loss and classification error.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">SGD, Bayesian, RMSprop, Adam</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We formulated SGD as a Bayesian filtering problem, and show that this gives rise to RMSprop, Adam, AdamW, NAG and other features of state-of-the-art adaptive methods</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">22 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SkxmoLr53Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>I do not find the results of the paper particularly convincing though I would not rule out Bayesian filtering as a framework for analyzing adaptive methods</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygREjC9YQ&amp;noteId=SkxmoLr53Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper44 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper44 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Paper summary: The authors analyze stochastic gradient descent through the lens of Bayesian filtering. In doing so they (approximately) recover several common adaptive gradient optimization schemes. The paper focuses on a theoretical construction of this framework and offers a limited empirical study.

Detailed comments:

I thought that the paper presented some interesting ideas but amongst the many things discussed there is very little which is empirically gratified. While the Bayesian filtering framework is interesting in that it recovers slight variations of existing algorithms, and also caters for some recent practical tricks, I do not feel that it substantially improves our theoretical understanding of these methods.

1) I found the notation difficult to follow in the introduction and parts of section 2. I have highlighted several places explicitly below. I found paragraphs 2 and 3 of the introduction particularly challenging.

2) I found the introduction of Bayesian filtering challenging to follow. For example, which form of the likelihood is assumed for the Taylor expansion? How/why is $\mu_{like}$ identified using the gradient? Linking to Kalman filtering made things easier to follow.

3) I think that the related work, and possibly a chunk of section 2, should include a discussion of Noisy Natural Gradient [1]. While the derivation differs, the motivation and final form of the updates seem to have a large overlap but this work is not cited.

4) Start of 2.1: "z will have on element representing a single parameter", after which z is treated as a vector. I believe this sentence is present to distinguish RMSProp from Adam when momentum is added but I found it confusing at first.

5) I found the comparisons between BRMSProp-vs-RMSProp and BAdam-vs-Adam fairly unconvincing. The assumptions are not clearly demonstrated to have little practical significance and Figure 2. does not seem to support the claim that these methods are strongly related. Is it possible to demonstrate empirically that these algorithms have equivalent behaviour under some limiting factors? And if not, is there a good reason for this that still justifies the comparison? I would appreciate some clarifications on these points.

6) I am not sure what you mean by "We now assume that the data is strong enough to reduce the uncertainty in the momentum below its levels under the prior". I believe that I am following the mathematical arguments correctly but I find this phrasing misleading. Furthermore, this section uses e.g. ppth and wpth to refer to coordinates, I think it would be clearer to simply write Sigma_{pp}, etc.

7) Section 4.2 is lacking justification in my opinion (am I missing something?). I think that this section needs to have the derivation clearly laid out (in the appendix would be fine). Furthermore, the NAWD algorithm is not explored empirically, or analyzed theoretically at all. I would argue that more evidence is needed that this is a reasonable thing to do before it is meaningful to include it in the final print of this paper. In general, sections 4.4 - 4.7 feel a little out-of-place and thrown together. I think there are interesting comments here which are certainly worth including but their presentation should be rethought and some empirical investigation would be valuable.

Minor comments:

- In introduction, how exactly does $w'_i$ differ from $w_i$?
- In introduction, after para 2, the notation in the equation is confusing, e.g. overloading w_i(t) and w_i(mu_{-i}(t)).
- In introduction,  para 3, "must depend on other parameters" - this seems like an obvious statement but it is presented as being crucial
- Should "Related Work" start at 1 or 2?
- (VERY MINOR) In section 2.2 and 2.3, "christen" seems like an add choice of word. Perhaps just "call"?
- Equations 10 and 11 introduce an independence assumption on the dimensions of the parameter vector. I think this should be explicitly stated.
- Section 7.2 heading typo: MOMEMTUM

Clarity: I found the paper challenging to follow in places due to choices of notation (and a weak background in Kalman filtering and related techniques).

Significance: I do not feel that this work offers a strong case for significance. The empirical evaluation is very limited. The theoretical framework introduces is interesting but is not justified particularly well in the paper and does not directly offer explanations for many of the observations noted in this paper and elsewhere.

Originality: To my knowledge, the ideas presented in the paper are original and hint at potentially interesting viewpoints of optimization.

References:

[1] Zhang et al. "Noisy Natural Gradient as Variational Inference" <a href="https://arxiv.org/pdf/1712.02390.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1712.02390.pdf</a></span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryxGTLVjam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response (1/2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygREjC9YQ&amp;noteId=ryxGTLVjam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper44 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper44 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">To my knowledge, this is the first work that is able to reconcile Bayesian inference with adaptive SGD methods such as RMSprop or Adam.  (Please see the response to reviewer 3, or the new introduction for details).  To do this, we introduce an entirely new approach to relating optimization and Bayesian inference, where we define a Bayesian generative model for the backpropagated gradients.  This model implicitly converts intractable high-dimensional correlations in the original posterior into tractable low-dimensional temporal changes.  Empirically, this approach gives very promising initial results.

1) I have replaced paragraphs 2 and 3 with a section entitled "Factorisation implies a rich dynamical prior", which takes a far more in-depth look at how writing down a factorised generative model forces us to include a rich dynamical prior.  Also see the response to reviewer 3.

2) I have replaced the title of this section with Bayesian (Kalman) filtering as adaptive SGD to emphasise the link to Kalman filtering.  Hopefully the new introduction has made this clearer.  Otherwise, the approach is relatively standard (e.g. Zhang et al. 2017).

3) We have included a discussion of [1] in the introduction.  In short, they state: "These update rules are similar in spirit to methods such as Adam, but with the addition of adaptive weight noise. We note that these update rules also differ from Adam in some details: (1) Adam keeps exponential moving averages of the gradients, which is equivalent to momentum, and (2) Adam applies the square root to the entries of f in the denominator. We define noisy Adam by adding momentum term to be consistent with Adam. We regard difference (2) inessential. The choice of squaring or divison may affect optimization performance, but they don't change the fixed points, i.e. they are fitting the same functional form of the variational posterior using the same variational objective."  Thus, they cannot be considered as recovering Adam from natural gradient VI, because natural gradient VI doesn't give momentum or the root-mean-square form for the normalizer.

4) I've flipped the sentence around so hopefully it's a bit less confusing.  It now reads:
"For Adam, z will have two elements representing a parameter and the associated momentum, whereas for RMSprop it will have only one element representing a single parameter."
I have also added a new equation defining z for RMSprop and Adam here, to clarify the shape of z in the subsequent derivation.

5) I was also surprised to see such substantive performance improvements with BRMSprop and BAdam, as compared with RMSprop and Adam: this is the benefit of obtaining updates as inference under a well-motivated prior.  The justification for the comparison comes from the sections "Recovering RMSprop from BRMSprop" and "Recovering Adam from BAdam".  The correspondence is close for RMSprop (becoming exact as t increases, if g^2 remains constant), and somewhat less so for Adam.  But our goal wasn't to match RMSprop and Adam exactly.  Our goal was to improve upon RMSprop and Adam by capturing their essential dynamical assumptions under a well-motivated Bayesian prior.  As such, there's a sense in which we don't actually want RMSprop and BRMSprop to be exactly equivalent, because if they were equivalent, we wouldn't be able to get inspiration for new adaptive methods, and we couldn't get improved performance.

Otherwise, in Fig. 2, we attempted to make these comparisons as fair as possible otherwise (same network, same initialization, same momentum for Adam and BAdam, sweeping out all sensible learning rates for both methods).  As such, it is also possible to take Fig. 2 (now Fig. 3) as showing considerably improved performance over standard adaptive baseline methods.

6) In the supplementary section entitled "Setting the momentum decay", we show that under the prior, the uncertainty in momentum is eta_p/2.  Here, we make the assumption that data is reasonably strong, and so our uncertainties are well below those under the prior, i.e. Sigma_pp &lt;&lt; eta_p/2.  If this were not true, the network would be unlikely to be able to learn anything.  As such, we might expect BAdam and Adam to differ in the case where the data is weak, and it would be interesting to establish which approach has better empirical performance in that case.

I have replaced references to the ppth element with the "lower-right" element of the matrix equation, and explicitly stated the component equation that I'm referring to.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1lwDgEi6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response (2/2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygREjC9YQ&amp;noteId=r1lwDgEi6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper44 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper44 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">7) We have added an Appendix section spelling out how inference in this filtering model implements NAG.  The empirical evaluations in Fig. 2 include NAWD as part of the full BRMSprop and BAdam algorithms.  We agree that it would be great to do an empirical investigation of all the features discussed in this paper, but this becomes an empirical analysis of a pretty large swathe of techniques for adaptive stochastic gradient descent, which is out of scope for the present paper.

Minor comments:
- w' has been removed.
- While we have rewritten this section, we have retained the notation.  w is the underlying unknown random variable, whereas mu is our current mean estimate of that variable. Hopefully this distinction is clearer in the new version.  We are also using the recommended DL book notation for random variables.
- I agree, it is obvious.  But it is also crucial: it is the key for why we need to introduce a rich dynamical prior.  Again, hopefully this discussion has been improved in the new version.
- Related work has been incorporated into the introduction.  I've also explicitly introduced an Introduction section, so that next section is 2.
- I have replaced christen
- The independence assumption comes right at the start when we write down a factorised generative model (as we explain in the new section "Factorisation implies a rich dynamical prior"  in Eq. 10 in the original submission, we're doing filtering in a 1D inference problem.
- Fixed</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BkeCieGqn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Promising novel research, high practical relevance</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygREjC9YQ&amp;noteId=BkeCieGqn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper44 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper44 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">* Description

The paper considers the following random process on the parameters z (modeled as Gaussians):
- shrink z towards zero and add Gaussian i.i.d. noise to it.
- update the parameters to the posterior w.r.t. a batch, where the likelihood is approximated as a diagonal multivariate normal distribution.
This results in a Kalman filter like updates. There have been related methods proposed performing Bayesian learning in the form of assumed density filtering, considered as separate learning algorithms. At the same time methods such as RMSprop and Adam were previously derived from completely different considerations. The work can derive these methods in the Bayesian framework with certain additional assumptions / simplifications. It allows to naturally explain tracking the gradient statistics as uncertainties and the normalization of the gradient in the existing methods as the update of the mean parameters in the Kalman filter taking into account these uncertainties. 
The experiments on MNIST show that derived more Bayesian variants of RMSprop and Adam can improve generalization in terms of test likelihood and test error. 

* Assessment

The provided derivation of Bayes like learning algorithms is relatively simple and could be very useful in practice and in further improvement of the learning methods. The approximations used are not completely clear. The clarification of the idea of a separate optimization problem per variable is necessary. The provided experiments, if there is nothing subtle, are clearly done and would be sufficient.
There are some open questions such as: does the method in fact learn useful variances of the parameters, i.e. really performs an approximate Bayesian learning? Overall if find it a promising novel research direction of high practical relevance.

* Clarity

Intro:
Why is the unnumbered equation on page 1 is called a “Bayesian optimization problem”? There is so many sings called Bayesian that one cannot be sure what it means. In the context of the paper it should be a Bayesian learning problem, but I do not see a posterior distribution over the parameters. Overall, I did not get the point of the discussion in the introduction and Figure 1 altogether. Everything it says to me is that global minimize coordinates are dependent through the objective. I do not see what the unnumbered equation on page 1 has to do with Bayesian inference and how the correlation of parameters in the posterior distribution is related to the dependencies in the minimizer. Could authors please seriously consider clarifying this section?
In what follows the paper keeps a factorize approximation to the posterior of parameters of a NN in the form of a Gaussian distribution per coordinate. It thus does not in any way avoid making this restrictive assumption.

Results:
Sorry, I am not familiar with the background behind (6). Which value of z is assumed in the conditional expectation, is it conditioning on “z = \mu_{prior}”? How come the approximation to the variance of the data likelihood does not depend on the data? If we make this approximation, how much it is still relevant to the Bayesian learning?

What are the overheads of the proposed methods? I expect they scale as easily to large problems as SGD?

* Experiments

From Figure 2 it seems that BRMSprop and BAdam can achieve relatively good results for large range of eta in 10^-5 to 10^-2 and it seems from the trend that even smaller eta would work. Does it mean they do not need in fact tuning of the learning rate? 
The experiment uses 50 epochs, do the compared methods reach the convergence? Could the authors consider an experiment running best setting of parameters per method with twice as many epochs?
Some artificial toy experiments could be of interest. For example, consider a classification problem with a 1D Gaussian data distribution in each class and the logistic regression model with 2 parameters. Does the method approximate the posterior distribution?

* Related work

The approach to Bayesian learning taken in the paper needs to be better discussed. I think it is from the family of methods known as “assumed density filtering”, occurring in:
Ghosh et al. “Assumed Density Filtering Methods for Scalable Learning of Bayesian Neural Networks”
with earlier works well described in 
Minka T. “Expectation propagation for approximate Bayesian inference”. 
In particular equation (5) of the submission is well known.
The work  Khan et al. 2018 “Fast and scalable Bayesian deep learning by weight-perturbation in Adam” also derives Bayesian learning algorithms in the forms closely similar to RMSprop and Adam and interprets the running statistics as uncertainties. However it takes the variational Bayesian learning approach, which means the reverse KL divergence is used somewhere. Could the authors discuss conceptual similarities and differences to this work?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryeqhwEjpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response (1/2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygREjC9YQ&amp;noteId=ryeqhwEjpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper44 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper44 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Assessment: We have hopefully clarified the approximations (esp. in Adam) and the introduction of a separate inference problem per variable (see the new sectinon "Factorisation implies a rich dynamical prior".  It should be noted that the methods we propose --- BRMSprop and BAdam --- don't use these approximations.  Instead, the approximations are only necessary to understand the similarities and differences betweeen BRMSprop and BAdam and the corresponding classical methods: RMSprop and Adam.  Finally, I agree that assessing the effectiveness of the Bayesian model for the optimal weight would make interesting future work, and we envisage including it in a broader investigation of the statistics of optimal weights under optimization.

Intro: we have deleted the reference to Bayesian optimization, and rewritten this section in order to clarify it considerably (see the section "Factorisation implies a rich dynamical prior").  In this section, we note that we aren't making a factorised approximation in the usual sense, where you write down an inference problem over all N parameters jointly, and use a factorised posterior.  Instead, we change the inference problem itself, by writing down N inference problems, one for each parameter, where the data for each individual problem is the backpropagated gradient.  This bakes factorisation into the problem setting. 

When we look at the generative process for the gradients, we find that the potential for a rich dynamical prior has emerged automatically, because the gradients for one parameter depend on all the others, and those other parameters change slowly as they are also being optimized. Critically, this converts intractable high-dimensional correlations in the original problem into tractable low-dimensional dynamics.  In contrast, these dynamics do not emerge in a traditional approach where you simply approximate the high-dimensional posterior.

Results: this is an interesting point, which bears further discussion.  I agree that the obvious approach here would be to use the Hessian (i.e. the second derivative of the actual likelihood).  However, using Hessian directly has two problems.  First, the Hessian isn't necessarily positive definite, and if such a likelihood is combined with a sufficiently weak prior under our approximations, it can result in a meaningless posterior (e.g. a Gaussian with a non-positive semidefinite covariance matrix).  Second, the Hessian (i.e. the full matrix of second derivatives) is difficult and expensive to compute in modern autodiff software such as PyTorch.  Because of these two disadvantages, we chose to work with a closely related quantity: the Fisher Information.  This is the Hessian for the expected log-likelihood of data drawn from the model, and it therefore represents an approximation to the actual Hessian.  However, in the case of classification, we expect the approximation to be reasonable, because it is based on the same input data, with output labels sampled from the model.  And as the classification error gets smaller, the model outputs and the true classification become more similar, and thus the approximation becomes becomes increasingly good.  Importantly, the Fisher Information resolves the two issues we had above: it is always positive definite, and it can be computed by taking the covariance of gradients, which are easy to compute in standard autodiff frameworks.  This is a standard approach (e.g. Zhang et al. 2017).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BkeX5DNjTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response (2/2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygREjC9YQ&amp;noteId=BkeX5DNjTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper44 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper44 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Experiments: usually, when we set the learning rate to be too small, it is impossible to move the parameters far enough from their initializations, and so we obtain very poor performance.  One of the interesting things about our framework is that as eta goes to zero, it converges to Bayesian inference without the dynamical prior, which, in effect, gives an adaptive learning rate that goes as the sum-of-square gradients.  In practice, we still expect to need to tune the learning rate to find the minimum of the curves in Fig. 2 (now Fig. 3).  As regards toy experiments, we expect the benefits of our approach to become more evident in complex models with strong posterior correlations, so the simplest relevant toy experiment is linear regression with highly correlated inputs --- which isn't all that simple.  Regarding convergence, tried a range of numbers of epochs, and found that the displayed curves are pretty stable at 50 epochs, but that if anything, the difference between the Bayesian and classical methods actually increased for larger number of epochs.

We have discussed the ADF-style approach (Ghosh et al.), and have noted that the Kalman filtering approach is well-understood. Finally, it should be noted that Khan et al. (2018) does not recover the root-mean-square-gradient form for the normalizer.  To quote from Khan et al. (2018):
"Using ... an additional modification in the VON update, we can make the VON update very similar to RMSprop. Our modification involves taking the square-root over s_{t+1} in (7)"
Both they and Zhang et al. (2017) of these approches use natural gradient VI, and they both encounter the same problem: that natural gradient gives you a mean-square-gradient, rather than a root-mean-square gradient form for the normalizer.  Khan et al. (2018) deal with this by reaching in and replacing the mean-square with a root-mean-square normalizer (without a principled justification based on approximate inference), and Zhang et al. (2017) regard the difference between a root-mean-square and a mean-square gradient normalizer as "inessential".</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJxLfaOlCQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Further clarification requests</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygREjC9YQ&amp;noteId=rJxLfaOlCQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper44 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper44 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I appreciate very much what authors explained in their response and the improvements made to the paper. The following details need further clarification in my opinion.

I understand the the issue of symmetries in model parameterization is avoided by considering the Laplace approximation to the posterior (a multivariate Gaussian around a mode of the likelihood). Furthermore, because the factorization of the posterior is an undesirable assumption, it is proposed to consider the distribution (1) and its Laplace approximation, which matches the respective conditional distribution of the global Laplace approximation.

The mode of this approximation is very unfortunately denoted the same way as the random variable itself, w*_i. Why not to use mu_i for the conditional mode? Also, expanding the log likelihood as the expectation seems unnecessary at this point.
The following notation is confusing:

P (w*_i | w_{−i} = µ_{−i}, D)
because w*_i is already defined as a shorthand for w_i | w_{−i} = µ_{−i}, D in (1). The conditioning accomplishes nothing.
The argument of this distribution being intractable  is unclear because it is a 1D distribution and given the data its empirical distribution or moments can be computed.

P(w*_i | D) - this does not make sense to me if definition (1) is used. If the authors want to speak about the marginal distribution P(w_i | D), it has been already discussed in the beginning of sec. 2. 

Equation (3) models the predictive probability for each batch of data as Gaussian. This would break down for e.g. batch size 1 and logistic model. I.e. some comment on batch size is needed. After this assumption is made, the distribution of  the whole data D is seen to be Gaussian (as a product of Gaussians over all batches) as well as the posterior distribution w_i | w_{-i}, D as desired, these distributions should be possibly given instead of the chain (6)-(7). In particular this will define w^*_i and Lambda_i as parameters of 
w_i | w_{-i}, D under this approximation.

In (6), I do not understand µ_{like,i} | Lambda_i, w^*_i. Does this mean conditioning on the r.v. w*_i being equal to the mode of its approximate distribution w*_i? This is very confusing. The remark that "w*_i must be a valid sample" is not helpful at all. It is further unclear which distribution is assumed and which is being approximated. The right hand side Lambda_i cannot be the batch-dependent Lambda_i in (4), can it?

The discussion about factorization matter reduces in the end to the difference in estimating
the marginal distribution of the Laplace approximation of w | D (which is a single Gaussian around a mode)
versus conditional distribution w_i| w_{-i}, D of that very same approximation at w_{-i} fixed to its mean. Interestingly, the conditional distribution will have the same mean, but different variance from the marginal distribution (and seems to be the one that can be connected to the variance of the gradient).
Now with this difference visible I do buy the arguments about coupling of parameters and not making the factorization assumption. However, speaking of multiple independent estimation problems still seems a suboptimal explanation. Can it be replaced with iteratively estimating the conditional distributions?

Please address these issues, I believe the intro could be still significantly clarified and made more accessible.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HkxH12v-RX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygREjC9YQ&amp;noteId=HkxH12v-RX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper44 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper44 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your comments.

Eq (1) and Eq (2) are actually equivalent: I agree, it would be crazy to use w*_i for two different things.  I think of Eq (1) as the "primary" definition of w*_i.  The confusion comes in with Eq (2).  In particular, Eq. (2) is *not* the mode of the usual likelihood, conditioned on all the data.  Instead, the expectation is taken over data, d, drawn from the underlying, unknown, true data distribution (which is why we need to write it as an expectation).  As such, Eq. (2) in effect does maximum likelihood with infinite data, which picks out the true value for the parameter, w*_i.  I have altered the text to clarify this point.

There was a slight problem with the notation here: we have changed P(w*_i | w_{−i}=µ_{−i}, D) to P(w*_i | µ_{−i}, D).  That said, w*_i as defined by Eq. (1) is a standard random variable, so it can be marginalised and conditioned, just like any other.  In particular, we can marginalise over µ_{−i}, using,
P(w*_i | D) = \int dµ_{−i} P(w*_i | µ_{−i}, D) P(µ_{−i}| D),
while it is difficult to write down the distribution over µ_{−i}, it could be sampled. In fact, this is just what we do in transitioning from Fig. 1A to Fig. 1B: the integral over µ_{−i} is difficult, so instead we directly write down simplified dynamics over w*_i.

Computing one of these distributions is order(N), where N is the number of parameters, so it should be tractable.  However, computing all the distributions is order(N^2), which is intractable for all but the smallest networks.

Note that while Eq. (3) takes a "Gaussian-like" form, d doesn't appear in the correct place on the RHS.  Instead, this is a Gaussian over a parameter, w_i.  What's going on here is that we're only interested in the dependence on w_i, so we have put all the dependence on the minibatch into const, mu_{like,i} and Lambda_i.  As these are arbitrary functions of the data, we can, in fact have arbitrary data.  More broadly here, we're treating the likelihood as function of the parameters (e.g. <a href="http://www.inference.org.uk/mackay/Bayes_FAQ.html#likelihood)." target="_blank" rel="nofollow">http://www.inference.org.uk/mackay/Bayes_FAQ.html#likelihood).</a>

mu_{like,i} is the mode for one minibatch.  w*_i is expected mode, given infinite data.  Because mu_{like,i} depends on the minibatch, it will exhibit some variability across minibatches, mu_{like, i}, but it will have mean w*_i.  I have updated this section, to note that we can also get the variability of mu_{like, i} by considering the Fisher Information, which gives us the variance of g.  Lambda_i can be batch-dependent (though the derivation simplifies considerably, if we assume that it is fixed, and I did this initially).  For instance, consider classification with noisy labels (e.g. from mechanical turk), where some labels are known to be high-quality, and others are low-quality.  The model should use higher Lambda_i for high-quality, and thus more informative labels.

I think the same basic approach could be used in many domains: importing some aspects of the approximations into the inference problem, such that we can use Bayes theorem to reason under the approximations.  However, I'm not sure this would simplify the presentation much.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HklQLC5WCX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarity / Technical correctness</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygREjC9YQ&amp;noteId=HklQLC5WCX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper44 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper44 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I thank the authors for their response. I will consider it in more detail when I have time. At the moment the following is important. I like very much the idea of the paper. However, when it comes to the final recommendation, clarity and technical correctness are necessary. At the moment the paper says:

w*_i ~ P(w_i | ...),
which I read as "w*_i is a r.v. distributed as P(...)". Right next, it is
"Note that w*_i  is close to, but not quite the mode of the likelihood of the actual data" 
However, it cannot possibly be the mode, because it is a random variable. If it was not a random variable, one cannot write P(w∗i|µ−i, D). 

At present, due to non-standard and inconsistent notation, it is not possible to follow the work. I mean also from the point of view of the future readers.

I would recommend the following:
-clearly distinguish random variables and their possible values
-avoid condition on random variables, only on events (or expect the result of e.g. E[stuff|w*_i] to be a random variable).
-make all events you condition on clear, for example, conditioning on \mu_i is not clear.
- check that all entities are defined before they are used. For example, Lambda_i is not defined other than a data-dependent r.v. (4). Nevertheless it is present on the RHS of (9) while not being conditioned on in the LHS. This step in particular, silently omiting Lambda_i in P(g|w*) is currently a gap in the derivation. Making everything precise should help to identify and resolve such gaps.
- make all claims step-by step verifiable. For example the variance in (6) is not. One have to guess about the meaning, assumptions and the proof.

Also, cannot the expression (6) for the precision of the batch likelihood be used instead of the estimation in (9.7)?

Please take you time to make the work solid.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_S1epgrIt2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Unnatural approximations</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygREjC9YQ&amp;noteId=S1epgrIt2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper44 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper44 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=S1epgrIt2Q" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this work, the authors attempt to unify existing adaptive gradient methods under the Bayesian filtering framework with the dynamical prior.  In Ollivier, 2017, a framework is proposed to connect Bayesian filtering and natural gradient.  On the other hand,  in Khan et al., 2018. an approach is proposed to connect natural gradient and adaptive gradient methods.  The main contributions of this work are (1)  introducing a dynamical prior and (2) recovering RMSProp and Adam as special cases. 

However, the proposed dynamical prior is very similar to the fading memory technique used in Ollivier, 2017. (see Proposition 3 of Ollivier, 2017) 
Furthermore, the authors argue that this work recovers a root-mean-square form while Khan et al., 2018 recovers a different sum-square form. Unfortunately, the authors have to use a series of unnatural approximations to recover the root-mean-square form. In fact, as mentioned in Khan, 2017b  this proposed method without these approximations is also a mean-square form. (also see Eq (2.28-2.29) of Ollivier, 2017)

Since the authors mainly follow Ollivier, 2017 and make unnatural approximations,  the work has a limited impact.  To get a higher rating, the authors should clearly give justifications and insights of these approximations.

Detailed comments:
(1) On Page 1,  "The typical approach to Bayesian filtering, where we infer a distribution, ... jointly, forces us to use extremely strong, factorised approximations, and it is legitimate to worry that these strong approximations might meaningfully disrupt the ability of Bayesian filtering to give close-to-optimal updates.   ... we instead consider ... that incorporates factorisation into the problem setting, and therefore requires fewer approximations downstream. "
The proposed method is equivalent to jointly perform Kalman filtering with full-covariance with an additional diagonal-approximation step. This additional step might also meaningfully disrupt the ability of Bayesian filtering. Furthermore, such approximation ignores the off-diagonal terms in the low-rank approximation at Eq (8). 

Minor: You should use \approx at Eq (8) since a rank-1 approximation is used.  

(2) On page 2, "It has been noted that under specific circumstances, natural gradient is approximate Bayesian filtering (Ollivier, 2017), allowing us to link Bayesian filtering to the rich literature on natural gradients.  However, this only occurs when the dynamical prior in the Bayesian filtering problem has a specific form: the parameters being fixed over time (i.e.  arguably an online data, rather than a true Bayesian filtering setting)." 
The authors should comment the difference between the dynamical prior and the fading memory technique (see Proposition 3 of Ollivier, 2017) where at page 14 of Ollivier, 2017, Ollivier mentions that "this is equivalent ... or to the addition of an artificial process noise ... in the model".  I think Ollivier's idea is very similar to the dynamical prior used at Eq (1) of this submission.  Furthermore, the second-order Taylor expansion with a Fisher information-based estimation of Hessian (see the equation below Eq(1) of this submission) is exactly the same as Ollivier's Extended Kalman filter (see  Eq 2.25 at Lemma 9  and Lemma 10 of Ollivier, 2017).  The authors should cite Ollivier, 2017.

Minor: Eq (6) should be E_p [ - \nabla_z^2 \log p(d|z) ] = E_p  [ e e^T ], where "-", the negative sign is missing. Please see the definition of the Fisher information matrix.
 
(3) On page 2, "While there have been attempts to use natural gradients to recover the Adam or RMSprop root-mean-square form for the gradient normalizer, in practice a different sum-square form emerges (Khan &amp; Lin, 2017; Khan et al., 2018). In contrast, we show that to recover the Adam or RMSprop form for the gradient normalizer." 
Khan et al., 2018 is a mean-square form for variational inference due to the entropy term of the variational distribution. (see Sec 3 and 5 of  Khan et al., 2018 and Khan, 2017b )
Unfortunately, the "root-mean-square form" does not appear naturally in this submission. In practice, the proposed update is also a mean-square form  (see Eq (2.28-2.29) of Ollivier, 2017 and Khan, 2017b) without a series of unnatural approximations used in this submission.
To justify these assumptions, the authors should explain when "the steady state posterior variance" (see sec 2.21) and  "a self-consistent solution" (see sec 7.1) achieve.  As far as I know, \sigma^2_t = \sigma^2_{t+1} in sec 2.2.1 only holds in the limit case when t-&gt; \inifity.  Why does the equality hold at each time step t? The authors should give a justification or an intuition about these approximations since this paper is a theory paper. Please also see my next point.

(4) Section 7.1 is also confusing.
In sec 7.1, the authors assume that A \in O(\eta). However, A=\eta^2/(2\sigma^2) in sec 2.2 and A_{1,1} =  ( \eta_w^2+\eta^2 )/ (2\sigma^2) at Eq (14). In both cases, A can be \in O(\eta^2). This is very *critical* since the authors argue that O(\eta^3) can be neglected in sec 7.1.  The authors use this point to show that Adam is a special case. 
If A \in O(\eta^2), we know that "A \Sigma_{post}" \in O(\eta^3) should be neglected. At the last equation on page 10,  the authors do not neglect "A \Sigma_{post}". Why?  The authors should clarify this point to avoid doing *selective* neglection.  Again, the impact of this paper should be inspiring new adaptive methods.
The authors also mention that the second-order term in A is neglected in sec 7.2. Any justification? 


References
[1] Ollivier, Yann. "Online Natural Gradient as a Kalman Filter." arXiv preprint arXiv:1703.00209 (2017).
[2] Khan, Mohammad Emtiyaz, and Wu Lin. "Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models." arXiv preprint arXiv:1703.04265 (2017).
[3] Khan, Mohammad Emtiyaz, et al. "Vprop: Variational Inference using RMSprop." arXiv preprint arXiv:1712.01038 (2017b).
[4] Khan, Mohammad Emtiyaz, et al. "Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam" (2018)

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJgJEO4iTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response (1/2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygREjC9YQ&amp;noteId=BJgJEO4iTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper44 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper44 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">First, it should be noted that neither Khan et al. (2018) nor Zhang et al. (2017) recover the root-mean-square-gradient form for the normalizer.  To quote from Khan et al. (2018):
"Using ... an additional modification in the VON update, we can make the VON update very similar to RMSprop. Our modification involves taking the square-root over s_{t+1} in (7)"
And to quote from Zhang et al. (2017):
"These update rules are similar in spirit to methods such as Adam, but with the addition of adaptive weight noise. We note that these update rules also differ from Adam in some details: (1) Adam keeps exponential moving averages of the gradients, which is equivalent to momentum, and (2) Adam applies the square root to the entries of f in the denominator. We define noisy Adam by adding momentum term to be consistent with Adam. We regard difference (2) inessential. The choice of squaring or divison may affect optimization performance, but they don't change the fixed points, i.e. they are fitting the same functional form of the variational posterior using the same variational objective."
Both of these approches use natural gradient VI, and they both encounter the same problem: that natural gradient gives you a mean-square-gradient, rather than a root-mean-square gradient form for the normalizer.  Khan et al. (2018) deal with this by reaching in and replacing the mean-square with a root-mean-square normalizer (without a principled justification based on approximate inference), and Zhang et al. (2017) regard the difference between a root-mean-square and a mean-square gradient normalizer as "inessential".
Moreover, neither approach gives rise to momentum.

Olliver (2017) works hard to ensure that their filtering technique is equivalent to natural gradient VI, and therefore, while they don't examine it, they are also likely to be unable to recover the root-mean-square normalizer.  More problematic is the (as you quoted) "the addition of an artificial process noise".  However, it is important to complete the quote, "the addition of an artificial process noise Q_t proportional to [the posterior covariance] P_{t-1}"  There are two things that they could potentially be referring to as "artificial" here:
1.) The introduction of any process noise into the generative process.
2.) The introduction of a generative process that depends on inferences under that process.
Notably, 2 is very, very artificial: I have never seen a Bayesian generative model which depends on inferences under that model.  Our approach does not use this process noise, and as such, if nothing else, our generative process is more meaningful, in the very basic sense that it doesn't depend on the posterior.

Detailed comments:

(1) There is no such equivalence, because, as we make clear in a new section, entitled "Factorisation implies a rich dynamical prior", these two methods don't even perform inference over the same random variable. The approach that you suggest towards factorising the problem fails immediately because P(w_i| D) (i.e. the distribution over one parameter conditioned on all the data), is meaninglessly broad distribution, because the posterior distribution is highly symmetric (e.g. unit-swapping symmetries).  Further, under this model, there is no principled motivation for the introduction of dynamics.

Instead, we note that the updates in most algorithms for neural network optimization are based on just the gradient for that parameter.  As such, if we define a factorised model for each parameter separately, we can take the "data" to be not the underlying input-output pairs, but the backpropagated gradient for that parameter.  This generative model naturally gives rise to the emergence of a new latent variable, w_i^*, the optimal value for the ith parameter, conditioned on the current estimate of all the other parameters.  If we choose to work with this parameter, then dynamics emerge automatically: w_i^* must change over time, because it depends on our current estimate of all the other parameters, which are changing as they are optimized.

In essence, this approach converts the intractable high-dimensional correlations in the full posterior into tractable low-dimensional temporal dynamics.  Notably, dynamics do not and cannot emerge from a straightforward factorised approximation to the original full posterior.  I'd be grateful for any further assistance to clarify the presentation.

Minor: We have rewritten this section, pushing the discussion of Fisher Information into the Appendix, and defining e e^T such that it equals \mLambda.  This is possible because the Hessian is always rank 1 (as we now clarify).
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1lCMu4iTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response (2/2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygREjC9YQ&amp;noteId=S1lCMu4iTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper44 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper44 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">(2) We have included a paragraph in the introduction on the difference with Olliver's (2017) fading memory technique.  To reiterate, they use a highly unnatural generative process under which the process noise depends on inferences under that generative model (in particular, the posterior covariance).  In contrast, the process noise under our model is "meaningful" in the sense that it does not depend on inferences under the model.

Minor: we have delete this equation.

(3) Steady-state is indeed only reached as t -&gt; infinity, and we have added a note to this effect.  At the very least, we don't expect to obtain a mean-square form for the normalizer, because that would require us to follow Olliver (2017) in using process noise proportional to our uncertainty, which we categorically do not do.

(4) The first point is that we agree, the impact of this paper should be inspiring new adaptive methods.  And we do just that, with BRMSprop and BAdam.  Importantly, these approximations are never used in simulations/updates for BRMSprop and BAdam.  Instead, they are only used to think about the similarities and differences between our Bayesian approaches (BRMSprop and BAdam) and classical methods such as RMSprop or Adam.  As such, in some sense, the stronger the approximations we need to make BAdam close to Adam, the more scope we have for developing improved adaptive methods!

In response to your specific questions.
Here, we are considering the limit of \eta -&gt; 0, so assuming A scales with \eta is weaker than assuming A scales with \eta^2.  While A for RMSprop indeed scales with \eta^2, A for Adam scales with \eta (the top-right element is -eta). As A in general scales with \eta, the terms you refer to, A S_post, scale with \eta^2, and cannot be neglected in general.  However, for the case of RMSprop, A indeed scales with eta^2, and so we can neglect these terms, and indeed we do just that in the original draft (the approximate variance updates that we solve for in steady state do not include any weight-decay terms).  We have clarified this point in the supplementary, including both the RMSprop and Adam cases.

As a technical issue, the usual definition of big-O notation considers the limit as \eta -&gt; \infty, whereas the relevant limit in our case is \eta -&gt; 0.  I have therefore replaced the \in big-O notation, with the physics-inspired \sim (for "scales with").</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BklHJvro6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>incorrect statements</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygREjC9YQ&amp;noteId=BklHJvro6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper44 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper44 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value"> I  list some incorrect statements made in this revised version.


(1) "Moreover, neither approach gives rise to momentum." 
This statement is not true.
The authors should read Sec 4 of [4].

(2) "Notably, 2 is very, very artificial: I have never seen a Bayesian generative model which depends on inferences under that model.  Our approach does not use this process noise, and as such, if nothing else, our generative process is more meaningful, in the very basic sense that it doesn't depend on the posterior."
"Thus, their generative model depends on inferences made under that model: a highly unnatural assumption that most likely does not correspond to any “real” generative process."

Note that the fading memory technique and the extended Kalman filter both depend on inferences under that model. (please read [5]) 
As far as I know, the proposed method is an extended Kalman filter, which also depends on inferences under that model. 
I strongly do not like this statement since the authors make unnatural approximations hidden in the derivation while the authors claim their method is more natural than [1].  

(3) "the Hessian is always rank 1"
I do not think this statement is correct. The main reason to use g g^T (or e e^T) is due to the empirical Fisher estimation, which implies that the true Hessian is not a rank 1 matrix.


References:
[1] Ollivier, Yann. "Online Natural Gradient as a Kalman Filter." arXiv preprint arXiv:1703.00209 (2017).
[4] Khan, Mohammad Emtiyaz, et al. "Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam" (2018)
[5] Humpherys, Jeffrey, Preston Redd, and Jeremy West. "A fresh look at the Kalman filter." SIAM review 54.4 (2012): 801-823.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJggX1UoTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygREjC9YQ&amp;noteId=rJggX1UoTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper44 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper44 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">(1) Presumably, you mean AR(2)?  Vanilla gradient descent has AR(1) like dynamics, as the next position depends on the previous one, whereas momentum-based methods can be formulated as depending on the past two positions, where we estimate the velocity using their difference.

Regarding [1], we can always take an inference problem, convert it to an optimisation problem using VI, and apply momentum to perform the optimisation.  This is always possible, and is always a valid approach.  But this is using, rather than recovering momentum.  In contrast, in our work, we are interested in how momentum might emerge as the solution to a principled inference problem.  

An alternative viewpoint is to think about your reference (Olliver 2017).  As long as they're doing natural gradient, the link to Bayesian filtering and hence to Bayesian inference is retained. As soon as they stop doing natural gradient (e.g. by adding momentum) the link to Bayesian filtering is broken, and they are simply using some optimisation strategy (though perhaps a very effective one) to optimize a loss function (albeit one that happens to be motivated by Bayesian considerations).

(2) Under the standard set-up for an EKF, the generative model does not depend on inferences made under that model.  For instance, one might sample the latent variable z from the following process,
z(t+1) ~ N((I - A) z(t), Q),
and the observed data, g, from another Gaussian,
g(t+1) ~ N(f(z(t)), I)
Note that as we have an arbitrary function, f(z(t)), here, we are in the EKF setting that you describe.

To reiterate, this is the true, underlying generative model, and it never depends on inferences made under the model.  To perform extended Kalman filtering, we can define an approximate model as a linearisation of f around the current mean.  However, that is categorically not what is going on in Olliver (2017).  There they set the process noise, Q, equal to their filtering uncertainty.  Olliver (2017) do not claim, and it is not, to my knowledge possible to set up a valid generative model (i.e. one for which the process noise, Q, is a function of z) under which this emerges in an EKF-like fashion.

Please let me know if there are any other issues I can clarify.  I await with interest your opinion on our new derivations that treat the back propagated gradient as the data in a Bayesian inference problem!</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HygmvPLo6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>issue (2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygREjC9YQ&amp;noteId=HygmvPLo6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper44 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper44 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">"  Olliver (2017) do not claim, and it is not, to my knowledge possible to set up a valid generative model (i.e. one for which the process noise, Q, is a function of z) under which this emerges in an EKF-like fashion"

On Page 10 of  Ollivier 2017, " (Note, however, that the online natural gradient and extended Kalman filter are identical at every time step, not only asymptotically.)"  Also see Definition 5 on page 16 of  Ollivier 2017.
 
To reiterate, EKF itself depends on inferences under that model due to the Gaussian approximation.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1eKdWDiam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygREjC9YQ&amp;noteId=S1eKdWDiam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper44 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper44 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">To reiterate.  The EKF is an inference technique.  The underlying prior (just like all other prior distributions in the Bayesian literature) does not depend on inferences under that model.  For instance, on the Wikipedia page for the EKF (<a href="https://en.wikipedia.org/wiki/Extended_Kalman_filter)," target="_blank" rel="nofollow">https://en.wikipedia.org/wiki/Extended_Kalman_filter),</a> under "formulation", they define the prior, which makes reference to nonlinear functions, but not to inferences made under the model.

Of course, when performing inference, ("Discrete-time predict and update equations" on Wikipedia), they do make reference to the inferred means and uncertainties.  To perform inference (and only to perform inference), they linearise the nonlinear functions, around the current inferred mean.  To reiterate, there is --- and has to be --- an underlying nonlinear model that does not depend on the inferences.  Only the surrogate, approximate linearised model depends on the inferences, and then, only through the location at which you perform the Taylor expansion.

I agree Olliver 2017 obtain Kalman Filter updates that are equivalent to natural gradient at every step.  However, this requires them to use process noise, Q, proportional to the filtering covariance, P, i.e. they use Q = Q(P).  It is not possible, to my knowledge, to define a nonlinear EKF style model as in (https://en.wikipedia.org/wiki/Extended_Kalman_filter), under "formulation" which displays the same behaviour i.e. Q(P) = Q(z), and, again to reiterate, Olliver 2017 do not claim as such.

That said, for the main thrust of our argument, the artificiality or otherwise for the Olliver 2017 process noise is a side point.  Olliver 2017 work hard to ensure that their approach is equivalent to natural gradient, but that means they can't recover momentum, and don't recover the root-mean-square form for the gradient normaliser.  In contrast, we give a principled justification for a different Kalman filtering approach (with a different choice for the process noise) that is able to recover momentum, and the root-mean-square gradient normaliser.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1gRViDo6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>no</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygREjC9YQ&amp;noteId=H1gRViDo6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper44 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper44 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">(1) "The underlying prior (just like all other prior distributions in the Bayesian literature) does not depend on inferences under that model.  For instance, on the Wikipedia page for the EKF (<a href="https://en.wikipedia.org/wiki/Extended_Kalman_filter)," target="_blank" rel="nofollow">https://en.wikipedia.org/wiki/Extended_Kalman_filter),</a> under "formulation", they define the prior, which makes reference to nonlinear functions, but not to inferences made under the model."

Note that the EKF linearizes the non-conjugate/non-Gaussian likelihood or the observation likelihood. For example, the observation can be a positive definitive matrix. In this case, a Wishart likelihood should be used.  In Ollivier 2017, this approximation is clearly stated. Needless to say, the likelihood depends on the model.   

(2) "In contrast, we give a principled justification for a different Kalman filtering approach (with a different choice for the process noise) that is able to recover momentum, and the root-mean-square gradient normalizer."

I think the authors should define "a principled justification." 
If my understanding is correct, a principled justification should come without unjustified approximation. To recover the root-mean-square gradient normalizer, additional approximations have to be made.  Do the authors justify these approximations? Why these approximations are natural.  
My concern is that whether these approximations come from the first principle.  Honestly, it feels contrived in order to recover RmsProp and Adam, which implies that the paper has a limited impact. To address this, the authors should show that non-diagonal adaptive gradient methods can be derived naturally without case-by-case approximations. For example, the authors should give a non-diagonal version of (Bayesian) RmsProp or Adam using the *exactly* same approximations made in the paper. 

</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkg-ctln6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygREjC9YQ&amp;noteId=rkg-ctln6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper44 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper44 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">(1) Indeed, the EKF defines an approximate linearised likelihood, as a Taylor series expansion of the underlying nonlinear model.  The underlying non-linear model cannot, and should not depend on inferences made under the model.  Only the linearised model depends on the inferred mean for the latent variable, and then, only through the location at which you perform the linearisation.

We entirely agree that the approximate, linearised likelihood does indeed depend on inferences made under the model, and that this a perfectly natural, sensible thing to do.  This is not the question.  The question is whether the dynamical prior (in particular the process noise) depends on inferences made under the model.  Olliver 2017 set the process noise proportional to the posterior uncertainty (covariance), and it is this that can't be written down within the EKF framework.

(2) I would kindly request that you read the new section, entitled "Factorisation implies a rich dynamical prior", which provides the principled justification that you request.

Again, it isn't really an approximation, but a new way of setting up the problem.  In particular, we note that the updates in most algorithms for neural network optimization are based on just the gradient for that parameter.  As such, if we define a factorised model for each parameter separately, we can take the "data" to be not the underlying input-output pairs, but the backpropagated gradient for that parameter.  This generative model naturally gives rise to the emergence of a new latent variable, w_i^*, the optimal value for the ith parameter, conditioned on the current estimate of all the other parameters.  If we choose to work with this parameter, then dynamics emerge automatically: w_i^* must change over time, because it depends on our current estimate of all the other parameters, which are changing as they are optimized.

The exact same exercise can trivially be extended to non-diagonal approximations using the derivations in our paper ("Bayesian (Kalman) filtering as adaptive SGD"), and we are considering the Kronecker factored approach in future work.  However, we believe that the we have already made considerable contributions: an entirely novel connection between high-dimensional correlations and temporal changes in a principled Bayesian model, that recovers state-of-the-art adaptive methods including RMSprop, Adam, AdamW and NAG.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJl-3AvnaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review of the revised version</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygREjC9YQ&amp;noteId=HJl-3AvnaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper44 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper44 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Since this paper is a theory paper, my evaluation criterion is based on the mathematical justification. 
As claimed, the main contribution of this paper is (1) introducing the dynamical prior and (2) recovering a root-mean-square normalizer such as (non-Bayesian) RMSProp/Adam as a special case.

Although the authors claim their framework is more natural than [1], 
the main concern is the underlying unnatural approximations made in the derivation. As suggested by [1], the Bayesian RmsProp and Bayesian Adam indeed admit a mean-square normalizer without further approximations.   To recover (non-Bayesian) RmsProp/Adam, which is a root-mean-square normalizer, the authors have to make additional approximations to get the exact RmsProp/Adam update. I do not think these additional approximations are natural.   

Detailed Issues
(1) On page 1, "their generative model depends on inferences
made under that model: a highly unnatural assumption that most likely does not correspond to any “real” generative process."
The paper and [1] use the extended Kalman filter to linearize the non-Gaussian likelihood.
Extended Kalman filter and fading-memory technique are known as standard estimation techniques. (see [5] and [6]). The fading-memory technique can be viewed as "an artificial process noise proportional to [the posterior covariance" as mentioned at [1]. To be fair, I do not think the fading memory technique is a highly unnatural assumption given that non-standard and unjustified approximations are made later in paper to recover the (non-Bayesian) RmsProp/Adam.

(2) I do not think that the equivalence between Eq (1) and Eq (2) is obvious. A proof is required.
 
(3) In figure 1, in order to get a traceable algorithm, an additional approximation is made. (figure 1.C is an approximation of figure 1.B)
Is this kind of approximation an estimation technique? Why this is better/(more natural) than the fading memory technique? 

(4) If A ~ \eta and Q ~ \eta^2 and O(\eta^3) terms are ignored, I do not think  the first equation below Sec 9.1 can exactly recover the followling equation on page 12.
\Sigma_post^(t + 1) =\Sigma_post − A \Sigma_post − \Sigma_post A^T + Q − \Sigma_post ee^T  \Sigma_post

(5) " Neglecting the second-order term in A" from the line above Eq 34 on page 13
To obtain Eq 34, the authors have to ignore  "A \Psi A^T". Why do the authors neglect "A \Psi A^T" in the derivation?  Any justification?

(6) Finally, with approximations made in (4) and (5), (non-Bayesian) RmpProp/ Adam is recovered in the limit case when t -&gt; \infinity. (see sec 4.1 and 5.1). The proposed method cannot recover (non-Bayesian) RmsProp/Adam or a root-mean-square normalizer at an early stage since t is not large enough.

I may add more approximation issues made in Sec 2. 

References
[1] Ollivier, Yann. "Online Natural Gradient as a Kalman Filter." arXiv preprint arXiv:1703.00209 (2017).
[5] Humpherys, Jeffrey, Preston Redd, and Jeremy West. "A fresh look at the Kalman filter." SIAM review 54.4 (2012): 801-823.
[6] Musoff, Howard, and Paul Zarchan. Fundamentals of Kalman filtering: a practical approach. American Institute of Aeronautics and Astronautics, 2009. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Syx3r6dp6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygREjC9YQ&amp;noteId=Syx3r6dp6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper44 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper44 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">(1) Indeed, the fading-memory technique can be viewed as "an artificial process noise proportional to [the posterior covariance]".  We use a different process noise that is more natural (in the sense that the process noise is fixed, rather than depending on inferences made under the model).  Further, this different process noise gives a very different, behaviour of the update rules from the mean-square normaliser that requires this form for the process noise.  I should also note that all of our approximations are very standard and well-understood in the Physics and Engineering literature, to the extent that they are arguably high school, rather than undergraduate level (e.g. steady state, neglecting terms such as A Phi A proportional to dt^2).

(2) This is obvious, because for a well-calibrated model, the posterior is well-calibrated, and thus the true parameter can be understood as being drawn from the posterior.  Further, in (2), we take the expectation under the true data distribution --- in effect, giving us infinite data.  Of course, in the infinite data regime, we again recover the underlying parameters.

To confirm the validity of the approximations in this section, we show that the second-order approximation to the likelihood is the same under the model conditioned on the gradient, and under the original model.

(3) It is well known that an autoregressive process of any order can be written as an AR(1) process by defining an expanded state space.  See Example 1 in lecture notes: <a href="http://faculty.washington.edu/ezivot/econ584/notes/statespacemodels.pdf" target="_blank" rel="nofollow">http://faculty.washington.edu/ezivot/econ584/notes/statespacemodels.pdf</a>
In that link, they go so far as regarding this equivalence as "obvious".

(4) Could you be more specific?  What do you get?  If you're worried about the denominator, that term is basically,

Sigma e e^T Sigma / (1 + e^T Sigma e)

Taking a first-order Taylor series expansion of the denominator, we obtain,

Sigma e e^T Sigma (1 - e^T Sigma e)

But as Sigma ~ eta, and we neglect terms that scale with eta^3, we have, this reduces to just Sigma e e^T Sigma

(5) This is a standard approximation made in almost all work on dynamical systems in engineering and physics.  Specifically, it is valid because Phi~1, and
for RMSprop:
A~eta^2
so A Phi A ~ eta^4 is dominated by A Phi ~ eta^2, and can be neglected
and for Adam:
A~eta
so A Phi A~ eta^2 is dominated by A Phi ~ eta, and can be neglected.  Note that, phrased in this way, this constitutes taking the "leading order" terms in an expression, and is again a standard, well-understood approximation.

(6) Indeed, there is a difference at small t, and this may be at the root of our improved performance.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkxIyCtaTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a> Ad hoc approximations </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygREjC9YQ&amp;noteId=SkxIyCtaTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper44 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper44 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">(1) "I should also note that all of our approximations are very standard and well-understood in the Physics and Engineering literature, to the extent that they are arguably high school, rather than undergraduate level (e.g. steady state, neglecting terms such as A Phi A proportional to dt^2). "

I think in the Physics and Engineering literature, approximation conditions are well-defined and studied.  If many ad-hoc approximations are introduced, do these approximation conditions jointly hold in this case? 

For example,in Sec 9.1, the authors argue that "to obtain a self-consistent solution, we need, \Sigma_{post} ~ O(\eta)".
 Why does this assumption hold? Why the self-consistent solution implies that \Sigma_{post} ~ O(\eta)? A proof is required.  Does the update (such as Eq 15) meet this assumption?  As t -&gt; \initify, can the authors show that  "\Sigma_{post} ~ O(\eta)"?
In Sec 9.1, the authors further use "a first-order Taylor series expansion of the denominator" while some terms are neglected in the numerator. Is it a standard, well-understood approximation? Note that Sec 9.1 is critical since the authors use the results in Sec 9.1 to recover non-Bayesian RmsProp and Adam. 

To reiterate, do the authors consider the *joint* side effect of these local approximations hidden in the whole derivation? Note that the approximation error can be cumulative. The dynamical system could be unstable due to the cumulative error.

Every time the authors make an approximation, they should consider the side effect of the approximation. If they make many approximations, they should also consider the interactions between these approximations and the cumulative approximation error. The authors can cite existing works to justify the approximations as long as existing works use the same approximations. Please remember this is a *theory* paper. 
 A theory paper should have formal proofs to justify all approximations made in the paper.  There are ad hoc approximations hidden in the derivation.  Also, the empirical results are weak, which is fine since this is a theory paper. Moreover, if the authors make assumptions, these assumptions should be clearly mentioned before going to the derivation/proof. Unfortunately, many approximations and assumptions are hidden in the derivation in this paper. I do not think this paper should be considered as a theory paper. I suggest that the authors should re-frame this paper as an algorithm paper, propose a non-diagonal version of adaptive gradient methods, and empirically examine the approximations made in this paper. 
 

(2) "because for a well-calibrated model" "in the infinite data regime" A big IF. The argument is very hand-waving.  I do not think that the argument is formal.   In Eq(1), w^* is drawn from a posterior while in Eq (2), w^* is the argmax of the expectation. Why is the equivalence obvious? A proof is required. The authors can cite an existing work.
 
(3) It is an approximation. Is it correct? As mentioned at <a href="http://faculty.washington.edu/ezivot/econ584/notes/statespacemodels.pdf," target="_blank" rel="nofollow">http://faculty.washington.edu/ezivot/econ584/notes/statespacemodels.pdf,</a> to recover an autoregressive process of any order, the state space must be augmented. For a full order with infinite time steps as suggested in Figure 1.A, the dimensionality of the augmented state space will be infinite.  Note that in the stochastic gradient setting, the original state space is the parameter space. If Figure 1.C is exactly equivalent to Figure 1.B, the dimensionality of z should be |T| \times dim(w), where |T| is the number of time steps and dim(w) is the dimensionality of the parameter space. However, in Eq 10,  dim(z_{rmsprop}) = dim(w). It is clear that the authors use the first-order approximation in the RmsProp case, where Figure 1.C is an approximation of Figure 1.B.  


(4)  " for Adam: A~eta
so A Phi A~ eta^2 is dominated by A Phi ~ eta, and can be neglected. " 

In Sec 9.1,  the authors argue that "As \Sigma_post is O(\eta), and updates to \Sigma_post are O(\eta^2), we can neglect O(\eta^3) terms"

Why not ignore updates to \Sigma_post  since "updates to \Sigma_post are O(\eta^2), which is dominated by \Sigma_post \in O(\eta)?

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>