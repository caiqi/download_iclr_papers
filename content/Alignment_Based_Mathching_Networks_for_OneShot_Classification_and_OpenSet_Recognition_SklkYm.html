<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Skl6k209Ym" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Alignment Based Mathching Networks for One-Shot Classification and..." />
      <meta name="og:description" content="Deep learning for object classification relies heavily on convolutional models. While effective, CNNs are rarely interpretable after the fact. An attention mechanism can be used to highlight the..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Skl6k209Ym" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition</a> <a class="note_content_pdf" href="/pdf?id=Skl6k209Ym" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019alignment,    &#10;title={Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Skl6k209Ym},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Deep learning for object classification relies heavily on convolutional models. While effective, CNNs are rarely interpretable after the fact. An attention mechanism can be used to highlight the area of the image that the model focuses on thus offering a narrow view into the mechanism of classification. We expand on this idea by forcing the method to explicitly align images to be classified to reference images representing the classes. The mechanism of alignment is learned and therefore does not require that the reference objects are anything like those being classified. Beyond explanation, our exemplar based cross-alignment method enables classification with only a single example per category (one-shot). Our model cuts the 5-way, 1-shot error rate in Omniglot from 2.1\% to 1.4\% and in MiniImageNet from 53.5\% to 46.5\% while simultaneously providing point-wise alignment information providing some understanding on what the network is capturing. This method of alignment also enables the recognition of an unsupported class (open-set) in the one-shot setting while maintaining an F1-score of above 0.5 for Omniglot even with 19 other distracting classes while baselines completely fail to separate the open-set class in the one-shot setting.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Bye9YWnphm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Sound empirical study</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skl6k209Ym&amp;noteId=Bye9YWnphm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1029 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1029 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose a deep learning method based on image alignment to perform one-shot classification and open-set recognition. The proposed model is an extension of Matching Networks [Vinyals et al., 2016] where a different image embedding is adopted and a pixel-wise alignment step between test and reference image is added to the architecture. 

The work relies on two strong assumptions: (i) to consider each point mapping as independent, and (ii) to consider the correct alignment much more likely than the incorrect ones. The manuscript doesn’t report arguments in favour of these assumptions. The motivation is partially covered by your statement “marginalizing over all possible matching is intractable”, nevertheless an explanation of why it is reasonable to introduce these assumptions is not clearly stated.

The self-regularization allows the model to have a performance improvement, and it is considered one of the contribution of this work. Nevertheless the manuscript doesn’t provide a detailed explanation on how the self regularization is designed. For example it is not clear whether the 10% and 20% pixel sampling is applied also during self regularization.

The model is computationally very expensive and force the use of only 10% of the target image pixels and 20% of the reference images’ pixels. The complexity is intrinsic of the pixel-wise alignment formulation, but in any case this approximation is a relevant approximation that is never justified. The use of hyper column descriptors is an effective workaround to achieve good performance even though this approximation. The discussion is neglecting to argue this aspect.

One motivation for proposing an alignment-based matching is a better explanation of results. The tacit assumption of the authors is that a classifier driven by a point-wise alignment may improve the interpretation. The random uniformly distributed subsampling of pixels makes the model less interpretable.It may occur for example as shown in figure 3 where the model finds some points that for human interpretation are not relevant and at the same time these points are matched with points that have some semantic meaning.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hye4nbNtaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author Response to Reviewer 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skl6k209Ym&amp;noteId=Hye4nbNtaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1029 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the comments. 

1) (i) Our method computes an approximate, sampled matching between two images, not an exact matching as computing an exact matching while enforcing locality constraints is computationally difficult. We hence make the independent point matching assumption. Given the full image and a point, let us assume there is an oracle that performs the matching. In this case, the independent point matching assumption is reasonable as the oracle will assign P_{i,j,k}=1 when the correct matching is done for a given pixel i and 0 otherwise. In our method, points are given increasing context as we use the features of higher layers, so given enough filters at each layer, an oracle would essentially have access to the full image using the hyper-column description of a particular point. Self-regularization trains the system to learn to perform the matching task of the oracle. Hence, we expect the approximate solution to be good enough to be able to perform the original task - one-shot learning, and still learning a reasonable matching.


(ii) If we assume that (i) gets us close to oracle level performance, then log P_{i,j,k} -&gt; 0 when the correct matching is found for pixel i and log P_{i,j,k} &lt;&lt; 0 otherwise. The similarity between the hyper-column feature vectors is expected to be high (and is trained to be high with self-regularization). With the independence assumption, since the image alignment score = \sum_i max_{j,k} log P_{i,j,k}, we expect that score -&gt; 0 when the matching is correct and score &lt;&lt; 0 already when there are a few misalignments. We expect most of the probability mass to be concentrated around the correct alignment due to the hyper-column representation of points. 

2) When it comes to self-regularization, it does not matter as much whether 10-20 sampling is used or if the full alignment is computed, as long as the target pixels are also selected in the reference set (for convenience of determining the self-alignment matching matrix). As the regularization is selected as the categorical loss, we have for an image I, S = {I}

self-regularization loss = CrossEntropyLoss(P(M | I) , Identity) = \sum_i CrossEntropyLoss(P(M_{i,*,1} |I), 1_i)  where 1_i is a vector with 1 at position i and 0 everywhere else. In other words, we want P(M_{i,j,1}|I) = 1 if i=j and 0 otherwise. Here, P(M_{i,j,1} | I, S) is proportional to C_{i,j}(I,I) as in Equation (2).

In our experiments, we used the 10% sampling during self-regularization as well. We could have sampled an additional 10% for the "reference" portion of the self-alignment, but this has marginal benefit as we are already contrasting the possible locations for a pixel across 10% of the image.

3) The justification for the sampling is that as we are performing independent alignment (ie the net alignment cost is the sum of the alignment costs of the individual pixels), a reasonable pixel sampling will well approximate the alignment score of the full image. If the images are misaligned (or can't be aligned), we expect a reasonable fraction of the pixels to have poor alignment scores. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BkxJlR5ahX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A new way of learning key point correspondence which can reflect visual concept</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skl6k209Ym&amp;noteId=BkxJlR5ahX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1029 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1029 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposed a new way of learning point-wise alignment of two images, and based on this idea, one-shot classification and open-set recognition can be further improved. The idea is interesting. As a human, when we say two images are similar, we may compare them locally and globally in our mind. However, traditional CNN models do not make direct comparisons. And this work give a good direction to further improve this motivation.

The paper is well written and easy to understand. 

For the experiments, MNIST, Omniglot and MiniImageNet are used to demonstrate the effectiveness of the proposed method. From Figure 2. we can see many interesting correspondences.  </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1gHbfVFaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author Response to Reviewer 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skl6k209Ym&amp;noteId=S1gHbfVFaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1029 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the review. Your summary captures the essence of our paper. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SylnZdjD37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review for ABM-Nets</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skl6k209Ym&amp;noteId=SylnZdjD37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1029 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1029 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this work, the authors tackle the problem of few-shot learning and open-set classification using a new type of NNs which they call alignment-based matching networks or ABM-Nets for short. They main idea is to benefit from binary maps between the query image and the support set (for the case of few-shot learning for the sake of discussion here) to guide the similarity measure. 

I have quite a few concerns;

- After reading the paper two times, I still couldn't find a clear explanation as how the binary map C is constructed. The paper says the cost of M,i,j,k = 1 is C. So what exactly happens given I_t and I_k. My understanding is that a vector representation of each image is obtained and then from those representations the matrix C is constructed (maybe an outer product or something). This does not come out clearly. 

- Nevertheless, I suspect if such a construction (based on my understanding) is the right approach. Firstly, I guess the algorithm should somehow encourage to match more points between the images. Right now the loss  does not have such a term so hypothetically you can match two images because they just share a red pixel which is obviously not right. 

- Aside from the above (so basically regularizing norm(C) somehow), one wonders why matching a point to several others (as done according to matrix C) is the right choice. 

- Related to the issues mentioned before, I may also complain that matching far away points might not be ideal. Currently I do not see how this can be avoided nor a solid statement as why this should not be a problem.  


- Another comment is how the alignment here differs from attention models? They surely resemble each-other though the alignment seems not that rich.


-  last but not least, I have found the language confusing. Some examples,
   -p2 bandwidth signal than the traditional label-only signal : I am very confused by how bandwidth comes to the picture and how this can be measured/justified

  - fig.1, what are \phi and \psi. paper never discussed these.

  - the authors say M is a tensor with 3dimensions. Then the marginalization before eq.1 uses M_{i,\cdot,\cdot} = 1 . What does this really mean?

    </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1eVQB6FTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author Response to Reviewer 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skl6k209Ym&amp;noteId=S1eVQB6FTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1029 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the detailed comments. 

1) C is not a binary map, but a continuous cost matrix where C_{i,j,k} tells us how well pixel i of the target image aligns to pixel j of reference image k. If you are viewing this as a binary matching problem, then M is the binary matching matrix and C (when normalized row-wise) is the continuous relaxation of M.

To construct C_{i,j,k}, we take the hyper-column descriptor of pixel i in the target image \phi_i(I) and the hyper-column descriptor of pixel j in reference image k \phi_j(S_k) and compute the similarity between them using cosine similarity as the normalized dot product (below Equation 2). P(M_{i,j,k} | I_t, S) is inverse-exponentially related to C_{i,j,k} (Equation 2)

2) The contextual feature encoding of a point is the hyper-column representation of the point and not just the pixel colors. Hence, the system will not align two red points unless the contextual information is also in agreement.

3) Our approach is motivated by finding correspondences. We express the solution as an alignment between sampled points in the two images which we force the system to perform, aided by self-regularization. The label of the target image is the reference image that best aligns with the various parts of the target image.

4) [same response as to AnonReviewer3] Our method computes an approximate, sampled matching between two images, not an exact matching as computing an exact matching while enforcing locality constraints is computationally difficult. We hence make the independent point matching assumption. Given the full image and a point, let us assume there is an oracle that performs the matching. In this case, the independent point matching assumption is reasonable as the oracle will assign P_{i,j,k}=1 when the correct matching is done for a given pixel i and 0 otherwise. In our method, points are given increasing context as we use the features of higher layers, so given enough filters at each layer, an oracle would essentially have access to the full image using the hyper-column description of a particular point. Self-regularization trains the system to learn to perform the matching task of the oracle. Hence, we expect the approximate solution to be good enough to be able to perform the original task - one-shot learning, and still learning a reasonable matching.

5) Visual attention corresponds to learning a mask over the input where the model should pay attention and then expressing the feature descriptor of the image as a combination of the feature descriptors across the image according to the attention weight. If one were to directly augment Matching Networks [Vinyals et al., 2016] with attention, a comparison between two images would be performed in the feature embedding space corresponding to a single similarity score between the resulting image feature vectors. On the other hand, alignment corresponds to the matching of individual points in a manner that incorporates the context around the points. Thus the comparison between two images is computed as the sum of similarity scores of individual pixels (hyper-column representations). Feature vectors are not compressed to a single feature descriptor. 

6) a) By bandwidth, we mean that only a single label is provided per target image for training in the traditional one-shot learning case (low bandwidth). With alignment, self-regularization provides a label per point corresponding to the index of each matched pixel, thus leading to rich set of labels (feedback) in addition to the original one-shot label (hence high-bandwidth). We will revise the terminology to make it more understandable.

b)  \phi and \psi are discussed below equation 2 in Section 3.1. \psi is the hyper-column descriptor of the pixel using the encoding network while \phi is an additional embedding of \psi. When we are looking at symmetric encoding of the target and reference images, then \phi = \psi. However, we note that the reference and target images need not be the same (for example, we could try to align a 1-channel gray-scale image to a 3-channel color image). In this case, we need to project the two hyper-column pixel descriptors into the same space, which corresponds to \phi being an embedding of \psi.

c) We meant to say M_{i,correspondingMatch(i)} = 1. M_{i, \cdot, \cdot} was a typo. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>