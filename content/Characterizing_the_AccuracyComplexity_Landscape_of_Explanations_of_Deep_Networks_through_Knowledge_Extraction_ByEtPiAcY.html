<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Characterizing the Accuracy/Complexity Landscape of Explanations of Deep Networks through Knowledge Extraction | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Characterizing the Accuracy/Complexity Landscape of Explanations of Deep Networks through Knowledge Extraction" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ByEtPiAcY7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Characterizing the Accuracy/Complexity Landscape of Explanations of..." />
      <meta name="og:description" content="Knowledge extraction techniques are used to convert neural networks into symbolic descriptions with the objective of producing more comprehensible learning models. The central challenge is to find..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ByEtPiAcY7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Characterizing the Accuracy/Complexity Landscape of Explanations of Deep Networks through Knowledge Extraction</a> <a class="note_content_pdf" href="/pdf?id=ByEtPiAcY7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019characterizing,    &#10;title={Characterizing the Accuracy/Complexity Landscape of Explanations of Deep Networks through Knowledge Extraction},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=ByEtPiAcY7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Knowledge extraction techniques are used to convert neural networks into symbolic descriptions with the objective of producing more comprehensible learning models. The central challenge is to find an explanation which is more comprehensible than the original model while still representing that model faithfully. The distributed nature of deep networks has led many to believe that the hidden features of a neural network cannot be explained by logical descriptions simple enough to be understood by humans, and that decompositional knowledge extraction should be abandoned in favour of other methods. In this paper we examine this question systematically by proposing a knowledge extraction method using M-of-N rules which allows us to map the complexity/accuracy landscape of rules describing hidden features in a Convolutional Neural Network (CNN). Experiments reported in this paper show that the shape of this landscape reveals an optimal trade off between comprehensibility and accuracy, showing that each latent variable has an optimal M-of-N rule to describe its behaviour. We find that the rules with optimal tradeoff in the first and final layer have a high degree of explainability whereas the rules with the optimal tradeoff in the second and third layer are less explainable. Furthermore, we show that by replacing the final layer with the optimal extracted rules the network becomes more robust against adversarial examples. The results shed light on the feasibility of rule extraction from deep networks, and point to the value of decompositional knowledge extraction as a method of explainability and to improve robustness when applied selectively.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Deep Networks, Explainability, Knowledge Extraction</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Systematically examines how well we can explain the hidden features of a deep network in terms of logical rules.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BkeTdJNv6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByEtPiAcY7&amp;noteId=BkeTdJNv6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper285 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper285 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=BkeTdJNv6m" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper is looking at how to extract knowledge from CNNs to help improve explainability and robustness against an adversarial attack.  It is using a known technical call M-of-N rules.

This problem of explainability of NN's is an important one and rules are a good step in that direction.  

+ The paper is generally well written

- The contribution seems to be relatively small
- The evaluation is limited, only 1 dataset and only 1 technique evaluated

General advice for work in AI explainability:
When one looks at the problem of AI explainability it is important to describe who the target audience is for the explanation.  Is it a machine learning expert, who wants to debug the model?  Is it an end user who wants to better understand why the prediction was made?  Is it a regulator who is trying to ensure the model's predictions are fair?

Each of these personas will come with different needs and different technical backgrounds, so an assumption that some artifact (a rule set?) is "explainable" may apply to one group, but not the other group.  For example, rules are likely to be more explainable to a ML expert, but may not be to an end user, unless they are very small.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HkgINMZPp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>M-of-N rule extraction is back</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByEtPiAcY7&amp;noteId=HkgINMZPp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper285 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper285 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes to rewrite each neuron of a neural network as a M-of-N decision rule. An measure of rule complexity (which takes into the account the number of terms in the rule) is proposed, and an approximate rule induction algorithm which binarizes the neurons using an information gain criterion is provided.

The paper gives no evaluation of the accuracy of extracted rules on a test set. Instead, the fidelity to network is evaluated on the train set for individual neurons (see below).

Since even for moderately-sized networks the method would result in lots of rules, the authors propose instead to use the proposed algorithm and complexity criterion as a tool for understanding the complexity of concepts detected by a layer. The results, gathered in Figure 1, suggest that for some layers complex rules do approximate the behavior of neurons, while for other layers a neuron can't be replaced with a single but complex M-of-N rule.

The ideas presented in this paper seem rudimentary and require further exploration before being publishable. First of all, the main result of complexity-vs-layer doesn't differentiate between failures of the approximate rule induction algorithm (the terms in rules are considered for inclusion in a single order, reducing the search space) and the genuine complexity of the rules - this can be verified by evaluating a more exhaustive algorithm on at leas a few neurons (not necessarily on all of them).

Second, if the rules are extracted in a layerwise fashion,  their errors accumulate for deeper layers. However, Algorithm 1 suggests that each neuron is replaced by a rule independently from others, and moreover that it requires the true value of the neurons in the layer below, not of their rule counterparts. This means that the rules can't be combined and explains why the paper doesn't provide any measure for aggregate rule accuracy.

Similarly, the robustness of rules to adversarial examples is meaningless - it seems that applying the rules results in a system which is less accurate (only 8/10 of rules mimick their neurons with no rule complexity penalty) overall, but also makes fewer adversarial examples.

Minor remarks.
Please don't ever produce Figures such as Figure 1: no legend (description in text), color selection is not black and white friendly, font is so small that the axis labels are hard to read... In fact, the poor quality of the Figure by itself should be sufficient  to reject the paper for not abiding to the author guidelines (sec. 4.3: All artwork must be neat, clean, and legible.)</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SyeC_idL67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting contribution but still rudimentary</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByEtPiAcY7&amp;noteId=SyeC_idL67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper285 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper285 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: This paper proposes a novel knowledge extraction method using M-of-N rules
to help interpret hidden features in a Convolutional Neural Network (CNN). While the idea itself is interesting, I think that the paper is still in a very early stage and needs more work before it can be accepted. Detailed comments below.

Pros: 
1. The paper proposes a new algorithm to interpret CNNs.
2. The paper is reasonably well written.

Cons: 
1. The experimental evaluation is quite weak. The authors present their (partial) results on a single dataset and also seem to generalize some of the findings in a rather misleading way. 
2. The proposed method is not compared against any baseline though there are ample rule-based methods to understand NNs in literature.
2. It seems like the literals in the generated rules are actually outputs of previous stages of NNs. Are the rules even human understandable in that case? 

Detailed Comments:
1. I think that the paper is missing a very clear discussion on what is novel about the proposed method in contrast with recent work on explaining NNs (or black box models) using rule based approaches. Examples of relevant papers include "Anchors: High-Precision Model-Agnostic Explanations" by Ribeiro et. al. and "Interpretable &amp; Explorable Approximations of Black Box Models" by Lakkaraju et. al. among others. 
2. Another important piece of discussion that is missing is how the proposed search technique for extracting M-of-N rules is novel compared to a lot of prior literature which deals with the same problem
3. I would strongly encourage the authors to experiment with at least three different datasets and multiple CNN architectures. 
4. I would really like to see the output of the proposed approach. What kinds of rules are being generated at each stage? It seems like the literals in the generated rules are actually outputs of previous stages of NNs. Are the rules even human understandable in that case? 
5. It would be good to do a simple user study to demonstrate that human users are able to understand something useful from the generated explanations. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1xgWbPq2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Characterizing the Accuracy/Complexity Landscape of Explanations of Deep Networks through Knowledge Extraction</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByEtPiAcY7&amp;noteId=r1xgWbPq2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper285 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper285 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Overall, this is a interesting paper on an important topic: knowledge extraction from Neural Networks.
Even though the authors seem propose a novel approach to knowledge extraction, the paper would 
dramatically benefit from two additions:
- an empirical evaluation on at least two more datasets (as is, the paper uses a single dataset)
- an illustrative-but-realistic example of how at least one rule is extracted from each layer of the neural network 

Other comments:
- on page 4 (1st paragraph in 3.3), the authors talk about a "test set" that, it turns out, it is extracted from the actual training set (1st paragraph of 4.1); the authors should use a more careful terminology
- from the paper, it seems that the authors tried a single randomly chosen set 1000 random inputs in 4.1; they should most definitely try several such sets
- Figure 1 should have a legend in the image, rather than as a 2-line caption
 </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>