<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Visceral Machines: Reinforcement Learning with Intrinsic Physiological Rewards | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Visceral Machines: Reinforcement Learning with Intrinsic Physiological Rewards" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SyNvti09KQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Visceral Machines: Reinforcement Learning with Intrinsic..." />
      <meta name="og:description" content="The human autonomic nervous system has evolved over millions of years and is essential for survival and responding to threats.&#10;   As people learn to navigate the world, ``fight or flight'' responses..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SyNvti09KQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Visceral Machines: Reinforcement Learning with Intrinsic Physiological Rewards</a> <a class="note_content_pdf" href="/pdf?id=SyNvti09KQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 14 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019visceral,    &#10;title={Visceral Machines: Reinforcement Learning with Intrinsic Physiological Rewards},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SyNvti09KQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=SyNvti09KQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">The human autonomic nervous system has evolved over millions of years and is essential for survival and responding to threats.
 As people learn to navigate the world, ``fight or flight'' responses provide intrinsic feedback about the potential consequence of action choices (e.g., becoming nervous when close to a cliff edge or driving fast around a bend.) Physiological changes are correlated with these biological preparations to protect one-self from danger. We present a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses. Our hypothesis is that such reward functions can circumvent the challenges associated with sparse and skewed rewards in reinforcement learning settings and can help improve sample efficiency. We test this in a simulated driving environment and show that it can increase the speed of learning and reduce the number of collisions during the learning stage.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Reinforcement Learning, Simulation, Affective Computing</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We present a novel approach to reinforcement learning that leverages a task-independent intrinsic reward function trained on peripheral pulse measurements that are correlated with human autonomic nervous system responses. </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_B1l7Gno_67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Revision uploaded.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyNvti09KQ&amp;noteId=B1l7Gno_67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper456 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper456 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank the reviewers again for their constructive and insightful comments.  We have uploaded a revised version of our manuscript with the changes described below in the "initial response and clarifications".  We highlight that we have added additional related work, the experiments and results with a time varying lambda, clarified the error bars and significant of the results and added comparisons with a baseline pulse amplitude prediction.  </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SylwTdykpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting approach, but analysis of the results should be improved</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyNvti09KQ&amp;noteId=SylwTdykpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper456 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper456 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
This submission proposes a reinforcement learning framework based on human emotional reaction in the context of autonomous driving. This relies on defining a reward function as the convex combination of an extrinsic (goal oriented) reward, and an intrinsic reward. This later reward is learnt from experiments with humans performing the task in a virtual environment, for which emotional response is quantified as blood volume pulse wave (BVP). The authors show that including this intrinsic reward lead to a better performance of a deep Q networks, with respect to using the extrinsic reward only. 
Evaluation:
Overall the proposed idea is interesting, and the use of human experiments to improve a reinforcement learning algorithm offers interesting perspectives. The weakness of the paper in my opinion is the statistical analysis of the results, the lack of in depth evaluation of the extrinsic reward prediction and the rather poor baseline comparison.
Detailed comments:
1.	Statistical analysis
The significance of the results should be assessed with statistical methods in the following results:
Section 4.1: Please provide and assessment of the significance of the testing loss of the prediction. For example, one could repetitively shuffle blocks of the target time series and quantify the RMSE obtained by the trained algorithm to build an H0 statistic of random prediction.
Section 4.2: the sentence “improves significantly when lambda is either non-zero or not equal to 1” does not seem valid to me and such claim should in any case be properly evaluated statistically (including correction for multiple comparison etc…).
Error bars: please provide a clear description in the figure caption of what the error bars represent. Ideally in case of small samples, box plots would be more appropriate.
2.	Time lags in BVP
It would be interesting to know (from the literature) the typical latency of BVP responses to averse stimuli (and possible the latency of the various mechanisms, e.g. brain response, in the chain from stimuli to BVP). Moreover, as latency is likely a critical factor in anticipating danger before it is too late, it would important to know how the prediction accuracy evolves when learning to predict at different time lags forward in time, and how such level of anticipation influence the performance of the Q-network.
3.	Poor baseline comparison
The comparison to reward shaping in section 4.4 is not very convincing. One can imagine that what counts is not the absolute distance to a wall, but the distance to a wall in the driving direction, within a given solid angle. As a consequence, a better heuristic baseline could be used. 
Moreover, it is unclear whether the approaches should be compared with the same lambda: the authors need to provide evidence that the statistics (mean and possibly variance) of the chosen heuristic is match to the original intrinsic reward, otherwise it is obvious that the lambda should be adapted.
4.	Better analysis of figure 5-6(Minor)
I find figure 5-6 very interesting and I would suggest that the authors fully comment on these results. E.g. : (1) why the middle plot of Fig. 6 mostly flat, and why such differences between each curve from the beginning of the training. (2) Why the goal oriented task leads to different optimal lambda, is this just a normalization issue?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rye3RNPGp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Initial response and clarifications.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyNvti09KQ&amp;noteId=rye3RNPGp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper456 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018 (modified: 11 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper456 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the detailed comments and constructive suggestions. We are running additional experiments and will upload the updated manuscript when they are complete in the next few days.  We have calculated the baseline RMSE using a random target as suggested by the reviewer. We have added these results and the significance of the difference between the baseline and reported results using an unpaired T-Test. The RMSE of the model predictions is significantly lower for all participants than the RMSE with the random target. The RMSE of the model predictions was 0.21 lower on average. These results have been added to Table 1.

The error bars in the plots correspond to standard error. Non-overlapping error bars correspond to 84% confidence according to z-test (Please see: <a href="https://tminka.github.io/papers/minka-errorbars.pdf)." target="_blank" rel="nofollow">https://tminka.github.io/papers/minka-errorbars.pdf).</a> Scaling of these by c=1.64, leads to 95% significance when the bars don’t overlap. We will add this information to the figure captions as well as the text in the paper. This should clarify which of the comparisons are significant. We will also modify the claim in the text to correspond to this.

We agree that there is a time delay between a stimulus and the physiological response to that stimulus. Our system was trained to mimic the physiological response of a person, this delay is already modeled into the prediction .  While it might be interesting to introduce an artificial time delay (i.e., to reflect a response that is faster or slower that was actually experienced), we believe our experiments are the most representative of the delays experienced. Please do clarify if we misunderstood your comment.

In this work we were trying to mimic a self-driving car in which the sensors are only facing a single direction and thus distance to the wall was an appropriate assumption.  We are not sure what the reviewer meant in the comment about the statistics of the “chosen heuristic matching to the original intrinsic reward.” We believe that a comparison with the same lambda values is the best initial test to run, without additional reasons to adjust the value of lambda on a case-by-case basis.

Fig. 6 (Distance) shows that the average length of the episodes does not increase with the number of episodes.  This is because with increasing numbers of episodes the car travels further but also faster. These two factors cancel one another out resulting in episodes with similar duration. The goal-oriented task leads to a different optimal lambda due to the different nature of the reward compared to the velocity or distance task.  However, it should be noted that consistently a lambda that is non-zeros and not equal to 1 is optimal.  We are adding more discussion of these results.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Hye3ipRChX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting application of RL, but scientifically sloppy</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyNvti09KQ&amp;noteId=Hye3ipRChX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper456 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper456 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Starting from the hypothesis that humans have evolved basic autonomic visceral responses that influence decision making in a meaningful way and that these are at work in driving a car, the authors propose to use such signals within the RL framework. This is accomplished by augmenting the RL reward function with a model learned directly from human nervous system responses. This leads to a 
convex combination of extrinsic rewards and visceral responses, with the goal to maximize extrinsic rewards and minimizing the physiological arousal response. The authors first show that they can train a CNN to predict systolic peaks from the pulse waveform based on the input images. The output of this network is then used with parametrically altered weightings in combination with the task related reward to evaluate performance on different driving tasks. The authors show that for different weightings performance on a number of driving tasks performance as measured by the collected extrinsic rewards is better.

Overall, this is an interesting application of RL. It is OK to be inspired by biology, neuroscience, or psychology, but further reaching claims or interpretations of results in these fields need to be chosen carefully. The discussion of neuroscience and psychology are only partially convincing, e.g. there is extensive evidence that autonomic responses are highly dependent on cognition and not just decisions dependent on visceral, autonomic responses of the SNS. Currently, the manuscript is rather loosely switching between inspirations, imprecise claims, and metaphorical implementations with relation to neuroscience. The authors are encouraged to relate their work to some of the multi-criteria and structural credit assignment literature in RL, given the convex combination of rewards.  It may also be important to relate this work to imitation learning, given that the physiological measurements certainly also reflects states and actions by the human agents. While one indication for the reasons of higher extrinsic rewards with the augmented system is mentioned by the authors, namely that the autonomic signal is continuous and while the extrinsic rewards are sparse is convincing, it is not at all clear, why the augmented system performs better as shown in figure 5. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryx-NNvGaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Initial response and clarifications.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyNvti09KQ&amp;noteId=ryx-NNvGaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper456 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper456 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your thoughtful review and suggestions of related work. We are running the additional experiments and will upload the updated manuscript when they are complete in the next few days. In the meantime, here is a summary of the changes and responses to your questions. We are tightening up the description of how our system is inspired by biological processes. We want to emphasize that this work is leveraging the peripheral blood volume pulse as a signal that indicates changes in autonomic nervous system arousal, related to stress.  But the system is not mimicking the nervous system or attempting to replicate its processes. We are revising the introduction to make this clear.

We appreciate the comment to relate this work to the imitation learning and credit assignment literature. We are adding to the background sections to tie in this related work. The imitation learning literature is relevant as the physiological reward could be considered similar to feedback from an expert. The structural credit assignment problem, or generalization problem, is related in the way that it helps to address cases in which the parameter space is very large. Our method helps reduce the sparsity of the reward signal and thus makes learning more practical in a large parameter space. We are adding references to the following work on imitation learning and credit assignment:

Search-based Structured Prediction
Hal Daumé III, John Langford, Daniel Marcu

A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning
Stéphane Ross, Geoffrey J. Gordon, J. Andrew Bagnell

Reinforcement and Imitation Learning via Interactive No-Regret Learning
Stephane Ross, J. Andrew Bagnell

Generative Adversarial Imitation Learning
Jonathan Ho, Stefano Ermon

Learning to Search Better Than Your Teacher
Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daumé III, John Langford

Self-Improving Reactive Agents Based On Reinforcement Learning, Planning and Teaching
Long-Ji Lin

Regarding explaining why the augmented system performs better, we believe that the sparsity is the best explanation.  The qualitative examples also shed light on the types of situations in which the system is likely to get a high or low reward. Having a less sparse reward that helps the vehicle avoid collisions is beneficial.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rylqztph2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice idea, not sure it generalizes to other tasks</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyNvti09KQ&amp;noteId=rylqztph2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper456 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper456 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The method proposes to use physiological signals to improve performance of reinforcement learning algorithms. By measuring heart pulse amplitude the authors build an intrinsic reward function that is less sparse that the extrinsic one. It helps to be risk averse and allows getting better performances than the vanilla RL algorithm on a car-driving task. 

I found the paper well written and the idea is quite nice. I like the idea that risk aversion is processed as a data-driven problem and not as an optimisation problem or using heuristics. I think this general idea could be pushed further in other cases (like encourage fun, surprise, happiness etc. ). 

There are some issues with this paper yet. First, modifying the reward function also modifies the optimal policy. In the specific case of car driving, it may not be bad to modify the policy so that it makes passenger less stressed but in general, it is not good. This is why most of works based on intrinsic motivation also schedule the lambda parameter to decrease with time. This is not something explored in this paper. Also, this work is well suited to the car-driving scenario because stress is closely related to risk and accident. But it may not work with other applications. I would thus suggest that the title of the paper reflects the specific case of risk aversion. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJlZQHDMp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Initial response and clarifications.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyNvti09KQ&amp;noteId=BJlZQHDMp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper456 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper456 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the review and the suggestion of additional experiments.  We are running experiments regarding the time-varying lambda and will upload the updated manuscript when they are complete in the next few days. We agree that exploring a time varying lambda is of interest. Our goal with this paper was to show that the blood pulse amplitude could be used effectively as an intrinsic reward function that is less sparse than the extrinsic reward from the environment. We feel the experiments capture that and we are happy to include additional results from experiments in which the lambda parameter decreases temporally to illustrate how that influences performance.  The experiments take several days to complete after which we will upload the revised paper.

We are happy to change the title of the paper to reflect the specific case of risk aversion. But we do believe that the principal applies beyond applications in driving to contexts in which stress may be an undesirable state. We propose the following title: “Visceral Machines: Risk-Aversion in Reinforcement Learning with Intrinsic Physiological Rewards”. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>