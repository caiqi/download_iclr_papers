<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Quaternion Recurrent Neural Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Quaternion Recurrent Neural Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ByMHvs0cFQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Quaternion Recurrent Neural Networks" />
      <meta name="og:description" content="Recurrent neural networks (RNNs) are powerful architectures to model sequential data, due to their capability to learn short and long-term dependencies between the basic elements of a sequence...." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ByMHvs0cFQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Quaternion Recurrent Neural Networks</a> <a class="note_content_pdf" href="/pdf?id=ByMHvs0cFQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 10 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019quaternion,    &#10;title={Quaternion Recurrent Neural Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=ByMHvs0cFQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=ByMHvs0cFQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Recurrent neural networks (RNNs) are powerful architectures to model sequential data, due to their capability to learn short and long-term dependencies between the basic elements of a sequence.  Nonetheless, popular tasks such as speech or images recognition, involve multi-dimensional input features that are characterized by strong internal dependencies between the dimensions of the input vector. We propose a novel quaternion recurrent neural network (QRNN), alongside with a quaternion long-short term memory neural network (QLSTM), that take into account both the external relations and these  internal  structural  dependencies with the quaternion algebra. Similarly to capsules, quaternions allow the QRNN to code internal dependencies by composing and processing multidimensional features as single entities, while the recurrent operation reveals correlations between the elements composing the sequence.  We show that both QRNN and QLSTM achieve better performances than RNN and LSTM in a realistic application of automatic speech recognition. Finally, we show that QRNN and QLSTM reduce by a maximum factor of 3.3x the number of free parameters needed, compared to real-valued RNNs and LSTMs to reach better results, leading to a more compact representation of the relevant information.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Quaternion recurrent neural networks, quaternion numbers, recurrent neural networks, speech recognition</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_B1ebqu4y6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Explores a good and important direction, with encouraging results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByMHvs0cFQ&amp;noteId=B1ebqu4y6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper261 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper261 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper takes a good step toward developing more structured representations by exploring the use of quaternions in recurrent neural networks.  The idea is motivated by the observation that in many cases there are local relationships among elements of a vector that should be explicitly represented.  This is also the idea behind capsules - to have each "unit" output a vector of parameters to be operated upon rather than a single number.   Here the authors show that by incorporating quaternions into the representations used by RNNs or LSTMs, one achieves better performance at speech recognition tasks using fewer parameters.

The quaternionic representation of the spectrogram chosen here seems a bit arbitrary.  Why are these the attributes to be packaged together?  its not obvious.  Shouldn't this be learned?

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJg4KAOmTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks you + further informations</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByMHvs0cFQ&amp;noteId=SJg4KAOmTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper261 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper261 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The authors thank the reviewer for the positive and constructive feedback. We appreciate that the reviewer finds that our paper on QRNN is clearly explained, viable and thoroughly evaluated.

In this work, we decided to show that even with traditional acoustic features (Mel-filter-bank + derivatives), we could motivate and introduce quaternion-valued recurrent neural networks. Nonetheless, as underlined by reviewer 2, a future work will be to investigate proper quaternion acoustic features (or even other domains features). Indeed, current features are mostly engineered for a real-valued representation, and there is plenty of rooms to explore quaternion-valued features (such as complex-valued features in the case of speech recognition with complex neural networks, or quaternion Fourier Transform). 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SkxejkS5hX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Quaternion Recurrent Neural Networks</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByMHvs0cFQ&amp;noteId=SkxejkS5hX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper261 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper261 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Quality: sufficient though there are issues. Work done in automatic speech recognition on numerous variants of recurrent models, such as interleaved TDNN and LSTM (Peddinti 2017), is completely ignored. The description of derivatives needs to mention the linear relationship between input features and derivatives (see trajectory HMMs by Zen and Tokuda). TIMIT is a very simple task. Derivations in the appendices could be connected better. 
 
Clarity: sufficient. It would be good to see some discussion of 1) split activations and other possible options if any 2) expressions of derivatives and their connection to standard RNN derivatives, 3) computational complexity. 

Originality: sufficient. This paper describes the extension of quaternion feed-forward neural networks to recurrent neural networks and a parameter initialisation method in the quaternial domain.

Significance: sufficient. 

Pros: Audience interested in quaternial neural networks would benefit from this publication. Experimental results even if limited suggest that quaternial representation may offer a significant reduction in the number of model parameters at no loss in performance.  

Cons: The choice of derivatives to yield quaternions as there are other more interesting views to contemplate both in speech and other fields. A simple task makes it hard to judge how the quaternion extension would scale.  

Other:

The format of references, the use of a number in parentheses, is unusual and distractive. 
Please at least name all the terms used in the main paper body even if they are defined later in the appendix (e.g. h_{t}^{*} in equation 10).
Do both W_{hh} and b_{h} contain the same \delta_{hh}^{t} term in their update equation 11?
Page 7 by mistake mentions 18.2% which cannot be found in the Table 1. 
Page 12 "is equals to"
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJlaJJFQTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks you + answers</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByMHvs0cFQ&amp;noteId=HJlaJJFQTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper261 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper261 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to first thank reviewer 1 for the useful feedback. In the following, we address typos and general comments.

——— Typos and general comments

The format of references has been modified to match the standard of the ICLR format (Name et al, year).

The authors agree with the fact that the notation have to be better explained to make the paper more clear for the reader. Therefore, we have added a sentence to clarify h_{t}^{*}. 

We would like to thank the reviewer 1 to highlight that b_{h} was given the wrong delta during the backpropagation. The right equation for b_{h} has been added to the paper as well as in the supplementary material. 

Others typos have been corrected.

——— Quality

As suggested by reviewer 1, we added the missing references of prior works on ASR systems in the introduction, and the linear relation between input features and derivatives.

The authors agree with fact that TIMIT is a very simple task,  but this framework allows us to evaluate the relevance of RNNs in terms of performance and the number of parameters required. Therefore, in the revised version of the paper, we have added experiments on the Wall Street Journal (WSJ) speech recognition task (in the supplementary materials) based on both the 14 and the 81 hours training data-sets. Experiments are conducted with the same configurations than the ones from the models that have obtained the best results observed during the experiments on speech recognition on the TIMIT data-set. As expected, QLSTMs scale well (such as for real-valued LSTM) to larger data-sets, and the performances observed during these experiments support the fact that QLSTMs perform better (w.r.t WER), and  with fewer parameters..  

——— Clarity

Quaternion NNs suffer from the fact of being still little employed. Therefore, we understand that many concepts such as the « split activation functions » raise legitimate questions. We added some words on the paper to motivate the use of split activation functions. Nevertheless, and as we have mentioned throughout new citations, the split activation functions have already been well investigated, and we could only paraphrase what original authors demonstrated. However, this point raised by Reviewer 1 have to be investigated during a dedicated work, to allow the reader to easily follow the study that will compare different activation functions as well as different function methods (split or not). 

The authors agree with reviewer 1 and, therefore,  we have added a paragraph to clarify the decomposition of derivatives (Below Eq. 9) in the paper and in the appendix (Below Eq. 40). Indeed, the derivatives of different elements/views of a same feature with a real valued BPTT process, do not allow the RNN based model to learn how to compute the whole dynamic of the error (dE/dW), due to the fact that the dynamic of each element composing the features, are not merged/mixed to compose the derivative of the whole error observed. This process of merging partial derivatives from each elements (r, i, j, k for quaternions and 4 different input features for real numbers) is managed by the weight matrices and hidden states in the context of real-valued RNN based models during the learning process. The author agree that these intuitions have to be supported by solid experiments and model analyses. Therefore, we also plan to investigate the internal dynamic (through partial derivatives that contribute to the total dynamic) of QRNNs compared to real-valued RNNs ones to better understand the benefits from the QBPTT (in addition to better results and less parameters). 

Computational complexity is also a very good point, but hard to fairly answer in the current state of QNNs. Nonetheless, and as requested by reviewer 1, we added a paragraph to the paper (Appendix 6.1.2)), to acknowledge the fact that computational complexity can be a problem. From a pure computational complexity perspective: QLSTM = LSTM = O(n^2). Indeed, due to the real-valued representation of quaternions, QLSTMs perform the same matrices operations, but with 4 times bigger matrices. From a computational time perspective, a simple forward propagation between two quaternion neurons involves 28 computations. Therefore QNNs are slower to train (2 to 3 times slower, depending on the model) due to this much higher number operations. Nonetheless, we also know that such computations are matrices products. We believe that a proper GPU engineering (cuDNN kernel) of the Hamilton product could drastically reduce the computation time by doing these 28 computations in parallel, implying a more efficient usage of the available resources. Furthermore, with a proper cuDNN kernel, one will obtain a better memory / computation rate. Indeed, QNNs are doing more computations, but with fewer parameters. This point will be detailed in a proper section in the appendix of the final version of the paper.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1evr1F76m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks you + answers [2/2]</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByMHvs0cFQ&amp;noteId=H1evr1F76m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper261 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper261 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">——— Pros and Cons

As stated above, we added WSJ experiments to validate the results observed with the small TIMIT dataset. Reviewer 1’s statement about actual quaternion acoustic feature is definitely true, and we propose in the conclusion to investigate novel multi-view features that could be better adapted. 

We truly hope that we answered all the remarks and questions of reviewer 1, and we are available for any further discussion. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_HJgC3kCNh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Simple but nice development of the framework based on mostly well-known algebra but lacks experimental validation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByMHvs0cFQ&amp;noteId=HJgC3kCNh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper261 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper261 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">1.	In “Consequently, for each input vector of size N, output vector of size M, dimensions are split into four parts: the first one equals to r, the second is xi, the third one equals to yj, and the last one to zk to compose a quaternion Q = r1 + xi + yj + zk”, are you splitting dimension M or M\times N? And if you split M \times N (I believe that’s what you are doing), in which order you are splitting (row major right?) Please explain.
2.	I did not understand why authors didn’t go in the negative direction of the gradient in Eq. (10-11)?
3.	In section 3.4, authors mentioned “Moreover, an hyper-complex parameter cannot be simply initialized randomly and component-wise, due to the interactions between components.” which I strongly agree. But in Eq. (7) and (9) why the update rules and activation function are applied component wise?
4.	I really like the elegance in the parameter initialization. Couple of minor things here: (1) It’s better to mention in Eq. (16) why E(|W|) is 0 because of symmetry. (2) Reference should be 6.1 instead of 5.1.
5.	Another reasonable baseline will be using a complex network like (<a href="https://openreview.net/forum?id=H1T2hmZAb)" target="_blank" rel="nofollow">https://openreview.net/forum?id=H1T2hmZAb)</a> and use the first two terms in Eq. (19) for representation. This will also possibly justify the usefulness of using higher order partials. 
6.	The authors mentioned multiple times about the achieved state-of-the-art results without giving any citation. As a reader not well versed in the acoustic domain, it will be nice to see some references to cross-validate the claim made.



General Comments:
1.	I understand the necessity of defining RNN/ LSTM model in the space of quaternions. But unit quaternions can be identified with other spaces where convolution is defined recently, e.g., with S^3 (https://arxiv.org/abs/1809.06211). I can see that this paper is contemporary, but at least can authors comment on the applicability of this general method in their case? Given that in NIPS’18 the following paper talked about RNN model on non-Euclidean spaces (https://arxiv.org/pdf/1805.11204.pdf), one can extend these ideas to develop an RNN model in the space of quaternions. Authors should look into it rigorously as future directions? But at least please comments on the applicability.
2.	The experimental results section is somewhat weak, the overall claim of using fewer parameters and achieving comparable results is only validated on TIMIT data. More experimentation is necessary. 
3.	In terms of technical novelty, though quaternion algebra is well-known, I like the parameter initialization algorithm. I can see the merit of this in ML/ vision community.  

Pros: 
1. Nice well grounded methodological development on well-known algebra. (simple but elegant, so that's good).
2. Nicely written and all the maths check out (that's good).
3. Experimental result on TIMIT dataset shows usefulness in terms of using fewer parameters (but still can achieve SOA results).

Cons:
1. See my comments above. I expect the authors to rebut/ address the aforementioned comments. Overall though simple but nice (and necessary) development of RNN/ LSTM framework in the space of quaternions. 
2. Lacks extensive experimental validation.

My reason for my rating is mainly because of (1) lack of experimental validation. (2) being aware of the recent development of general RNN model on non-Euclidean spaces, I want some comments in this direction (see detailed comment and reference above).
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryxPHgY7p7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks you + answers[1/2]</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByMHvs0cFQ&amp;noteId=ryxPHgY7p7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper261 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper261 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to first thank reviewer 3 for the detailed and useful feedback. We start by addressing each one of the initial point raised:

1. Let us take the example of an input vector X of size N=256 and an output vector O of size M=512. During computations, both X and O are one-dimensional real-valued vectors. Numbers contained in X[0,…,63] are real components, while X[64,…,127] belong to the component i. Therefore the first quaternion is X[0] + X[64]i + X[128]j + X[192]k, and the same considerations can be done for the output vector O. At the end, we have N/4 input quaternions and M/4 output quaternions. 

2. The update phase of the NN parameters with respect to the gradient direction is actually depending on the task. We corrected it to go in the negative direction of the gradient, we thank reviewer 2 for suggesting this correction. 

3.The authors agree with the fact that the split activation functions does not seem to perfectly suit quaternion networks. Therefore we added a statement in the paper to motivate the use of split activation functions. Nonetheless, these quaternion activation functions have been found to be more stable (purely quaternion functions have singularities), and easier to compute, making them interesting for QRNNs. We plan in a near future to investigate QNNs that will use quaternions from the input to the output (pure quaternion activations, full rotations), but we believe that these networks might be harder to train due to singularities deriving from the use of quaternion algebra. Furthemore, the BPTT for quaternions is defined based on the initial work of back propagation in the quaternial domain proposed by P. Arena. In the latter, the loss (Eq. 9) has to be calculated with respect to each component of the quaternion. Indeed, we have to evaluate how much each component of a given quaternion parameter affects the whole loss. Then by applying the chain rule, we end up with a component-wise product in Eq. 13 due to the split activations, which is simpler and way less computationally intensive than calculating the derivative of a quaternion-valued function. 

4. Fixed.

5. We thought a lot about baselines during the experiments. The main issue is that it is not possible to compare complex-valued NN (CVNNs) to QNNs in a fair setting. Indeed, in the case proposed by reviewer 2, CVNNs will clearly have less information and will give worst results. Then we could use CVNNs with magnitude and phase directly from the signal, but the input space would be different compared to QNN, and we won’t have comparable results. Many papers use well engineered features, more-complex structures (attention mechanisms, gates,..,), or even training regularization (Batch-normalization), and it won’t be fair to compare our « vanilla » QRNN and QLSTM to such models. For these reasons, we have decided to add real-valued RNN and LSTM, that are exactly the same than quaternion-valued ones, to obtain fair comparisons. However, it is clear that it could be interesting to build and investigate a complex and state-of-the art quaternion-valued model, but we have to  first introduce the basics of QRNN-QLSTM based models.

6. The authors agree with this remark. As suggested by reviewer 3, we added more citations and written results of the literature in the paper (in term of PER) to help the reader to better compare and evaluate the observed results. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SkxSXlFmT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks you + answers [2/2]</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByMHvs0cFQ&amp;noteId=SkxSXlFmT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper261 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper261 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">——— General comments

1. This is a critical point, indeed. The question « Complex and quaternion valued networks are ok, but what if we want to go in higher dimensions ? » is a common question. We are aware that many works exist in higher dimensions (Octonions, sedenions ..) or even on « generic » models that could apply to any dimensionality (like Clifford algebra based neural networks, or ManifoldNets). Therefore, we acknowledge the fact that it should be possible to reduce the dimension of such different spaces to 4, but then, we would end up with neural networks that will behave as quaternion-valued neural networks. Quaternion-valued neural networks are a special case of such high-dimensional algebras, and are thus suitable to perfectly solve specific problematics that could be very useful for many domain areas, such as image processing, human pose estimation, 3D transformations,…,. It is therefore important to first clearly define a quaternion-valued neural network as a specific neural network using a particular algebra. However, it is clear that there is a need for higher dimensional neural networks (such as ManifoldNets), and there is plenty of rooms for investigations. 

2. We agree that TIMIT is too small. Therefore, and as described in the answer to questions of  reviewer 2, we added results on a larger speech recognition task (Wall Street Journal) in the supplementary materials. Reported results confirm those observed on the TIMIT task with fewer parameters and slightly lower Word Error Rates. 

3. We thanks reviewer 3 for this encouraging statement.

We truly hope that we answered all your questions and remarks, and we are still open to any discussion on this work. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>