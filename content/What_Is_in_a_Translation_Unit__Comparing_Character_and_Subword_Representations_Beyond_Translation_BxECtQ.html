<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>What Is in a Translation Unit?  Comparing Character and Subword Representations Beyond Translation | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="What Is in a Translation Unit?  Comparing Character and Subword Representations Beyond Translation" />
        <meta name="citation_author" content="Nadir Durrani" />
        <meta name="citation_author" content="Fahim Dalvi" />
        <meta name="citation_author" content="Hassan Sajjad" />
        <meta name="citation_author" content="Yonatan Belinkov" />
        <meta name="citation_author" content="Preslav Nakov" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1x0E2C5tQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="What Is in a Translation Unit?  Comparing Character and Subword..." />
      <meta name="og:description" content="Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1x0E2C5tQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>What Is in a Translation Unit?  Comparing Character and Subword Representations Beyond Translation</a> <a class="note_content_pdf" href="/pdf?id=B1x0E2C5tQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=ndurrani%40qf.org.qa" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="ndurrani@qf.org.qa">Nadir Durrani</a>, <a href="/profile?email=faimaduddin%40qf.org.qa" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="faimaduddin@qf.org.qa">Fahim Dalvi</a>, <a href="/profile?email=hsajjad%40qf.org.qa" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="hsajjad@qf.org.qa">Hassan Sajjad</a>, <a href="/profile?email=belinkov%40mit.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="belinkov@mit.edu">Yonatan Belinkov</a>, <a href="/profile?email=pnakov%40hbku.edu.qa" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="pnakov@hbku.edu.qa">Preslav Nakov</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Recent work has shown that contextualized word representations derived from neural machine translation (NMT) are a viable alternative to such from simple word predictions tasks. This is because the internal understanding that needs to be built in order to be able to translate from one language to another is much more comprehensive. Unfortunately, computational and memory limitations as of present prevent NMT models from using large word vocabularies, and thus alternatives such as subword units (BPE and morphological segmentations) and characters have been used. Here we study the impact of using different kinds of units on the quality of the resulting representations when used to model syntax, semantics, and morphology.  We found that while representations derived from subwords are slightly better for modeling syntax, character-based representations are superior for modeling morphology and are also more robust to noisy input.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">subwords, representations, word embeddings, transfer learning, machine translation, natural language processing</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We study the impact of using different kinds of subword units on the quality of the resulting representations when used to model syntax, semantics, and morphology.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SyxCJzHN6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1x0E2C5tQ&amp;noteId=SyxCJzHN6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1499 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1499 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BylOU5xyT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Contribution weak</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1x0E2C5tQ&amp;noteId=BylOU5xyT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1499 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1499 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: To carry out "analytic" NLP tasks focusing on morphology, syntax, semantics, and assuming we take embeddings from a neural translation model, which translation unit type (word, BPE, morpheme, character) should I use? What is the impact of character-level errors on performance on these tasks.

Is the task which is undertaken in this paper really useful? I would have wished for conclusions to help one decide, based on, say, linguistic knowledge about tasks (is it based more on morphology, orthography, agreement, word order, etc) and language (is the language inflection-rich, does it have flexible word order, etc), which is the best unit size. The research questions and the conclusions are very limited. They make the entire exercise seem academic. The contribution is not really clear: how does one exploit this in practice? What should I have expected before doing the experiments, which I don't find at the end? Did I really need this to realise that (sec 6, Best of All Worlds) different aspects of the language are learnt by different unit sizes?

The paper is mostly clearly written, and well presented. The experiments are well executed. I am not convinced by the choice of tasks, which is not motivated in the paper (the paper only says "we don't do sentiment analysis and question answering", but why?). I could well have imagined a paper checking my linguistic intuition on tasks such as language modelling, agreement, sentence completion.

Overall, despite the good presentation, I am skeptical about the contribution and impact of this paper.


Miscellaneous criticism and praise
* Table 3 is unclear: are the numbers word or character or sentence counts? Here and in the text (sec 5 "Only 1863... for German"), it is not clear why and how the de corpus wouold only have a cross validation set and no training/ test set? what does that mean?
* sect 4: "The classifier...trained for 10 epochs": Log reg training is a convex problem, so the number of epochs should only affect the final result very little.
* Figure 2: I don't understand the token frequency axis scale: absolute frequency? but then how to compare different unit sizes?
* 5.1 "character-based..much better..on less frequent and OOV words": seems to apply on in Russian and German
* based on the WMT18 systems, it seems that it would have been closer to the state of the art to study Transformer models rather than LSTM-based models.
* footnote 4 is good. I wish the variance of results had been calculated. I regularly suspect that the papers draws conclusions from insignificant differences in performance.

Suggestions and typos
1 first bullet at end: I suggest to clarify "when used to model a,b, and c at word level", idem in line 2 of sec 5
3 the connection between z (eg BPE) and x (word) is unclear
3.1 gerund: why use teletype font?
3.2 "ranging from" has no "to": suggest remove
below table 3: I disagree that the tag S\NP/NP indicates that "in" attaches to the verb
4 The classifier is a *multi-category* logistic regression
4 500 dim *2 layers * 2 directions (encoder) resp. 1 direction (decoder)
5.4 performed worse
6 present
6 results</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkeau-5cnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice comparison but lacking variance discussion and clear way to usefulness.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1x0E2C5tQ&amp;noteId=rkeau-5cnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1499 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1499 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors study the impact of using different representation units â€“ words, characters, subword units, and morphological segments on the representations learned in neural machine translation systems. The paper is very well-written and the study is convincing: there are 4 NMT data-sets used and 4 tasks used to asses the quality of representations and their robustness to noise. The results are a bit mixed: character-level representations are most robust to noise and perform best in the morphological tagging task, morphological segments perform best in syntax tasks, and so on. One problem with the study is that the architecture of the NMT network is fixed to be an LSTM. It is unclear how this affects the results: does the fact that LSTM can make many steps on character-level input help? Does it hurt? Architectural variance is not measured nor discussed at all. In the introduction, the authors say "We make practical recommendations based on our results." -- this is hard to see, as the results are very mixed and using a fixed architecture makes it hard to draw any recommendations from them in a wider setting.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJeNmedc27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Weak reject</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1x0E2C5tQ&amp;noteId=rJeNmedc27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1499 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1499 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper investigates the representations learned by NMT models.  Specifically it compares the representations learned by NMT models with different types of input/output units.  The representations are compared by concatenating the hidden states into a feature vector and training classifiers for 3 different tagging tasks (morphology, syntax, semantics) on those features.

The study is complete and rigorous, and the methods are clearly explained.

One finding that is somewhat surprising is that word representations performed worst across the board.   A few concerns arise from this finding:
-- Is this a consequence of limiting the vocabulary to 50K?  Are the word representations somewhat handicapped by this limitation?
--  As the authors state, word vectors are currently the representation of choice in NLP applications.  Are the authors suggesting that subword representations would be preferable in these applications?  If so, it would be more convincing if the representations were compared in the context of one of these applications, such as question answering or entailment.  If this is not what the authors are suggesting, then what is the broader significance of this finding?

Learning pre-trained representations of language is certainly important in many NLP applications, particularly those for which there is little available labeled data.  This appears to be the first study comprehensively comparing different units in terms of the representation quality.  It is thorough and original.  However, the authors have measured performance on morphological, syntactic, and semantic tagging.  While these tasks seem to have been chosen as being representative of raw language understanding, I'm not sure these would also reflect performance in actual NLP applications.

Pros
- Experiments are rigorous and comprehensive.
- Very clearly written and easy to understand.

Cons
- Significance/relevance of these particular tasks is limited.
- Limiting word vocabulary to 50K may be impacting results.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>