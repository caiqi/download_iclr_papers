<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Reinforcement Learning: From temporal to spatial value decomposition | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Reinforcement Learning: From temporal to spatial value decomposition" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1GRtj05t7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Reinforcement Learning: From temporal to spatial value decomposition" />
      <meta name="og:description" content="Value function lies in the heart of reinforcement learning, as it can capture the long-term dependency between current state/action and delayed reward. Value function is defined as expected total..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1GRtj05t7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reinforcement Learning: From temporal to spatial value decomposition</a> <a class="note_content_pdf" href="/pdf?id=B1GRtj05t7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019reinforcement,    &#10;title={Reinforcement Learning: From temporal to spatial value decomposition},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=B1GRtj05t7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Value function lies in the heart of reinforcement learning, as it can capture the long-term dependency between current state/action and delayed reward. Value function is defined as expected total reward. Following this definition, it's natural to derive a temporal decomposition for value function, as a sum of discounted reward on different time steps. Most existing RL algorithms estimate value function based on this temporal decomposition.

In our formulation, value function $Q(s,a)$ is a weight sum of reward $R(s')$, and the weight is the discounted visiting frequency $d(s, a, s')$. Here $d(s, a, s')$ is the key part that captures long-term dependency between current state/action with future state. This formulation inspires us to study $d(s, a, s')$ in depth, with function approximation as usual.
Under the function approximation setting, $d(s, a, s')$ enjoys the benefit of generalization between similar state $s'$. Due to this generalization property, we can get better estimation for $d(s, a, s')$, thus resulting in a better estimation for $Q(s,a)$. We propose spatial monte-carlo (SMC) method, and apply the idea of SMC to actor-critic algorithms with $m$-step advantage estimation and GAE. Experiments demonstrate that our algorithms work empirically well.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">reinforcement learning</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_r1luSeas2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1GRtj05t7&amp;noteId=r1luSeas2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper494 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper494 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper described a spatial value decomposition for RL. The idea is: instead of temporally sum reward for Q function, multiply future states will be visited at the same time. Each future state is weighted by visiting frequency. The new Q function will be weighted average of future states reward.  In order to make the problem tractable, a function approximation method by using Gamma function of state, action and future state is introduced for visiting frequency. In appendix, the authors show equivalence between temporal and spatial formulation of Q function.

A few experiments with different environments are performed, showing spatial value decomposition is able to converge faster.

Comment:

To my knowledge, it is a novel decomposition. However the major concern is: this method is introducing extra bias, because visiting frequency may or may not have strong correlation to final reward. The experiments also indicate that, with larger $m$, the bias effect significantly reduces return. A more detailed discussion of the bias effect will be helpful. Detailed explanation on bias effect will be helpful to understand whether this method will be helpful for sparse reward as well.

I don't have much hands-on experiences on RL experiments. As one of the anonymous pointed, it seems less significant. 

In summary, given the novelty of the paper, I tend to accept it.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SyxeC-kKhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper is missing key related literature, and thus the significance of the contributions is not clear.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1GRtj05t7&amp;noteId=SyxeC-kKhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper494 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper494 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a notion of spatial decomposition of the value function, whereby the Q-function is a linear combination of rewards and discounted visiting frequency.  This seems to be analogous to the successor feature representation (Barreto et al. 2017).  Unfortunately the authors seem to be unaware of this work.  In general, the Related Works section is insufficient, and many other related work on spatial regularization in RL is also missing (from fitted Q-iteration, to work by Ghavamzadeh et al.).  I encourage the authors to thoroughly consult this literature, to better position their contributions.

Other comments:
The experiments currently in the paper are not fully convincing; only 5 random seeds were used yet mujoco can be quite noisy. The results don't seem statistically significant.  I also suspect thereâ€™s some cherry picking; some hyper-parameter are used for some game and different one for other game.

(I had a colleague independently review the paper, and he pointed out exactly the same major issue.)

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">2: Strong rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hyg9GolSnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The spatial value decomposition is just successor representation?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1GRtj05t7&amp;noteId=Hyg9GolSnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper494 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper494 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper is quite clearly written and the authors has explained the ideas nicely. There are also some convincing results that that the proposed algorithm, spatial monte-carlo method improved agent's performance in several continuous control tasks.

However, a critical problem I found is that the so-called 'spatial value decomposition' seems just the same as the the well known successor representation. (i.e. to learn the expected discounted future state occupancy G(s, a, s'), and the reward function R(s)). Learning successor representation instead of Q-function has been investigated quite extensively by Kulkarni et al. in Deep Sucessor Reinforcement Learning (<a href="https://arxiv.org/pdf/1606.02396.pdf)." target="_blank" rel="nofollow">https://arxiv.org/pdf/1606.02396.pdf).</a> So, I'm afraid I do not see enough novelty in this manuscript.

I would be happy to discuss further if the authors could point out how the proposed method is different from the deep successor RL one. Even if they are different enough, I think the manuscript would be much strengthened if the authors could mention the relationship (difference?) between successor representation and the spatial value decomposition.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BylwyYzx9X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>I am afraid that the results are not statistical significant at all</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1GRtj05t7&amp;noteId=BylwyYzx9X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Oct 2018</span><span class="item">ICLR 2019 Conference Paper494 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">To be fair, I did not read the paper thoroughly,  that being said, I read the experiments carefully, however. 
I have to say that the results in your paper are NOT statistical significant at all, there are a couple of reasons.
Firstly, Swimmer has two "local minimum modes"  which your paper exactly shows, however, from my experiences, randomly run PPO for some random seeds will give you half chance to run into the "better" mode which gives you higher reward.
Secondly, in other environments like HalfCheetah, Walker, and Hopper, consider the variance there in your figures is so large, you should have tried many more instead of only 5 random seeds, which is totally not convincing especially there is a small difference in performance between baseline and your method.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJgBrfPaiX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Experimental details</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1GRtj05t7&amp;noteId=SJgBrfPaiX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper494 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">24 Oct 2018</span><span class="item">ICLR 2019 Conference Paper494 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the comments! We want to point out several details in our experiment.
1) We build our code based on OpenAI baselines. In our implementation, PPO can never run into the "better" mode in more random seeds that we've tried.
2) For all MuJoCo experiments, our five random seeds are set to be 0~4 without cherry-picking.
3) We've run experiments with more random seeds (e.g., 0~10), and we observed that the performances are roughly the same as shown in our paper. We will update the results with 10 random seeds during paper revision.
4) We will release the code with paper publication, so the reproducibility of our experimental results should not be an issue.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJgdwnNGnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Publish code</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1GRtj05t7&amp;noteId=BJgdwnNGnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Oct 2018</span><span class="item">ICLR 2019 Conference Paper494 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">In the context of reproducibility of the results, would you be able to share an anonymous version of the code now such that the results can be replicated ?</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>