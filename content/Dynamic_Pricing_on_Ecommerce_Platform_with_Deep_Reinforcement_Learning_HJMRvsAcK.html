<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Dynamic Pricing on E-commerce Platform with Deep Reinforcement Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Dynamic Pricing on E-commerce Platform with Deep Reinforcement Learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HJMRvsAcK7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Dynamic Pricing on E-commerce Platform with Deep Reinforcement..." />
      <meta name="og:description" content="Dynamic pricing problem has been studied for decades and varieties of methodologies were developed under different assumptions. We developed an approach based on deep reinforcement learning (DRL)..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HJMRvsAcK7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Dynamic Pricing on E-commerce Platform with Deep Reinforcement Learning</a> <a class="note_content_pdf" href="/pdf?id=HJMRvsAcK7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019dynamic,    &#10;title={Dynamic Pricing on E-commerce Platform with Deep Reinforcement Learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HJMRvsAcK7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Dynamic pricing problem has been studied for decades and varieties of methodologies were developed under different assumptions. We developed an approach based on deep reinforcement learning (DRL) to address the dynamic pricing problem on an E-commerce platform with few assumptions. This paper first modeled dynamic pricing as a Markov Decision Process, defined various reward functions, with both discrete and continuous pricing action space. And then it introduced the methods to pre-train the model with the historical sales data. Offline evaluations and field experiments were designed and conducted to validate our approach. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">reinforcement learning, dynamic pricing, e-commerce, revenue management, field experiment</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">This paper describes a methodology for pre-training, evaluating and online dynamic pricing on E-commerce platform using deep reinforcement learning.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">5 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HJgUtyV6nm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting and relevant, but still needs work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJMRvsAcK7&amp;noteId=HJgUtyV6nm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper312 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper312 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a reinforcement learning approach to dynamically set the price of items on sale on an e-commerce website, based on a state description consisting of several kinds of features: price, sales, customer traffic and competitiveness. The actions (possible prices) are constrained to lie in item-specific lower and upper bounds. Against a proposed method relying a continuous action space (an implementation of the Deep Deterministic Policy Gradient of Lillicrap et al., 2015), alternatives relying on multi-armed bandits and Deep Q Networks are evaluated. Experimental results from an online deployment are presented.

The paper is at times hard to understand. In particular, it would benefit from a thorough review of English grammar and style. For instance, the alternating descriptions between the past and present tenses (e.g. the abstract and the introduction) are quite non-natural and somewhat irritating [see specific instances in the detailed comments, below]. 

From an algorithmic standpoint, the paper does not introduce new methods and relies on well-known reinforcement learning techniques. The proposed methodology of applying specific RL techniques such as DDPG to pricing appears novel. However, there is an abundant literature on optimal pricing and discounting in operations research, much of it based on dynamic programming techniques, and more links to this literature could be provided.

One could question the choice of the reward function chosen (eq. 1): since it is a ratio, it is extremely sensitive to variations in the denominator, and this could severely impact convergence. 

The primary weakness of the paper lies in the experimental evaluation: at least the following informations are missing to truly understand the methodological impact and economic benefits of the proposed approach:

1. A complete description of the experimental setting, i.e. how many SKUs (stock keeping units) were evaluated, over which product categories, over what time horizon. Of those, how are they distributed in the fast-mover / slow-mover plane (Syntetos et al., 2005)? What special events were material during the time period? How many of those were predicted and incorporated into the state representation?
2. One or more tables of results giving expected utility gains over baseline of the proposed methods, along with confidence intervals.
3. For an ICLR submission, there should be additional investigations as to the structure of the learned representations, at least by the DQN and the DDPG models — is the state embedding learned by the networks somewhat suggestive of economically meaningful properties of the items or the sales environment?

As such, even though the paper is interesting, it is in too early a state to recommend acceptance at ICLR.

Detailed comments:

* p. 1: many past tenses that should be in the present tense, e.g. (just in the abstract): modeled ==&gt; models, defined ==&gt; defines, then it introduced ==&gt; it then introduces, were designed ==&gt; are designed. Many other cases in the rest of the paper.
* p. 1: has draw ==&gt; has drawn
* p. 2: Forth ==&gt; Fourth
* p. 2: overtime ==&gt; over time
* p. 2: is assumed, to ==&gt; is assumed to
* p. 2: described ==&gt; describe
* p. 3: “looks deep inside learning while earning approaches” ==&gt; sentence not clear
* p. 3: this is not clear: “since the number of page visitors may ﬂuctuate dramatically and this could lead to non-concavity“ ==&gt; why would it lead to non-concavity?
* p. 4: The whole paragraph before eq. (1) is not clear
* p. 5: The D in eq. (5) should be explained immediately, not after eq. (6).
* p. 5: In eq. (5), $\theta’$ is not explained: how does it differ from $\theta$ ?
* p. 5: In eq. (6), how is $\theta^{Q’}$ different from $\theta^{Q}$ ?
* p. 6: in eq. (8), the denominator r_t could be a small number, leading to a noisy error; this should be discussed.
* p. 6: Below eq. (12), these sentences are not clear: “For dynamic pricing problem, we particularly concern the outcome from changing between prices. To have a well knowledge between two prices before and after pricing.”</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rye65HF52m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review for Dynamic Pricing on E-commerce Platform with Deep Reinforcement Learning</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJMRvsAcK7&amp;noteId=rye65HF52m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper312 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper312 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors proposed a deep reinforcement learning for dynamic pricing problem. The major contribution is on the problem formulation and application end. From the algorithm point of view, the authors adopt the existing deep reinforcement learning algorithms like DQN and policy gradient. The experiments are conducted both online and offline using dataset from Tmall.

[Advantage Summary]
1. A very interesting application to apply deep RL on dynamic pricing.

2. Experimented on industry dataset based real users. Both online and offline experiments are conducted.

[Weakness Summary]
1. This is not the first work to apply deep reinforcement learning for dynamic pricing problem as claimed by the authors.

2. Limited technical contribution.

3. Illustration and analysis of the experiment can be further approved.

[Details in weakness and questions]

1. This is not the first work to apply deep reinforcement learning for dynamic pricing problem as claimed author.
For instance, 
"Reinforcement Learning for Fair Dynamic Pricing"
Since applying deep RL to dynamic pricing is one of the significant contributions of this paper, this limits the overall contribution.

2.  The technical contribution is very limited by just applying existing algorithms.

3. How to determine the step t seems to be a very important issue that can affect the performance of the algorithm and not well explained. From the experiment, the authors seem to set the period as one-day and update the price daily. But in reality, the time to update the price in a dynamic pricing system should not be a fixed value. For instance, the system should adjust the price in real time if there are changes in the environment(e.g., demand-supply change)

4. Experiment part needs more analysis. For instance, day 16 seems to be an outlier, and conversion rate drops dramatically in the following days. Why? How is the conversion rate from day 16 gets calculated in the final evaluation?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HklnqASCoX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Dynamic Pricing with RL</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJMRvsAcK7&amp;noteId=HklnqASCoX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper312 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper312 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this paper, the authors study the problem of Dynamic Pricing. This is a well-studied problem in Economics, Operations Research, and Computer Science. The basic problem is to find the right price for a product based on repeated interaction with the market. The significant challenge in this problem is to figure the right set of prices without knowing the future demand. The authors in this paper propose a deep reinforcement learning based approach to tackle this problem. Algorithms to this problem in the past have made assumptions such as concavity of the demand curve to make it tractable mathematically. In this paper, they do not make assumptions (although they do not provide theoretical guarantees) and one of the contributions of this paper is to handle the full generality of the problem. The other contribution is to test their methods in both offline and real-time setting and compare it against a natural MAB style algorithm (LinUCB to be precise). I have several comments on this paper.

First, the paper is not well-written. To begin with, the authors do not formally define the Dynamic Pricing problem. In literature, there are many versions of this problem. Is there limited supply or is the supply unlimited? Should we find the price for a single item or multiple items separately? It is unclear. There are pieces of information across the paper which indicate that the seller is selling multiple products for which the algorithm is pricing separately. I am still unsure about whether the supply is finite or not. Along these lines, the authors should spend some time reading through the draft carefully to fix many grammatical mistakes throughout the paper. 

Second, the authors have missed out some critical related work, both in the related work section *and* in their experimental comparison. The paper Dynamic Pricing with limited supply [Babiaoff et al EC 12] and Bandits with Knapsacks (BwK) [Badanidiyuru et al FOCS 2013] provide MAB style algorithms for this problem. These algorithms overcome the reasoning they provide in their experiments - 

"We noticed that, LinUCB outperformed DRL methods in offline policy evaluation. One of the main reasons is that, MAB methods aim to maximize immediate reward while RL methods maximize long-term reward". 

The BwK algorithm precisely optimizes for long-term rewards under limited supply constraints. I think it will be useful to add experiments that compare the current approaches of the paper with this MAB algorithm. Overall I find the baselines in this paper to be weak and suggest more experiments and comparison against tougher relevant benchmarks. In fact, they have failed to compare, in any meaningful way, to a long line of work on Dynamic Pricing (the two Besbes and Zeevi papers they have cited for instance). I wonder how the experiments compare against those algorithms. Or they should clearly state why such a comparison is not relevant.

Overall I find the goal to be interesting and novel. But I think the current state of the draft and the experiments make it weak. In particular, the paper falls below the bar on clarity and quality in this current state.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkxsVD65jX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The Question of the Reward Function</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJMRvsAcK7&amp;noteId=rkxsVD65jX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">22 Oct 2018</span><span class="item">ICLR 2019 Conference Paper312 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">According to the paper, the reward is defined as the difference of the conversion rate in the current step and that of the last step, which means that the long-term rewards are indeed equal to the conversion rate of the last step. However, in reinforcement learning, the objective is to maximize the averaged revenue conversion rate of the entire episode. Thus, the reward function defined in the paper is meaningless.  Also, I think some missing explanations in the paper are: 1) why the reward does not approach to 0 in your offline experiments 2) why the conversion rate is not stable in your online experiments.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJxWopi3om" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: The Question of the Reward Function</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJMRvsAcK7&amp;noteId=rJxWopi3om"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper312 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">23 Oct 2018</span><span class="item">ICLR 2019 Conference Paper312 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your comments. Let’s clarify some of your misunderstanding: 

We defined two different types of reward functions in the paper (please refer to Section 3 and 4 for details): the first reward function maximizes the total expected conversion rates overtime (with discount), which corresponds to the revenue management notion of optimality in pricing. The second reward function maximizes the total expected conversion rate differences overtime (with discount). This is a different objective in general, however these two objectives are closely and positively related, and if needed we can give their precise relationship in mathematical terms. 

Real world commercial environment is not stationary, as a result, the real conversion rates and their differences may not converge to zero or stabilize at a point on-line or off-line. We will think about adding more explanation in the final version.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>