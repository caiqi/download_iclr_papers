<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Differentially Private Federated Learning: A Client Level Perspective | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Differentially Private Federated Learning: A Client Level Perspective" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SkVRTj0cYQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Differentially Private Federated Learning: A Client Level Perspective" />
      <meta name="og:description" content="Federated learning is a recent advance in privacy protection. &#10;  In this context, a trusted curator aggregates parameters optimized in decentralized fashion by multiple clients. The resulting model..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SkVRTj0cYQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Differentially Private Federated Learning: A Client Level Perspective</a> <a class="note_content_pdf" href="/pdf?id=SkVRTj0cYQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019differentially,    &#10;title={Differentially Private Federated Learning: A Client Level Perspective},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SkVRTj0cYQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Federated learning is a recent advance in privacy protection. 
In this context, a trusted curator aggregates parameters optimized in decentralized fashion by multiple clients. The resulting model is then distributed back to all clients, ultimately converging to a joint representative model without explicitly having to share the data. 
However, the protocol is vulnerable to differential attacks, which could originate from any party contributing during federated optimization. In such an attack, a client's contribution during training and information about their data set is revealed through analyzing the distributed model. 
We tackle this problem and propose an algorithm for client sided differential privacy preserving federated optimization. The aim is to hide clients' contributions during training, balancing the trade-off between privacy loss and model performance. 
Empirical studies suggest that given a sufficiently large number of participating clients, our proposed procedure can maintain client-level differential privacy at only a minor cost in model performance. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Machine Learning, Federated Learning, Privacy, Security, Differential Privacy</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Ensuring that models learned in federated fashion do not reveal a client's participation.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_r1g9TUss3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Differentially private variant of the federated learning framework</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkVRTj0cYQ&amp;noteId=r1g9TUss3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper853 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper853 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper revisits the federated learning framework from McMahan in the context of differential privacy.  The general concern with the vanilla federated learning framework is that it is susceptible to differencing attacks. To that end, the paper proposes to make the each of the interaction in the server-side component of the gradient descent to be differentially private w.r.t. the client contributions. This is simply done by adding noise (appropriately scaled) to the gradient updates.

My main concern is that the paper just described differentially private SGD, in the language of federated learning. I could not find any novelty in the approach. Furthermore, just using the vanilla moment's accountant to track privacy depletion in the federated setting is not totally correct. The moment's accountant framework in Abadi et al. uses the "secrecy of the sample" property to boost the privacy guarantee in a particular iteration. However, in the federated setting, the boost via secrecy of the sample does not hold immediately. One requirement of the secrecy of the sample theorem is that the sampled client has to be hidden. However, in the federated setting, even if one does not know what information a client sends to the servery, one can always observe if the client is sending *any* information. For a detailed discussion on this issue see <a href="https://arxiv.org/abs/1808.06651" target="_blank" rel="nofollow">https://arxiv.org/abs/1808.06651</a> .</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1eXjyItn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Well-motivated problem, but incremental improvement over previous work?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkVRTj0cYQ&amp;noteId=r1eXjyItn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper853 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper853 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">[Summary] The authors propose a protocol for training a model over private user data in a federated setting. In contrast with previous approaches which tried to ensure that a model would not reveal too much about any individual data point, this paper aims to prevent leakage of information about any individual client. (There may be many data points associated with a single client.)

[Key Comments] The submission generally seems polished and well-written. However, I have the impression that it's largely an incremental improvement over recent work by McMahan et al. (2018).
* If the main improvement of this paper over previous work is the dynamic adaptation of weight updates discussed in Section 3, the experimental results in Table 1 should compare the performance of the protocol with vs. without these changes. Otherwise, I think it would be helpful for the authors to update the submission to clarify their contributions.
* Updating Algorithm 1 / Line 9 (computation of the median weight update norm) to avoid leaking sensitive information to the clients would also strengthen the submission.
* It would also be helpful if the authors could explicitly list their assumptions about which parties are trusted and which are not (see below).

[Details]
[Pro 1] The submission is generally well-written and polished. I found the beginning of Section 3 especially helpful, since it breaks down a complex algorithm into simple/understandable parts.

[Pro 2] The proposed algorithm tackles the challenging/well-motivated problem of improving federated machine learning with strong theoretical privacy guarantees.

[Pro 3] Section 6 has an interesting analysis of how the weight updates produced by clients change over the course of training. This section does a good job of setting up the intuition for the training setup used in the paper, where the number of clients used in each round is gradually increased over the course of training.
 
[Con 1] I had trouble understanding the precise threat model used in the paper, and I think it would be helpful if the authors could update their submission to explicitly list their assumptions in one place. It seems like the server is trusted while the clients are not. However, I was unsure whether the goal was to protect against a single honest-but-curious client or to protect against multiple (possibly colluding) clients.

[Con 2] During each round of communication, the protocol computes the median of a set of values, each one originating from a different client, and the output of this computation is used to perform weight updates which are sent back to the clients. The authors note that "we do not use a randomized mechanism for computing the median, which, strictly speaking, is a violation of privacy. However, the information leakage through the median is small (future work will contain such privacy measures)." I appreciate the authors' honesty and thoroughness in pointing out this limitation. However, it does make the submission feel like a work in progress rather than a finished paper, and I think that the submission would be a bit stronger if this issue was addressed.

[Con 3] Given the experimental results reported in Section 4, it's difficult for me to understand how much of an improvement the authors' proposed dynamic weight updates provide in practice. This concern could be addressed with the inclusion of additional details and baselines:
* Few details are provided about the model training setup, and the reported accuracy of the non-differentially private model is quite low (3% reported error rate on MNIST; it's straightforward to get 1% error or below with a modern convolutional neural network). The authors say they use a setup similar to previous work by McMahan et al. (2017), but it seems like that paper uses a model with a much lower error rate (less than 1% based on a cursory inspection), which makes direct comparisons difficult.
* The introduction argues that "dynamically adapting the dp-preserving mechanism during decentralized training" is a significant difference from previous work. The claim could be strengthened if the authors extended Table 1 (experimental results for differentially private federated learning) in order to demonstrate the effect of dynamic adaptation on model quality.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkgvwEsl2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The scale of the S</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkVRTj0cYQ&amp;noteId=rkgvwEsl2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">26 Oct 2018</span><span class="item">ICLR 2019 Conference Paper853 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">It is unclear about the scale of the clip bound S. Could you please
add some details about the scale of the S, due to the S
is a key factor to the final performance. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BygeEICS3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Choosing S</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkVRTj0cYQ&amp;noteId=BygeEICS3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018</span><span class="item">ICLR 2019 Conference Paper853 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for your question.
On page 4, in the section 'Choosing S' we provide information about the scale. As proposed by Abadi et al. (2016) we chose S to be the median of the second norms of the client contributions. We thereby ensure that the noise does not explode when a client provides very large updates but also do not trim too much of the true updates. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJl-Xe68j7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Please give a more clear desribtion of the algorithm part.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkVRTj0cYQ&amp;noteId=rJl-Xe68j7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Oct 2018</span><span class="item">ICLR 2019 Conference Paper853 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">It remains unclear in the algorithm. Firstly, what's the purpose to introduce the parameter variance V?
Then, the author used \sigma_t as the noise scale,  but used \sigma  in the following updating.
Please give some comment on how to change the noise scale at each iterative step. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkx4AgGjim" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Missing _t</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkVRTj0cYQ&amp;noteId=Bkx4AgGjim"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">22 Oct 2018</span><span class="item">ICLR 2019 Conference Paper853 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the comment. Indeed this is a mistake. In line 10 of the algorithm, \sigma must be replaced with \sigma_t. 
The parameter variance is introduced when defining the between clients variance (it is just an in-between step to make that definition easier). 

In the discussion we explain: 
For a certain noise scale at iteration t: \frac{sigma^2_t}{m_t}, the privacy loss is smaller for both sigma_t and m_t being small. Now if the clients provided very similar updates we would therefore go for small sigma_t and small m_t. But if the clients provided very distinct updates, a communication round with a small m_t would not improve the model even if the overall noise scale didn't change. (remember: In federated learning client-data might be non-IID).

We show that over the course of federated learning (for highly non-IID clients) the similarity of updates decreases (between clients variance increases) and it is therefore advantageous to start with a low m_t and keep increasing it during subsequent iteration rounds. If \sigma_t is held constant for all t that means the noise scale decreases over the course of training. 

The precise choices of \sigma_t and \m_t over the course of training highly depend on the  federated learning scenario (the privacy budget, the data, the amount of clients and how data is distributed among them). We therefore cannot give a general iterative rule in the algorithm but just provide a tendency to be followed when these parameters are to be chosen for a new setting.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1x_i3hUsm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>What's the contribution of this paper?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkVRTj0cYQ&amp;noteId=S1x_i3hUsm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Oct 2018</span><span class="item">ICLR 2019 Conference Paper853 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">This work is similar paradigm to LEARNING DIFFERENTIALLY PRIVATE RECURRENT LANGUAGE MODELS. So, what's the
difference to previous work?  </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BklIfi7voQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Opposite extreme cases</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkVRTj0cYQ&amp;noteId=BklIfi7voQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Oct 2018</span><span class="item">ICLR 2019 Conference Paper853 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for your question.
We do point out the similarity to LEARNING DIFFERENTIALLY PRIVATE RECURRENT LANGUAGE MODELS in the introduction. The research was conducted at the same time as ours but published at last year’s ICLR-conference, whereas we presented ours at a workshop.

The reason why we now decided to aim for a conference-publication is that the two research projects aimed at opposite extreme cases and we want to motivate research in ours. 
LEARNING DIFFERENTIALLY PRIVATE RECURRENT LANGUAGE MODELS shows that with lots of clients, performance of language models can be maintained high while privacy is ensured. The work is centered around mobile phone users where hundreds of millions of clients are a realistic scenario. 
DIFFERENTIALLY PRIVATE FEDERATED LEARNING: A CLIENT LEVEL PERSPECTIVE aims at the other extreme. We were primarily interested in institutions such as hospitals jointly learning models. In these scenarios, the number of clients (e.g. hospitals) could be as low as a hundred. We want to motivate research of differentially private federated learning in this less commercial area and point out its potential for hospitals, laboratories and universities that have high privacy standards but could greatly benefit from one another (e.g. the authors of [Multi-Institutional Deep Learning Modeling Without Sharing Patient Data: A Feasibility Study on Brain Tumor Segmentation] state that the integration of differential privacy into their research would make it applicable to sensitive data.) In our research we focus on different phases of federated learning and how low numbers of participants influence these phases and the privacy loss during them. 

TLDR;
We did not want to show: 'We can include privacy into already existing language models learned from millions of clients without drawbacks'
But instead: 'Hospitals, labs or universities, that do not cooperate in learning models as of today, could greatly benefit from one another without revealing sensitive information.'</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SkenUlAliQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting direction but confusing presentation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkVRTj0cYQ&amp;noteId=SkenUlAliQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper853 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper853 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The main claim the authors make is that providing privacy in learning should go beyond just privacy for individual records to providing privacy for data contributors which could be an entire hospital. Adding privacy by design to the machine learning pipe-line is an important topic. Unfortunately, the presentation of this paper makes it hard to follow. 

Some of the issues in this paper are technical and easy to resolve, such as citation format (see below) or consistency of notation (see below). Another example is that although the method presented here is suitable only for gradient based learning this is not stated clearly. However, other issues are more fundamental:
1.	The main motivation for this work is providing privacy to a client which could be a hospital as opposed to providing privacy to a single record – why is that an important task? Moreover, there are standard ways to extend differential privacy from a single record to a set of r records (see dwork &amp; Rote, 2014 Theorem 2.2), in what sense the method presented here different than these methods?
2.	Another issue with the hospitals motivation is that the results show that when the number of parties is 10,000 the accuracy is close to the baseline. However, there are only 5534 registered hospitals in the US in 2018 according to the American Hospital Association (AHA): <a href="https://www.aha.org/statistics/fast-facts-us-hospitals." target="_blank" rel="nofollow">https://www.aha.org/statistics/fast-facts-us-hospitals.</a> Therefore, are the sizes used in the experiments reasonable?
3.	In the presentation of the methods, it is not clear what is novel and what was already done by Abadi et al., 2016
4.	The theoretical analysis of the algorithm is only implied and not stated clearly
5.	In reporting the experiment setup key pieces of information are missing which makes the experiment irreproducible. For example, what is the leaning algorithm used? If it is a neural network, what was its layout? What type of cross validation was used to tune parameters?
6.	In describing the experiment it says that “For K\in\{1000,10000} data points are repeated.” This could mean that a single client holds the same point multiple times or that multiple clients hold the same data point. Which one of them is correct? What are the implications of that on the results of the experiment?
7.	Since grid search is used to tune parameters, more information is leaking which is not compensated for by, for example, composition bounds
8.	The results of the experiments are not contrasted against prior art, for example the results of Abadi et al., 2016.

Additional comments
9.	The introduction is confusing since it uses the term “federated learning” as a privacy technology. However federated learning discusses the scenario where the data is distributed between several parties. It is not necessarily the case that there are also privacy concerns associated, in many cases the need for federated learning is due to performance constraints.
10.	In the abstract the term “differential attacks” is used – what does it mean?
11.	“An independent study McMahan et al. (2018), published at the same time”- since you refer to the work of McMahan et al before your paper was reviewed, it means that the work of McMahan et al came out earlier.
12.	In the section “Choosing $\sigma$ and $m$” it is stated that the higher \sigma and the lower m, the higher the privacy loss. Isn’t the privacy loss reduced when \sigma is larger? Moreover, since you divide the gradients by m_t then the sensitivity of each party is of the order of S/m and therefore it reduces as m gets larger, hence, the privacy loss is smaller when m is large. 
13.	At the bottom of page 4 and top of page 5 you introduce variance related terms that are never used in the algorithm or any analysis (they are presented in Figure 3). The variance between clients can be a function of how the data is split between them. If, for example, each client represents a different demography then the variance may be larger from the beginning.
14.	In the experiments (Table 1), what does it mean for \delta^\prime to be e-3, e-5 or e-6? Is it 10^{-3}, 10^{-5} and 10^{-6}?
15.	The methods presented here apply only for gradient descent learning algorithms, but this is not stated clearly. For example, would the methods presented here apply for learning tree based models?
16.	The citations are used incorrectly, for example “sometimes referred to as collaborative Shokri &amp; Shmatikov (2015)” should be “sometimes referred to as collaborative (Shokri &amp; Shmatikov, 2015)”. This can be achieved by using \citep in latex. This problem appears in many places in the paper, including, for example, “we make use of the moments accountant as proposed by Abadi et al. Abadi et al. (2016).” Which should be “we make use of the moments accountant as proposed by Abadi et al. (2016).” In which case you should use only \cite and not quote the name in the .tex file.
17.	“We use the same deﬁnition for differential privacy in randomized mechanisms as Abadi et al. (2016):” – the definition of differential privacy is due to Dwork, McSherry, Nissim &amp; Smith, 2006
18.	Notation is followed loosely which makes it harder to follow at parts. For example, you use “m_t” for the number of participants in time t but in some cases,  you use only m as in “Choosing $\sigma$ and $m$”.
19.	In algorithm 1 the function ClientUpdate receives two parameters however the first parameter is never used in this function. 
20.	Figure 2: I think it would be easier to see the results if you use log-log plot
21.	Discussion: “For K=10000, the differrntially private model almost reaches accuracies of the non-differential private one.” – it is true that the model used in this experiment achieves an accuracy of 0.97 without DP and the reported number for K=10000 is 0.96 which is very close. However, the baseline accuracy of 0.97 is very low for MNIST.
22.	In the bibliography you have Brendan McMahan appearing both as Brendan McMahan and H. Brendan McMahan


It is possible that underneath that this work has some hidden jams, however, the presentation makes them hard to find. 

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>