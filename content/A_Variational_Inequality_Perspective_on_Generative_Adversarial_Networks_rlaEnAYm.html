<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>A Variational Inequality Perspective on Generative Adversarial Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="A Variational Inequality Perspective on Generative Adversarial Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=r1laEnA5Ym" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="A Variational Inequality Perspective on Generative Adversarial..." />
      <meta name="og:description" content="Generative adversarial networks (GANs) form a generative modeling approach known for producing appealing samples, but they are notably difficult to train. One common way to tackle this issue has..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_r1laEnA5Ym" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A Variational Inequality Perspective on Generative Adversarial Networks</a> <a class="note_content_pdf" href="/pdf?id=r1laEnA5Ym" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 18 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019a,    &#10;title={A Variational Inequality Perspective on Generative Adversarial Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=r1laEnA5Ym},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=r1laEnA5Ym" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Generative adversarial networks (GANs) form a generative modeling approach known for producing appealing samples, but they are notably difficult to train. One common way to tackle this issue has been to propose new formulations of the GAN objective. Yet, surprisingly few studies have looked at optimization methods designed for this adversarial training. In this work, we cast GAN optimization problems in the general variational inequality framework. Tapping into the mathematical programming literature, we counter some common misconceptions about the difficulties of saddle point optimization and propose to extend methods designed for variational inequalities to the training of GANs. We apply averaging, extrapolation and a novel computationally cheaper variant that we call extrapolation from the past to the stochastic gradient method (SGD) and Adam.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">optimization, variational inequality, games, saddle point, extrapolation, averaging, extragradient, generative modeling, generative adversarial network</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We cast GANs in the variational inequality framework and import techniques from this literature to optimize GANs better; we give algorithmic extensions and empirically test their performance for training GANs.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rygmGXa_p7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>an interesting perspective and missing important references</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1laEnA5Ym&amp;noteId=rygmGXa_p7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1491 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">It is an interesting perspective for training GAN. 

I would like to point out several important references that the paper is missing regarding the theoretical contributions of this work. 

1. the linear convergence for strongly monotone VI has been proved by Nesterov in 2011, though for a different algorithm. 
Yurii Nesterov and Laura Scrimali. Solving strongly monotone variational and quasi-variational inequalities. Discrete and Continuous Dynamical Systems - A, 2011.

2. the idea of using one gradient in the extragradient method has been used in online optimization algorithms, e.g., 
 Chiang, C.K., Yang, T., Lee, C.J., Mahdavi, M., Lu, C.J., Jin, R., Zhu, S.: Online optimization with gradual variations. In: COLT 2012.

</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkgcGClCa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to "an interesting perspective and missing important references"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1laEnA5Ym&amp;noteId=BkgcGClCa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1491 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1491 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hello, 
first of all we would like to thank this anonymous reader for his interest on the paper. We agree that [Chiang et al. 2012] is a relevant and we will consider to incorporate it in the revision.

However, note that we already mentioned in our paper a more general or a more seminal related work:

1.
We are aware of existing convergence proof for strongly monotone VI: We actually mention in Section 3 a seminal work on strongly monotone VIPs: “These iterates are known to converge linearly under an additional assumption on the operator\footnote{ Strong monotonicity, a generalization of strong convexity. See §A.} (Chen and Rockafellar, 1997)”.

As you pointed out, Nesterov and Scrimali (2011) consider another algorithm. The Forward-Backward algorithm presented in (Chen and Rockafellar, 1997)  is another denomination (a bit more general though) for what we called “gradient method” (in Section 3). The Forward-Backward algorithm is more related to our work than Nesterov and Scrimali’s method is. Moreover, the proof of linear convergence of what we called “extrapolation from the past” algorithm (Theorem 1) is non trivial and, to our knowledge, does not directly extend from any existing work.

2.
We mention right after (21), “ This update scheme can be related to the optimistic mirror descent (Rakhlin and Sridharan, 2013)”. Rakhlin and Sridharan (2013) explain in the beginning of Section 2 that “[they] exhibit a Mirror Descent type method which can be seen as a generalization of the recent algorithm of [9]” [9] being (Chiang et al. 2012).  

As developed in our paper right after the definition of “extrapolation from the past” (and pointed out by the anonymous reviewers) we are bringing a new perspective on this method: “However our technique comes from a different perspective, it was motivated by VIP and inspired from the extragradient method” and “ Using the VIP point of view we are able to prove a linear convergence rate for a projected version of the extrapolation from the past (see details and proof of Theorem 1 in §B.3). We also extend these results to the stochastic operator setting in §4”.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJeE2n3_TX" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1laEnA5Ym&amp;noteId=rJeE2n3_TX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper1491 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJlRNFEa3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Principled optimization for GANS</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1laEnA5Ym&amp;noteId=SJlRNFEa3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1491 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1491 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
The authors take a variational inequality perspective to the study of the saddle point problem that defines a GAN. By doing so, they are able to profit from the corresponding literature and propose a few methods that are variants of SGD. The authors show in a simple example (a bilinear function) these exhibit better performance than Adam and a basic gradient method. After showing theoretical guarantees of these methods (linear convergence) the authors propose to combine them with existing techniques, and show in fact this leads to better results.

Evaluation
This is a very good paper and I cannot but recommend its acceptance:
It is clear and well written. 
It has the right level of balance between theory and experiments. 
Theoretical results are far from trivial. 
I haven't seen something similar.
The authors's do not make overstatements: they do not claim to have solved the GAN problem, but they do report improvements which are due to a thorough analysis (see above points). These results are much appreciated.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJeigrZCaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for the positive comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1laEnA5Ym&amp;noteId=SJeigrZCaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1491 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1491 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The authors first would like to thank AnonReviewer3 for his careful evaluation and his detailed comments.
We would like to point out that we addressed the points raised by AnonReviewer2  in our updated version, particularly, we tried our methods to train a ResNet architecture with the WGAN-GP objective (see the experimental section 7.2, table 1 and figure 4 for the new results).  After few days of experiments, we were able to match the current state-of-the art results of 8.2 on this architecture by using ExtraAdam with averaging (and without using spectral normalization). A similar time budget was spent for fine tuning each algorithm (SimAdam, AltAdam1, AltAdam5, ExtraAdam). We observed that with quite few hyperparameter tuning it was possible to match the state of the art with ExtraAdam. We also observed that ExtraAdam is less sensitive to the choice of learning rate, making the hyperparameter tuning easier and enabling the use of higher learning rate.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Bye1MVvq2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A new perspective on optimization problems arising in GANs which helps provide insights into why averaging helps, why certain type of updates are bad, and how extrapolation can be used to obtain even better solvers.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1laEnA5Ym&amp;noteId=Bye1MVvq2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1491 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1491 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper looks at solving optimization problems that arise in GANs, via a variational inequality perspective (VIP). VIP entails solving an optimization problem that is related to the first order condition of the optimization problem that we wish to solve. VIP have been very successful in solving min-max style problems. Given that, GAN formulations tend to be min-max style problems (though not necessarily 0 sum) the VIP perspective is very natural, though under-explored in machine learning. Two techniques that have been widely used to solve VIP problems are averaging and extragradient methods. The authors look at a simple GAN setup where both the generator and the discriminator are linear models. In this case two kinds of gradient updates can be derived. First are simultaneous updates, and the other is alternated updates. The authors show that simultaneous updates are not even bounded and diverge to infinity, whereas alternated updates are more stable and stay bounded, but need not necessarily converge. However, I think this behaviour is limited to only linear discriminator/generator and might not extend beyond the linear case. The second key idea is the use of extra-gradient updates. Extra-gradient updates perform an "extra" or fake gradient step to get to a new point, and then kind of retracks back and perform a gradient step using the gradient step obtained from the "extra step".  This extra-gradient method is a close approximation to Euler's method, though far more computationally efficient.  However, the extragradient step requires one to calculate gradient twice, which can be expensive in large models. For this reason, the authors suggest using gradients from past as the "extragradient" in the extragradient method. 

For strongly-monotone operators (a generalization of strongly-convex functions) extrapolation updates are shown to have linear convergence.  Furthermore, the authors show that using extrapolation and averaging under the assumption that the operator is monotonic, and using constant step size SGD the rates of convergence are better than the rates obtained using plain SGD with averaging but without extrapolation. Authors also show how one can use these ideas using other first order methods such as ADAM instead of SGD. Experiments are shown on the DCGAN architecture. 

On the whole this is a really nice paper, that shows how standard ideas from VIP can be useful for training GANs. I recommend acceptance</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJe-3rW0pQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for the knowledgeable review </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1laEnA5Ym&amp;noteId=HJe-3rW0pQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1491 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1491 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The authors first would like to thank AnonReviewer1 for his meticulous analysis and his insightful comments.

“I think this behaviour is limited to only linear discriminator/generator and might not extend beyond the linear case.”
This behavior is a local behavior (and then can be true for a globally non-monotone operator), i.e, if the objective is bilinear in a neighborhood of an (local) equilibrium this behavior is true in that neighborhood. It means that the iterates of the simultaneous method will be expelled from this neighborhood geometrically, the ones of the alternated method will stay in the neighborhood but will not converge to the equilibrium. On the contrary the averaged iterates  and the iterate of the extragradient method will converge to the equilibrium. We also think that these results could be generalized to any game which is locally Hamiltonian (see [1]) around the (local) equilibrium. 

“Experiments are shown on the DCGAN architecture. “
As suggested by AnonReviewer2, we tried our methods to train a ResNet architecture with the WGAN-GP objective (see the experimental section 7.2, table 1 and figure 4 for the new results). After few days of experiments, we were able to match current the state-of-the art results of ~8.2 on this architecture by using ExtraAdam with averaging (and without using spectral normalization). Contrary to the previous experiments of the paper, the hyperparameter search was less exhaustive (due to time reason) but a similar time budget was spent for fine tuning each algorithm. We observed that with quite few hyperparameter tuning it was possible to match the state of the art with ExtraAdam. We also observed that ExtraAdam is less sensitive to the choice of learning rate, making the hyperparameter tuning easier and enabling the use of higher learning rate.

[1] Balduzzi, D., Racaniere, S., Martens, J., Foerster, J., Tuyls, K., &amp; Graepel, T. The Mechanics of n-Player Differentiable Games. ICML 2018</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HkekxnDu3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good review on algorithms for VIs in the context of GANs</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1laEnA5Ym&amp;noteId=HkekxnDu3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1491 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1491 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Overall, the paper is well-written and of high quality, therefore I recommend acceptance. 

Pros:
+ The work gives an accessible but still rigorous introduction to the literature on VIs which I find highly valuable, as it creates a bridge between the classical mathematical programming literature and applications in AI. 

+ The theory for optimization of VIs with stochastic gradients (though only in monotone setting) was very interesting to me and contains some novel results (Theorem 2, Theorem 4)

Cons:
- I'm a bit skeptical about the experiments on GANs. They indicate that for the specific choice of architectures and hyper-parameters "ExtraAdam" works better, but the chosen architectures are not state-of-the art. What would convince me if the algorithm can be used to improve a current best inception score of 8.2 reached with SNGANs. Also with WGAN-GP, scores of ~7.8 are reported which are much higher than the 6.4 reported in the paper. But I understand that producing state-of-the-art inception scores is not the focus of the paper, therefore I would suggest that the authors release an implementation of the proposed new optimizers (ExtraAdam) for a popular DL framework (e.g. pytorch) such that practitioners working with GANs can quickly try them out in a "plug-and-play" fashion.

- Proposition 2 is a bit misleading. While for \eta \in (0, 1) implicit and extrapolation are similar, adding the remark that implicit method is stable for any \eta &gt; 0 (and therefore can lead to an arbitrary fast convergence) would give a more balanced view. Right now, only the advantages of extrapolation method and disadvantages of implicit method are mentioned which I find unfair for the implicit method.

- The theory is presented for variational inequalities with monotone operators. For clarity it should be mentioned that GANs parametrized with neural nets lead to non-monotone VIs. A provably convergent algorithm for that setting is still an open problem, no?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryT6uWAa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Many thanks for the constructive comments. **New experimental results**</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1laEnA5Ym&amp;noteId=ryT6uWAa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1491 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1491 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The authors first would like to thank AnonReviewer2 for his thorough evaluation and interesting remarks. In the following, we try to address as clearly as possible the points risen by AnonReviewer2.

“I'm a bit skeptical about the experiments on GANs.:”
As suggested, we tried to train a WGAN-GP with a ResNet architecture. The new results have been included in the updated version of the paper (see the experimental section 7.2, table 1 and figure 4 for the new results). After few days of experiments, we were able to obtain state-of-the art results on this architecture using ExtraAdam with averaging. As developed in our new experimental section (see the revised paper), we are not claiming that our principled methods are the solution to the many challenges of practical GAN optimization ( it is possible that a very fine-grained hyperparameter tuning on a standard method may give similar results) but after spending a similar limited time budget on optimizing the hyperparameters of each algorithm it looks clear to us that for this task the ExtraAdam method is much more robust to hyperparameter tuning, i.e, it yields reasonably good results for a large range of step-sizes.
The code in pytorch containing all the algorithms presented in the paper as well as the exact experimental setup is ready and will be released after the anonymity period due to the reviewing process.

“Proposition 2 is a bit misleading.”
Our goal was not to weaken the value of implicit methods. When a closed form for the implicit updates is known, this method is very effective, but unfortunately for neural network optimization we are not aware of any practical way to implement the implicit steps. More precisely, an implicit step is equivalent to computing a minimization step of the original objective with a l2 regularization (see [1] for more details on implicit SGD and its applications), this subproblem is supposed to be simpler because of the strong convexity of the l2 regularization. Unfortunately, for neural networks, the optimization problem remains non-convex for any small step-size (which are the step-sizes of interest). Hence, we considered that implicit steps were prohibitively expensive for our applications of interest.

“The theory is presented for variational inequalities with monotone operators.”
As we mentioned it in the second paragraph of Sec. 2.2 “Standard GAN objectives are non-convex (i.e. each cost function is non-convex),”, meaning that they are non-monotone since as we explain it right after the definition of monotonicity, “If F can be written as (6), it implies that the cost functions are convex.“. We added a clarification in the paper right after the definition of monotonicity (page 7) stating that “GANs parametrized with neural networks lead to non-monotone VIPs” to clarify this. For further discussion about the extension of the VI to non-monotone operators we refer the reviewer to App. C.3.

“A provably convergent algorithm for that setting is still an open problem, no?*”
To our knowledge, general convergence results in the context of general non-monotone VIPs is still an open question. The only partial results we are aware of are mentioned in our related work section: 
-“for a new notion of regret minimization, by Hazan et al. (2017) and in the context of GANs by Grnarova et al. (2018)“
-“ Mertikopoulos et al. (2018) also independently explored extrapolation providing asymptotic convergence results (i.e. without any rate of convergence) in the context of coherent saddle point. The coherence assumption is slightly weaker than monotonicity”.



[1] TOULIS, Panagiotis, AIROLDI, Edoardo, et RENNIE, Jason. Statistical analysis of stochastic gradient methods for generalized linear models. In : International Conference on Machine Learning. 2014.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>