<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>MERCI: A NEW METRIC TO EVALUATE THE CORRELATION BETWEEN PREDICTIVE UNCERTAINTY AND TRUE ERROR | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="MERCI: A NEW METRIC TO EVALUATE THE CORRELATION BETWEEN PREDICTIVE UNCERTAINTY AND TRUE ERROR" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ryG8UsR5t7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="MERCI: A NEW METRIC TO EVALUATE THE CORRELATION BETWEEN PREDICTIVE..." />
      <meta name="og:description" content="As deep learning applications are becoming more and more pervasive, the question of evaluating the reliability of a prediction becomes a central question in the machine learning community. This..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ryG8UsR5t7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>MERCI: A NEW METRIC TO EVALUATE THE CORRELATION BETWEEN PREDICTIVE UNCERTAINTY AND TRUE ERROR</a> <a class="note_content_pdf" href="/pdf?id=ryG8UsR5t7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019merci:,    &#10;title={MERCI: A NEW METRIC TO EVALUATE THE CORRELATION BETWEEN PREDICTIVE UNCERTAINTY AND TRUE ERROR},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=ryG8UsR5t7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">As deep learning applications are becoming more and more pervasive, the question of evaluating the reliability of a prediction becomes a central question in the machine learning community. This domain, known as predictive uncertainty, has come under the scrutiny of research groups developing Bayesian approaches to deep learning such as Monte Carlo Dropout. Unfortunately, for the time being, the real goal of predictive uncertainty has been swept under the rug. Indeed, Bayesian approaches are solely evaluated in terms of raw performance of the prediction, while the quality of the estimated uncertainty is not assessed. One contribution of this article is to draw attention on existing metrics developed in the forecast community, designed to evaluate both the sharpness and the calibration of predictive uncertainty. Sharpness refers to the concentration of the predictive distributions and calibration to the consistency between the predicted uncertainty level and the actual errors. We further analyze the behavior of these metrics on regression problems when deep convolutional networks are involved and for several current predictive uncertainty approaches. A second contribution of this article is to propose an alternative metric that is more adapted to the evaluation of relative uncertainty assessment and directly applicable to regression with deep learning. This metric is evaluated and compared with existing ones on a toy dataset as well as on the problem of monocular depth estimation. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">evaluation metric, predictive uncertainty, deep learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We review existing metrics and propose a new one to evaluate predictive uncertainty in deep learning</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BJeLx9UeRX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Preface</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryG8UsR5t7&amp;noteId=BJeLx9UeRX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper178 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper178 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Foremost, we would like to thank the reviewers for their time and thorough reviews. Globally, all reviewers unveiled similar issues to the submitted work. We will address each specific point in detailed responses below. Just as a recall and to introduce our discussion, we would like to insist on the properties we wanted our metric to have:

       •	Insensitivity to scale, so that we can compare predictive uncertainty techniques with various ranges that are often not in relation with the absolute error;
       •	Robustness to outliers, so that we can fairly evaluate tasks for which data are known to be noisy;
       •	Easy applicability to deep learning, where we usually get two scalars to define the couple prediction/uncertainty instead of a whole distribution;
       •	Representativity of the true error, in the sense that we want it to evaluate jointly calibration and sharpness, ensuring that the estimated uncertainty varies with the true error.

We will also notify each submission updates we propose below.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BkgP0S052m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Ok, but not good enough</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryG8UsR5t7&amp;noteId=BkgP0S052m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper178 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper178 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose mean rescaled confidence interval (MERCI) as a way to measure the quality of predictive uncertainty for regression problems. The main idea is to rescale confidence intervals, and use average width of the confidence intervals as a measure for calibration. Due to the rescaling, the MERCI score is insensitive to the absolute scale; while this could be a feature in some cases, it can also be problematic in applications where the absolute scale of uncertainty matters. 

Overall, the current draft feels a bit preliminary. The current draft misses discussion of other relevant papers, makes some incorrect claims, and the experiments are a bit limited. I encourage the authors to revise and submit to a different venue. 
	
There’s a very relevant ICML 2018 paper on calibrating regression using similar idea: 
Accurate Uncertainties for Deep Learning Using Calibrated Regression
<a href="https://arxiv.org/pdf/1807.00263.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1807.00263.pdf</a>
Can you clarify if/how the proposed work differs from this? I’d also like to see a discussion of calibration post-processing methods such as Platt scaling and isotonic regression.

The paper unfairly dismisses prior work by making factually incorrect claims, e.g. Section 2 claims
“Indeed, papers like (Hernandez-Lobato &amp; Adams, 2015; Gal &amp; Ghahramani, 2016; Lakshminarayanan et al., 2017; Kendall &amp; Gal, 2017) propose quantitative evaluations on several datasets, those classically used for evaluating the task, but only compare their average test performances in terms of RMSE. It is the quality of the prediction which is measured, and not the quality of the estimated uncertainty. They also show some qualitative results, where maps of the estimated uncertainty are displayed as images and visually evaluated. Yet, to the best of our knowledge, the literature on deep neural networks does not propose any method for the quantitative evaluation of the uncertainty estimates.”
This is incorrect.  To just name a few examples of prior work quantitatively evaluating the quality of uncertainty: (Hernandez-Lobato &amp; Adams, 2015) and (Gal &amp; Ghahramani, 2016) report log-likelihoods on regression tasks, (Lakshminarayanan et al. 2017) report log-likelihoods and Brier score on classification and regression tasks. There are many more examples. 

The experiments are a bit limited. Figure 1 is a toy dataset and Table 2 / Figure 4 focus on a single test case which does not seem like a fair comparison of the different methods. The authors should at least compare their method to other work on the UCI regression benchmarks used by (Hernandez-Lobato &amp; Adams, 2015).</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyxT09LeAX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>More references and experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryG8UsR5t7&amp;noteId=SyxT09LeAX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper178 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper178 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your helpful review and constructive comments.
Because we proposed a new evaluation metric, we were concerned by the question of how to compare very different predictive uncertainty. For example, the use of Bayesian deep learning techniques leads to compute variations that don’t have meaning in terms of absolute error. That is the reason why we took sides for the insensitivity to scale.

Thank you also for pointing out this very relevant ICML paper that we were not aware of. We still need to study it further but it seems that they take into account calibration and sharpness separately, while we were driven by the will of assessing both these properties jointly. For example, the sharpness score they propose in section 3.5 seem very close to MeRCI as it corresponds to the mean variance of the evaluated method. However this only assesses the sharpness of the predictive uncertainty and do not allow to compare techniques with different ranges of uncertainties.

We indeed made incorrect claims about some prior work and apologize for this mistake. We rather wanted to point out that systematic assessment of the uncertainty estimate itself was largely underestimated in the deep learning community. We will rephrase this properly in the next update.

Finally we plan to use several other datasets from the UCI regression benchmark, as you advised. We believe that proving the generalization of the metrics behavior will support our claims more strongly. We will also add several other qualitative analysis in the appendix for a fairer comparison. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SygUVxWc3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting paper for the deep learning community, but the experimental section is not convincing enough</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryG8UsR5t7&amp;noteId=SygUVxWc3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper178 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper178 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This works presents an overview of different techniques to obtain uncertainty estimates for regression algorithms, as well as metrics to assess the quality of these uncertainty estimates.
It then introduces MeRCI, a novel metric that is more suitable for deep learning applications.

Being able to build algorithms that are good not only at making predictions, but also at reliably assessing the confidence of these predictions is fundamental in any application. While this is often a focus in many communities, in the deep learning community however this is not the case, so I really like that the authors of this paper want to raise awareness on these techniques. The paper is well written and I enjoyed reading it. 
I feel that to be more readable for a broader audience it would be relevant to introduce more in depth key concepts such as sharpness and calibration, an not just in a few lines as done in the end of page 2. 

While I found the theoretical explanation interesting, I feel that the experimental part does not support strongly enough the claims made in the paper. First of all, for this type of paper I would have expected more real-life experiments, and not just the monocular depth estimation one. This is in fact the only way to assess if the findings of the paper generalize.
Then, I am not convinced that keeping the networks predictions fixed in all experiments is correct. The different predictive uncertainty methods return both a mean and a variance of the prediction, but it seems that you disregard the information on the mean in you tests. If I understood correctly, I would expect the absolute errors to change for each of the methods, so the comparisons in Figure 4 can be very misleading. 
With which method did you obtain the predictions in Figure 4.c? 

Typos:
- "implies" -&gt; "imply" in first line of page 3
- "0. 2" -&gt; "0.2" in pag 6, also you should clarify if 0.2 refers to the fraction of units that are dropped or that are kept

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkgNvoLlCm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>More experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryG8UsR5t7&amp;noteId=SkgNvoLlCm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper178 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper178 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your considerate review and insightful comments. 
About the concepts of calibration and sharpness, we will follow your advice and introduce them earlier in the paper to be more easily understandable for a broader audience.

We also understand your concerns that the experiments do not support strongly enough our theoretical claims. As proposed by the reviewer 1, we plan to conduct similar experiments on several other datasets from the UCI benchmark. In this way, we should be able to demonstrate that our findings generalize well to other real-life setups.

Finally, your last point is also very relevant. We indeed disregarded the means of the predictive uncertainty methods in our tests for comparison purposes. Instead, we use a unique prediction for all methods, represented in Figure 4. and obtained with the standard network described in section 7.1. However, we also computed all the results using both the mean and variance for each method to unbias the comparisons. Because the results were not mixed up, we originally decided to omit this part for simplicity and clarity. We realize that we should have left it and we will reintroduce the corresponding section in our next update.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ryeyZSlc37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Lacking novelty and rigor.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryG8UsR5t7&amp;noteId=ryeyZSlc37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper178 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper178 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The main contribution of this paper is a new proposed score to evaluate models that yield uncertainty values for regression.

As constituted, the paper can not be published into one of the better ML conferences. The novelty here is very limited. Furthermore there are other weaknesses to the study.

First, the stated goal of the "metric" is that "reflects the correlation between the true error and the estimated uncertainty ... (and is) scale independent and robust to outliers." Given this goal (and the name of the paper) it is perplexing why the correlation (<a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)" target="_blank" rel="nofollow">https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)</a> of true error and predicted error (\sigma_i) was not tried as baseline score. The correlation would have some "scale independence" and I'm sure that there are robust estimates (a simple thing would be to consider the median instead of mean, but there are probably more sophisticated approaches). This just feels like an obvious omission. If one wants to mix both predictive quality and correctness of uncertainty assessments then one could just scale the mean absolute error by the correlation: MAE/Corr, which would lead to a direct comparison to the  proposed MeRCI.

Second, the paper does a poor job of justifying MeRCI. On toy data MeRCI is justified by a confluence with existing scores. Then on the depth prediction task, where there are discrepancies among the scores, MeRCI is largely justified qualitatively  on a single image (Figure 4). A qualitative argument on a single instance in a single task can not cut it. The paper must put forth some systematic and more comprehensive comparison of scores.

Even with the above issues resolved, the paper would have to do more for publication. I would want to see either some proof of a property of the proposed score(s), or to use the proposed score to inform training, etc.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJenao8lC7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>More analysis and experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryG8UsR5t7&amp;noteId=rJenao8lC7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper178 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper178 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your time and critical feedback.
The score you propose is indeed interesting and we would need to look further into it. However, it seems that its main disadvantage compared to our proposed metric is the lack of robustness to outliers. Indeed, there is no direct way to handle a known amount of outliers and the correlation itself is error-prone and will be sensitive to erroneous data. 

We believe that robustness to outliers is of major importance, especially for problems where the learning is based on potentially erroneous ground-truth. Depth estimation is one such typical case since it involves sensor measurement uncertainty. To highlight this issue and also to address your concern about our qualitative evaluation made on a single image, we plan to add several images in the appendix with and without ground truth problems.

We finally would like to respectfully disagree concerning the novelty of the work since there is no reliable metric used in the deep learning community to assess the quality of a predictive uncertainty with respect to the true error, while the need for it is growing.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>