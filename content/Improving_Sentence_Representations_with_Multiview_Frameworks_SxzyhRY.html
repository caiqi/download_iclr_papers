<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Improving Sentence Representations with Multi-view Frameworks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Improving Sentence Representations with Multi-view Frameworks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=S1xzyhR9Y7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Improving Sentence Representations with Multi-view Frameworks" />
      <meta name="og:description" content="Multi-view learning can provide self-supervision when different views are available of the same data. Distributional hypothesis provides another form of useful self-supervision from adjacent..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_S1xzyhR9Y7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Improving Sentence Representations with Multi-view Frameworks</a> <a class="note_content_pdf" href="/pdf?id=S1xzyhR9Y7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019improving,    &#10;title={Improving Sentence Representations with Multi-view Frameworks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=S1xzyhR9Y7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=S1xzyhR9Y7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Multi-view learning can provide self-supervision when different views are available of the same data. Distributional hypothesis provides another form of useful self-supervision from adjacent sentences which are plentiful in large unlabelled corpora. Motivated by the asymmetry in the two hemispheres of the human brain as well as the observation that different learning architectures tend to emphasise different aspects of sentence meaning, we present two multi-view frameworks for learning sentence representations in an unsupervised fashion. One framework uses a generative objective and the other a discriminative one. In both frameworks, the final representation is an ensemble of two views, in which, one view encodes the input sentence with a Recurrent Neural Network (RNN), and the other view encodes it with a simple linear model. We show that, after learning, the vectors produced by our multi-view frameworks provide improved representations over their single-view learned counterparts, and the combination of different views gives representational improvement over each view and demonstrates solid transferability on standard downstream tasks.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">multi-view, learning, sentence, representation</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Multi-view learning improves unsupervised sentence representation learning</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Skex_lDyCm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Revision 1 to include experiments and ablation study suggested by Reviewer 1 and Reviewer 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xzyhR9Y7&amp;noteId=Skex_lDyCm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper967 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper967 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We first include an ablation study suggested by Reviewer 1, and an additional experiment suggested by Reviewer 2, and they are in the appendix now. 



1/ The effect of the post-processing step. (suggested by Reviewer 1)

Six models trained on BookCorpus, UMBC news and Amazon Book Review with generative or discriminative objective are evaluated without the post-processing step that removes the first principal component. The results are presented in Table 2 in the appendix.

Overall, the postprocessing step overall improves the performance of our models on unsupervised evaluation tasks, and also improves the models with generative objective on supervised sentence similarity tasks.  However, it doesn’t have a significant impact on single sentence classification tasks, including sentiment analysis task and question-type classification.



2/ On combining both generative and discriminative objectives into a single multi-view framework. (suggested by Reviewer 2)

Models with both generative and discriminative objectives are trained to see if further improvement can be provided by combining an RNN encoder, an inverse of a linear decoder in the generative objective and a linear encoder in the discriminative objective.  The results of models trained on BookCorpus and UMBC News are presented in Table 3 in the appendix. 

As presented in the table, no further improvement against models with only one objective is shown. In our understanding, the inverse of the linear decoder in generative objective behaves similarly to the linear encoder in the discriminative objective, which is presented in Table 6 in the main paper. Therefore, combining two objectives doesn’t perform better than only one of them.



3/ We will update our paper later to address more detailed issues mentioned by all three reviewers.



Thanks,

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SygDgAxi3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Impressive results and interesting model</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xzyhR9Y7&amp;noteId=SygDgAxi3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper967 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper967 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper is about a multi-view framework for learning sentence representations. Two objective functions (a generative one and a discriminative one) are proposed that make use of two encoders, one of them is based on an RNN and the other on a linear projection of averaged word embeddings. Each of these objective functions has a multi-view framework where their respective objective functions are in part based on making sure their is some relationship between the two different encoders. This multi-view framework is shown to be helpful over having independent encoders in their ablation study.

The authors evaluate on the SentEval benchmark (a collection of tasks where a shallow neural network is learned and the sentence embeddings are kept fixed) and a collection of STS tasks (where the cosine between two vectors is used to estimate their semantic similarity).

The results are impressive. A closer examination of them though leaves me with some questions and thoughts.

Regarding the SentEval numbers, I would like to know the dimensionality of your models in Table 5. I am somewhat unclear of how the final embeddings were produced, it seems that you concatenate mean, max, min, and last_h from the RNN encoder and then mean, max, min from the projection encoder. Is that correct? That would make your feature vector 7*1024 dimensions, which is a bit bigger than most of what you compare to (some of these methods are 4096 dims). With this type of evaluations, larger feature vectors do help performance, thought I am certain that you would have nice performance even if your dimension was reduced (this is from looking at the ablation). I think making the dimensions more explicit and clarifying in the text how the final feature vector was created would be helpful for readers.

Another thing to consider in the evaluation, is that a paper recently pointed out that max pooling in a certain way in SentEval can artificially inflate results for some of these datasets. I noticed that max-pooling is used in your experiments. This paper also shows how big of an effect larger feature vectors have on performance: <a href="https://openreview.net/forum?id=BkgPajAcY7." target="_blank" rel="nofollow">https://openreview.net/forum?id=BkgPajAcY7.</a> I'd like to know if your results are affected by this max-pooling operation as is the case for several well-known papers in this area.

I also noticed that a lot of the best SentEval numbers came from using the book review dataset. This makes a lot of sense in that a lot of these are based on sentiment and is something that was in part used by (Radford et al. 2017) to obtain strong performance on these tasks. A similar thing happens with the STS data and the news domain as noted in this paper.

I noticed you did the principal component removal trick for InferSent, but it did not have a large effect on performance. How big of an effect did if have with your methods? I'm glad you included it in InferSent, but I'd like to see this as well in the ablation.

Overall I do think this paper has value for the community. It shows how strong results can be obtained using just raw text and using less parameters and training faster than other recent approaches. I do think a lot of the gains here are due to clever design choices in their experiment (for instance using different types of raw data which help more on certain tasks, removing the first principal component, etc.) but putting everything together to get very competitive results with across all these tasks with an interesting approach and an accompanying analysis is a nice contribution.

Minor comment: The paper was tough to understand in parts due to symbols/abbreviations not being defined or motivated clearly. It'd be nice if the authors could define the symbols/abbreviations that are in the tables in the captions. An example of this would be WR in Table 4. The left-most column in Table 6 could also be clearer (I know it is in the text, but I was confused about what f1 and f2 represent etc. in my first pass). This also occurs in the text as well like when g is introduced in Section 2.2.

PROS:
- Interesting and novel model combining RNNs and word-averaging
- I find the multiview framework to be a nice contribution, having the models tied in this way also improves performance.
- Model is fast to train and requires only raw text
- Competitive results with SOA on many datasets - both those requiring a trained classifier using the fixed embedding and STS tasks.

CONS
- Some of their gains are due to choice of dataset for training or removing the first principal component - advantages that other comparable models may or may not have. Not really a con though, more of an observation. I would like to see an ablation to see the effect of removing the first principal component.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hye46WcE6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for your detailed review; we really appreciate all the time you put into it.  We will definitely also explain the symbols in the captions of tables.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xzyhR9Y7&amp;noteId=Hye46WcE6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper967 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper967 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1/ Our paper makes separate contributions.   A significant contribution is that training the views to agree with each other results in better representations for each of the views after training.   This finding is not dependent on the benefits of max pooling or large dimensionality as our comparisons in the ablation tests use the same representation pooling methods and dimensionality.

In Table 6, we compared our proposed method with other variants that don’t maximise the agreement between two views, and the results show that 1) the individual encoders in our method perform better and 2) the ensemble of two views also overall works better than other ensembles. The hyperparameters, including representation pooling and dimensionality of the models, are the same, which indicate that our multi-view learning frameworks help and provide better performance. 


2/ Another contribution is that the combination after multi-view training is uniformly better on the unsupervised tasks and on average better on the supervised tasks.   We believe that a good representation should perform well in both types of tasks and our goal was to create the best system for finding a great representation for both types of tasks.
 

3/ The ICLR submission [3] you mentioned is very interesting, and we think it is definitely important to understand how much gain we can get by learning against an ensemble of randomly initialised models. However, their paper only demonstrates the performance gain from increased model dimensionality on supervised evaluation tasks; no results on unsupervised evaluation tasks are provided.   We have found that larger dimensionality often harms performance when evaluated on unsupervised tasks.  Following the findings of [4], we have also noticed that good performance on supervised tasks is not consistent with good performance on unsupervised tasks.  This observation was a motivating point for our paper.

We are interested in finding a representational system that works for both types of tasks and believe that our simple method of freely (without training a different model) creating a larger dimensionality representation (that works well for supervised tasks) from a relatively lower dimensional model (that works well for unsupervised tasks)  is another generally useful contribution.

Also, all of our studies in the ablation table (Table 6) use similar representation pooling methods and still demonstrate a benefit to multi-view learning.


4/ In our proposed multi-view frameworks, removing the first principal component leads to better performance on all unsupervised evaluation tasks, but not on the supervised evaluation tasks. It has been shown by at least two papers [1][2] that removing first principal component provides stronger performance on unsupervised tasks, and the results show that the assumption of representations being isotropic is helpful.

While on supervised evaluation tasks, additional learning of a linear classifier is required, it is unclear if removing the first principal component would definitely help or not. We will report the results in our next version.



[1] Arora et al. A simple but tough-to-beat baseline for sentence embeddings. In ICLR2017.
[2] Mu et al. “All-but-the-Top: Simple and Effective Postprocessing for Word Representations.” In ICLR2018.
[3] Anonymous, “No Training Required: Exploring Random Encoders for Sentence Classification.” In submission to ICLR2019.
[4] Hill et al. “Learning Distributed Representations of Sentences from Unlabelled Data.” HLT-NAACL (2016).
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1ggt-L93m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>contributions for ICLR community are unclear</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xzyhR9Y7&amp;noteId=B1ggt-L93m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper967 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper967 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper describes experiments in learning sentence embeddings from unlabeled text. The paper compares a few different compositional architectures and training objectives. The story of the paper focuses on the training of multiple architectures jointly for a single sentence, then ensembling those architectures at test time to represent sentences. One architecture is an RNN and the other is a word averaging model, and the idea is that these two architectures capture different "views" of the sentence. 

Pros:

As a general-purpose method to get sentence embeddings without using any resources other than unannotated text documents, this approach has strong results, including solid performance on the SentEval tasks and relatively-low training times. 

It was also nice to see how the results depend on the domain of the training data. Review data definitely helps on the several sentiment-related tasks, which provides further evidence of a worrisome aspect of SentEval. 

Cons:

Overall, the paper feels incremental and is likely a better fit for an NLP conference. What are the generalizable contributions to the ICLR community? Given the known differences between RNNs and word averaging models for sentences (especially on the SentEval tasks, which, as the authors note, was discussed by Hill et al.), it's entirely unsurprising that combining the two would be a good idea. But even if this were not the case, the ubiquity of ensembles outperforming single models in deep learning also makes it unsurprising that combining these two kinds of model architectures would be beneficial. So I'm just not sure if there is a significant contribution beyond the NLP results. These results seem solid (though a bit incremental), but if the primary contribution is empirical, then the paper would be a better fit for an NLP venue. 

In addition, I'm not sure if "multi-view" is an appropriate description of the approach. In Sec. 1, we find the sentence "Compared to earlier work on multi-view learning (de Sa, 1993; Dhillon et al., 2011) that takes data from various sources or splits data into disjoint populations, our framework processes the exact same input data in two distinctive ways."  Therefore, maybe it's not quite accurate to describe this approach using the term "multi-view learning"? I think it would make more sense to use a different term rather than stretch the definition for a well-known one. 

I kept expecting the paper to present results when combining the generative and discriminative objectives, but as far as I can tell, this was never done. What would happen if one were to use multitask learning and just optimize the sum of the two losses? 

I'd suggest citing and comparing to the results from "Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning" by Subramanian et al. (ICLR 2018) and the Byte mLSTM from "Learning to generate reviews and discovering sentiment" by Radford et al. 

I'm not sure how excited we should get about not using any annotations or structured resources for learning sentence embeddings. The authors do not motivate this goal. 


Below are more specific comments/questions:

Sec. 2.2 contains the sentence "Ideally, the inverse of h should be easy to compute so that during testing we can set g = h^-1." At this point in the paper, it is not clear what g is going to be applied to at test time, since presumably the following sentence is not going to be available at test time, right? I think it would be good to discuss how the model is going to be used at test time before discussing the inverse of h. 

Sec. 2.3: 
In Eq. (3), why does the denominator sum always start at 1 no matter what i and j are? That is, why would the denominator always sum over the first N sentences in the dataset?

I think the pooling methods in Table 2 should be described in Section 2. 

In Table 2, it is not clear what h_i^{M_i} is. If M_i is the number of words in sentence i, that should be mentioned somewhere. 

Sec. 3.1:
"For a given sentence input s with M words, suggested by (Pennington et al., 2014; Levy et al., 2015), the representation is calculated as z = (\hat{z}_f + \hat{z}_g) / 2, where \hat{z} refers to the post-processed and normalised vector, and is mentioned in Table 2."  I don't understand. Where is this mentioned in Table 2?


Minor issues follow:

Sec. 1:
"Distributional hypothesis" --&gt; "the distributional hypothesis"
"in machine learning community" --&gt; "in the machine learning community"
"and distributional hypothesis" --&gt; "and the distributional hypothesis"
"the linear/log-linear models" --&gt; "linear/log-linear models"
"based on distributional hypothesis" --&gt; "based on the distributional hypothesis"
"contraint" --&gt; "constraint"

Sec. 2:
"marry RNN-based sentence encoder" --&gt; "marry RNN-based sentence encoders" or "marry the RNN-based sentence encoder"

Sec. 2.1:
"only hidden state" --&gt; "only the hidden state"

Sec. 2.2:
"prior work with generative objective" --&gt; "prior work with generative objectives"

Sec. 2.3:
"with discriminative objective learns" --&gt; "with the discriminative objective learns"

Sec. 3.1:
In Table 3, I don't see where superscript 5 is shown in the table.

Sec. 3.2:
In Table 5, the numbered superscripts at the top of the table do not show up next to the methods in the actual table rows. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rke0Pfc4aX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for your review and your suggestions. However, we believe our contributions to ICLR community are clear and valuable.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xzyhR9Y7&amp;noteId=rke0Pfc4aX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper967 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper967 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We believe our contributions to ICLR community are clear and valuable.


1/ Multi-view learning is the appropriate term for our paper.

From the introduction to the Proceedings of the ICML 2005 Workshop on Learning with Multiple Views [1], the 3rd paragraph goes 

“Contributions to this workshop from fields of machine learning, such diverse as clustering, semi-supervised learning, named entity recognition or ensemble learning show that there is a strong interest in learning problems with multiply represented instances and consensus maximizing learning methods in a variety of communities”.     

Our method is using doubly represented instances and consensus maximization and is a novel type of unsupervised multi-view learning.  Recognizing that different types of networks create different features that can be effectively used for multi-view learning is part of our contribution.



2/ We agree with you that combining different representations is a good idea.  However, our contribution goes well beyond that (in fact that is the baseline that we compare to in the ablation studies).  The point of our paper is that training to maximise the agreement between the views
improves their individual results (with no ensembling used)  and that combining them (after multi-view training) does better than an ensemble from independently trained views (f: an RNN and g: linear).



3/ It is not guaranteed that the ensemble of single models outperforms each single one of them; it also depends on the tasks.

The essence of most ensemble learning methods, including boosting, bag-of-trees, random forest is the majority voting mechanism when making predictions, and it has been proved that the performance improvement is guaranteed.

However, in the unsupervised evaluation tasks presented in our paper, the similarity of two sentences are determined by directly measuring the cosine similarity of two sentence representations, and a direct ensemble of different encoding functions won’t necessarily give better performance than each one of them.

In fact, as presented in Table 6, in a model with the generative objective but without invertible constraint (not our multi-view frameworks), the ensemble of two encoding functions performs poorly, and even worse than the linear encoding function solely. Meanwhile, our model with the invertible constraint (where the inverted decoder must be in agreement with the forward encoder)  provides better performance when taking an ensemble of two encoding functions. It is also the case in the models with the discriminative objective. Furthermore, the ensemble representations in our multi-view frameworks (where the two representations are trained to agree) perform better than other variants in general. 

Given the results we presented, we think that it is important to encourage consensus from different views. Multi-view frameworks we proposed in our paper could also be applied in other areas, as it is an idea of marrying multiple information processing methods in a single learning algorithm.



4/ We will motivate learning sentence representations from unlabelled corpora better in our next version.   We would like to note that we are not using completely unstructured data as the corpora contain temporally meaningful data and our methods critically use this consistency in meaning between neighbouring sentences.

In our paper, we referred to several interesting accepted-to-ICLR papers working on unsupervised sentence representation learning [2][3], and also papers that were published on other venues on the same topic.  As the multi-view learning aspect provides the core novelty of our paper, we focused more on motivating that contribution and the importance of multi-view learning.  We will update our paper to include detailed motivation for the importance of unsupervised sentence representation learning.


5/ It is interesting to try the multi-task learning that combines generative and discriminative objectives, but we think the final performance will be similar to learning with only one of them, given the results of generative and discriminative objective are comparable to each other. We will try it and report results in our later version.


Replies to specific questions:
Sec. 2.2: How to use the decoder at test time is explained later in the same section, and we directly take the transpose of the linear decoder as another encoder at test time.
Sec. 2.3: Agreed. The summation in the denominator should range from j-N/2 to j+N/2-1


[1] Rüping and Scheffer. "Learning with multiple views." Proc. ICML Workshop on Learning with Multiple Views. 2005.
[2] Arora et al. “A simple but tough-to-beat baseline for sentence embeddings.” In ICLR2017.
[3] Logeswaran and Lee. “An efficient framework for learning sentence representations.” In ICLR2018.

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HyljWGq4aX" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xzyhR9Y7&amp;noteId=HyljWGq4aX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper967 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1lcRhycnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper presents a multiview framework for sentence representation in NLP tasks. But novelty appears too limited here.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xzyhR9Y7&amp;noteId=S1lcRhycnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper967 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper967 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=S1lcRhycnX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a multiview framework for sentence representation in NLP tasks. Authors propose two architectures, one using a generative objective, while the other uses a discriminative objective. Both combine a recurrent based encoding function and a linear model. Large experiments have been conducted on several NLP tasks and datasets, showing improvement of the introduced frameworks compared to baselines.

The paper is globally well written and has a clear presentation. But I'm not sure to understand why authors motivate their work on the asymmetric information processing in the two hemispheres of the human brain. It sounds like a nice motivation, but the work presented here does not show any clear answer for this, except the idea of combining two different encoders for sentence representation..

My main concern is about the term multiview since the merging step is somewhat trivial (min/max/averaging vectors or concatenation). This is far from significant works on multiview learning, see: "Multi-view learning overview: Recent progress and new challenges".

Table 3, where G and D refer respectively to Generative and Discriminative models. But what differences between G1, G2, G3 ; D1, D2, D3 ?

Invertible constraint is a nice idea for using inverse of the decoder as the encoder. Is it really to take advantages of decoder information on the encoder/representation part? Or also to reduce the amount of parameters learnt in the model? Moreover, it is unclear on the ablation study: did you consider the original encoder ; or still the inverse of decoder but without the constraint? Unfortunately, it seems to not give significant gain, according to ablation study in table 6.

In this current form, I feel this paper does not give sufficient novelty to be accepted at ICLR.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkeakQq46Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for your review, but we vehemently disagree that our multi-view method is only merging (min/max/averaging or concatenation).   </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xzyhR9Y7&amp;noteId=rkeakQq46Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper967 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper967 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Our comparison methods in the ablation study do just merging and don’t perform well. (for instance merging an independently trained two encoding functions: f and g)  We show however that when we force the views to agree (consensus maximisation) (and thus change the learnt representations of each view) we get improved performance of the individual views (and also of their subsequently merged representation).   The main point is by requiring consensus maximisation, we achieve better representations for each view.   The merging after that gives an additional improved performance.

We agree that we have not proved anything related to brain hemispheric processing but are only giving it for motivation.  We will make it clear that we have not proved anything in that regard.

We are using the understanding of the term multi-view as originally envisioned in the introduction to the Proceedings of the ICML 2005 Workshop on Learning with Multiple Views, which states “Contributions to this workshop from fields of machine learning, such diverse as clustering, semi-supervised learning, named entity recognition or ensemble learning show that there is a strong interest in learning problems with multiple represented instances and consensus maximizing learning methods in a variety of communities”. Our method is using doubly represented instances and consensus maximisation and is a novel type of unsupervised multi-view learning.


Clarifications:


1/ “G” and “D” refer to generative objective and discriminative objective respectively. “1”, “2” and “3” refer to three corpora used in our experiments, which are Toronto BookCorpus, UMBC News and Amazon Book Review. “G1” refers to the model trained with generative objective on Toronto BookCorpus. (see Table 1, we also mentioned it in Table 2, but we didn’t mention it again in other tables.)


2/ In both generative and discriminative objective, an RNN encoder (f) and a linear encoder (g) are trained under our multi-view frameworks, and also both applied during testing. Table 6 states comparisons between our multi-view frameworks and other variants.

Specifically, in models with generative objective, the RNN encoder (f) is still used after learning, and its performance is in rows indicated by z^f.


3/ Interestingly, the z^g’s are worse in the generative without invertible constraint but the ensembled (from invertible constraint) with z^f(which is usually worse than z^g) is better than the ensembled without invertible constraint.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>