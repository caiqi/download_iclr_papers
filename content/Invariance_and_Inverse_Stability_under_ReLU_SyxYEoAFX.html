<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Invariance and Inverse Stability under ReLU | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Invariance and Inverse Stability under ReLU" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SyxYEoA5FX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Invariance and Inverse Stability under ReLU" />
      <meta name="og:description" content="We flip the usual approach to study invariance and robustness of neural networks by considering the non-uniqueness and instability of the inverse mapping. We provide theoretical and numerical..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SyxYEoA5FX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Invariance and Inverse Stability under ReLU</a> <a class="note_content_pdf" href="/pdf?id=SyxYEoA5FX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 13 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019invariance,    &#10;title={Invariance and Inverse Stability under ReLU},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SyxYEoA5FX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=SyxYEoA5FX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We flip the usual approach to study invariance and robustness of neural networks by considering the non-uniqueness and instability of the inverse mapping. We provide theoretical and numerical results on the inverse of ReLU-layers. First, we derive a necessary and sufficient condition on the existence of invariance that provides a geometric interpretation. Next, we move to robustness via analyzing local effects on the inverse. To conclude, we show how this reverse point of view not only provides insights into key effects, but also enables to view adversarial examples from different perspectives.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">deep neural networks, invertibility, invariance, robustness, ReLU networks</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We analyze the invertibility of deep neural networks by studying preimages of ReLU-layers and the stability of the inverse.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">5 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Hkl2yixyCX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting investigation but needs work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxYEoA5FX&amp;noteId=Hkl2yixyCX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper17 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper17 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">



Review

This paper discusses invariances in ReLU networks. The discussion is anchored around the observation that while the spectral norm of neural networks layers (their product bounds the Lipschitz constant) has been investigated as a measure of robustness of nets, the smallest singular values are also of interest as these indicate directions of invariance. 

The paper consists mostly of a theoretical analysis with little empirical support, focusing on a property of matrices called omnidirectIonality. The definition given seems weird — an A \in R^{m \times n} is omnidirectional if there exists a unique x \in R^n such that Ax \leq 0. 

If there is a *unique* x then that x must be 0. Else if there were a nonzero x for which Ax \leq 0, then A(cx) also \leq 0 for any positive scalar 0 and thus x is not unique. Moreover if x must be equal to 0 Ax \leq 0 and at that point Ax = 0, then that means there exists no x for which Ax &lt; 0, so why not just say this outright? Perhaps a cleaner definition would just be “A is full rank and there does not exist any X such that Ax &lt; 0? Also perhaps better to use the curly sign for vector inequality. 

Overall the paper, while interesting is unacceptably messy. 
The first two pages have no paragraph breaks!!! This means either that the author are separating paragraphs with \\ \noindent or that they have modified the style file to remove paragraph breaks to save space. Either choice is unreadable and unacceptable. The paper is also littered with typos and vague statements (many enumerated below under *small issues*). In this case, they add up to make a big issue. 


The notation at the top of page 4 — see (1) and (2) — comes out of nowhere and requires explanation. |_{y&gt;0} x + b |_{y&gt;0}  &lt;— what is the purpose of the subscripts here? Why is this notation never introduced?

Ultimately this paper focuses on the question on whether the pre-image of a ReLU layer can be concluded (based on the post-image) to be a singleton,  a compact polytope, or if it has infinite volume. The paper offers some analysis, suggesting when each of the conditions occurs, upper bounds the smallest singular value of D A (where the example dependent diagonal matrix D incorporates the ReLU activation (shouldn’t this be more clearly introduced and notated?). 

Ultimately this paper is interesting but falls well below the standards of exposition that I expect from a theory paper and doesn’t go very far at connecting the analysis back to the claimed motivation of investigating practical invariances. If the authors significantly improve the quality of the draft, I’ll be happy to revisit it and re-evaluate my score. 


Small issues

The following is a *very* incomplete list of small bugs found in the paper:

“From a high-level perspective both of these approaches” --&gt; missing comma after “perspective”

"as well as the gradient correspond to the highest
possible responds for a given perturbation" --&gt; incomprehensible "corresponding?" "possible responds?" do you mean "response", and if so what is the precise technical meaning here?

"analyzing the lowest possible response" what does "response' mean here?

"We provide upper bounds on the smallest singular value" -- the singular value of what? This hasn't been stated yet.

"reverse view on adversarial examples" --- what this means isn't clear from the preceding text.

"we aim to theoretically derive means to uncover mechanisms of rectifier networks without assumptions on the weights" -- what does "mechanisms" mean here?

Notation section -- need a sentence here at the beginning, can't just have a section heading followed by bullets. 

"realated"
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkg7S_D93X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>not entirely novel with few concerns but includes results leading to interesting insights</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxYEoA5FX&amp;noteId=rkg7S_D93X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper17 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper17 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper has two distinct parts. In the first part (section 2) it studies the volume of preimage of a ReLU network’s activation at a certain layer as being singular, finite, or infinite. This part is an extension of the work in the study of (Carlsson et al. 2017). The second part (section 3) builds on the piecewise linearity of a ReLU network’s forward function. As a result, each point in the input space is in a polytope where the model acts linearly. In that respect, it studies the stability of the linearized model at a point in the input space. The study involves looking at the singular values of the linear mapping. 

The findings of the paper are non-trivial and the implications potentially interesting. However, I have some concerns about the study.

There is a key concern about the feasibility of the numerical analysis for the first part. That is, a layer-by-layer study can have a computational problem where the preimage is finite at each layer but can become infinite by the mapping of the preceding layers. In that regard, I would like the authors to comment on the worst-case computational complexity of the numerical analysis for determining the volume of a preimage through multiple layers.

As for the second part, the authors mention the increase in the dimensionality of the latent space in the current deep networks. However, this observation views convolutional networks as MLPs. However, there is more structure in a convolutional layer’s mapping function. The structure is obtained by the shared and sparse rows of matrix A. I would like the authors to comment on how the studies will be affected by this property of the common networks.

All in all, while there are some concerns and the contributions are not entirely novel, the reviewer believes the findings of the paper is generally non-trivial and shed more light on the inner workings of the ReLU networks and is thus a valuable contribution to the field.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1xwrnD_pQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Added discussion on raised points in revision (Scope section)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxYEoA5FX&amp;noteId=H1xwrnD_pQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper17 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper17 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank you for acknowledging our findings to be useful to shed more light on the inner workings of ReLU-networks. 
We respond to your raised points below:

---------
- Q: Algorithm applied layer-by-layer:
As correctly observed, the application of our algorithm to classify the preimage of one data point of one ReLU layer does not easily translate to more than one layer. On the one hand, as pointed out, as soon as the preimage is no longer only a point itself it is no longer applicable. On the other hand it is a first step towards a multilayer analysis and allows a localized layer-by-layer analysis for the first time.
-&gt; For more on this we refer to the newly added Section “Scope” in the revision.

--------
- Q: Applicable to CNNs:
It is true that our analysis is quite general considering MLPs and not specifically CNNs and indeed we find it very likely that there are stronger results possible for CNNs than the ones we presented. 
-&gt; Added a discussion on CNNs in the new “Scope” Section in the revision
------------
- Q: Relation to Carlsson et al. (2017):
While the work of Carlsson et al. (2017) rather focus on a general analysis on the shape of preimages of activities at arbitrary levels and gives a first geometrical view as a piecewise linear manifold, we present in our work an in-depth understanding for preimages and the inverse mapping of ReLU networks:
1) We perform a qualitative analysis for the preimages and give computable conditions when the inverse image of an output is finite, infinite or a single point by performing an intuitive mathematical derivation.
2) We analyze the stability of the inverse mapping by investigating the singular values of the linearization of the network and confirm our theoretical results by numerical experiments.

---------
We therefore think that our work can be seen as a significantly different approach to the one presented by Carlsson et al. (2017).

We thank the reviewer for the helpful comments and would appreciate further discussions.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Skx34ZHc37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting and original idea, not sure about practical implications</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxYEoA5FX&amp;noteId=Skx34ZHc37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper17 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper17 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents an analysis of the inverse invariance of ReLU networks. It makes the observation that one can describe the pre-image of an image point z = F(x) using linear algebra arguments. They provide necessary conditions for the pre-image to be a singleton or a finite volume polytope. They also provide upper-bounds on the singular values of a train network and measure those in standard CNNs.

The paper is well-written but the structure is a bit disconnected; most notably, I didn't see clearly how Section 2 and 3 fit together. The proofs seem correct and rely mostly on elementary linear algebra argument; this simplicity makes the analysis quite interesting. The argument about a different kind of adversarial examples is also very interesting; instead of looking for small perturbation that affect the mapping in drastic ways, find large perturbations that in invariant directions of the network. However, the experiments are overall not very useful to the comprehension of the paper and not that illustrative.

I have several questions for the authors:
- the conditions presented in Theorem 4, seem hard to check in practice; what is the time complexity of this operation? I believe that checking if A is omnidirectional is equivalent to an LP but how do you solve the combinatorial size of doing that over all set of indices?
- I understand the upper bounds on the singular values, but I am not sure how they relate to inverse stability. Maybe more explanation and quantitative analysis (e.g. relating the volume of the preimage of an epsilon ball around z to the singular values) could be helpful.
- Is there actionable consequences one could draw from your papers? The way the results are presented seem like they are only useful inspection after training; are your results able to derive methods to enforce conditions on the pre-images for example?

In conclusion, this paper does an interesting and original analysis which can help us understand better the polytopes composing the input space. The experiments are not very convincing or illustrative of the theoretical results in my opinion. It is not clear how those observations can affect practical algorithms and this is something I hope the author can address.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkxr33DOTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Added discussion of raised points in revision (Scope and Practical Implications)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyxYEoA5FX&amp;noteId=rkxr33DOTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper17 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper17 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank you for acknowledging the novelty our findings and your appreciation for the elementary nature of our theory.

----------------
- Q: How do Section 2 &amp; 3 fit together?
Although it is true that our paper can roughly be divided into two section, we want to stress that these sections are inextricably linked due to the nature of their topics, since we see invariance as a limit case of inverse stability. We therefore think it is natural to study both of them.
However, the analysis of the limit case, invariance, admits more powerful tools (see e.g. Theorem 4), since one is only interested in whether a singular value is zero or not. Hence, the invariance is qualitative, whereas for stability we need to quantify singular values.

-----------------
-Q: Combinatorial problem to check Theorem 4:
While there are indeed a combinatorial number of possible tuples that the Theorem 4 describes, we can use the following trick in the design of the Algorithm 1 (Appendix A3) to circumvent these computations: The set of tuples (A, b) that form omnidirectional tuples is a null-set in all tuples of same form, we therefore ignore this case in our numerical analysis. Hence, we only have to check whether we have a compact or unbounded preimage. This can be done by simply checking whether A is omnidirectional or not.

----------------
-Q: Upper bounds and inverse stability:
The smallest singular values are directly linked to inverse stability for points from the same input polytope (where the linearization is exact). The upper bounds (Lemma 9) and the correlation effect are interesting, as they show how a well-conditioned matrix (subset of rows almost orthogonal) may become instable due to the removal of rows via ReLU. If the correlation of some rows is arbitrarily small (but non-zero) between remaining and removed rows, the upper bounds can be arbitrarily small. Thus, this Lemma provides an intuition how hard it is to globally control inverse stability with a vanilla architecture (linear mapping followed by ReLU). 

However, when considering an epsilon ball around activations, two main questions arise: 1) Are all points in the ball reachable from the considered input polytope? 2) Do points from other input polytopes map to the epsilon ball? If the second case holds, one would need to consider different linearizations of the network and thus extend the analysis to movements between the polytopes. 
-&gt; Added a comment in the newly written “Scope” section in the revision


-------------------
-Q: Actionable consequences from paper: 
One consequence of our paper is that it is close to impossible (each layer need at least to double the number of neuron) to enforce invertibility and it is similarly hard to enforce compactness in ReLU layers. This leads to the conclusion that if one wants invertibility or even just compactness reliably over the whole space, vanilla architectures using ReLU are not a good tool for the task.
Hence, our analysis can be seen as an argument for additional structure like dimension splitting in reversible networks (see e.g. Jacobsen et al. (2018)). These structures allow for guarantees as they are by design bijective, while vanilla architectures show a breadth of possible effects as shown in our analysis.
-&gt; Added a comment to “Practical Implications” in the revision

- Q: Illustrative experiments:
We currently thinking about an experiment to better illustrate the intuition of our theory and would appreciate any suggestions.

We thank the reviewer for the helpful comments and we would appreciate further suggestions.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>