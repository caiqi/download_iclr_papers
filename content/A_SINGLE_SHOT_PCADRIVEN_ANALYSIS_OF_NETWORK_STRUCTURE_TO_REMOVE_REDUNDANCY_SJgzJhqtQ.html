<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>A SINGLE SHOT PCA-DRIVEN ANALYSIS OF NETWORK STRUCTURE TO REMOVE REDUNDANCY | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="A SINGLE SHOT PCA-DRIVEN ANALYSIS OF NETWORK STRUCTURE TO REMOVE REDUNDANCY" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SJgzJh0qtQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="A SINGLE SHOT PCA-DRIVEN ANALYSIS OF NETWORK STRUCTURE TO REMOVE..." />
      <meta name="og:description" content="Deep learning models have outperformed traditional methods in many fields such&#10;  as natural language processing and computer vision. However, despite their&#10;  tremendous success, the methods of..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SJgzJh0qtQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A SINGLE SHOT PCA-DRIVEN ANALYSIS OF NETWORK STRUCTURE TO REMOVE REDUNDANCY</a> <a class="note_content_pdf" href="/pdf?id=SJgzJh0qtQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019a,    &#10;title={A SINGLE SHOT PCA-DRIVEN ANALYSIS OF NETWORK STRUCTURE TO REMOVE REDUNDANCY},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SJgzJh0qtQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Deep learning models have outperformed traditional methods in many fields such
as natural language processing and computer vision. However, despite their
tremendous success, the methods of designing optimal Convolutional Neural Networks
(CNNs) are still based on heuristics or grid search. The resulting networks
obtained using these techniques are often overparametrized with huge computational
and memory requirements. This paper focuses on a structured, explainable
approach towards optimal model design that maximizes accuracy while keeping
computational costs tractable. We propose a single-shot analysis of a trained CNN
that uses Principal Component Analysis (PCA) to determine the number of filters
that are doing significant transformations per layer, without the need for retraining.
It can be interpreted as identifying the dimensionality of the hypothesis space
under consideration. The proposed technique also helps estimate an optimal number
of layers by looking at the expansion of dimensions as the model gets deeper.
This analysis can be used to design an optimal structure of a given network on
a dataset, or help to adapt a predesigned network on a new dataset. We demonstrate
these techniques by optimizing VGG and AlexNet networks on CIFAR-10,
CIFAR-100 and ImageNet datasets.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">deep learning, model compression, pruning, PCA</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We present a single shot analysis of a trained neural network to remove redundancy and identify optimal network structure</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SkxO-ono2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper proposes a PCA based analysis of a pre-trained network to determine the number of neurons that are relevant in a Deep network</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJgzJh0qtQ&amp;noteId=SkxO-ono2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper965 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper965 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
Claims in the paper are there is no need for iterations or retraining and proposing design heuristics for optimal network design. 
The analysis is not only about width but also depth without compromissing accuracy.

From the paper, one of the findings is a consistent answer independently of the architecture. 
- What sounds interesting to me is finding the depth. However, does seem quite heuristic and not really relevant. 
- The idea of using PCA to analyze the relevance of each filter has been approached in several papers as postprocessing and some others  (for instance Compression-aware training of Neural nets, NIPS 2017) during training and therefore, in this case, no finetuning would be required. 
- How does this differ from those?

- What is really the take home message from the analysis of the layers? In the end, after the architecture is found, there is a need to retrain (as it is still an approximation)

- Results on imagenet are interesting but based on VGG networks that, from related works, are extremely overparameterized and therefore easy to prune. 

- As in the conclusions, what will happen with relus and batch norms? I have to say those conclusions seem more a discussion than conclusions


- One other claim is this could be used while designing the network for new data. I do not see that point. Why? In the end, you need the network to converge and have proper accuracy, and if that is the case, worth trying to just do regularization, compression aware pruning or simple pruning.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1ggYKOi2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Heuristics for choosing architectures</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJgzJh0qtQ&amp;noteId=S1ggYKOi2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper965 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper965 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a set of heuristics for identifying a good neural network architecture, based on PCA of unit activations over the dataset. The method shows modest accuracy drops and improved parameter counts in several settings.

Major comments:

The proposed method is not optimal, in the sense that it does not provide any guarantees on the loss in performance or generalization error. It is largely heuristic, particularly the notion that the depth can be determined to be the layer at which expansion of the representation stops. This itself depends on the initial architecture one uses. 

There is no clear theoretical link between PCA of the filters and performance of the network. In general, important information in features could come from low variance directions. Moreover, the PCA step is meant to give the dimensionality of the transformations, but this will depend on the choice of nonlinearity. Optimizing the method’s design parameter (% variance retained) may be comparably difficult as optimizing the number of filters in the networks directly.

The experiments show that the proposed method can often reduce the number of operations and parameters, at usually slight decreases in test performance. The experiments could be greatly improved by adding comparison to related parameter reduction mechanisms. It is hard to evaluate the benefit of the proposed method without these. Additionally, the drop in performance highlights the question of what exactly is being optimized if these design principles are ‘optimal.’

It could be interesting to look at the patterns in reduction of filters in the experimental results run here. Do adjacent layers typically increase the number of filters by certain factors after pruning? 

The paper is mostly clear but the high level organization could be improved.

Overall the proposed method offers a simple heuristic for choosing the architecture of a deep network, but this seems unlikely to yield principled improvements at present.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Bkehr9Oghm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting investigation based on reasonable heuristics. Benchmarking and comparison with respect to other compression methods is needed </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJgzJh0qtQ&amp;noteId=Bkehr9Oghm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper965 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">26 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper965 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a framework for optimising neural networks architectures through the identification of redundant filters across layers. 
The intuition behind this work is that the redundancy in a given layer can be modelled through linear transformation of a low-dimensional set of basis functions. This motivates the use of PCA at each layer to identify a novel low-dimensional representation of the filters. 
The PCA is actually performed on the set of concatenated outputs obtained by convolving a given input patch with each filter. The practical implementation of the scheme is based on the computation of PCA on the activations obtained from a small sample of training images. 

The results provide some interesting insights. The pruning seems to be more effective for deeper layers, where the overall accuracy seems to be less sensitive to filters removal. The experimental validation shows that re-training a network by using the reduced filter set results in a minimal accuracy drop. Overall, the paper is based on a nice intuition and points to interesting research directions. However, the development looks quite heuristic and not completely intuitive. Moreover, the work completely lacks the comparison with respect to the state of the art.

Detailed comments.

- One may argue that the heuristics 1-4 provided in section 3.1 would be enough to effectively reduce the networks parameters. The authors should have compared the proposed framework with respect to the network reduction obtained by using these approaches (even without retraining).  

- The logic from section 3.1 to 3.2 is counterintuitive. The idea is introduced by studying the spatial correlation analysis across filters. 
However, the  actual dimensionality reduction is performed on stacked convolution outputs, which are non-trivially related to the filter appearance. Again, the impression is that the development of the work is not clearly motivated from the methodological perspective.

- The procedure based on iterative convolutions on randomised training samples is quite tedious, and again completely dependent on heuristics. Moreover, the required sample size may be prohibitive for many real world applications. 

- The post-hoc assessment of the redundancy may lacks of consistency, as it would be beneficial to prune filters and weights directly during training. For example, it is not clear under which sense the proposed compression is optimal. Since no relationship across layers is taken into account it is difficult to assess the impact of pruning across different layers.  

- One of the major issue of this work concerns the lack of comparison with respect to the state of art. For example, one would expect a comparison with respect to some of the several proposed techniques on network compression. 

- Section 6. Figure 9 is mentioned, but the authors are perhaps referring to 8b and 8c?

- I found the paper presentation not optimal. The focus of the first part of the paper (Section 3.1) provides some evidence about the paper’s idea. However it distracts the reader from the contribution of the work. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>