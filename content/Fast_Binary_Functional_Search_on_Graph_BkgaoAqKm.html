<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Fast Binary Functional Search on Graph | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Fast Binary Functional Search on Graph" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Bkg5aoAqKm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Fast Binary Functional Search on Graph" />
      <meta name="og:description" content="The large-scale search is an essential task in modern information systems. Numerous learning based models are proposed to capture semantic level similarity measures for searching or ranking...." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Bkg5aoAqKm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Fast Binary Functional Search on Graph</a> <a class="note_content_pdf" href="/pdf?id=Bkg5aoAqKm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019fast,    &#10;title={Fast Binary Functional Search on Graph},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Bkg5aoAqKm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">The large-scale search is an essential task in modern information systems. Numerous learning based models are proposed to capture semantic level similarity measures for searching or ranking. However, these measures are usually complicated and beyond metric distances. As Approximate Nearest Neighbor Search (ANNS) techniques have specifications on metric distances, efficient searching by advanced measures is still an open question. In this paper, we formulate large-scale search as a general task, Optimal Binary Functional Search (OBFS), which contains ANNS as special cases. We analyze existing OBFS methods' limitations and explain they are not applicable for complicated searching measures. We propose a flexible graph-based solution for OBFS, Search on L2 Graph (SL2G). SL2G approximates gradient decent in Euclidean space, with accessible conditions. Experiments demonstrate SL2G's efficiency in searching by advanced matching measures (i.e., Neural Network based measures).</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Binary Functional Search, Large-scale Search, Approximate Nearest Neighbor Search</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Efficient Search by Neural Network based searching measures.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Byx3R3E03m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Fast Binary Functional Search on Graph</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkg5aoAqKm&amp;noteId=Byx3R3E03m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper830 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper830 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This work extends the approximate nearest neighbor search (ANNS) algorithm to a more general setting. Instead of search with a "separable" similarity measure, the authors propose Optimal Binary Functional Search (OBFS), where the scoring function f() is in general non-separable. The exact construction of the Binary Function Graph wrt f() and X is computationally expensive. The specific approximate algorithm of OBFS proposed in the paper is to:
1) First construct an L2 Delaunay graph for based on the dataset X only and;
2) Perform greedy search with the L2 Delaunay graph.

The authors also discuss various conditions under which, the approximation method can achieve close to optimal value.

Some of the concerns I have with this work:

1) The authors do not demonstrate sufficient value of performing approximation in this specific fashion. For instance, in  Theorem 2, the authors start with the concavity assumption of the scoring function f(). Then it is natural to apply a gradient ascent method on the neighborhood graph. And the authors did not quantitatively or qualitatively justify their specific approach.

2) Lately, numerous publications have shown that distilled models can achieve very high quality and render scoring function separable. The authors should at least compare their method against distillation and Maximum Inner Product Search based approaches.

Overall, this research direction is interesting, but this specific work falls short for a publication at ICLR.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJxBPmP9pm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Responses to the comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkg5aoAqKm&amp;noteId=SJxBPmP9pm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper830 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper830 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The authors do not demonstrate sufficient value of performing approximation in this specific fashion. For instance, in Theorem 2, the authors start with the concavity assumption of the scoring function f(). Then it is natural to apply a gradient ascent method on the neighborhood graph. And the authors did not quantitatively or qualitatively justify their specific approach.

[Response] Thanks for your comments. Nodes on the neighborhood graph are discrete points in the space. Searching on the neighborhood graph is quite different from gradient descent in the continuous space. That is why we try to figure out the conditions in which the proposed method will work well. To the best of our knowledge, this is the first work discusses this point. We provided the theoretical analysis and empirical experiments for the proposed approach. 

Lately, numerous publications have shown that distilled models can achieve very high quality and render scoring function separable. The authors should at least compare their method against distillation and Maximum Inner Product Search based approaches. 

[Response] For related distilled models, could you specify the particular papers? Thanks. 

The MIPS problem is a special case of the Binary Functional Search problem. Although the proposed method (SL2G) is not designed for the MIPS problem but for more complex searching measures, it can be applied for MIPS, the corresponding empirical study can be found in Appendix D.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BklgOiC2jQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Promising novel idea; needs further clarification and development</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkg5aoAqKm&amp;noteId=BklgOiC2jQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper830 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">24 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper830 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Pros
-------
[Originality/Significance] The manuscript focuses on a very general and important problem and proposes a scheme to solve this general problem. The authors present some theoretical and empirical results to demonstrate the utility of the proposed scheme.

Limitations
----------------
[Clarity] While the problem being addressing is extremely important, and the proposed solution seems reasonable, the manuscript is really hard to follow. For example, Definition 3 and Theorem 1 are extremely hard to understand. 

[Clarity/Significance] Moreover, I feel that the authors should be more precise in pointing out why current graph based search algorithms are just not trivially applicable to OBFS. The nature of the approximate Delaunay graph is that it can be built for any given similarity function (the level of approximation obviously depends on the similarity function, but that is an existing issue with graph-based methods). Given the graph, I do not understand why the basic search algorithm on this similarity graph would not be an approximate solution to OBFS. Hence I believe the authors need to clarify why the existing graph based algorithms do not directly translate. 

[Significance] While Definition 1 considers topological spaces, SL2G is assuming that X and (maybe) Y are in R^d (for different values of d). So does that mean that SL2G does not solve the general OBFS?

[Significance/Correctness/Clarity] The assumptions in Theorem 2 (as well as the supporting Proposition 1 in Appendix B) seems quite unreasonable. In moderately high dimensional X, doesn't the curse of dimensionality imply that this condition will not hold in most case? In there any reason why/how this would be circumvented? Moreover, in Proposition 1 (in Appendix B), the quantity C_r needs to be precisely defined since it could in general be exponential in the number of dimensions. Also, the assumption in Proposition 1 where \lambda^* &gt; 0 is fairly strong in high dimensional data since data gets really sparse in high dimensions. Finally, the last step in Proposition 1 (where the failure probability obtained from the union bound is connected to condition (b) in Theorem 2) is not clear at all -- it is not apparent how E and F related to S and how p relates to every ball containing a point in S. This is a very important step and needs better exposition. 

[Clarity/Significance] I am unable to understand the baseline HNSW-SBFG (or the motivation for it) in the empirical section. It would be good to clarify this. 


General comments
---------------------------
[Significance] Finally, I believe that it would be good to see a connection between the success of SL2G to relationship between |f(x1, q) - f(x2, q)| and ||x1 - x2 ||_2 since the author emphasize that the proposed scheme can be seen as "gradient descent in Euclidean space" (although the authors would need to also precisely explain what they mean by that statement).

[Originality] Some related work that the authors should position their proposed problem/solution against:
- There is some work on "max-kernel search" which can perform similarity search with general notions of similarity (than just Euclidean metrics).
- There is some work on search with Bregman divergences which handle asymmetric similarity functions and also incorporate notions of gradient descent over convex sets.

Minor comments/typos
---------------------------------
- The authors should present the precise SL2G algorithm given the graph in the manuscript.
- l^2 --&gt; \ell_2
- gradient decent --&gt; gradient descent
- Table 1, f(q, x) --&gt; f(x, q)</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkebSdD9aX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Responses to the Comment 6, 7&amp;8 </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkg5aoAqKm&amp;noteId=BkebSdD9aX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper830 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper830 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">6. [Significance] Finally, I believe that it would be good to see a connection between the success of SL2G to relationship between |f(x1, q) - f(x2, q)| and |x1 - x2 |_2 since the author emphasize that the proposed scheme can be seen as "gradient descent in Euclidean space" (although the authors would need to also precisely explain what they mean by that statement).

[Response] As we mentioned in the paragraph after Theorem 2, the success and accuracy of our algorithm depend on the radius of curvature of the level sets. We believe |f(x_1, q) - f(x_2, q)| and |x_1 - x_2 |_2 together with the density of dataset might affect the speed of convergence, which we have not covered in this paper.

7. [Originality] Some related work that the authors should position their proposed problem/solution against…

[Response] Thank you for pointing out these related works. First of all, we would like to emphasize that our work is very different from similarity search, so most of the existing methods in this field does not apply to our problem. 
``Max-kernel search" is defined on a Hilbert space, so it has to be symmetric. Bregman divergence does not have to be symmetric, but both variables must come from the same convex set. Even if we apply Bregman ball tree to a similarity search problem, we do not think it performs well on finding top-10 nearest neighbors.
We will analyze these works in our updated manuscript. 

8. The authors should present the precise SL2G algorithm given the graph in the manuscript.

[Response] Thanks for your suggestion. To make the manuscript more self-contained, we will list the algorithms (graph construction and greedy search) in the Appendix, although they are common algorithms for search on graph methods.

Finally, we really appreciate your time and detailed comments.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1gakuPcTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Responses to the Comment 3, 4&amp;5</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkg5aoAqKm&amp;noteId=S1gakuPcTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper830 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper830 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">3. [Significance] While Definition 1 considers topological spaces, SL2G is assuming that X and (maybe) Y are in $R^d$ (for different values of d). So does that mean that SL2G does not solve the general OBFS?

[Response] Thanks for pointing this out. The answer is “no”, it only solves OBFS when X and Y are subsets of Euclidean spaces. Actually, we are usually interested in Euclidean spaces or its subsets in real applications, as mentioned below Definition 1.

4. [Significance/Correctness/Clarity] The assumptions in Theorem 2 (as well as the supporting Proposition 1 in Appendix B) seems quite unreasonable. In moderately high dimensional X, doesn't the curse of dimensionality imply that this condition will not hold in most case? …

[Response]Thank you for reading the proposition and theorem very carefully. 
We consider an asymptotic setting that the number of data points growing to infinity and the dimension of X is fixed. When the region E is fixed, \lambda^* is proportional to the number of data points, so it goes to infinity. 
Of course, one can consider a high dimensional setting when n and d increase simultaneously. However, C_r is still not critical since it is not in the exponent. In the failing probability formula, the volume of r/2 ball depends on d and plays a much more important role than C_r when d increases. If we hope the failing probability still goes to 0, then we should require log \lambda^* is much greater than d log d.
About the implication from Proposition 1 to condition (b) in Theorem 2, it is a simple geometry property. We recall that F is a set of centers of open r/2-balls whose union covers E. For a fixed open r-ball, say B, its center is covered by at least one open r/2-ball with the center in F. This open r/2-ball is contained in B by triangle inequality. The open r/2-ball contains at least one data point, which also belongs to B. This implies every open r-ball contains a data point, which is the assumption (b) in Theorem 2. We believe our proof is mathematically correct and clear.
We believe this is a nontrivial result. As the number of data points increases, we have more “bad” data points. Here, “bad” data point means it is far away from the local optimum of $f$, but it is a local optimum in greedy search on the graph. The theorem and proposition show that even if we have more bad data points, the failure probability of the greedy search still goes to 0.

5. [Clarity/Significance] I am unable to understand the baseline HNSW-SBFG (or the motivation for it) in the empirical section. It would be good to clarify this. 

[Answer] HNSW-SBFG is quite similar to the original HNSW. We just replace the metric measure in HNSW, such as l2 or cosine, with the focusing search binary functional f. Beyond that, the graph construction and greedy search approaches of HNSW-SBFG are same as the original HNSW. Note that, to let HNSW-SBFG be applicable, we set X and Y in the same space (both 64-dimensional). In this way, f(x_i,x_j) will output a value no matter f is symmetrical (e.g., MLP-Em-Sum) or asymmetrical (e.g., MLP-Concate). If f is asymmetrical, f(x_i,x_j) is problematic actually. That why HNSW-SBFG works even worse on MLP-Concate datasets. If X and Y have different dimensions, HNSW-SBFG will be not applicable.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HkehwwDqTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Responses to the Comment1&amp;2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkg5aoAqKm&amp;noteId=HkehwwDqTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper830 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper830 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1. While the problem being addressing is extremely important, and the proposed solution seems reasonable, the manuscript is really hard to follow. For example, Definition 3 and Theorem 1 are extremely hard to understand. 

[Response] Thanks for your comments. We will add more explanations for the theory part and make it easier to access. Specifically, although the problem is in an asymmetric setting, readers can still assume f(x,y) = -|x-y| as a typical example to understand the definitions and theorem. For example, assuming f is the negative l2-norm, then definition 3 means we will connect two data points in the Delaunay graph if the Voronoi cell is “adjacent” to each other. Here, adjacency means their boundary has nonempty intersection. Theorem 1 means that, for an arbitrary query, a greedy search on Delaunay graph with any initial point can find the nearest neighbor of the query.

2. [Clarity/Significance] Moreover, I feel that the authors should be more precise in pointing out why the current graph-based search algorithms are just not trivially applicable to OBFS. …

[Response] Thank you for this comment. We are going to assume "basic search algorithm on similarity graph" indicates the previous search on graph methods, such as HNSW or Bregman ball tree (you mention in a later comment). These algorithms require f(x,y) defined on the product of two identical spaces. OBFS is much more general and does not have such an assumption. 

Suppose we still assume x and y are from the same space and plug in f as a "similarity function" in HNSW, which is exactly the baseline, HNSW-SBFG, we used in experiments. Particularly, in the recommendation-system scenario, we embed users and items in the same Euclidean space. As shown in the experimental results on page 8, the performance of HNSW-SBFG is much poorer than HNSW-SL2G. We believe original HNSW or any other existing similarity graph based algorithms require f performs like a similarity function. A well behaved f in recommendation system should not measure the similarity between user and item.

It is also worth to mention that, although we provide guarantees for SBFG, but most of general f's, e.g., neural networks, does not satisfy the condition in Theorem 1.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>