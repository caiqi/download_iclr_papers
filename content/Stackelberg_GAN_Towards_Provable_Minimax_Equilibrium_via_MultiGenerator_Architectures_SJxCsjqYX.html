<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Stackelberg GAN: Towards Provable Minimax Equilibrium via Multi-Generator Architectures | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Stackelberg GAN: Towards Provable Minimax Equilibrium via Multi-Generator Architectures" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SJxCsj0qYX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Stackelberg GAN: Towards Provable Minimax Equilibrium via..." />
      <meta name="og:description" content="We study the problem of alleviating the instability issue in the GAN training procedure via new architecture design. The discrepancy between the minimax and maximin objective values could serve as..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SJxCsj0qYX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Stackelberg GAN: Towards Provable Minimax Equilibrium via Multi-Generator Architectures</a> <a class="note_content_pdf" href="/pdf?id=SJxCsj0qYX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 10 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019stackelberg,    &#10;title={Stackelberg GAN: Towards Provable Minimax Equilibrium via Multi-Generator Architectures},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SJxCsj0qYX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=SJxCsj0qYX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We study the problem of alleviating the instability issue in the GAN training procedure via new architecture design. The discrepancy between the minimax and maximin objective values could serve as a proxy for the difficulties that the alternating gradient descent encounters in the optimization of GANs. In this work, we give new results on the benefits of multi-generator architecture of GANs. We show that the minimax gap shrinks to \epsilon as the number of generators increases with rate O(1/\epsilon). This improves over the best-known result of O(1/\epsilon^2). At the core of our techniques is a novel application of Shapley-Folkman lemma to the generic minimax problem, where in the literature the technique was only known to work when the objective function is restricted to the Lagrangian function of a constraint optimization problem. Our proposed Stackelberg GAN performs well experimentally in both synthetic and real-world datasets, improving Frechet Inception Distance by 14.61% over the previous multi-generator GANs on the benchmark datasets.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">generative adversarial nets, minimax duality gap, equilibrium</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We study the problem of alleviating the instability issue in the GAN training procedure via new architecture design, with theoretical guarantees.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">10 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Hkx_aMpapQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Summary of the revision</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxCsj0qYX&amp;noteId=Hkx_aMpapQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper671 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper671 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The authors would like to thank the reviewers for bringing up valuable questions/suggestions to improve our paper. We have updated manuscripts to address all concerns from the reviewers appropriately. We summarize our revision below.
----------------------------------------------------------------------------------------------------------------------------------------
Positions                     For which comment                          Revised version
----------------------------------------------------------------------------------------------------------------------------------------
Section D                    Density approximation                     New results for density approximation

Table 1                        Comparison with MAD-GAN            New experiments for MAD-GAN

Sections 2                   Concave closed hull?                        Avoid using the term by redefining \hat{cl} f

Bullet 1, Page 2          Difference with prior GANs            Clarify algorithmic difference

Whole paper              Typos                                                   Fix all typos
-----------------------------------------------------------------------------------------------------------------------------------------

Again, we would like to emphasize that our contributions focus more on the approximate Nash equilibrium in Theorems 1, 3, and Corollary 2, improving over previous best-known bounds. We argue that the algorithmic similarity is an advantage of this paper, which means that our theoretical results work for broader class of multi-generator GANs.

Finally, we would like to mention that the technical contents in the revised version are the same with those in the previous version. We would like to kindly ask reviewers to re-evaluate the paper focusing more on the technical contributions of the paper. Thank you.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkxGpTi9hX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting view of GANs from game-theory perspective, but algorithmically the Stackelberg GAN is similar with previous multiple-generator GANs</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxCsj0qYX&amp;noteId=rkxGpTi9hX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper671 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper671 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes the Stackelberg GAN framework of multiple generators in the GAN architecture. The architecture is similar with previous multiple-generator GANs (MAD-GAN and MGAN). In fact, it's even simpler in the sense that Stackelberg GAN has simpler loss function for the discriminator compared with the previous two. The authors prove that the minimax duality gap shrinks as the number of generators increases. And this proof has no assumption on the expressive power of generators and discriminator. With this proof, the authors argues that because the duality gap shrinks as the number of generators increases, the training of GANs gets more stable.

From the algorithm part, I think the algorithm is very similar (and even simpler) than MAD-GAN and MGAN. The MAD-GAN and MGAN even proposed some specific loss for the discriminator so that it will encourage different generator to generate different modes in the target distribution. The Stackelberg GAN does not do this, but "partially" achieved the same goal. However, from Figure 9, we see that the simpler the generator is, the easier different generator will capture different modes. I think that this is due to the simplicity of discriminator loss. Therefore, on the algorithm part, the author may want to address the difference between Stackelberg GAN and MAD-GAN and MGAN. On the experiment part, we need to see more comparison between these three methods. In the current experiment, MGAN result is very similar to the proposed method, and MAD-GAN result is missing. Personally, I think that on cifar dataset (or larger datasets), these three methods should have very similar behavior. 

From the theoretical part, the authors derived a bound of the minimax duality gap for the Stackelberg GAN, without the assumption on the expressive power of generators and discriminator. Although the bound may not be practical, these are nice efforts. There are many typos in the paper (and appendix), which make me difficult to follow the proofs. For example, "Let clf (bclf) be the convex(concave) closure of f, which is defined as the function whose epigraph (subgraph) is the convex
(concave) closed hull of that of function f." Do we have concave closed hull of subgraph of function f? What is the concave closed hull of a set? The usage of sub(sup)-script is also very confusing, like in the definition of h_i(u_i). The authors may want to correct typos and improve the presentation. In the conclusion, the authors conclude "We show that the minimax gap shrinks to \eps as the number of generators increases with rate e O(1/\eps)." This is an over-claim, because the authors only proved this under the assumption of concavity of the maximization w.r.t. discriminators. 

Finally, the authors may want to provide some simple results of the Stackelberg GAN from the perspective of density approximation, even assuming infinite capacity of the discriminator set, as other GANs does. Whether the distance defined by the maximization problem a distance or divergence. If we exactly minimizing that objective function, do we get the target distribution? 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rylVJXBmaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our theory works broadly including some forms of previous multi-generators GANs as special cases.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxCsj0qYX&amp;noteId=rylVJXBmaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper671 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018 (modified: 18 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper671 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the valuable comments. We will address all concerns from the reviewer in the following form of Q/A.

Q1. (The first paragraph of review) From the algorithm part, I think the algorithm is very similar (and even simpler) than MAD-GAN and MGAN. Therefore, on the algorithm part, the author may want to address the difference between Stackelberg GAN and MAD-GAN and MGAN. On the experiment part, we need to see more comparison between these three methods. In the current experiment, MGAN result is very similar to the proposed method, and MAD-GAN result is missing.
A1. On one hand, the population form of Stackelberg GAN includes some forms of previous multi-generators GANs as special cases. We believe this is a *plus* of our paper, because it implies that our theory works for broader GAN models, providing a unified and improved framework for multi-generator GANs. Note that in our theory, we make no assumption on the capacity and architecture of discriminator. Thus, our theory even works for more complicated discriminator such as that of MGAN and MAD-GAN, whose theory on equilibrium is missing in their original papers. On the other hand, the empirical losses of Stackelberg GAN and prior GANs are different. Our choice of sampling scheme is flexible as we claimed in the previous post. Furthermore, MGAN requires shared network parameters among various generators, while Stackelberg GAN enjoys free parameters for each generator. To make the paper clearer, we restate the difference among various models in Page 2 of our revised version (the first bullet).
On the experiment part, we supplement new experiments on MAD-GAN on CIFAR-10 as the reviewer suggested. We did not find existing Inception Score of MAD-GAN on CIFAR-10, so we run it by ourselves. Here is a thorough comparison among MGAN, MAD-GAN, and Stackelberg GAN with the same network capacity and 10 generators. A potential reason of the unsatisfactory performance of MAD-GAN is that the method involves a multi-class discriminator with classes as many as I+1, which leads to an imbalance between real and generated data and the unstable training.
-----------------------------------------------------------------------------------------
Model                          Inception Score            Frechet Inception Distance
----------------------------------------------------------------------------------------
MAD-GAN                    6.67+-0.07                        34.10
MGAN                           7.52+-0.1                          31.34
Stackelberg GAN        7.62+-0.07                        26.76

Q2. (The second paragraph of review) There are many typos in the paper (and appendix). In the conclusion, the sentence "we show that the minimax gap shrinks to eps as the number of generators increases with rate O(1/eps)" is an over-claim, because the authors only proved this under the assumption of concavity of the maximization w.r.t. discriminators.
A2. We have tried our best to fix all the typos that we find in the paper and appendix. In particular, we avoid the use of “concave closed hull of a set” by redefining \hat{cl}f:=-\br{cl}(-f). We clarify our use of sub-script in h_i(u_i) by saying “The subscript i in h_i indicates that the function h_i is derived from the i-th generator. The argument of h_i should depend on i, so we denote it by u_i. Intuitively, h_i serves as an approximate convexification of -\phi(\gamma_i,\cdot) w.r.t the second argument due to the conjugate operation”. We also modified the sentence in the conclusion as “we show that the minimax gap shrinks to eps as the number of generators increases with rate O(1/eps), when the maximization problem w.r.t. the discriminator is concave”.

Q3. (The third paragraph of review) The authors may want to provide some simple results of the Stackelberg GAN from the perspective of density approximation. Whether the distance defined by the maximization problem a distance or divergence. If we exactly minimizing that objective function, do we get the target distribution?
A3. Thanks for the comment. As the reviewer suggested, in Theorem 9 of the revised version we provide new results of Stackelberg GAN from the perspective of density approximation under the standard assumption of Goodfellow’14. Our result shows that Stackelberg GAN can learn a mixture of distributions. This theorem gives a positive answer to the reviewer’s question about whether minimizing the objective function gets the target distribution. For the question concerning whether the distance defined by the maximization problem a distance or divergence, it depends on the choice of function f. For example, when f is the log function, the distance defined by the maximization problem of Stackelberg GAN (i.e., the \tilde{L} in the proof of Theorem 9) is the Jensen-Shannon divergence between the mixture generative distribution and the real distribution.

We are looking forward to a re-evaluation from the reviewer based on our revision.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_r1ekBWtq3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A nice approach for training multi-generator</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxCsj0qYX&amp;noteId=r1ekBWtq3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper671 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper671 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a way of training multi-generator in the GAN setting.
While a proposed approach is simply to put N generators and form a sum of GAN losses to train a model, the paper carefully presents a theoretical analysis on the duality gap, and shows as N goes infinity, the duality gap can shrink to zero.
One can think of this as a usual ensemble approach to increase model's capacity and performance, but the main difference to the usual ensemble approach is to form a sum of losses (ensemble losses) instead of a loss on output of ensemble.
The paper shows this can be more effective approach to train a multi-generator architecture and I believe that this can be an effective approach to capture multi-modal sample distributions.
Finally, a paper is well-written and well-organized. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyxWuDuj3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The difference between a sum of losses (ensemble losses) and a loss on output of ensemble is very minor, even negligible</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxCsj0qYX&amp;noteId=SyxWuDuj3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper671 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 06 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper671 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The authors overstate the difference between a sum of losses (ensemble losses) and a loss on output of ensemble. In fact, in terms of algorithm, this difference is very minor, even negligible.

When we have a loss on output of ensemble, the loss is:
\sum_{n=1}^N f(D_{\theta}(x_n) + \sum_{i=1}^I \sum_{n=1}^{N_i} f(1-D_{\theta}(G_{\gamma_i}(z_{i,n}))),
where {x_n}_{n=1}^N are iid samples from the dataset, {G_{\gamma}(z_{n,i}): 1\le i\le I, 1\le n \le N_i} are iid samples from the mixture generative model, and \sum_{i=1}^I N_i = N.

When we have a sum of losses (ensemble losses), the loss is:
\sum_{n=1}^N f(D_{\theta}(x_n) + \sum_{i=1}^I \sum_{n=1}^{N/I} f(1-D_{\theta}(G_{\gamma_i}(z_{i,n}))),
where {x_n}_{n=1}^N are iid samples from the dataset, {G_{\gamma}(z_{n,i}): 1\le n \le N/I} are iid samples from the i'th generative model, and all the I generator components contribute N/I samples to the loss equally.

Therefore, the only difference is that in a loss on output of ensemble, we are truly sampling from the ensemble model, while in a sum of losses (ensemble losses), we enforce that each component must contribute the same number of training samples.

This difference even makes Stackelberg GAN more difficult to prove its convergence in the density approximation sense, because now its training samples are not iid sampled from the ensemble model.



</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BygSR2UR37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Misunderstandings about the ensemble loss by AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxCsj0qYX&amp;noteId=BygSR2UR37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper671 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper671 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank AnonReviewer2 for the comment. We believe there are misunderstandings about the loss by the reviewer here. Our ensemble loss is E_x f(D_{\theta}(x)) + \frac{1}{I}\sum_{i=1}^I E_z f(1-D_{\theta}(G_{\gamma_i}(z))). This is totally different from the loss that the reviewer mentioned, as the index i is imposed on the generator parameter \gamma in our loss. Our loss involves optimizing *multiple* generators jointly, while the ensemble loss that the reviewer mentioned only involves learning one generator. Therefore, there is huge difference between the ensemble loss (as in this paper) and the loss on output of ensemble (as the reviewer mentioned).

Furthermore, we do not require that each component must contribute the same number of training samples in the ensemble loss. Rather, we only restrict the *weight* of all generators to be the same. Our analysis focuses on the population form where many sampling methods are consistent with it by the law of large number. For example, we allow the generator mixture model with uniform distribution over all generators. We also allow an empirical ensemble loss \frac{1}{N}\sum_{n=1}^N f(D_{\theta}(x_n)) + \frac{1}{I}\sum_{i=1}^I \frac{1}{N_i}\sum_{n=1}^{N_i} f(1-D_{\theta}(G_{\gamma_i}(z_{i,n}))) with i.i.d. z_{i,n}. We even allow the case that N_1=…=N_I. So, our model does not have the issue that “Stackelberg GAN is more difficult to prove its convergence in the density approximation sense”, since the training samples indeed can be i.i.d. sampled from the ensemble model.

Thanks again for your revaluation.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hkl61NqR2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The difference is in the empirical ensemble loss</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxCsj0qYX&amp;noteId=Hkl61NqR2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper671 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper671 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the reply!

First, sorry that in my empirical ensemble losses I missed typed G_{\gamma_i} as G_{\gamma}. Both the sum of losses (ensemble losses) and a loss on output of ensemble have G_{\gamma_i}. After I corrected my typo, I did not see much difference there.

In fact, if we consider the population loss, the sum of losses (ensemble losses) and the loss on output of ensemble are exactly the same. Both of them are E_x f(D_{\theta}(x)) + \frac{1}{I}\sum_{i=1}^I E_z f(1-D_{\theta}(G_{\gamma_i}(z))). The loss on output of ensemble can have different weights on different generators, and you method can do it, too.

The difference is in the empirical ensemble loss, as I wrote in my last post. In the loss on output of ensemble, (N_1, N_2, ..., N_I) is a random vector with multinomial distribution (N, 1/I, 1/I, ..., 1/I). It is not clear to me that in your sum of losses (ensemble losses), how will you choose your (N_1, N_2, ..., N_I)? It seems to be that you choose N_1=…=N_I=N/I?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HyebUgh0nQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We do not need to choose N_1=…=N_I=N/I, although we can as well. The choice is flexible.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxCsj0qYX&amp;noteId=HyebUgh0nQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper671 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper671 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank AnonReviewer2 for the quick reply! Yes, the difference is in the empirical loss. However, we do not necessarily need to choose N_1=…=N_I=N/I, although we can as well. We believe the key to your question is on the relationship between population loss and empirical loss --- the unbiased estimator. Note that by the uniform convergence, an unbiased empirical loss asymptotically converges to the population loss. There are multiple ways of samplings which lead to an unbiased empirical loss to the reviewer’s population loss. Here are three examples: (1) the multinomial distribution with parameter (1/I,…,1/I) as the reviewer mentioned. Note that even in this case, with high probability N_1=…=N_I=N/I does not hold. (2) Each generator samples a fixed but unequal number of data points independently, e.g., N_1=1.5N/I, N_2=…=N_{I-1}=N/I, N_I=0.5/I. (3) Each generator samples a fixed and equal number of data points independently, i.e., N_1=…=N/I. All the three sampling schemes are unbiased to the population loss, although N_1=…=N_I=N/I does not always hold true.

Thanks again for your question.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_HJxvQznKn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The name Stackelberg GAN is misleading as the underlying problem formulation is not a Stackelberg game but (still) a zero-sum game. The argument why more data generators help is not convincing.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxCsj0qYX&amp;noteId=HJxvQznKn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper671 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper671 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">A Stackelberg competition is a nonzero-sum game where 1) each player has their own objective, which do not sum up to a constant, and 2) there is an order at which the players interact. The proposed formulation only assumes that parameters of one player (data generator) partition in I tuples \gamma_i of parameters, where each tuple parameterizes a different data generator component (e.g., a separate neural network). Further, each of those components is assumed to contribute a term to the game's objective that only depends on the corresponding parameter tuple \gamma_i, and the other player's parameters \theta (e.g., weights of the discriminator). From a game theoretic perspective, this still yields a 2-player zero-sum game where the action space of the data generator is the product space of the I tuple spaces. Hence, I have doubts about the general finding that more data generating components decreases the duality gap.

The gap between the a maximin and minimax solution is determined by the shape of the objective \phi(\gamma,\theta) and is zero, for example, if \phi is (quasi) convex in \gamma=[\gamma_1, ..., \gamma_I], and (quasi) concave in \theta. The authors bound the violation of this property w.r.t. the data generator components' parameters \gamma_i, and argue that this degree of violation is the same for the whole data generator parametrized by \gamma=[\gamma_1, ..., \gamma_I] if the data generator components are from the same family of mappings (e.g., having the same network architecture). While this conclusion is true under worst cast assumption, e.g., the globally maximal possible gap, this would also imply that all data generator components find the same global best solution, that is, yield the same mapping, in which case the gap would be identical to just using one of those components.

Intuitively, the only reason to have multiple data generator components is to learn different mappings such that the joint data generator -- mixing the outputs of the different components -- is more expressiv than just a single mapping. If the different mappings only result from the inability of finding the global best solution, a worst case argument is not very insightful; in this case, one should study the duality gap in the neighborhood of the starting solutions. On the other hand, if we assume a different family of mappings for each component, the convexity violation of the joint data generator is higher than for each component; hence, the gap does not necessarily decrease with more components.

So why do multiple data generator components help in practice, and why does the proposed model outperform single-component GANs and the multi-branch GAN in the experiments? Solving a maximin/minimax problem for highly non-convex-concave functions is challenging; there is an infinity of saddle point solutions which yield different "performances". The multi-branch GAN can be seen as a model averaging approach giving more stable results, whereas the proposed GAN seems more of an ensemble approach to stabilize the result. Though, this is speculative and I would encourage the authors to study this in-depth; the reasoning in Remark 1 is not convincing to me.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByghlSHQTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Stackelberg game can be a zero-sum game. We provide a necessary condition about why more data generators help by analyzing the existence of approximate Nash equilibrium.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJxCsj0qYX&amp;noteId=ByghlSHQTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper671 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018 (modified: 18 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper671 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the comments. However, we respectfully disagree with a few points from the reviewer. We will address all concerns from the reviewer in the following form of Q/A.

Q1 (The first paragraph of review). A Stackelberg competition is a non-zero-sum game. From a game theoretic perspective, Stackelberg GAN still yields a 2-player zero-sum game. Hence, I have doubts about the general finding that more data generating components decreases the duality gap.
A1. Stackelberg competition can be a zero-sum game. There are many references supporting this claim. We cite some of them below. In fact, zero-sum Stackelberg games are equivalent to solving for the minimax equilibrium in zero-sum games. So, people usually don't talk about Stackelberg equilibrium in zero-sum games, instead they talk about minimax equilibrium. Here we mainly use the concept of leader-follower model in stackelberg game to represent the sequential adversarial process between one discriminator and multiple generators. Therefore, we respectfully disagree that the name Stackelberg GAN is misleading, since the underlying problem formulation is indeed a zero-sum Stackelberg game.
[1] Stackelberg Security Games: Looking Beyond a Decade of Success, 2018.
[2] <a href="http://coral.ise.lehigh.edu/wp-content/uploads/coralseminar/ipsem/talks/2005_06/scott_bicrit.pdf" target="_blank" rel="nofollow">http://coral.ise.lehigh.edu/wp-content/uploads/coralseminar/ipsem/talks/2005_06/scott_bicrit.pdf</a>
[3] Sequential Stackelberg Equilibria in Two-Person Games, 1988.

Q2 (The second paragraph of review). Suppose that the data generator components have the same network architecture. This would also imply that all data generator components find the same global best solution, in which case the gap would be identical to just using one of those components.
A2. We argue that having the same network architecture for each data generator component does not necessarily imply all data generator components find the same global best solution. Here we provide two reasons about this. (1) Neural network is highly non-convex: starting from different random initializations, each generator would converge to different solutions even with the same network architecture, as the globally optimal solutions might not be unique. (2) In Appendix D, we show that Stackelberg GAN can learn a mixture of distributions under standard assumptions as Goodfellow et al.’14. This implies that all generators cannot find the same globally best solution when each generator does not have enough capacity to learn the real data distribution but a mixture of generators has; otherwise, we have P_{G_{gamma_i}(z)}=P_d for all i, contradicting with the condition that “each generator does not have enough capacity to learn the real data distribution”. From (1) and (2), we see that “different initializations” and “generator capacity” are two factors which might prevent generators from finding the same solution.
Our main conclusion holds not only for the worst case, but also holds true for the practical cases. For example, in Figure 1 we use the same network architecture for all generator components. We do not observe the phenomenon that all data generator components find the same globally best solution as the reviewer mentioned.

Q3 (The third paragraph of review). If we assume a different family of mappings for each component, the convexity violation of the joint data generator is higher than for each component; hence, the gap does not necessarily decrease with more components.
A3. We respectfully disagree with the comment. Denote by Delta_i the convexity violation of the i-th generator and let Delta_max=max{Delta_1,…,Delta_I}. Our result shows that the convexity violation of the joint data generator (i.e., the duality gap) is no larger than Delta_max/I. Since Delta_max can be a bounded value, this shows that the gap decreases with more components. Indeed, the convexity violation of the joint data generator Delta_max/I is smaller than that of the most non-convex generator, and can even be smaller than the most convex generator when I is sufficiently large.

Q4 (The fourth paragraph of review) So why do multiple data generator components help in practice, and why does the proposed model outperform single-component GANs and the multi-branch GAN in the experiments?
A4. We answer both questions from the game-theoretic perspective in this paper --- when does approximate equilibrium exist. We believe both points of view (optimization and game theory) are worth studying, while we focus on the latter one. We argue that there are strong connections between the two points of view: if approximate Nash equilibrium does not exist as in the single-component GANs, all optimization methods would suffer from instability and finally fail. In contrast, our study shows that approximate Nash equilibrium exists for multi-component GANs and improves over Arora’s result. So, our study of Nash equilibrium serves as a necessary condition for the success of GANs.

We are looking forward to your re-evaluation based on our reply. Thanks for your consideration.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>