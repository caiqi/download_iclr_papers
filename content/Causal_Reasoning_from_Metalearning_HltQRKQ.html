<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Causal Reasoning from Meta-learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Causal Reasoning from Meta-learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1ltQ3R9KQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Causal Reasoning from Meta-learning" />
      <meta name="og:description" content="Discovering and exploiting the causal structure in data is a crucial challenge for intelligent agents. Though powerful formalisms for causal reasoning have been developed, applying them in..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1ltQ3R9KQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Causal Reasoning from Meta-learning</a> <a class="note_content_pdf" href="/pdf?id=H1ltQ3R9KQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019causal,    &#10;title={Causal Reasoning from Meta-learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=H1ltQ3R9KQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Discovering and exploiting the causal structure in data is a crucial challenge for intelligent agents. Though powerful formalisms for causal reasoning have been developed, applying them in real-world domains is often difficult because the frameworks make idealized assumptions. Here we explore whether modern deep reinforcement learning can be used to train agents to perform causal reasoning, without incorporating explicit principles of causal reasoning. We adopt a meta-learning approach, where the agent learns a policy for conducting experiments via causal interventions, in order to support a subsequent task which rewards making accurate causal inferences. We also found the agent could make sophisticated counterfactual predictions, as well as learn to draw causal inferences from passive observational data, when such inferences were possible. Our results suggest that applied causal reasoning in complex settings may benefit from powerful learning-based approaches. More generally, this work may offer new strategies for structured exploration in reinforcement learning, by providing agents with the ability to perform and interpret experiments.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">meta-learning, causal reasoning, deep reinforcement learning, artificial intelligence</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">meta-learn a learning algorithm capable of causal reasoning</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">11 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SyxDjPH10m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Promising paper on an appealing topic, but needs a bit more work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ltQ3R9KQ&amp;noteId=SyxDjPH10m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1376 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1376 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Note: This review is coming in a bit late, already after one round of responses. So I write this with the benefit of having read the helpful previous exchange. 

I am generally positive about the paper and the broader project. The idea of showing that causal reasoning naturally emerges from certain decision-making tasks and that modern (meta-learning) RL agents can become attuned to causal structure of the world without being explicitly trained to answer causal questions is an attractive one. I also find much about the specific paper elegant and creative. Considering three grades of causal sophistication (from conditional probability to cause-effect reasoning to counterfactual prediction) seems like the right thing to do in this setting.

Despite these positive qualities, I was confused by many of the same issues as other reviewers, and I think the paper does need some more serious revisions. Some of these are simple matters of clarification as the authors acknowledge; others, however, require further substantive work. It sounds like the authors are committed to doing some of this work, and I would like to add one more vote of encouragement. While the paper may be slightly too preliminary for acceptance at this time, I am optimistic that a future version of this paper will be a wonderful contribution.

(*) The authors say at several points that the approach “did not require explicit knowledge of formal principles of causal inference.” But there seem to be a whole of lot of causal assumptions that are critically implicit in the setup. It would be good to understand this better. In particular, the different agents are hardwired to have access to different kinds of information. The interventional agent is provided with data that the conditional agent simply doesn’t get to see. Likewise, the counterfactual agent is provided with information about noise. Any sufficiently powerful learning system will realize that (and even how) the given information is relevant to the decision-making task at hand. A lot of the work (all of the work?) seems to be done by supplying the information that we know would be relevant.

(*) Previous reviewers have already made this point - I think it’s crucial - and it’s also related to the previous concern: It is not clear how difficult the tasks facing these agents actually are, nor is it clear that solving them genuinely requires causal understanding. What seems to be shown is that, by supplying information that’s critical for the task at hand, a sufficiently powerful learning agent is able to harness that information successfully. But how difficult is this task, and why does it require causal understanding? I do think that some of the work the authors did is quite helpful, e.g., dividing the test set between the easy and hard cases (orphan / parented, unconfounded / confounded). But I do not feel I have an adequate understanding of the task as seen, so to say, from the perspective of the agent. Specifically:

(*) I completely second the worry one of the reviewers raised about equivalence classes and symmetries. The test set should be chosen more deliberately - not randomly - to rule out deflationary explanations of the agents’ purported success. I’m happy to hear that the authors will be looking more into this and I would be interested to know how the results look.

(*) The “baselines” in this paper are often not baselines at all, but rather various optimal approaches to alternative formulations of the task. I feel we need more actual baselines in order to see how well the agents of interest are doing. I don’t know how to interpret phrases like “close to perfect” without a better understanding of how things look below perfection. 

As a concrete case of this, just like the other reviewers, I was initially quite confused about the passive agents and why they did better than the active agents. These are passive agents who actually get to make multiple observations, rather than baseline passive agents who choose interventions in a suboptimal way. I think it would be helpful to compare against an agent who makes the same number of observations but chooses them in a suboptimal (e.g., random) way. 

(*) In relation to the existing literature on causal induction, it’s telling that implementing a perfect MAP agent in this setting is even possible. This makes me worry further about how easy these tasks are (again, provided one has all of the relevant information about the task). But it also shows that comparison with existing causal inference methods is simply inappropriate here, since those methods are designed for realistic settings where MAP inference is far from possible. I think that’s fine, but I also think it should be clarified in the paper. The point is not (at least yet) that these methods are competitors to causal inference methods that do “require explicit knowledge of formal principles of causal inference,” but rather that we have a proof-of-concept that some elementary causal understanding may emerge from typical RL tasks when agents are faced with the right kinds of tasks and given access to the right kinds of data. That’s an interesting claim on its own. The penultimate paragraph in the paper (among other passages) seems to me quite misleading on this point.

(*) One very minor question I have is why actions were softmax selected even in the quiz phase. What were the softmax parameters? And would some of the agents not perform a bit better if they maximized?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BkgPvmG96X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Potentially interesting, but possibly not ready yet</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ltQ3R9KQ&amp;noteId=BkgPvmG96X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1376 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1376 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper aims at training agents to perform causal reasoning with RL in three settings: observational (the agent can only obtain one observational sample at a time), interventional (the agent can obtain an interventional sample at a time for a given perfect intervention on a given variable) and a counterfactual setting (the agent can obtains interventional samples, but the prediction is about the case in which the same noise variables were sampled, but a different intervention was performed) . In each of these settings, after T-1 steps of information gathering, the algorithm is supposed to select the node with the highest value in the last step. Different types of agents are evaluated on a limited simulated dataset, with weak and not completely interpretable results.

Pros:
-Using RL to learn causal reasoning is a very interesting and worthwhile task.
-The paper tries to systematize the comparison of different settings with different available data.


Cons:
-Task does not necessarily require causal knowledge (predict the node with the highest value in this restricted linear setting)
-Very limited experimental setting (causal graphs with 5 nodes, one of which hidden, linear Gaussian with +/- 1 coefficients, with interventions in training set always +5, and in test set always -5) and lukewarm results, that don’t seem enough for the strong claims. This is one of the easiest ways to improve the paper.
-In the rare cases in which there are some causal baselines (e.g. MAP baseline), they seem to outperform the proposed algorithms (e.g. Experiment 2)
-Somehow the “active” setting in which the agent can decide the intervention targets seems to always perform worse than the “passive” setting, in which the targets are already chosen. This is very puzzling for me, I thought that choosing the targets should improve the results...
-Seem to be missing a lot of literature on causality and bandits, or reinforcement learning (for example: <a href="https://arxiv.org/abs/1606.03203," target="_blank" rel="nofollow">https://arxiv.org/abs/1606.03203,</a> https://arxiv.org/abs/1701.02789, http://proceedings.mlr.press/v70/forney17a.html)
-Many details were unclear to me and in general the clarity of the description could be improved

In general, I think the paper could be opening up an interesting research direction, but unfortunately I’m not sure it is ready yet. 


Details:
-Abstract: “Though powerful formalisms for causal reasoning have been developed, applying them in real-world domains is often difficult because the frameworks make idealized assumptions”. Although possibly true, this sounds a bit strong, given the paper’s results. What assumptions do your agents make? At the moment the agents you presented work on an incredibly small subset of causal graphs (not even all linear gaussian models with a hidden variable…), and it’s even not compared properly against the standard causal reasoning/causal discovery algorithms...
-Footnote 1: “this formalism for causal reasoning assumes that the structure of the causal graph is known” - (Spirtes et al. 2001) present several causal discovery (here “causal induction”) methods that recover the graph from data.
-Section 2.1 “X_i is a potential cause of X_j” - it’s a cause, not potential, maybe potentially not direct.
-Section 3: 3^(N-1)/2 is not the number of possible DAGs, that’s described by this sequence: https://oeis.org/A003024. Rather that is the number of (possibly cyclic) graphs with either -1, 1 or 0 on the edges. 
-“The values of all but one node (the root node, which is always hidden)” - so is it 4 or 5 nodes? Or it is that all possible DAGs on N=6 nodes one of which is hidden? I’m asking because in the following it seems you can intervene on any of the 5 nodes… 
-The intervention action is to set a given node to +5 (not really clear why), while in the quiz phase (in which the agent tries to predict the node with the highest variable) there is an intervention on a known node that is set to -5 (again not clear why, but different from the interventions seen in the T-1 steps). 
-Active-Conditional is only marginally below Passive-Conditional, “indicating that when the agent is allowed to choose its actions, it makes reasonable choices” - not really, it should perform better, not “marginally below”... Same for all the other settings
-Why not use the MAP baseline for the observational case?
-What data does the Passive Conditional algorithms in Experiment 2? Only observations  (so a subset of the data)?
-What are the unobserved confounders you mention in the results of Experiment 2? I thought there is only one unobserved confounder (the root node)? Where do the others come from?
-The counterfactual setting possibly lacks an optimal algorithm? 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1eTXE52TX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply from authors, thank you for the review (1/3)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ltQ3R9KQ&amp;noteId=S1eTXE52TX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1376 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1376 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">TL;DR: We will significantly improve clarity of our paper and will update our results with larger graphs, addressing the reviewers’ concerns about generalizability. 

We thank the reviewer for their interest, time, and detailed review! We feel that most of the criticisms are not fundamental concerns about the content but rather pertain to lack of clarity in the text. We are grateful for the opportunity to improve our exposition. We have made extensive clarifications below, and hope the reviewer will find our paper much improved as a result.

We have grouped some of the suggestions below so as to make responses more coherent.

--------------------------------------------------


&gt;&gt;-Task does not necessarily require causal knowledge (predict the node with the highest value in this restricted linear setting)


&gt;&gt;-Very limited experimental setting (causal graphs with 5 nodes, one of which hidden, linear Gaussian with +/- 1 coefficients, with interventions in training set always +5, and in test set always -5) and lukewarm results, that don’t seem enough for the strong claims. This is one of the easiest ways to improve the paper.

We agree that generalization to more settings is important  but we think that our setting is sufficient to answer the question posed in this work. We respectfully disagree that the simplicity of the domain means that causal knowledge is not required. That the agent successfully makes predictions about the effect of an intervention, makes correct causal inference despite the presence of a hidden confounder, and makes counterfactual predictions, are all hallmarks of causal knowledge. The agent outperforms the best possible non-causal algorithm. We also point to standard textbooks [Causal Inference in Statistics - A Primer, Judea Pearl, Madelyn Glymour, Nicholas P. Jewell, 2016] describing and analysing causal inference in exactly such linear Gaussian settings. A non-linear setting would possibly make this more challenging for the agent, but our main goal was to demonstrate that our meta-learning approach using model-free RL, can learn to exploit causal structure changing at each episode -- an entirely new area of research. We felt that this simple setting afforded the most unencumbered test for causal reasoning.

Nevertheless, we agree that some demonstration of the generalizability of our approach will strengthen our paper. In Appendix D, we present results on non-linear causal graphs, and in the revision we will include results with larger graphs. We used a test intervention (-5) far outside the learning distribution (always +5) because this is a strong test for having encoded the underlying causal graph. We agree that there are many ways to generalize our approach, and have made an effort to demonstrate it with some non-linear graphs and larger numbers of nodes;  but convincingly and fairly testing the limits of this generalizability would require using more sophisticated agents and training regimes, and is orthogonal to the purview of our current work.

--------------------------------------------------

&gt;&gt;-In the rare cases in which there are some causal baselines (e.g. MAP baseline), they seem to outperform the proposed algorithms (e.g. Experiment 2)

Our analyses are focused on looking for evidence that the RL agent takes advantage of causal information. The MAP baseline is an upper bound on performance. The key result for Experiment 2 is that the agent learns an important aspect of causal reasoning i.e. to resolve unobserved confounders with interventions. In Figure 4(a) we see that the agent with access to interventional data performs better than an agent with access to only observational data, reaching close to optimal MAP performance. Figure 4(b) shows that the performance increase is greater in cases where the intervened node shared an unobserved parent (a confounder) with other variables in the graph.

--------------------------------------------------


&gt;&gt;-Somehow the “active” setting in which the agent can decide the intervention targets seems to always perform worse than the “passive” setting, in which the targets are already chosen. This is very puzzling for me, I thought that choosing the targets should improve the results…

We apologize for not explaining this adequately in the manuscript. The intervention policy hard-coded into the passive agent (a single  intervention on each of the 4 observable nodes) is  near optimal. In the zero noise limit, it is optimal. The active agent, on the other hand, must learn a good exploration policy from scratch. This is an extra challenge that results in slightly worse performance. In future work, it might be interesting to examine domains where it is difficult to hand-craft an effective exploration policy, and determine whether the meta-RL approach can find such a policy.  We will improve our explanation of this issue in the revised text.

--------------------------------------------------

cont’d..
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1gd5N52pQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply from authors, thank you for the review (2/3) </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ltQ3R9KQ&amp;noteId=r1gd5N52pQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1376 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1376 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
&gt;&gt;-Seem to be missing a lot of literature on causality and bandits, or reinforcement learning (for example: <a href="https://arxiv.org/abs/1606.03203," target="_blank" rel="nofollow">https://arxiv.org/abs/1606.03203,</a> https://arxiv.org/abs/1701.02789, http://proceedings.mlr.press/v70/forney17a.html)

We thank the reviewer for these helpful references and will incorporate a better literature review in our updated manuscript.

--------------------------------------------------
&gt;&gt;-Many details were unclear to me and in general the clarity of the description could be improved

We hope that by addressing the reviewers concerns as detailed below, the clarity of the updated manuscript will be much improved.

--------------------------------------------------

&gt;&gt;In general, I think the paper could be opening up an interesting research direction, but unfortunately I’m not sure it is ready yet. 

&gt;&gt;Details:
&gt;&gt;-Abstract: “Though powerful formalisms for causal reasoning have been developed, applying them in real-world domains is often difficult because the frameworks make idealized assumptions”. Although possibly true, this sounds a bit strong, given the paper’s results. What assumptions do your agents make? At the moment the agents you presented work on an incredibly small subset of causal graphs (not even all linear gaussian models with a hidden variable…), and it’s even not compared properly against the standard causal reasoning/causal discovery algorithms…

We agree that our paper in its current form does not speak to improving causal reasoning in real-world domains. The goal of our paper is to demonstrate the proof of principle that causal reasoning can arise out of model-free reinforcement learning. The above statement was made to motivate the potential practical value of RL algorithms that can make fast, amortized causal inferences at run time, learned end-to-end from large and high dimensional data that might be intractable for traditional causal inference algorithms. However, we agree that this statement is strong and will edit it to make sure our contributions are more accurately represented.

--------------------------------------------------
&gt;&gt;-Footnote 1: “this formalism for causal reasoning assumes that the structure of the causal graph is known” - (Spirtes et al. 2001) present several causal discovery (here “causal induction”) methods that recover the graph from data.

It was not our intention to dismiss the literature, but to make clear to readers that the tasks our agent performs in the three experiments do not fully equate to the three levels of causal reasoning we formalize in the section on causality, as these assume that the graph structure is known. We will change this phrasing.

--------------------------------------------------


&gt;&gt;-Section 2.1 “X_i is a potential cause of X_j” - it’s a cause, not potential, maybe potentially not direct.

We are using the definition in the book Causal Inference in Statistics - A Primer, Judea Pearl, Madelyn Glymour, Nicholas P. Jewell, 2016, page 27:  “If X is a descendant of Y, then Y is a potential cause of X (there are rare intransitive cases in which Y will not be a cause of X, which we will discuss in Part Two).”, in the interest of being as general as possible in our introductory section on causality. 

--------------------------------------------------


&gt;&gt;-Section 3: 3^(N-1)/2 is not the number of possible DAGs, that’s described by this sequence: https://oeis.org/A003024. Rather that is the number of (possibly cyclic) graphs with either -1, 1 or 0 on the edges. 

Our sampling procedure was not adequately explained in our paper, we thank the reviewer for drawing our attention to our lack of clarity on this point. We have tried to delineate the process more clearly below, and will include an explanation in the updated manuscript.

We first consider the n*(n-1)/2 edges in the strictly upper-triangular part of the adjacency matrix. The number 3^(n*(n-1)/2) for the total number of graphs is derived from each of these edges independently having weights -1, 0, or 1. These are all guaranteed to be acyclic. We will revise to be more clear about what distribution of graphs we are sampling from.

--------------------------------------------------

cont’d...
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rylNDB5n6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply from authors, thank you for the review (3/3) </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ltQ3R9KQ&amp;noteId=rylNDB5n6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1376 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1376 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
&gt;&gt;-“The values of all but one node (the root node, which is always hidden)” - so is it 4 or 5 nodes? Or it is that all possible DAGs on N=6 nodes one of which is hidden? I’m asking because in the following it seems you can intervene on any of the 5 nodes… 

The DAGs consist of a total of 5 nodes, one of which is hidden. The agent can only intervene on 4 of the nodes. We will clarify this in the updated manuscript.

--------------------------------------------------

&gt;&gt;-The intervention action is to set a given node to +5 (not really clear why), while in the quiz phase (in which the agent tries to predict the node with the highest variable) there is an intervention on a known node that is set to -5 (again not clear why, but different from the interventions seen in the T-1 steps). 

Thanks for pointing out that we never explained our choices for these values. We will improve our exposition in the updated manuscript. 

The intervention action sets a node to a value (+5) outside the likely range of passive observations. This facilitates learning the causal graph. One of the important control conditions is an agent that receives samples from the graph conditioned on one of the nodes being +5, in order to directly assess the benefits of intervention. 

The intervention in the quiz phase sets a node to a value never before seen. This disallows the agent from memorizing the results of its interventions in the information phase. 

--------------------------------------------------

&gt;&gt;-Active-Conditional is only marginally below Passive-Conditional, “indicating that when the agent is allowed to choose its actions, it makes reasonable choices” - not really, it should perform better, not “marginally below”... Same for all the other settings

We hope to have clarified this in the explanation above.

--------------------------------------------------

&gt;&gt;-Why not use the MAP baseline for the observational case?

This is a good point. We will add a MAP baseline that records the optimal causal induction / discovery possible with purely observational data. However, the key observation from this experiment was that the conditional agent outperformed the optimal associative baseline. This indicates that that the conditional agent drew causal inferences from observational data (i.e., learned to perform do-calculus). This is highlighted by the fact that this improvement manifested only in test cases where do-calculus makes a prediction distinguishable from the predictions based on correlations. These are cases where the externally intervened node has a parent, so that graph surgery results in a different graph. 

--------------------------------------------------

&gt;&gt;-What data does the Passive Conditional algorithms in Experiment 2? Only observations (so a subset of the data)?

The Passive Conditional agent receives conditional samples from the distribution defined by the DAG. These samples are conditioned on one of the nodes having a value of +5. This is described in the subsection about conditional agents on Page 5 of the original manuscript. However, we appreciate that this description was somewhat buried. In the revision we draw more attention to this mechanism. 

--------------------------------------------------


&gt;&gt;-What are the unobserved confounders you mention in the results of Experiment 2? I thought there is only one unobserved confounder (the root node)? Where do the others come from?

The reviewer is right in noting that there is only one unobserved confounder. The plural was not intended to refer to multiple confounders within a single graph, however we realize that this was confusing, and we will remove the usage of the plural in the revision. 

--------------------------------------------------

&gt;&gt;-The counterfactual setting possibly lacks an optimal algorithm? 

There is an optimal algorithm in the counterfactual setting. Thanks for drawing attention to this. In the revision we will include a baseline that records the specific exogenous noise and draws the correct counterfactual prediction from it. Notwithstanding this, the key observation in this experiment was that the counterfactual agent earned more reward than the MAP baseline. This is sufficient to infer that the agent used information about the specific exogenous noise (i.e. counterfactual inference), and not just information about the causal structure. This observation is also consistent with the fact that the performance improvement manifested only in the presence of degenerate maximum valued nodes. 

We’d like to thank the reviewer again for their detailed and insightful comments, and look forward to their reply. Our manuscript has hugely benefited from this feedback.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_SJlspzn9nm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Promising but several shortcomings</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ltQ3R9KQ&amp;noteId=SJlspzn9nm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1376 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1376 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
The paper presents a meta-learning RL framework to train agents that
can learn and do causal reasoning.  The paper sets up three tasks for
agents to learn to do associational, interventional, and
counterfactual reasoning. The training/testing is done on all
5-variable graphs. The authors demonstrate how the agent can maximize
their rewards, which demonstrate that the agent might have learnt to
learn some causal structure and do reasoning in the data.

Review:

I think Causality is an important area, and seeing how RL can help in
any aspect is something really worth looking into.

However, I have a few qualms about the setting and the way the tasks
are modeled.

1. Why is the task to select the node with the highest "value"
(value=expected value?  the sample? what is it?) under some random
external intervention? It feels very indirect.

Why not explicitly model certain useful actions that directly query
the structure, such as:

- selecting nodes that are parents/children of a node
- evaluating p(x | y) or p(x | do(y))?

2. The way you generate test data might introduce biases:

- If you enumerate 3^(n(n-1)/2) DAGs, some of them will have loops.  Do you weed them out?
  Does it matter?

- How do you sample weights from {-1, 0, 1}? uniform?  What happens if
  wij = 0?  This introduces bias in your training data.  This means
  your distribution is over DAGs + weights, not just DAGs.

- Your training/test split doesn't take into account certain
  equivalence/symmetries that might be present in your training data,
  making it hard to rule out whether your agents are in effect
  memorizing training data, specially that the number of test graphs
  is so tiny (300, while test could have been in the thousands too):

Example, if you graph just has one causal connection with weight = 1:
  X1 -&gt; X2; X3; X4; X5, This is clearly equivalent to X2 -&gt; X1; X3; X4; X5.
  Or the structure X1 -&gt; X2 might be present in a larger graph, example with these two components:
  X1 -&gt; X2; X3 -&gt; X4 -&gt; X5;

3. Why such a low number of learning steps T (T=5 in paper) in each episode? no
experimentation over choice of T or discussion of this choice is
given.  And it is mentioned in the findings, in several cases, that
the active agent is only merely comparable to the passive agent, while
one would think active would be better. If T were reasonably higher
(not too low, not too high), one would expect to see a difference.

4. Although I have concerns listed above, something about Figure 2(a)
  seems to suggest that the agent is learning something.  I think if
  you had tried to probe into what the agent is actually learning, it
  would have clarified many doubts.

However, in Figure 2(c), if the black node is -5, why is the node
below left at -2.5?  The weight on the edge is 1 and other parent is
0, so -2.5 seems extremely unlikely, given that the variance is 0.1
(stdev ~ 0.3, so ~8 standard deviations away!).  (Similar issue in
Figure 3c)

5. Although the random choice would result in a score of -5/4, I think
  it's quite easy and trivial to beat that by just ignoring the node
  that's externally intervened on and assigned -5, given it's a small
  value. This probably doesn't require the agent to be able to do
  "causal reasoning" ...  That immediately gives you a lower bound of
  0.  That might be more appropriate.

  If you could give a distribution of the max(mean_i(Xi)) over all
  graphs (including weights in your distribution), it could give an
  idea of how unlikely it is for the agent to get a high score without
  actually learning the causal structure.

Suggestions for improving the work:

- Provide results on wider range of experiments (eg more even
  train-test split, choice of T), or at minimum justify choices
  made. And address the issues above.

- Focus on more intuitive notions that clearly require causal
  knowledge, or motivate your objective very clearly to show its
  sufficiency.

- Perhaps discuss simpler examples (e.g., 3 node), where it's easy to
  enumerate all causal structures and group them into appropriate
  equivalence classes.

- Please proof-read and make sure you've defined all terms (there are
  a few, such as Xp/Xf in Expt 3, where p/f are not really defined).

- You could show a few experiments with larger N by sampling from the space of all possible
  DAGs, instead of enumerating everything.

Of course, it would be great if you can probe the agent to see what it
really learnt. But I understand that could be a long-shot.

Another open problem  is whether this approach can scale to larger number of
variables, in particular the learning might be very data hungry.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HyeYcqHs6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply from authors, thank you for the review (1/3)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ltQ3R9KQ&amp;noteId=HyeYcqHs6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1376 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1376 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">TL;DR: We will significantly improve clarity of our paper and will update our results with larger graphs, addressing the reviewers’ concerns about train/test splits and scalability. 

We thank the reviewer for their detailed review! We think all of the points are readily addressable without fundamental changes to the paper. We hope that the reviewer will find our paper much improved based on the changes and clarifications detailed below.
--------------------------------------------------

&gt;&gt;1. Why is the task to select the node with the highest "value"
(value=expected value?  the sample? what is it?) under some random
external intervention? It feels very indirect.

&gt;&gt;Why not explicitly model certain useful actions that directly query
the structure, such as:

&gt;&gt;- selecting nodes that are parents/children of a node
&gt;&gt;- evaluating p(x | y) or p(x | do(y))?

&gt;&gt;The agent's reward was the value of the chosen node in the sample at that time step.

We did consider training the agent to perform explicit causal inference, but instead choose this more indirect objective to demonstrate that RL algorithms can learn to infer and utilize underlying causal structure when it is relevant to the rewarding task even when the task does not explicitly involve resolving that causal structure. This allows us to make a more general statement that also applies to the kinds of tasks prevalent in RL.

--------------------------------------------------

&gt;&gt;2. The way you generate test data might introduce biases:

&gt;&gt;- If you enumerate 3^(n(n-1)/2) DAGs, some of them will have loops.  Do you weed them out?
  Does it matter?

&gt;&gt;- How do you sample weights from {-1, 0, 1}? uniform?  What happens if
  wij = 0?  This introduces bias in your training data.  This means
  your distribution is over DAGs + weights, not just DAGs.

&gt;&gt;- Your training/test split doesn't take into account certain
  equivalence/symmetries that might be present in your training data,
  making it hard to rule out whether your agents are in effect
  memorizing training data, specially that the number of test graphs
  is so tiny (300, while test could have been in the thousands too):

&gt;&gt;Example, if you graph just has one causal connection with weight = 1:
  X1 -&gt; X2; X3; X4; X5, This is clearly equivalent to X2 -&gt; X1; X3; X4; X5.
  Or the structure X1 -&gt; X2 might be present in a larger graph, example with these two components:
  X1 -&gt; X2; X3 -&gt; X4 -&gt; X5;

Our sampling procedure was not adequately explained in our paper; we thank the reviewer for drawing our attention to our lack of clarity on this point. We have tried to delineate the process more clearly below, and will include an improved explanation in the updated manuscript.

We first consider the n*(n-1)/2 edges represented by the upper-diagonal of the adjacency matrix. Any graph that only contains only some subset of these edges is guaranteed to be a Directed Acyclic Graph, and contains no loops. The number 3^(n*(n-1)/2) for the total number of graphs is derived from each of these edges independently having weights -1, 0, or 1. As the reviewer pointed out, we are indeed sampling from a distribution of DAGs + weights. We do not uniformly sample over the space of DAGs, but rather we sample uniformly over the space of graphs formed by randomly assigning the edges in the upper triangular of the adjacency matrix uniformly from {0, -1, 1}. These are all guaranteed to be DAGs. Our sampling procedure means that nodes are more likely to be connected than if we had sampled the presence or absence of each edge uniformly, but this does not affect our results.

The observation about equivalence classes is an excellent point. We would like to point out however, that while such equivalences exist, they are not obvious to a neural network, and the examples outlined above by the reviewer cannot be solved with memorization. Nevertheless, we agree that generalization outside the equivalence class is a stronger claim and we will update our results with simulations where we exclude entire equivalence classes from the training set and test on these held-out classes. 

--------------------------------------------------

continued...</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1ldPsHiT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply from authors, thank you for the review (2/3)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ltQ3R9KQ&amp;noteId=S1ldPsHiT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1376 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1376 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt;&gt;3. Why such a low number of learning steps T (T=5 in paper) in each episode? no
experimentation over choice of T or discussion of this choice is
given.  And it is mentioned in the findings, in several cases, that
the active agent is only merely comparable to the passive agent, while
one would think active would be better. If T were reasonably higher
(not too low, not too high), one would expect to see a difference.

We thank the reviewer for this useful feedback. We choose an episode length of T = 5 (= number of nodes) with 4 learning steps and 1 test step since, in the noise-free limit, exactly 4 interventions (one on each of the 4 observable nodes) are sufficient to fully distill the causal structure required to get maximum performance on the test phase (since the hidden node is never intervened upon in the test phase). In a longer episode, training would be more difficult since the reward (only given once at the end of each episode) becomes sparser and credit assignment more challenging [1]. Meanwhile, in a shorter episode it would be impossible to infer the causal structure in general.

To clarify the relative performances of the passive and active agents: The intervention policy (a single intervention at each of the 4 observable nodes) of the passive agent is a good policy for this domain. In the limit of zero noise, as mentioned above, it is the optimal intervention policy. The active agent performs worse because in addition to learning to reason from the results of interventions, it must also learn an exploration policy.  Given a passive agent with a suboptimal fixed policy, in a task where smart exploration is crucial, indeed we expect the passive agent to perform worse than an agent that can actively learn an exploration policy. However, in this work, the passive policy is close to optimal and therefore acts as a benchmark for the active agent.

We will include clarification on both of these points in our updated manuscript. 

-------------------

&gt;&gt;4. Although I have concerns listed above, something about Figure 2(a)
  seems to suggest that the agent is learning something.  I think if
  you had tried to probe into what the agent is actually learning, it
  would have clarified many doubts.

&gt;&gt;However, in Figure 2(c), if the black node is -5, why is the node
below left at -2.5?  The weight on the edge is 1 and other parent is
0, so -2.5 seems extremely unlikely, given that the variance is 0.1
(stdev ~ 0.3, so ~8 standard deviations away!).  (Similar issue in
Figure 3c)

We appreciate the attention to detail! We did find an error in Figure 4(c) and have addressed it in the new version of the manuscript. Figure 2(c) however seems correct. The mean value of a child node is given by the weighted mean of its parents’ values.  So -2.5 = (-5.0 x 1 + 0.0 x 1)/2.

Regarding “what the agent is actually learning”:

In Section 4.1 and Figure 2, we show that the agent learns to perform some do-calculus. In Figure 2(a) we see that, compared to the highest possible reward achievable without causal knowledge, the trained agent received more reward. This observation is corroborated by Figure 2(b) which shows that performance increased selectively in cases where do-calculus made a prediction distinguishable from the predictions based on correlations. These are situations where the externally intervened node had a parent -- meaning that the intervention resulted in a different graph.

In Section 4.2 and Figure 4, we show that the agent learns to resolve unobserved confounders using interventions (a feat impossible with only observational data). In Figure 4(a) we see that the agent with access to interventional data performs better than an agent with access to only observational data. Figure 4(b) shows that the performance increase is greater in cases where the intervened node shared an unobserved parent (a confounder) with other variables in the graph. In this section we also compare the agent’s performance to a MAP estimate of the causal structure and find that the agent’s performance matches it, indicating that the agent is indeed doing close to optimal causal inference.

In Section 4.3 and Figure 6, we show that the agent learns to use counterfactuals. In Figure 6(a) we see that the agent with additional access to the specific randomness in the test phase performs better than an agent with access to only interventional data. In Figure 6(b), we find that the increased performance is observed only in cases where the maximum mean value in the graph is degenerate, and optimal choice is affected by the exogenous noise -- i.e. where multiple nodes have the same value on average and the specific randomness can be used to distinguish their actual values in that specific case.

In all of these three cases we also show an example graph (in Figures 2(c), 4(c), and 6(c)) where the agent’s behavior demonstrates what it has learned in each experiment.

We will make these three points more directly in the revised text.

contd.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Syl5shBjpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply from authors, thank you for the review (3/3)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ltQ3R9KQ&amp;noteId=Syl5shBjpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1376 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1376 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
&gt;&gt;5. Although the random choice would result in a score of -5/4, I think
  it's quite easy and trivial to beat that by just ignoring the node
  that's externally intervened on and assigned -5, given it's a small
  value. This probably doesn't require the agent to be able to do
  "causal reasoning" ...  That immediately gives you a lower bound of
  0.  That might be more appropriate.

We agree that this is a better baseline. In the original submission we described it in Appendix A. In the revision we will move it to the main text.

--------------------------------------------------

&gt;&gt;If you could give a distribution of the max(mean_i(Xi)) over all
  graphs (including weights in your distribution), it could give an
  idea of how unlikely it is for the agent to get a high score without
  actually learning the causal structure.

Thanks for this suggestion. We will add a plot showing the distributions of the values of mean_i(Xi) and max(mean_i(Xi)) over all graphs.
--------------------------------------------------

&gt;&gt;Suggestions for improving the work:

We have grouped some of the suggestions below so as to make responses more concise.
--------------------------------------------------

&gt;&gt;- Focus on more intuitive notions that clearly require causal
  knowledge, or motivate your objective very clearly to show its sufficiency.

&gt;&gt;Of course, it would be great if you can probe the agent to see what it
really learnt. But I understand that could be a long-shot.

We have tried to clarify above our analysis of what the agent learned.

--------------------------------------------------

&gt;&gt;- Perhaps discuss simpler examples (e.g., 3 node), where it's easy to
  enumerate all causal structures and group them into appropriate
  equivalence classes.

In the revision, we will explicitly partition equivalence classes between training and testing, as described above. We will then also show examples of 3-node graphs at test time whose equivalence class was excluded from training.

--------------------------------------------------

&gt;&gt;- Provide results on wider range of experiments (eg more even
  train-test split, choice of T), or at minimum justify choices
  made. And address the issues above.


&gt;&gt;- You could show a few experiments with larger N by sampling from the space of all possible
  DAGs, instead of enumerating everything.

&gt;&gt;Another open problem  is whether this approach can scale to larger number of
variables, in particular the learning might be very data hungry.


We hope to have addressed the reviewers' concerns about the setup in the replies above. The goal of our paper was to show the first evidence that causal reasoning can arise from model-free RL. We also demonstrate that the same agent discovers a good experimentation policy from which it learns the environment’s causal structure. This lays the groundwork for sophisticated reinforcement learning agents that can actively interact with and learn about their environments. Our synthetic environment of 5 node random DAGs is simple enough to concretely demonstrate causal reasoning, as well as to benchmark the learned intervention policy; but is not tailored so specifically to causal inference as to lose connection to typical RL tasks. While scalability is an important question, we think that it is outside the scope of this paper.

 However, in the revision, we will discuss issues related to scaling: a) compositional agent architectures might allow the agent to utilize symmetries such as equivalence classes, thus lowering the training data requirements [2], and b) more advanced training regimes might alleviate the credit assignment problem, allowing successful training with longer episode lengths [3].

--------------------------------------------------

&gt;&gt;- Please proof-read and make sure you've defined all terms (there are
  a few, such as Xp/Xf in Expt 3, where p/f are not really defined).
	
These points are duly noted and we will address them in our updated manuscript. 

We’d like to thank the reviewer again for their detailed and insightful comments, and look forward to their reply. Our manuscript has hugely benefited from this feedback.


[1] Hochreiter, S., Bengio, Y., Frasconi, P., &amp; Schmidhuber, J. (2001). Gradient flow in recurrent nets: the difficulty of learning long-term dependencies.
[2] Battaglia, P. W., Hamrick, J. B., Bapst, V., Sanchez-Gonzalez, A., Zambaldi, V., Malinowski, M., ... &amp; Gulcehre, C. (2018). Relational inductive biases, deep learning, and graph networks.
[3] Bengio, Y., &amp; Frasconi, P. (1994). Credit assignment through time: Alternatives to backpropagation.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_H1gHF1d4nQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good paper on important topic</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ltQ3R9KQ&amp;noteId=H1gHF1d4nQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1376 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1376 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This submission is an great ablation study on the capabilities of modern reinforcement learning to discover the causal structure of a synthetic environment. The study separates cases where the agents can only observe or they can also act, showing the expected gains of active intervention.

The experiments are so far synthetic, but it would be really interesting to see how the lessons learned extend to more realistic environments. It would also be very nice to have a sequence of increasingly complex synthetic environments where causal inference is the task of interest, such that we can compare the performance of different RL algorithms in this task (the authors only used one).

I would change the title to "Causal Reasoning from Reinforcement Learning", since "meta-learning" is an over-loaded term and I do not clearly see its prevalence on this submission.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkeS2DrjpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a> Reply from authors, thank you for the review.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ltQ3R9KQ&amp;noteId=rkeS2DrjpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1376 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1376 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for their interest and time. Please find below our responses to their suggestions and comments.

&gt;&gt;The experiments are so far synthetic, but it would be really interesting to see how the lessons learned extend to more realistic environments. It would also be very nice to have a sequence of increasingly complex synthetic environments where causal inference is the task of interest, such that we can compare the performance of different RL algorithms in this task (the authors only used one).
--------------------------------------------------
We strongly agree that scaling our method to complex and realistic environments is worthwhile. However, we think it is outside the scope of this paper. Our goal here is to determine whether it is possible to learn a causally-aware algorithm through model-free reinforcement learning. For this purpose, a simple environment -- and correspondingly simple agent architecture -- facilitates interpretation. We therefore think it is important to start here. The current results already stretch the page limit, so we plan to follow up with scaling results in a separate paper. 

&gt;&gt;I would change the title to "Causal Reasoning from Reinforcement Learning", since "meta-learning" is an over-loaded term and I do not clearly see its prevalence on this submission.
--------------------------------------------------
We thank the reviewer for this great suggestion, and will accordingly update our title.

We’d like to thank the reviewer again for their time and encouragement, and look forward to hearing back!
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>