<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Exploiting Cross-Lingual Subword Similarities in Low-Resource Document Classification | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Exploiting Cross-Lingual Subword Similarities in Low-Resource Document Classification" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=S1e_H3AqYQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Exploiting Cross-Lingual Subword Similarities in Low-Resource..." />
      <meta name="og:description" content="Text classification must sometimes be applied in situations with no training data in a target language.  However, training data may be available in a related language.  We introduce a cross-lingual..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_S1e_H3AqYQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Exploiting Cross-Lingual Subword Similarities in Low-Resource Document Classification</a> <a class="note_content_pdf" href="/pdf?id=S1e_H3AqYQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019exploiting,    &#10;title={Exploiting Cross-Lingual Subword Similarities in Low-Resource Document Classification},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=S1e_H3AqYQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Text classification must sometimes be applied in situations with no training data in a target language.  However, training data may be available in a related language.  We introduce a cross-lingual document classification framework CACO between related language pairs.  To best use limited training data, our transfer learning scheme exploits cross-lingual subword similarity by jointly training a character-based embedder and a word-based classifier.  The embedder derives vector representations for input words from their written forms, and the classifier makes predictions based on the word vectors.  We use a joint character representation for both the source language and the target language, which allows the embedder to generalize knowledge about source language words to target language words with similar forms.  We propose a multi-task objective that can further improve the model if additional cross-lingual or monolingual resources are available.  CACO models trained under low-resource settings rival cross-lingual word embedding models trained under high-resource settings on related language pairs.
</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">cross-lingual transfer, character-based method, low-resource language</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a cross-lingual document classification framework for related language pairs.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rJe977-j2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Straightforward model, sub-par experiment setup</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1e_H3AqYQ&amp;noteId=rJe977-j2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1554 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1554 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes to transfer document classifiers between (closely) related languages by exploiting cross-lingual subword representations in a cross-lingual embedder jointly with word-based classifier: the embedder represents the words, while the classifier labels the document. The approach is reasonable, albeit somewhat unexciting, as the basic underlying ideas are in the vein of Pinter et al. (2017), even if applied on a different task.

The main concern I have with the paper is that it leaves much open in terms of exploring the dimension of (dis)similarity: How does the model perform when similarity decreases across language pairs in the transfer? The paper currently offers a rather biased view: the couplings French-Italian-Spanish, Danish-Swedish are all very closely related languages, and Amharic-Tigrinya are also significantly related. Outside these couplings, there's a paragraph to note that the method breaks down (Table 5 in the appendix). Sharing between Romance and Germanic languages is far from representative of "loosely related languages", for all the cross-cultural influences that the two groups share.

While the experiment is reasonably posed, in my view it lacks the cross-lingual breadth and an empirical account of similarity. What we do in cross-lingual processing is: port models from resource-rich to low-resource languages, and to port between very similar languages that already have resources is a purely academic exercise. This is not to say that evaluation by proxy should be banned, but rather that low-resource setups should be more extensively controlled for.

Thus, in summary, a rather straightforward contribution to computational modeling paired with sub-par experiment setup in my view amounts to a rejection. The paper can be improved by extending the experiment and controlling for similarity, rather than leaving it as implication.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hylf35yihX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Need more insights</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1e_H3AqYQ&amp;noteId=Hylf35yihX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1554 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1554 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: The authors address the task of cross language document classification when there is no training data available in the target language but data is available a closely related language. The authors propose forming character-based embeddings of words to make use of sub-word similarities in closely-related languages. The authors do an extensive evaluation using various combinations of related languages and show improved performance. In particular,  the performance is shown to be competitive with word-based models, which are tied to a requirement of resources involving the original language (such as MT systems, bilingual lexicons, etc). The authors show that their results are boosted when some additional resources (such as bilingual dictionaries of minimal size) are used in a multi-task learning setup.


- I would have liked to see some comparison where your model also uses all the resources available to CLWE based models (for example, larger dictionary, larger parallel corpus, etc)

- It is mentioned that you used parallel projection only for Amharic as for other languages you had enough RCV2 training data. However, it would be interesting to see if you still use parallel projection on top of this.

- I do not completely agree with the statement that CACO models are "not far behind" DAN models. IN Table 1, for most languages the difference is quite high. I understand that your model uses fewer resources but can it bridge the gap by using more resources? Is the model capable of doing so ?

- How did you tune the lambdas in Eqn 11? Any interesting insights from the values of these lambdas? Do these lambda values vary significantly across languages ?

- The argument about why the performance drops when you use language identifiers is not very convincing. Can you please elaborate on this ?

- Why would the performance be better in one directions as compared to another (North Germanic to Romance v/s ROmance to North Germanic). Some explanation is needed here.

- One recurring grievance that I have is that there are no insights/explanations for any results. Why are the gains better for some language pairs? Why is there asymmetry in the results w.r.t direction of transfer ? In what way do 2 languages help as compared to single source language? What is you use more that 2 source languages?

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1gw8ImdhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Well-presented but some shortcomings in experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1e_H3AqYQ&amp;noteId=S1gw8ImdhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1554 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1554 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Overview:

This paper proposes an approach to document classification in a low-resource language using transfer learning from a related higher-resource language. For the case where limited resources are available in the target low-resource language (e.g. a dictionary, pretrained embeddings, parallel text), multi-task learning is incorporated into the model. The approach is evaluated in terms of document classification performance using several combinations of source and target language.

Main strengths:

1. The paper is well written. The model description in Section 2 is very clear and precise.
2. The proposed approach is simple but still shows good performance compared to models trained on corpora and dictionaries in the target language.
3. A large number of empirical experiments are performed to analyse different aspects and the benefits of different target-language resources for multi-task learning.

Main weaknesses:

1. The application of this model to document classification seems to be new (I am not a direct expert in document classification), but the model itself and the components are not (sequence models, transfer learning and multitask learning is well-established). So this raises a concern about novelty (although the experimental results are new).

2. With regards to the experiments, it is stated repeatedly that the DAN model which are compared to uses "far more resources." The best ALL-CACO model also relies on several annotated but "smaller" resources (dictionaries, parallel text, embeddings). Would it be possible to have a baseline where a target-language model is trained on only a small amount of annotated in-domain document classification data in the target language? I am proposing this baseline in order to answer two questions. (i) Given a small amount of in-domain data for the task at hand, how much benefit do we get from additionally using data from a related language? (ii) How much benefit do we get from using target-language resources that do not address the task directly (dictionaries, embeddings) compared with using a "similar" amount of data from the specific task?

Overall feedback:

This is a well-written paper, but I think since the core of the paper lies in its empirical evaluation, the above experiments (or something similar) would greatly strengthen the work.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>