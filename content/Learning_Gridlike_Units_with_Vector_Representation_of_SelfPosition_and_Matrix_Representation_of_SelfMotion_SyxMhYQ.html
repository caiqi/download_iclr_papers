<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learning Grid-like Units with Vector Representation of Self-Position and Matrix Representation of Self-Motion | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learning Grid-like Units with Vector Representation of Self-Position and Matrix Representation of Self-Motion" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Syx0Mh05YQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learning Grid-like Units with Vector Representation of..." />
      <meta name="og:description" content="This paper proposes a simple model for learning grid-like units for spatial awareness and navigation. In this model, the self-position of the agent is represented by a vector, and the self-motion..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Syx0Mh05YQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning Grid-like Units with Vector Representation of Self-Position and Matrix Representation of Self-Motion</a> <a class="note_content_pdf" href="/pdf?id=Syx0Mh05YQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019learning,    &#10;title={Learning Grid-like Units with Vector Representation of Self-Position and Matrix Representation of Self-Motion},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Syx0Mh05YQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=Syx0Mh05YQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">This paper proposes a simple model for learning grid-like units for spatial awareness and navigation. In this model, the self-position of the agent is represented by a vector, and the self-motion of the agent is represented by a block-diagonal matrix. Each component of the vector is a unit (or a cell). The model consists of the following two sub-models. (1) Motion sub-model. The movement from the current position to the next position is modeled by matrix-vector multiplication, i.e., multiplying the matrix representation of the motion to the current vector representation of the position in order to  obtain the vector representation of the next position. (2) Localization sub-model. The adjacency between any two positions is a monotone decreasing function of their Euclidean distance, and the adjacency is modeled by the inner product between the vector representations of the two positions. Both sub-models can be implemented by neural networks. The motion sub-model is a recurrent network with dynamic weight matrix, and the localization sub-model is a feedforward network. The model can be learned by minimizing a loss function that combines the loss functions of the two sub-models. The learned units exhibit grid-like patterns (as well as stripe patterns) in both 2D and 3D environments. The learned model can be used for path integral and path planning. Moreover, the learned representation is capable of error correction.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">5 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Hkgw09yBaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Surprising new result (continued): multiple hexagon blocks with automatically learned metrics</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syx0Mh05YQ&amp;noteId=Hkgw09yBaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1317 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1317 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Reviewers, 

We have uploaded the second revision that includes a new Subsection 5.2 on learning multiple blocks of grid cells where the metrics or grid sizes alpha_k are automatically learned. 

Figure 2.a shows the learned blocks and their learned metrics alpha_k. You can see that the learned blocks again show hexagon patterns, and different blocks have different metrics or grid sizes. The metrics are explicitly defined as the curvatures of the local kernels and are learned together with the vector and matrix representations. 

In Figure 2.a, the number of cells in each block is 6. In Appendix D, we show the learned blocks with different numbers of cells. As long as the number is greater than or equal to 6, the hexagon patterns emerge (for smaller number, the learned cells tend to exhibit square lattice patterns). 

Figure 2.b shows the heat maps &lt;v_k, v_k(x)&gt; for inferring the location of v = (v_k, k = 1, ..., K). While individual heat maps have multiple firing locations, they add up to the Gaussian kernel with a unique location. The global Gaussian kernel is used to regulate the metrics of different constituent blocks, who vote for the inferred position by their heat maps. 

To summarize, our model, while being very simple, explains the following aspects of grid cells at the computational (not necessarily neuroscience) level: (1) hexagon grid patterns. (2) metrics or grid sizes. (3) path integral. (4) path planning. (5) error correction. 

We removed the original Subsection 5.2 on learning multiple blocks in the original version. We also shortened the discussion to stay within the page limit. 

We will continue to revise our paper according to your advice, and we will reply to your comments soon. 

Thank you for your consideration. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJxh001bTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Surprising new result: hexagon and metric, with theoretical analysis</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syx0Mh05YQ&amp;noteId=rJxh001bTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1317 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1317 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Reviewers, 

Thank you for your precious time and insightful comments. 

We have uploaded the first revision to include a new result that we find surprising, interesting and important. Please see Subsection 5.1 of the first revision. 

To summarize, we learn a single block of cells with a generic local kernel. Recall the adjacency A(x, y)=f(|x-y|). A second order Taylor expansion of f(r) at r = 0 gives us f(r) = 1-alpha r^2, for small r, where 2 alpha is the curvature of f(r) at 0. The first derivative is 0 because f(r) reaches maximum at 0. We use the localization loss term 

|&lt;v(x), v(y)&gt; - (1-alpha |x-y|^2)|^2,  for |x-y|^2 &lt;= 1.5/alpha. 

Together with the motion loss, our learning method unfailingly produces hexagon patterns, and alpha determines the metric or grid size. The hexagon patterns emerge as long as the number of units is greater than or equals to 6 (for smaller number we may learn rectangle patterns). 

We also provide a theoretical solution when the number of units equals to 6, based on a tight frame in 2D. 

We want to emphasize that both the localization loss and the motion loss are LOCAL, and yet the global hexagon patterns always emerge. Our loss function does not assume any global periodic pattern. It is perhaps the simplest loss function one can find: (1) a second order Taylor expansion for localization loss. (2) a matrix-vector product for motion loss. This is really minimalistic. That is, what we put in is far less than what we get out. 

We believe this is the most important result of our work, because after all, the grid cells are characterized by hexagon patterns of different sizes. This is why they are called grid cells in the first place. We now can explain this crucial piece of puzzle. 

We will incorporate this local kernel loss term into our original global kernel loss so that we will learn the metric alpha for each block automatically. 

To save space, we moved the 3D path planning to appendix. We also added 1D result in appendix. The 1D result can be interpreted as time2vec or time stamp for events. 

A few key points: 

We shall reply to your valuable comments soon and further revise our paper according to your comments and advice. But first please allow us to make a few key points here: 

(1)	In our representation scheme, we NEVER represent the coordinates x = (x1, x2) explicitly. We only represent the position by heat map or one-hot map. Without explicit coordinates, it is not a trivial task to do path planning, and it is very different from path planning in robotics based on explicit coordinates. 
(2)	About path planning. Consider a rat leaves his home to forage. He needs path integration to know where he is. But MORE importantly, when he needs to go back home, he needs path planning. Even when he is standing still, his grid cells are changing during path planning, i.e., he is imagining or fantasizing the steps. Our proposed steepest ascent algorithm is of this nature.  The rat can also fantasize much bigger step sizes beyond his physical capability in path planning, and our method enables him to do that, see Figure 4(a) for straight path planning. 
(3)	About error correction. When talking to people in CS and robotics, a common question is: how come the brain does not use two neurons to represent the two coordinates x = (x1, x2), and instead use many neurons to represent the position. Our error correction experiment may give a justification. The dropout experiment also points to the possibility that the grid cells can work asynchronously, which is typical of biological neural system. We shall explore this issue further. 

We shall reply to your comments and upload the second revision soon. 

Thank you for your consideration of our first revision and first reply. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJeFrS4J6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A simple and elegant approach to grid cells that begs for theoretical insight</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syx0Mh05YQ&amp;noteId=rJeFrS4J6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1317 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1317 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a simple and elegant approach to learning "grid-cell like" representations that uses a high-dimensional encoding of position, together with a matrix for propagating position that involves only local connections among the elements of the vector.  The vectors are also constrained to have their inner products reflect positional similarity.  The paper also shows how such a representation may be used for path planning.

By stripping away the baggage and assumptions of previous approaches, I feel this paper starts to get at the essence of what drives the formation of grid cells.   It is still steps away from having direct ties to neurobiology, but is trying to get at the minimal components necessary for bringing about a grid cell like solution.  But I feel the paper also stops just a few steps short of developing a fuller theoretical understanding of what is going on.  For example the learned solution is quite Fourier like, and we know that Fourier transforms are good for representing position shift in terms of phase shift.  That would correspond to block size of two (i.e., complex numbers) in terms of this model.  So what's wrong with this solution (in terms of performance) and what is gained by having block size of six, beyond simply looking more grid like?  It would be nice to go beyond phenomenology and look at what the grid-like solution is useful for.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1ls-yVtnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Elegant but simplistic model for grid cells; unnecessary extension to path planning</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syx0Mh05YQ&amp;noteId=S1ls-yVtnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1317 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1317 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper builds upon the recent work on computational models of grid cells that rely on trainable (parametric) models such as recurrent neural networks [Banino et al, 2018; Cueva &amp; Wei, 2018]. It focuses entirely on path integration in 2D and 3D, from velocity inputs only, and it relies on two sub-networks: the motion model (an RNN) and the localization model (a feed-forward network). The avowed goal of the paper is to build a very simple and linear model for grid cells.

By linearly embedding the position x into a high-dimensional hidden vector v(x) (e.g., 96 elements), it can model motion using a linear model relying on matrix-vector multiplication: v(x + dx) = M(dx) v(x), where dx is the 2D or 3D displacement, v(.) is a vector and M(.) is a matrix. The embeddings v(.) are learnable and the paper assumes a square or cubic grid of N*N or N*N*N possible positions x (with N=40); these embeddings are also normalized to unit length and obey the kernel constraint that the dot-product between any two positions' vectors v(x) and v(y) is a Gaussian or an exponential function. The motion matrix is represented as block diagonal, where each block is a rotation of subvector v_k(x) into v_k(x + dx), where each block corresponds to a specific grid cell, and where the diagonal block is further expressed as a quadratic function of dx_1, dx_2, dx_3 elements of the displacement vector.

The strengths of the paper are that:
1) The supervision of the localization subnetwork only depends on Euclidean proximity between two positions x and y and therefore uses relative positions, not absolute ones. Similarly, the path integration supervision of the motion model uses only relative displacements.
2) The resulting rate maps of the hidden units seem perfect; the model exhibits multi-scale grid behaviour.
3) The idea of using disentangled blocks, rather than injecting noise or using dropout and a softmax bottleneck as in [Banino et al, 2018], is interesting.
4) The model accumulates little path integration error over 1000 step-long episodes.

The weakness of the paper is its simplicity:
1) The assumption that A(x, y) can be modeled by a Gaussian or exponential (Laplacian?) kernel is limiting, in particular for positions x and y that are far apart.
2) There is no discussion about what egocentric vs. allocentric referentials, and dx is assumed to be aligned with (x, y) axes (which are also the axes defining the bounding box of the area).
3) Unlike the other work on learning path integration using an RNN, the linear matrix model can only handle allocentric displacements dx_1, dx_2 (and optional dx_3 in 3D).
4) No consideration is given to non-square areas: would the network also exhibit grid-like behavior if the area was circular?
5) What happens if the quadratic parameterisation of block diagonals is dropped?
6) The paper did not use metrics accepted in the neuroscience community for computing a gridness score of the grid cells (although the grid cell nature is evident). There should however be metrics for quantifying how many units represent the different scales, offsets and orientations.
7) The authors did not consider (but mentioned) embedding locations from vision, and did not consider ambiguous position embeddings.

The experiments about path planning are unconvincing. First of all, the algorithm requires to input absolute positions of every obstacle into equation (9) - (10), which assumes that there is perfect information about the map. Secondly, the search algorithm is greedy and it is not obvious how it would handle a complex maze with cul-de-sac. Saying that "there is no need for reinforcement learning or sophisticated optimal control" is very misleading: the problem here is simplified to the extreme, and fully observed, and any comparison with deep RL algorithms that can handle partial observations is just out of place.

In summary, the authors have introduced an interesting and elegant model for grid cells that suffers from simplifications. The part on path planning should be cut and replaced with more analysis of the grid cells and an explanation of how the model would handle egocentric velocity.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SkeifEYIhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The motivation for this work needs to be clarified</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syx0Mh05YQ&amp;noteId=SkeifEYIhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1317 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1317 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">

=Major Comments=
The prior work on grid cells and deep learning makes it clear that the goal of the work is to demonstrate that a simple learning system equipped with representation learning will produce spatial representations that are grid-like. Finding grid-like representations is important because these representations occur in the mammalian brain. 

Your paper would be improved by making a similar argument, where you would need to draw much more explicitly on the neuroscience literature. Namely, the validation of your proposed representations for position and velocity are mostly validated by the fact that they yield grid-like representations, not that they are useful for downstream tasks.

Furthermore, you should better justify why your simple model is better than prior work? What does the simplicity provide? Interpretability? Ease if optimization? Sample complexity for training?

This is important because otherwise it is unclear why you need to perform representation learning. The tasks you present (path integral and planning) could be easily performed in basic x-y coordinates. You wouldn’t need to introduce a latent v. Furthermore, this would mprove your argument for the importance of the block-diagonal M, since it would be more clear why interpretability matters.


Finally, you definitely need to discuss the literature on randomized approximations to RBF kernels (random Fourier features). Given the way you pose the representation learning objective, I expect that these would be optimal. With this, it is clear why grid-like patterns would emerge.

=Additional Comments=
What can you say about the quality of the path returned by (10)? Is it guaranteed to converge to a path that ends at y? Is it the globally optimal path? 

I don’t agree with your statement that your approach enables simple planning by steepest descent. First of all, are the plans that your method outputs high-quality? Second, if you had solved (10) directly in x-y coordinates, you could have done this easily since it is an optimization problem in just 2 variables. That could be approximately solved by grid search.

I would remove section 5.4. The latent vector v is a high-dimensional encoding of low-dimensional data, so of-course it is robust to corruptions. The corruptions you consider don’t come from a meaningful noise process, however? I can imagine, for example, that the agent observes corrupted versions of (x,y), but why would v get corrupted?

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>