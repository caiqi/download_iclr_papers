<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>A Unified View of Deep Metric Learning via Gradient Analysis | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="A Unified View of Deep Metric Learning via Gradient Analysis" />
        <meta name="citation_author" content="Xun Wang" />
        <meta name="citation_author" content="Xintong Han" />
        <meta name="citation_author" content="Weilin Huang" />
        <meta name="citation_author" content="Dengke Dong" />
        <meta name="citation_author" content="Matthew R. Scott" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Skf5qiC5KQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="A Unified View of Deep Metric Learning via Gradient Analysis" />
      <meta name="og:description" content="Loss functions play a pivotal role in deep metric learning (DML). A large variety of loss functions have been proposed in DML recently. However, it remains difficult to answer this question: what..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Skf5qiC5KQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A Unified View of Deep Metric Learning via Gradient Analysis</a> <a class="note_content_pdf" href="/pdf?id=Skf5qiC5KQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=xunwang%40malong.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="xunwang@malong.com">Xun Wang</a>, <a href="/profile?email=xinhan%40malong.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="xinhan@malong.com">Xintong Han</a>, <a href="/profile?email=whuang%40malong.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="whuang@malong.com">Weilin Huang</a>, Dengke Dong, Matthew R. Scott</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Loss functions play a pivotal role in deep metric learning (DML). A large variety of loss functions have been proposed in DML recently. However, it remains difficult to answer this question: what are the intrinsic differences among these loss functions?This paper answers this question by proposing a unified perspective to rethink deep metric loss functions. We show theoretically that most DML methods in deep metric learning, in view of  gradient equivalence, are essentially weight assignment strategies of training pairs. Based on this unified view, we revisit several typical DML methods and disclose their hidden drawbacks. Moreover, we point out the key components of an effective DML approach which drives us to propose our weight assignment framework. We evaluate our method on image retrieval tasks, and show that it outperforms  the state-of-the-art DML approaches by a significant margin on the CUB-200-2011, Cars-196, Stanford Online Products and In-Shop Clothes Retrieval datasets. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">metric learning, gradient equivalence, image retrieval</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SygscRF06X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skf5qiC5KQ&amp;noteId=SygscRF06X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper558 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper558 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJxi3xfe6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Why did you use InceptionBN?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skf5qiC5KQ&amp;noteId=SJxi3xfe6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper558 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">To best my knowledge, only LiftedStructed use InceptionBN network and all the other methods use Inception-v1.
(Sampling matters in deep embedding learning use ResNet-50. However when i compare the distance weighted sampling versus random negative, it increase the performance on inception-v1.)
When I tested on both networks, the recall is significantly different. 

Can you provide the ablation study on both networks?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkljRbJuam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>LIftedStructure doesn't use Inception-BN </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skf5qiC5KQ&amp;noteId=SkljRbJuam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018 (modified: 18 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper558 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">In fact, BN-Inception is one of the  most frequently used network architectures( proxy-nca [1], HTL[2], clustering[3]) . Moreover,  LiftedStructure  uses inception-v1 (GoogleNet), not BN-inception. You can refer to these papers for mored details.

[1] Y. Movshovitz-Attias, A. Toshev, T. K. Leung, S. Ioffe, and  S. Singh. No fuss distance metric learning using proxies. In
ICCV, 2017.
[2] W. Ge, W. Huang, D. Dong, and M. R. Scott. Deep metric learning with hierarchical triplet loss. In ECCV, 2018. 
[3] H. O. Song, S. Jegelka, V. Rathod, and K. Murphy. Deep metric learning via facility location. In CVPR, 2017.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rye9bQWFhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The novelty is limited and the empirical study is not convincing.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skf5qiC5KQ&amp;noteId=rye9bQWFhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper558 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper558 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">My major concerns are as follows.
1. Authors propose a new loss function in Eqn.17 which simply combines lifted structure loss and binomial deviance loss.
2. The comparison is not sufficient. For the loss function, authors only compare it to binomial in Table 1 where the improvement is not significant on two datasets. Authors should include the performance of lifted structure loss which is the other block in their new loss.
3. The improvement in Table 2&amp;3 is suspect. Baseline methods use either different backbone network or different size of embedding (usually with smaller size). For example, 'Sampling'('Margin' in tables?) uses a simplified ResNet-50 and the size of embedding is 128. The size of embedding in 'Proxy-NCA' is only 64 while this work uses 512. 
4. The parameters in the experiments seem quite sensitive. For example, they use $K=4$ on two data sets and $K=5$ on the others. Can authors elaborate how they tune the parameters?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1lyQr7Ppm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Architecture in "Sampling matters in deep embedding learning" paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skf5qiC5KQ&amp;noteId=S1lyQr7Ppm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper558 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Actually "Sampling matters in deep embedding learning" uses a regular ResNet-50 for all experiments except LFW faces dataset where they used a simplified version.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkexzyc82Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Strong experimental results but the theory is not that strong</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skf5qiC5KQ&amp;noteId=rkexzyc82Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper558 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper558 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper analyses the gradients some deep metric learning (DML) approaches. Most of them optimize over pair-wise or triplet-wise constraints.
The main claim of the paper is that, during training, most DML approaches ignore some informative pairs of examples or do not take into account the value of the distance gap between similar and dissimilar pairs.
A specific weighting strategy is then proposed to address those problems.

The main strength of the paper are the strong experimental results on different transfer learning benchmarks. The proposed approach seems to significantly outperform existing methods thanks to the proposed strategy. 
However, the theoretical aspect is weak.

I have some concerns about the paper.

- My first concern is about terminology: I disagree with the claims of the paper that there is any theoretical demonstration in the paper. 
I do not see the novelty of the gradient equivalence theorem (Theorem 3.1). It was already explained in (Law et al., ICCV 2013) (that is cited in the paper) that pairwise and triplet-wise constraints are just specific formulations of quadruplet-wise constraints. Pair-wise and triplet-wise can then be written in their quadruplet-wise formulation: we then obtain the induced gradient formulation that depends on positive and negative pairs (as formulated in Eq. (4)). The derivation of the proof is very straightforward once we have a formulation that generalizes both triplet-wise and pairwise constraints.

Moreover, the gradient equivalence definition (in Definition 3.1) is not applicable to most deep learning optimization frameworks that use momentum-based optimizers (e.g. Adam which is a momentum solver, and SGD which is often optimized with momentum). Indeed, definition 3.1 only considers the value of theta at some given iteration, but not at the previous iterations. However, momentum-based approaches keep a history of the gradients from previous iterations and will then return different gradients.

- One of the main claims of the paper against the triplet loss is that the sampling strategy ignores some informative pairs. This is mainly due to the fact that the triplet loss is generally used in the context of very large datasets and large mini-batches (Schroff et al., CVPR 2015) where it is computationally expensive to generate all the possible triplet constraints. Triplet sampling strategies are formulated to ensure fast convergence while avoiding degenerate solutions induced by the chosen triplet sampling strategy.

The submitted approach does not deal with very large datasets and then does not need to consider such sampling strategies. Although some sampling strategy is proposed, it is unclear if it would be scalable (i.e. trainable in reasonable time on the same dataset as FaceNet).
Moreover, as mentioned in Section 5.1, the proposed strategy is a straightforward extension of lifted structure.

- Why did you only report the recall@K evaluation when many methods report the normalized mutual information (NMI)? (Hyun Oh Song et al., CVPR 2017)
Could you please include NMI scores?

- If the problem of the contrastive loss is the fact that the gradient considers all pairs equally, why can it not be adapted to depend on the hardness of the constraint (e.g. by taking the squared of the loss for each pair)?

In conclusion, my opinion is borderline but only leans towards acceptance because the experimental results are strong.
Nonetheless, the reported results of baselines are often for different network architectures and using different output dimensions. Can the authors try their method with the same architecture and same output dimensionality as baselines?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkgUuhV8hQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting observation, needs further clarification</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skf5qiC5KQ&amp;noteId=rkgUuhV8hQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper558 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper558 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: The paper provides a unified view of the weight assignment strategies for pairwise similarities in DML. The authors analyze the weights assignment methods for multiple commonly used methods and provide a weighting scheme called RAW which combines the lifted structure with the binomial deviance approach. They also propose a hard mining triplet selection strategy for sampling informative triplets. The experiments indicate the advantage of combining these approaches.

Comments:
- The Theorem 3.1 should merely be stated as an observation. It is a trivial observation that the partial derivative of the parameters at every step will depend positively on s_ik and negatively on s_ij. Although the transition from line 2 to 3 in Equation (3) is very unclear and needs further expansion (how do you introduce indicator variables?). Moreover, there is no need to define the concept of gradient equivalence. The whole observation can be asserted as the fact that the partial derivative of the loss at every iteration can be seen as a weight. Additionally, to make the derivations mathematically sound, you should state that the weights in (2) are updated using the new similarity values at every iteration and are kept fixed during the next iteration.

- The proposed weighting strategy is a combination of two previous approaches. Although there is not much novelty in combining the two, the experiments indicate that the performance of DML can improve using the new approach. The triplet mining on the other hand is a bit dubious. Sampling only hard triplets, in my experience with triplets, may cause the loss to be saturated by unsatisfied hard triplets. Also, the embedding might not learn anything about the global structure of the dataset (i.e. how are the clusters relatively located w.r.t each other, which points are outliers, etc.). It would be nice to have a visualization of the learned embeddings to get an idea about the separability of the clusters.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1gmv8dd9X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Can you provide the details of the training process?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skf5qiC5KQ&amp;noteId=S1gmv8dd9X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Oct 2018</span><span class="item">ICLR 2019 Conference Paper558 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">How much is the batch-size? What is the total epoch of the training?  Do you use bound-box for car and cub datasets? </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJl823p_9X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Details of training</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skf5qiC5KQ&amp;noteId=rJl823p_9X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Oct 2018</span><span class="item">ICLR 2019 Conference Paper558 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your comments.

1.   Batch size:
We use a batch size of 80 for CUB-200 and Cars-196, and 640 for SOP and In-shop datasets. We have also experimented with smaller batch sizes (e.g, 80, 280, 320, 480) for SOP and In-shop and found that the performance is not sensitive to the choice of batch size.

2.     Epoch of training: 
We train on CUB-200 for 4K iterations, Cars-196 for 12K iterations, SOP for 60K iterations and In-shop for 100K iterations.

3.     Use of bounding boxes:
For fair comparison, we don’t use bounding boxes for CUB-200 and Cars-196 datasets.
 
We will release our code and all these training details will be included.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>