<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Identifying Generalization Properties in Neural Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Identifying Generalization Properties in Neural Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJxOHs0cKm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Identifying Generalization Properties in Neural Networks" />
      <meta name="og:description" content="While it has not yet been proven, empirical evidence suggests that model generalization is related to local properties of the optima which can be described via the Hessian. We connect model..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJxOHs0cKm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Identifying Generalization Properties in Neural Networks</a> <a class="note_content_pdf" href="/pdf?id=BJxOHs0cKm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019identifying,    &#10;title={Identifying Generalization Properties in Neural Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BJxOHs0cKm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">While it has not yet been proven, empirical evidence suggests that model generalization is related to local properties of the optima which can be described via the Hessian. We connect model generalization with the local property of a solution under the PAC-Bayes paradigm. In particular, we prove that model generalization ability is related to the Hessian, the higher-order "smoothness" terms characterized by the Lipschitz constant of the Hessian, and the scales of the parameters. Guided by the proof, we propose a metric to score the generalization capability of the model, as well as an algorithm that optimizes the perturbed model accordingly. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">generalization, PAC-Bayes, Hessian, perturbation</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">a theory connecting Hessian of the solution and the generalization power of the model</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HyelTxG53m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Worth publishing work deserving a more rigorous presentation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJxOHs0cKm&amp;noteId=HyelTxG53m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper101 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper101 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors study generalization capabilities of neural networks local minimums thanks to a PAC-Bayesian analysis that grasps the local smoothness properties. Even if some assumptions are made along the way, their analysis provides a metric that gives insight on the accuracy of a solution, as well as an optimization algorithm. Both of these result show good empirical behavior.

However, despite my favorable opinion, I consider that the paper presentation lacks rigor at many levels. I hope that the criticism below will be addressed in an eventual manuscript.

It is confusing that Equations (4) and (9) defines slightly differently \sigma*_i(w*,\eta,\gamma). In particular, the former is not a function of \eta. 

The toy experiment of Figure 1 is said to be self-explainable, which is only partly true. It is particularly disappointing because these results appear to be really insightful. The authors should state the complete model (in supplementary material if necessary). Also, I do not understand Figures (b)-(c)-(d): Why the samples do not seem to be at the same coordinates from one figure to the other? Why (d) shows predicted green labels, while the sample distribution of (b) has no green labels?

It is said to justify the perturbed optimization algorithm that Theorem 1 (based on Neyshabur et al. 2017) suggests minimizing a perturbed empirical loss. I think this is a weak argument for two reasons:
(1) This PAC-Bayes bounds is an upper bound on the perturbed generalization loss, not on the deterministic loss.
(2) The proposed optimization algorithm is based on Theorem 2 and Lemma 3, where the perturbed empirical loss does not appear directly.
That being said, this does not invalidate the method, but the algorithm justification deserves a better justification

There is a serious lack of rigor in the bibliography:
- Many peer-reviewed publications are cited just as arXiv preprints
- When present, there is no consistency in publication names. NIPS conference appears as "Advances in Neural ...,", 'NIPS'02", "Advances in Neural Information Processing Systems 29", "(Nips)". The same applies to other venues.
- Both first name initials and complete names are used 
- McAllester 2003: In In COLT
- Seldin 2012: Incomplete reference

 Also, the citation style is inconsistent. For instance, the first page contains both "Din et al, (2007) later points out..." and "Dziugaite &amp; Roy (2017) tries to optimize..." 

Typos:
- Page 3: ...but KL(w*+u | \pi) =&gt; KL(w*+u || \pi)
- In this/our draft: Think to use another word if the paper is accepted
- Line below Equation (5): \nabla^2 L =&gt; \nabla L (linear term)
- it is straight-forward -&gt; straightforward
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hkl3fxjisX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An extension of Neyshabur et al. PAC-Bayes bounds.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJxOHs0cKm&amp;noteId=Hkl3fxjisX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper101 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">23 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper101 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors prove a PAC-Bayes bound on a perturbed deterministic classifier in terms of the Lipschitz constant of the Hessian. They claim their bound suggests how insensitive the classifier is to perturbations in certain directions. 

The authors also “extract” from the bound a complexity measure for a particular classifier, that depends on the local properties of the empirical risk surface: the diagonal entries of the Hessian, the smoothness parameter of the Hessian, and the radius of the ball being considered.  The authors call this “metric” “PAC-Bayes Generalization metric”, or pacGen.

Overall, this seems like a trivial extension of Neyshabur et al. PAC-Bayes bounds. 

The experiments demonstrating that pacGen more or less tracks the generalization error of networks trained on MNIST dataset is not really surprising. Many quantities track the generalization error (see some of Bartlett’s, Srebro’s, Arora’s work). In fact, these other quantities track it more accurately. Based on Figure 2, it seems that pacGen only roughly follows the right “order” of networks generalizing better than others. If pacGen is somehow superior to other quantities, why not to evaluate the actual bound? Or why not to show that it at least tracks the generalization error better than other quantities?

The introduction is not only poorly written, but many of the statements are questionable. Par 2: What complexity are you talking about? What exactly is being contradicted by the empirical evidence that over-parametrized models generalize? 

Regarding the comment in the introduction: “ Dinh et al later points out that most of the Hessian-based sharpness measures are problematic and cannot be applied directly to explain generalization.”, and regarding the whole Section 5, where the authors argue that their bound would not grow much due to reparametrization:
If one obtains a bound that depends on the “flatness” of the minima, the bound might still be useful for the networks obtained by SGD (or other algorithms used in practice). The fact that Dinh et al. paper demonstrates that one can artificially reparametrize and change the landscape of a specific classifier does not contradict any generalization bounds that rely on SGD finding flat minima. Dinh et al. did not show that SGD finds classifiers in a sharp(er) minima that generalize (better).

In the experiment section, the authors compare train and test errors of perturbed (where the perturbation is based on the Hessian) and unperturbed classifiers. However, they don't compare their results to other type of perturbations, e.g. dropout. It’s been shown in previous work that certain perturbations improve generalization and test error.

There are numerous typos throughout the paper.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJlxUUUsj7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The paper does some extensive calculations but is weak on qualitative insights and empirical evaluation.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJxOHs0cKm&amp;noteId=rJlxUUUsj7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper101 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">22 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper101 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper gives various PAC-Bayesian generalization guarantees and some
empirical results on parameter perturbation in training using an algorithm
motivated by the theory.

The fundamental issue addressed in this paper is whether parameter
perturbation during training improves generalization and, if so, what
theoretical basis exists for this phenomenon.  For continuously
parameterized models, PAC-Bayesian bounds are fundamentally based on
parameter perturbation (non-singular posteriors).  So PAC-Bayesian
theory is naturally tied to parameter perturbation issues.  A more
refined question is whether the size of the perturbation should be
done on a per-parameter bases and whether per-parameter noise levels
should be adaptive --- should the appropriate noise level for each
parameter be adjusted on the basis of statistics in the training data.
Adam and RMS-prop both adapt per-parameter learning rate eta_i to be
proportional to 1/((E g_i^2) + epsilon) where E g_i^2 is some running
estimate of the expectation over a draw of a training point of the
square of the gradient of the loss with respect to parameter i.  At
the end of the day, this paper, based on PAC-Bayesian analysis,
proposes that a very similar adaptation be made to per-parameter noise
during training but where E g_i^2 is replaced by the RMS value \sqrt{E
g_i^2}.  It seems that all theoretical analyses require the square
root --- the units need to work.  A fundamental theoretical question,
perhaps unrelated to this paper, is why in learning rate adaptation the
square root hurts the performance.

This paper can be evaluated on both theoretical and empirical grounds.
At a theoretical level I have several complaints.  First, the
theoretical analysis seem fairly mechanical and without theoretical
innovation. Second, the analysis obscures the prior being used (the
learning bias). The paper first states an assumption that each
parameter is a-priori taken to be uniform over |w_i| &lt;= \tau_i and the
KL-divergence in the PAC-Bayes bound is then log tau_i/sigma_i where
sigma_i is the width of a uniform posterior over a smaller interval.
But later they say that they approximate tau_i by |w_i| + kappa_i with
kappa_i = \gamma |w_i| + epsilon.  I believe this works out to be
essentially a log-uniform prior on |w_i| (over some finite range of
log |w_i|).  This seems quite reasonable but should be made explicit.

The paper ignores the possibility that the prior should be centered at
the random initialization of the parameters.  This was found to be
essential in Dziugaite and Roy and completely changes the dependence
of k_i on w_i.

Another complaint is that the Hoefding bound is very loose in cases
where the emperical loss is small compared to its upper bound.  The
analysis can be more intuitively related to practice by avoiding the
rescaling of the loss into the interval [0,1] and writing expressions
in terms of a maximum bound on the loss L_max.  When hat{L} &lt;&lt; L_max
(almost always the case in practice) the relative Chernoff bound is
much tighter and significantly alters the analysis.  See McAllester's
PAC-Bayesian tutorial.

The theoretical discussion on re-parameterization misses an important
point, in my opinoin, relative to the need to impose a learning bias
(the no-free-lunch theorem).  All L_2 generalization bounds can be
interpreted in terms of a Gaussian prior on the parameters.  In all
such cases the prior (the learning bias) is not invariant to
re-parameterization.  All L_2 generalization bounds are subject to the
same re-parameterization criticism.  A prior tied to a particular
parameterization is standard practice in machine learning for in all
L_2 generalization bounds, including SVMs.  I do think that a
log-uniform prior (rather than a Gaussian prior) is superior and
greatly reduces sensitivity to re-parameterization as noted by the
authors (extremely indirectly).

I did not find the empirical results to very useful.  The value of
parameter perturbation in training remains an open question. Although
it is rarely done in practice today, it is an important fundamental
question. A much more thorough investigation is needed before any
conclusions can be drawn with confidence. Experimentation with
perturbation methods would seem more informative than theory given the
current state of the art in relating theory to practice.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>