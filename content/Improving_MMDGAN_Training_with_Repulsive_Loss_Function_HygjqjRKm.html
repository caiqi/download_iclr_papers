<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Improving MMD-GAN Training with Repulsive Loss Function | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Improving MMD-GAN Training with Repulsive Loss Function" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HygjqjR9Km" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Improving MMD-GAN Training with Repulsive Loss Function" />
      <meta name="og:description" content="Generative adversarial nets (GANs) are widely used to learn the data sampling process and their performance heavily depends on the loss functions used in training. This study revisits MMD-GAN that..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HygjqjR9Km" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Improving MMD-GAN Training with Repulsive Loss Function</a> <a class="note_content_pdf" href="/pdf?id=HygjqjR9Km" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019improving,    &#10;title={Improving MMD-GAN Training with Repulsive Loss Function},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HygjqjR9Km},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Generative adversarial nets (GANs) are widely used to learn the data sampling process and their performance heavily depends on the loss functions used in training. This study revisits MMD-GAN that uses the maximum mean discrepancy (MMD) as the loss function for GAN and makes two contributions. First, we argue that the existing MMD loss function may discourage the learning of data structures as it attempts to attract the discriminator outputs of real data. To address this issue, we propose a repulsive loss function to actively learn the difference among the real data by simply rearranging the terms in MMD. Second, inspired by the hinge loss, we propose a bounded Gaussian kernel to stabilize the training of MMD-GAN. The proposed methods are applied to the unsupervised image generation tasks on CIFAR-10, STL-10, CelebA, and LSUN bedroom datasets. Results show that the repulsive loss function significantly improves over the MMD loss at no additional computational cost and outperforms other representative loss functions. The proposed methods achieved an FID score of 16.21 on the CIFAR-10 dataset using a single DCGAN network and spectral normalization.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">generative adversarial nets, loss function, maximum mean discrepancy, image generation, unsupervised learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Rearranging the terms in maximum mean discrepancy yields a much better loss function for generative adversarial nets</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HklD4r-bTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygjqjR9Km&amp;noteId=HklD4r-bTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper565 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper565 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">OVERALL COMMENTS:

I haven't had much time to write this, so I'm giving a low confidence score and you should feel free to correct me.

I didn't think this paper was very clear. 
I had trouble grasping what the contributions were supposed to be
and I had trouble judging the significance of the experiments. 

That said, now that (I think) I understand what's going on,
the idea seems well motivated, the connection between the repulsion and the use of label information in other
GAN variants makes sense to me, and the statements you are making seem (as much as I had time to check them) correct. 

This leaves the issue of scientific significance. 
I feel like I need to understand what specifically contributed to the improvements in table 1 to evaluate significance. 
First of all, it seems like there are a lot of other 'good-scoring' models left out of this table. 
I understand that you make the claim that your improvement is orthogonal, but that seems like something that needs to
be tested empirically. You have orthogonal motivation but it might be that in practice your technique works for a reason
similar to the reason other techniques work. I would like to see more exploration of this. 
Second, are the models below the line the only models using spectral norm? I can't tell.
Overall, it's hard for me to envision this work really seriously changing the course of research on GANs,
but that's perhaps too high a bar for poster acceptance.

For these reasons, I am giving a score of 6.

DETAILED COMMENTS ON TEXT:

&gt; their performance heavily depends on the loss functions used in training.
This is not true, IMO. See [1]


&gt; may discourage the learning of data structures
What does 'data structures' mean in this case?
It has another more common usage that makes this confusing.

&gt; Several loss functions have been proposed
IMO this list doesn't belong in the main body of the text.
I would move it to an appendix.

&gt; We assume linear activation is used at the last layer of D
I'm not sure what this means?
My best guess is just that you're saying there is no activation function applied to the logits.

&gt; Arjovsky et al. (2017) showed that, if the supports of PX and PG do not overlap, there exists a perfect discriminator...
This doesn't affect your paper that much, but was this really something that needed to be shown?
If the discriminator has finite capacity it's not true in general and if it has infinite capacity its vacuous.


&gt; We propose a generalized power iteration method...
Why do this when we can explicitly compute the singular values as in [2]?
Genuine question.

&gt; MS-SSIM is not compatible with CIFAR-10 and STL-10 which have data from many classes;
Just compute the intra-class MS-SSIM as in [3].

&gt; Higher IS and lower FID scores indicate better image quality
I'm a bit worried about using the FID to evaluate a model that's been trained w/ an MMD loss where 
the discriminator is itself a neural network w/ roughly the same architecture as the pre-trained image classifier
used to compute the FID. What can you say about this?
Am I wrong to be worried?

&gt; Table 1: 
Which models use spectral norm?
My understanding is that this has a big influence on the scores.
This seems like a very important point.



REFERENCES:

[1] Are GANs Created Equal? A Large-Scale Study
[2] The Singular Values of Convolutional Layers
[3] Conditional Image Synthesis With Auxiliary Classifier GANs</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJe9UVmbaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Some clarifications on our study, experiments in preparation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygjqjR9Km&amp;noteId=BJe9UVmbaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper565 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018 (modified: 09 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper565 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your precious comments. Below I'll try to address your concerns, and will add more experimental results when ready.

Q1. What specifically contributed to the improvements in Table 1. Other good-scoring models need to be tested empirically. 
A1: In this study, we focused on comparing the proposed repulsive loss function with other representative loss functions. All experiments were done in an almost identical setup (DCGAN + spectral normalization + Adam + 16 learning rate combinations + 100k iterations) (see Section 5.1 Experiment Setup), that is, the differences among the several methods in Table 1 lie only in the loss functions used. To the best of our knowledge, this setup may not bias any loss function particularly. Thus, we attribute the improvements to our proposed repulsive loss. 

I said "almost identical" because, for MMD-related losses, the output layer of DCGAN has 16 neurons, while for logistic and hinge losses, it is one. In the revised manuscript, we will show that repulsive loss also performed the best when one output neuron is used. 

We agree that it would be interesting to test the repulsive loss in more general experiment setups, e.g., ResNet, gradient penalty, self-attention modules, supervised training, etc. However, we are afraid to admit that such a comprehensive study would require many computational resources. We would try our best to fill in this gap before and after the deadline of the rebuttal period. 

Q2: Does GAN performance heavily depend on the loss functions used in training?
A2: We agree that this is an overstatement and will change this in the revised paper as:
"their performance may heavily depend on the loss functions given a limited computational budget"
[1], [2] and our study did find that different loss functions lead to quite different performances in practice with a limited computational budget. 

Q3: What does 'data structure' mean in that case that MMD may discourage learning of data structure?
A3: By data structure, we mean the fine details in real data that separate them. For example, in CIFAR-10 dataset, "ship" and "cat" should be quite different, but discriminator trained using MMD may overlook such differences (see Figure 4). 

Q4: The Literature review on other loss functions does not belong to the main body of the text.
A4: We agree this could be moved to the appendix. 

Q5: What does it mean by assuming linear activation is used at the last layer of D.
A5: We mean there is no activation function applied to the discriminator outputs. In the case of minimax and non-saturating loss functions, we could absorb the sigmoid function into the loss which results in the formation with softmax function.

Q6: No need to include Arjovsky et al. (2017)'s statement on perfect discriminator.
A6: We agree with the reviewer. 

Q7: Why propose a generalized power iteration method when singular values can be computed as in [3]?
A7: The main reason is that, when only the first singular value is needed, the power iteration used in our study and [2] is computationally simpler than the method in [3] which uses Fourier transform and SVD. In addition, the method in [3] does not consider strides other than 1. The discriminator has downsampling layers where the stride is 2. However, the strength of [3] is that all singular values can be computed in a single run, which may eventually inspire more powerful regularization methods for GAN.

Q8: Calculate Intra-class MS-SSIM for CIFAR-10 and STL-10.
A8: We will revise corresponding texts and calculate the Intra-class MS-SSIM for CIFAR-10. STL-10 does not have labels so Intra-class MS-SSIM cannot be calculated.

Q9: Should FID be used to evaluate a model trained with an MMD-loss when the discriminator uses almost the same architecture as the Inception model in FID?
A9: We would like to point out that all loss functions in our study are paired with plain DCGAN architecture (see Appendix Table S1 and S2), which is much simpler than the Inception model.

Q10: Which models in Table 1 used the spectral norm?
A10: All models in Table 1 used the spectral normalization. In addition, their performances (as measured by FID) were better on CIFAR-10 than those in the corresponding papers [2][4][5] that proposed/used them, possibly due to the spectral normalization and TTUR training schedule [6]. 

-------------------------------------------------
[1] Are GANs Created Equal? A Large-Scale Study. NIPS, 2018.
[2] Spectral Normalization for Generative Adversarial Networks. ICLR, 2018
[3] The Singular Values of Convolutional Layers. Under review at ICLR 2019.   
[4] Demystifying MMD GANs. ICLR, 2018.
[5] MMD GAN: Towards a deeper understanding of moment matching network. NIPS, 2017.
[6] GANs trained by a two time-scale update rule converge to a local Nash equilibrium. NIPS, 2017.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJll-a6hnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>official review for "Improving MMD-GAN Training with Repulsive Loss Function"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygjqjR9Km&amp;noteId=rJll-a6hnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper565 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper565 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposed two techniques to improve MMD GANs: 1) a repulsive loss for MMD loss optimization; 2) a bounded Gaussian RBF kernel instead of original Gaussian kernel. The experimental results on several benchmark shown the effectiveness of the two proposals. The paper is well written and the idea is somehow novel. 

Despite the above strong points, here are some of my concerns:
1.The two proposed solutions seem separated. Do the authors have any clue that they can achieve more improvement when combined together, and why?

2. They are limited to the cases with spectral normalization. Is there any way both trick can be extended to other tricks (like WGAN loss case or GP).

3. Few missed references in this area:
a. On gradient regularizers for MMD GANs
b. Regularized Kernel and Neural Sobolev Descent: Dynamic MMD Transport</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rylo1DbthX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea but more evidence to show the significance of the work would be appreciated.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygjqjR9Km&amp;noteId=rylo1DbthX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper565 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper565 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a new discriminator loss for MMDGAN which encourages repulsion between points from the target distribution. The discriminator can then learn finer details of the target distribution unlike previous versions of MMDGAN. The paper also proposes an alternative to the RBF kernel to stabilize training and use spectral normalization to regularize the discriminator. The paper is clear and well written overall and the experiments show that the proposed method leads to improvements. The proposed idea is promising and a better theoretical understanding would make this work more significant. Indeed, it seems that MMD-rep can lead to instabilities during training while this is not the case for MMD-rep as shown in Appendix A. It would be good to better understand under which conditions MMD-rep leads to stable training. Figure 3 suggests that lambda should not be too big, but more theoretical evidence would be appreciated.
Regarding the experiments: 
- The proposed repulsive loss seems to improve over the classical attractive loss according to table 1, however, some ablation studies might be needed: how much improvement is attributed to the use of SN alone? The Hinge loss uses 1 output dimension for the critic and still leads to good results, while MMD variants use 16 output dimensions. Have you tried to compare the methods using the same dimension?
-The generalized spectral normalization proposed in this work seems to depend on the dimensionality of the input which can be problematic for high dimensional inputs. On the other hand, Myato’s algorithm only depends on the dimensions of the filter. Moreover, I would expect the two spectral norms to be mathematically related [1]. It is unclear what advantages the proposed algorithm for computing SN has.
- Regarding the choice of the kernel, it doesn’t seem that the choice defined in eq 6 and 7 defines a positive semi-definite kernel because of the truncation and the fact that it depends on whether the input comes from the true or the fake distribution. In that case, the mmd loss loses all its interpretation as a distance. Besides, the issue of saturation of the Gaussian kernel was already addressed in a more general case in [2]. Is there any reason to think the proposed kernel has any particular advantage?


[1]: Sedghi, Hanie, Vineet Gupta, and Philip M. Long. “The Singular Values of Convolutional Layers.” CoRR 
[2]: M. Arbel, D. J. Sutherland, M. Binkowski, and A. Gretton. On gradient regularizers for MMD GANs.



</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJxg9N3IcQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>questions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygjqjR9Km&amp;noteId=BJxg9N3IcQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Oct 2018</span><span class="item">ICLR 2019 Conference Paper565 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear authors,

- Do similar results with Appendix A hold when we use the proposed repulsive loss?
- The calculation of the spectral norm does not appear novel to me. [1] offers an efficient and exact calculation of the spectral norm, and [2, 3] proposed the generalized power iteration. But my concern is rather a computational cost than the novelty. Appendix B reports no performance improvements over the original method [4]. Concerning computational cost and memory consumption, the original method is superior.  Are there any reasons to estimate the true spectral norm with paying additional overheads?

Thanks,

[1] Hanie Sedghi, Vineet Gupta, Philip M. Long.  The Singular Values of Convolutional Layers. arXiv 1805.10408, 2018
[2] Yusuke Tsuzuku, Issei Sato, Masashi Sugiyama. Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks. NIPS, 2018
[3] Kevin Scaman, Aladin Virmaux. Lipschitz regularity of deep neural networks: analysis and efficient estimation. NIPS, 2018
[4] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida. Spectral Normalization for Generative Adversarial Networks. ICLR, 2018</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByeuVvPvqm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Local stability of repulsive loss and Modified spectral normalization</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygjqjR9Km&amp;noteId=ByeuVvPvqm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper565 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Oct 2018 (modified: 09 Oct 2018)</span><span class="item">ICLR 2019 Conference Paper565 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for your valuable comments. We would try to address your concerns as below.

Q1: Appendix A demonstrates the local stability of MMD loss. Can similar conclusion be drawn to the proposed repulsive loss?
A1: Yes.
For the realizable case (the equilibrium P_x = P_G, i.e., real and generated sample distributions equal), we will explicitly state the local stability of the repulsive loss and prove it in the revised manuscript. 
For the non-realizable case (the real sample distribution is impossible to be fit by the generator), the proof is indeed hard. In the submitted paper, we have used a simulation study (Figure S1) to show that both MMD loss and repulsive loss may be locally exponentially stable near equilibrium. 

Q2: What is the point of estimating the true spectral norm given the additional computational cost and no improvements?
A2: Upon submitting the paper, we were only able to test the proposed power iteration for convolution (PICO) against the one on a matrix (PIM) [4] on the CIFAR-10 dataset. The best result for PICO is FID 23.46, while for PIM it is FID 24.17, where the difference is small. For a fixed learning rate combination, the results are mixed. Thus, we reported no significant differences in performance, which, we agree, is not a rigorous statement. 
A preliminary test on other datasets gives the following results:

Table. The best FID scores for hinge loss among tested 16 learning rate combinations
-------------------------------------------------------------------------------------
Methods  |  CIFAR-10  |  STL-10  |  CelebA  |  LSUN-bedroom
-------------------------------------------------------------------------------------
PIM          |     24.17     |   47.17    |     10.77   |     29.00
PICO        |     23.46     |   49.44    |     8.60     |     16.73
-------------------------------------------------------------------------------------

However, we suspect that multiplying a constant other than 1 after each spectral normalization may affect the results for both PIM and PICO. We will do more tests and provide a more thorough comparison in the Appendix of the revised manuscript, and the main results in this comment as soon as possible.

The reasons for estimating the true spectral norm (sigma) are: 
1. Empirically sigma_PICO is always larger than sigma_PIM, thus we believe PICO imposes a tighter upper bound on the spectral norm of the whole discriminator (because we divide the weights by the spectral norms). For a very deep neural network, higher upper bound may make the training more unstable if we do not resort to methods like multiplication by a small constant. 
2. Perhaps more importantly, the spectral normalization process actually contributes to the gradients. [4] (Section 2.3 Gradient Analysis) argues that spectral normalization discourages the weight from being too sensitive in one direction. We believe estimating true spectral norm may make its gradient less biased than that produced by PIM. 
We will provide a discussion on this in the Appendix of the revised manuscript.

The cost of PICO roughly equals to increasing the batch size by two. Given a typical batch size of tens or hundreds of samples, PICO may still be considered as efficient though it indeed requires more computational cost than PIM. 

At last, regarding the novelty, we notice that our proposed PICO is similar to that of [2], which we were not aware of during this study. We will mention [1][2][3] as related work in the revised manuscript.

Again, thank you very much for your comments!

---------------------------------------------
[1] Hanie Sedghi, Vineet Gupta, Philip M. Long.  The Singular Values of Convolutional Layers. arXiv 1805.10408, 2018
[2] Yusuke Tsuzuku, Issei Sato, Masashi Sugiyama. Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks. NIPS, 2018
[3] Kevin Scaman, Aladin Virmaux. Lipschitz regularity of deep neural networks: analysis and efficient estimation. NIPS, 2018
[4] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida. Spectral Normalization for Generative Adversarial Networks. ICLR, 2018</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkxJC6g79Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Code for MMD-GAN with the repulsive loss function</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HygjqjR9Km&amp;noteId=rkxJC6g79Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper565 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Oct 2018</span><span class="item">ICLR 2019 Conference Paper565 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear researchers,
The code for this paper can be found at the anonymized GitHub repository:
<a href="https://anonymous.4open.science/repository/e8675209-4393-4dbc-ad04-aad36cd5d738/" target="_blank" rel="nofollow">https://anonymous.4open.science/repository/e8675209-4393-4dbc-ad04-aad36cd5d738/</a>
Any suggestion/feedback on the paper and code is much appreciated.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>