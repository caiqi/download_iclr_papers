<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Found by NEMO: Unsupervised Object Detection from Negative Examples and Motion | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Found by NEMO: Unsupervised Object Detection from Negative Examples and Motion" />
        <meta name="citation_author" content="Rico Jonschkowski" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Byl9bhA5F7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Found by NEMO: Unsupervised Object Detection from Negative Examples..." />
      <meta name="og:description" content="This paper introduces NEMO, an approach to unsupervised object detection that uses motion---instead of image labels---as a cue to learn object detection. To discriminate between motion of the..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Byl9bhA5F7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Found by NEMO: Unsupervised Object Detection from Negative Examples and Motion</a> <a class="note_content_pdf" href="/pdf?id=Byl9bhA5F7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=rjon%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="rjon@google.com">Rico Jonschkowski</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">This paper introduces NEMO, an approach to unsupervised object detection that uses motion---instead of image labels---as a cue to learn object detection. To discriminate between motion of the target object and other changes in the image, it relies on negative examples that show the scene without the object. The required data can be collected very easily by recording two short videos, a positive one showing the object in motion and a negative one showing the scene without the object. Without any additional form of pretraining or supervision and despite of occlusions, distractions, camera motion, and adverse lighting, those videos are sufficient to learn object detectors that can be applied to new videos and even generalize to unseen scenes and camera angles. In a baseline comparison, unsupervised object detection outperforms off-the shelf template matching and tracking approaches that are given an initial bounding box of the object. The learned object representations are also shown to be accurate enough to capture the relevant information from manipulation task demonstrations, which makes them applicable to learning from demonstration in robotics. An example of object detection that was learned from 3 minutes of video can be found here: <a href="http://y2u.be/u_jyz9_ETz4" target="_blank" rel="nofollow">http://y2u.be/u_jyz9_ETz4</a></span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">unsupervised learning, computer vision, object detection</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Learning to detect objects without image labels from 3 minutes of video</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">5 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_B1l6OY_3TX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byl9bhA5F7&amp;noteId=B1l6OY_3TX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1195 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1195 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1gZZYuhTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for the reviews</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byl9bhA5F7&amp;noteId=S1gZZYuhTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1195 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1195 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank our reviewers for the time and effort they have spent on reviewing the paper and for their constructive feedback. We appreciate that the reviewers liked the our idea  and we are using all comments to improve the paper by explaining the motivation / application more clearly and by adding more quantitative analysis of our method. 

Following one of the comments, we will withdraw the submission from CoRL to resubmit to a computer vision conference.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rygKheLq27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice idea with some interesting results but needs more work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byl9bhA5F7&amp;noteId=rygKheLq27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1195 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1195 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">OVERVIEW:
The paper presents an interesting approach to unsupervised (more accurately weakly supervised) object detection that uses motion as training cue. At a high level, the authors propose to learn what an object looks like from two input videos: (1) a positive example containing the object of interest undergoing some motion and (2) a negative example of the same background scene without the object. They use a spatial encoder network in conjunction with three losses (variation loss, slowness loss, presence loss) to learn the appearance of an object based on motion cues. The only supervision here is at the video level with a label indicating the positive example. Then, given a new video, they are able to detect the object under some nuisance factors. They show four experiments on (1) detection with static camera showing written text, (2) detection of a moving objects like a roomba, drone and toy car with a moving camera, (3) detection of multiple objects, and (4) comparison with baselines of object tracking and template matching.

PROS:
1. The authors are able to show object detection with video level labels solely using motion cues. This is an interesting idea which can have significant impact with the abundance of unlabeled data in the wild and where video level annotations are much cheaper than bounding box level or even image level annotations.
2. The spatial encoding (not a contribution of this work) to represent object location is simple but powerful and I feel can be extended to foreground segmentation. The authors use it neatly with reasonable loss functions to formulate the detection problem in an interesting way.
3. The experiments show qualitatively that their proposed approach works reasonably well under simple settings (distinct objects on uniform background).

CONS:
This is a nice idea and some preliminary experiments show positive qualitative results. However, for a publication especially at ICLR, it needs more work. The below points are some negative aspects of the current submission but also things I feel the authors can work on in their next submission:
1. The Problem Setup: I think the scenario where you have one video of a single moving object in the scene and another video of the same scene without the object is not common. That is not to say that there might be some practical applications where this kind of setup in natural (maybe drones?) but you haven't convinced me about it. It will work in your favor if you are able to convince the reader where this kind of a setup is easily present and data is easily available.
2. Concern about optimization: You mention in the second paragraph of Sec. 3.3 some weird stuff happening during optimization and how you counter it. It feels very hand-wavy and you need to either (1) investigate this behavior further and resolve the problem or (2) provide a more convincing answer as to why your proposed solutions make sense.
3. Lack of quantitative evaluation: The experimental evaluation shows qualitative performance. I think the paper will look better (and more mature) with some quantitative metrics, say percentage of frames where the object was detected correctly, AP scores using bounding box annotations on test videos, experiments on larger number of videos with some averaged metrics.
4. Stronger baselines: This is a continuation of the previous point. The only quantitative evaluation the authors have is MSE loss on position, between their proposed approach and baselines of template matching &amp; object tracking. In my opinion, these baselines are very weak. Seeing the example images/videos, I am almost tempted to say that video registration followed by background subtraction with median image might also work pretty well. An out-of-the-box object detection system, trained on COCO (that hasn't seen these categories) might still return "good" bounding boxes with bad category labels. Basically, convince the reader with some stronger experiments than just proof-of-concept (for this kind of idea).
5. Lack of variety in data: The data looks simple with one or few objects on plain background. I would trust the algorithm more if you show that it works on harder data, say textured background (drone in sky/field, car on road), appearance changes (train on one car, test on another), viewpoint changes, etc. 
6. One Shot Learning: The problem setup sounds very close to one-shot learning where you are seeing a video of the object once before trying to learn it. Some discussion along this direction in related work might be needed.

OVERALL:
I am rejecting the paper in it's version right now. I feel this is an interesting idea and can be resubmitted (possibly to a Vision conference) with additional work. If this was to be a vision paper, I can see a new dataset introducing this task with your approach and a stronger baseline comparison.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJxpq-M52Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Weak technical novelty and insight</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byl9bhA5F7&amp;noteId=SJxpq-M52Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1195 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1195 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">**Summary of the paper**

The paper introduces a novel approach for unsupervised object detection based on motion cues from two separate videos: one with and one without the object of interest. The main novelty is the usage of the negative video example which enables unsupervised detection of objects by being robust to distractor objects in the scene as the algorithm is able to differentiate between interesting motion and distractor object motion. The method trains a convolutional network to output dense probability maps for the object location and optimizes for strong/weak detection in positive/negative video respectively along with smoothness and variation objectives. The authors evaluate the proposed method on self-collected video datasets and test detection on held-out videos from the same scene as well as generalization to detecting the same object in new scenes. They compare to template matching and tracking baselines and give suggestions for a possible application of their method as automated demonstration annotation tool for pick-and-place tasks.


**Clarity**

Above average

**Significance**

Below Average

**Correctness**

The paper is technically correct.

**Detailed comments**

_Paper Strengths_

- the idea to use a negative video example for unsupervised detection learning seems novel
- the proposed method is simple and the needed data can be collected with widely available equipments
- the paper addresses the problem of being robust with respect to moving distractor objects for cases in which those are present in both the positive and negative video example
- the authors collected real-world data from different scenes with different objects, object counts and conditions (indoor/outdoor, still/moving camera)
- the authors compare to a number of non-learning approaches from open-source implementations (the reviewer cannot judge whether any relevant technique is missing)
- the authors provide anonymized links to videos demonstrating a representative sample of the algorithm's performance on the considered scenes


_Paper Weaknesses_

- the authors clearly reduced the horizontal margins of the standard ICLR style template leading to a wider text corpus, however, as the bottom margin seems to be increased the reviewer will review the paper nevertheless, but requires the authors to revert to the standard ICLR style template upon update of their manuscript
- the paper makes the relatively strong assumption that we have a video of the object of interest moving in the scene we plan to employ our algorithm on along with another video of the same scene with all relevant distractor objects but without the object of interest; given these assumptions the reviewer struggles to see a meaningful application of the approach (the only one provided by the authors, as tool for automated demonstration annotation, is not convincing, see below) that is outside of a very controlled setting in which more classic, e.g. maker-based approaches could be employed for detecting the object
- the claimed robustness of the algorithm only holds for variations that were extensively present during training time (e.g. lighting differences on the car), in contrast the algorithm seems to be very sensitive to partial occlusion (as can be seen in the multi-object examples) and seems to heavily overfit on the color of objects (e.g. the car generalization to the new scene is easy as the car is the only red object in the scene, once the car moves away from the camera and the red front part is self-occluded the detection fails, see e.g. minutes 1:15, 1:26 in the transfer video)
- other than the single transfer example mentioned in the previous point the authors do not prove generalization to more challenging non-training scenes with heavier clutter, non-seen lighting changes and occlusions to support their robustness claim
- the proposed method cannot operate in-the-wild (e.g. Youtube data) as it makes very strong assumptions about the required input data
- the fact that the method needs to be trained from scratch for each new scene and object reduces the number of possible applications/scalability and makes comparison to classic baselines unfair which are not specifically tuned towards a certain object or scene
- the authors make no comparison to other unsupervised detection approaches (e.g. to the self-cited Jonschkowski et al. (2017)) to prove short-comings of other methods on the newly generated dataset
- as mentioned by the authors the method does not use any temporal information for the detection which surely could help, the reviewer cannot follow this design decision, especially because the fact that the encoder gets the difference image to the previous video frame as input in case of a moving camera (for ego-motion estimation) makes applications to non-video data impossible, also none of the experiments exploit the non-temporal property of the approach to show single frame detection on a more varied set of scenes
- the experiment showcasing the proposed application to "learning from demonstration" is not convincing as the method is only used to detect the goal location of the object but, critically, treats every object in the scene separately, missing any relational information in the target configuration, therefore in case of the sorting in a vertical line task the learned goal representation does not represent the actual goal of the task
- the authors do not provide ablation studies proving the necessity of all three proposed objectives (slowness, variability, presence)
- the reviewer cannot follow the references to object-centric representation learning that are made in the paper, specifically because (as also noted by the authors themselves) the proposed method makes substantially stronger assumptions with respect to the input data and merely learns to detect the spatial coordinates of a single given object in a scene as opposed to learning an object-centric scene representation that can be useful for downstream tasks, therefore the reviewer would strongly suggest to tone down the scope of the presented work, especially in the first paragraph of the introduction and the last paragraph of the conclusion
- the "random search optimization" discussed in section 3.3 is not a valid method as it "solves" this problem of instable training by picking the best of N runs with varying random seeds
- Figure 2 is not very helpful for understanding, the details of the architecture (left part) could go to the appendix and be replaced with a figure that details the intended usage of the method for a concrete application to strengthen the motivation of the approach


_Reproducibility_

- Given the architectural information provided in the paper the reviewer believes it would be relatively straight forward to reproduce the results of the work.

_Conclusion_

- Overall, the reviewer appreciates the effort that went into the work but sees considerable need for improvement concerning the motivation and possible applications given the strong assumptions that are made about the input data. To truly position the paper against other detection/tracking algorithms more experiments are needed that show zero-shot (without retraining) generalization to substantially different scenes, or even architectural changes to be able to handle novel objects too. If the authors want to motivate the method as an automated tool for demonstration annotation an actual example of policy learning from unsupervisedly annotated sequences would strengthen the argument. The proposed method does not provide considerable technical novelty or insight to compensate for the lack in motivation. In the current state the paper is not convincing enough to warrant acceptance.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJx8kK6Uh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>using videos without objects as negative examples to detect objects</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byl9bhA5F7&amp;noteId=rJx8kK6Uh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1195 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1195 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper introduces a system that uses positive videos (with the objects of interest in motion) and negative videos (devoid of those objects but the same backgrounds) to detect those objects of interest. The motivation in clear and explanation and flow of the paper are good. The technique will allow building object detectors with less labeling effort.

However, the labeling of the first frame in many previous tracking and detection techniques can be compared to collecting a negative video (i.e. the agent needs to make sure that the negative video does not have that or related object in the negative video) and so both these approaches are similar. The current strategy can't by itself detect the absence of the object and hence there is supervision needed there.

One other important aspect I think was missing was the details of the training and test sets. How many training videos and test videos was this tried on? How different were the test videos? Did the test videos have objects similar to the training videos? Did the test videos have different external condition like lighting etc than the training videos? How similar were the backgrounds in these videos?

Previous approaches like TLD also used positive and negative samples (though from the same video) and track the object with view and lighting changes. When compared to the baseline, why did the current approach do better than these approaches (since both these methods and using a similar strategy of using positive and negative samples).

How does the system handle videos with different objects moving in the video with similar physical constraints? When does the system break i.e. what happens if objects are moving too fast, the camera motion or panning is not as expected? How would the manually set weight parameters need to be changed? How reliable are these parameters to new objects, far away objects etc?

How is variation or slowness being computed reliably without use of optical flow or other techniques? i.e. won't other objects due to camera motion negatively affect this?

In section 3.3, how do you detect if it has fully converged or close to it for the multiple runs especially since the ground truth is missing?

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>