<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>DISTRIBUTIONAL CONCAVITY REGULARIZATION FOR GANS | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="DISTRIBUTIONAL CONCAVITY REGULARIZATION FOR GANS" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SklEEnC5tQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="DISTRIBUTIONAL CONCAVITY REGULARIZATION FOR GANS" />
      <meta name="og:description" content="We propose Distributional Concavity (DC) regularization for Generative Adversarial Networks (GANs), a functional gradient-based method that promotes the entropy of the generator distribution and..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SklEEnC5tQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>DISTRIBUTIONAL CONCAVITY REGULARIZATION FOR GANS</a> <a class="note_content_pdf" href="/pdf?id=SklEEnC5tQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 09 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019distributional,    &#10;title={DISTRIBUTIONAL CONCAVITY REGULARIZATION FOR GANS},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SklEEnC5tQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=SklEEnC5tQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We propose Distributional Concavity (DC) regularization for Generative Adversarial Networks (GANs), a functional gradient-based method that promotes the entropy of the generator distribution and works against mode collapse. 
Our DC regularization is an easy-to-implement method that can be used in combination with the current state of the art methods like Spectral Normalization and Wasserstein GAN with gradient penalty to further improve the performance.
We will not only show that our DC regularization can achieve highly competitive results on ILSVRC2012 and CIFAR datasets in terms of Inception score and Fr\'echet inception distance, but also provide a mathematical guarantee that our method  can always increase the entropy of the generator distribution.  We will also show an intimate theoretical connection between our method and the theory of optimal transport.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Generative Adversarial Networks, regularization, optimal transport, functional gradient, convex analysis</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Skgk8OrDTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Sound method and good results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SklEEnC5tQ&amp;noteId=Skgk8OrDTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1441 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1441 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
This paper proposes distributional concavity regularization for GANs which encourages producing generator distributions with higher entropy. The paper motivates the proposed method as follows:
-       Using the concept of functional gradient, the paper interprets the update in the generator parameters as an update in the generator distribution
-       Given this functional gradient perspective, the paper proposes updating the generator distribution toward a target distribution which has *higher entropy and satisfies monoticity*
-       Then, the paper proves that this condition can be satisfied by ensuring that generator’s objective (L) is concave
-       Since it’s difficult to ensure concavity when parametrizing generators as deep neural networks, the paper proposes adding a simple penalty term that encourages the concavity of generator objective
Experiments confirm the validity the proposed approach. Interestingly, the paper shows that performance of multiple GAN variants can be improved with their proposed method on several image datasets
 
Strengths:
-   	The proposed method is very interesting and is based on sound theory
-   	Connection to optimal transport theory is also interesting
-   	In practice, the method is very simple to implement and seems to produce good results
 
Weaknesses:
-       Readability of the paper can be generally improved. I had to go over the paper many times to get the idea.
-       Figures should be provided with more detailed captions, which explain main result and providing context (e.g. explaining baselines).
 
Questions/Comments:
-       Equation (7) has typos (uses theta_old instead of theta in some places)
-       Section 4.1 (effect of monoticity) is a bit confusing. My understanding is that parameter update rule of equation (3) and (6) are equivalent, but you seem to use (6) there. Can you clarify what you do there and in general this experiment a bit more?
-       Comparing with entropy maximization method of EGAN (Dai et al, 2017) is a good idea, but I’m wondering if you can compare it on low dimensional settings (e.g. as in Fig 2). It is also not clear why increasing entropy with EGAN-VI is worse than baselines in Table 1.

 
Overall recommendation:
The paper is based on sound theory and provides very interesting perspective. The method seems to work in practice on a variety of experimental setting. Therefore, I recommend accepting it.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkxoIlOOaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Emphasize improving readability of the paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SklEEnC5tQ&amp;noteId=rkxoIlOOaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1441 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1441 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I would like to emphasize that the main weakness in the paper, in my opinion, is that it can be quite hard to read for the general ICLR community. The authors are strongly encouraged to try to make it more accessible, which will in fact will increase the impact of the paper eventually.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJeZ1SFh3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A good paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SklEEnC5tQ&amp;noteId=HJeZ1SFh3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1441 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1441 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors make use of the theory of functional gradient, based on optimal transport, to develop a method that can promote the entropy of the generator distribution without directly estimating the entropy itself. Theoretical results are provided as well as necessary experiments to support their technique's outperformance in some data sets. I found that this is an interesting paper, both original ideal and numerical results.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">1: The reviewer's evaluation is an educated guess</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryxaWURfTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you! </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SklEEnC5tQ&amp;noteId=ryxaWURfTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1441 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1441 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for the comment.  We believe that functional gradient based methods have much room to explore, and it is our hope that many aspects of GANs can be analyzed using this philosophy. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Hyx3bbVc27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>potentially useful heuristic for GANs with vague maths</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SklEEnC5tQ&amp;noteId=Hyx3bbVc27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1441 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1441 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">GANs (generative adversarial network) represent a recently introduced min-max generative modelling scheme with several successful applications. Unfortunately, GANs often show unstable behaviour during the training phase. The authors of the submission propose a functional-gradient type entropy-promoting approach to tackle this problem, as estimating entropy is computationally difficult.

While the idea of the submission might be useful in some applications, the work is rather vaguely written, it is in draft phase:
1. Abbreviations, notations are not defined: GAN, WGAN-GP, DNN, FID (the complete name only shows up in Section 4), softplus, sigmoid, D_{\theta_{old}}, ...
2. While the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.
3. Estimating entropies is a standard practice in statistics and machine learning, with an arsenal of estimators; the motivation of the submission is questionable.
4. Differentiation w.r.t. functions (or more generally elements in normed spaces) is a well-defined concept in mathematics, including the notions of Gateaux, Frechet and Hadamard differentiability. It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ... It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.

While the idea of the work might be useful in practice, the current submission requires significant revision and work before publication.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJllM_RzaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you very much for the suggestions and comment for the revisions! </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SklEEnC5tQ&amp;noteId=rJllM_RzaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1441 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1441 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for the comment.  We have revised the script to reflect the suggestions, and we would like to articulate on the changes we have made.   Because we are out of space, we will provide the  answers over several comments.  For the list of mentioned references, please see the 'last' comment. 

&gt; 1. Abbreviations, notations are not defined: GAN, WGAN-GP, DNN, FID (the complete name only shows up in Section 4), softplus, sigmoid, D_{\theta_{old}}, …

We clarified the abbreviations for commonly used acronyms, such as Deep Neural Networks and Frechet inception distance, and gave definitions to undefined  symbols. 

&gt; 2. While the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.

We have given more detailed explanation of mode collapse, and provided citation that can be consulted for further information regarding its definition. In short, Mode collapse collectively refers to the lack of diversity in generator distribution [1-5]. This phenomena happens even in the simplest case of generating a mixture of Gaussian Distribution (See Fig 22, [1] for example) . Without any notable countermeasure, GANs tend to produce a distribution with less number of modes than the target distribution . 

&gt; 3. Estimating entropies is a standard practice in statistics and machine learning, with an arsenal of estimators; the motivation of the submission is questionable. 

In the revision, we have emphasized how the classical empirical techniques in estimating the entropy are not suited for the training of GANs aimed toward the synthesis of high dimensional distribution. In fact, our method yields much better performance than Energy-based GANs(EGANs) [6], that uses a rather high calibre variational inference based technique to directly estimate the entropy, which was, in their experiment, performed the best among all classical techniques they tested. For more details, please consult the original text.  

In many application of GANs, the objective is to synthesize a generative model on high dimensional space, and it is a very difficult problem on its own to rely on classical techniques to estimate the entropy in such high dimensional space. In most study of generative models, the distribution is defined in term of latent variables, and its law is given by G# p where p is some known distribution---or the pushforward of p with a measurable function G with an euclidean range.  The most straightforward approach in entropy estimation uses some form of plugin Estimator based on Kernel density estimation and nearest neighbor estimation hat p: 
estimate  =  g(\hat p(X_i), X_i  \in Data ). 
For the real world applications of GANs (e.g image generations), the dimensions of the data space are high.  On the CIFAR 10 benchmark, the dimension is   32*32*3, and the dimension of the ImageNet benchmark is high as 64*64*3.   Even for Kernel density estimator on of  regular density on d dimensional Euclidean space for example, the mean square error is MSE(f*, \hat f) = O(h^4  +  1/(n h^{d})) where h is the bandwidth, n is the number of samples [7,8].
This is to say we need small enough bandwidth with tremendous size of n for large d. On the top of this, for  the density of p_g of G\# p is computed through the formula
H(p_g) = H(p(z)) + E_p(z)[det(D_z G(z))]. 
In order to compute E_p(z)[det(D_z G(z))], we need at least O(n d^2) (often d^3 in usual implementation)  and this expression does not include the massicve scaler that is dependent on the number of parameters, which again, is  often extremely large, and can be in order of millions（Fig 2 in [9]) .  Again, Energy-based GANs(EGANs)[6], that uses a rather high calibre variational inference based technique are not performing too well relative to our approach.  
Also, one of the fundamental motivation for the development of GANs techniques is to do away with the precise and explicit density of the target distribution in high dimensional space. For more details of these motives, please consult classical literatures in GANs [1,4,10].
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkl84uRMaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>response, con'd </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SklEEnC5tQ&amp;noteId=rkl84uRMaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1441 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1441 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt; 4. Differentiation w.r.t. functions (or more generally elements in normed spaces) is a well-defined concept in mathematics, including the notions of Gateaux, Frechet and Hadamard differentiability. It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ... It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.

We have added more descriptions in the theory section and expressed our intention that we are taking Frechet derivatives of a functional defined on a Hilbert space consisting of functions that are L2 integrable with respect to the probability measure of concern. In functional gradient applications to generative models [11, 12],  conditions required for  the Frechet differentiability of the objective functional are often assumed to hold. We hope that our revisions made the paper more readable to the wider audience, and hope that  the paper now assumes less knowledge of GANs in understanding our idea.

[1] Ian Goodfellow. NIPS 2016 tutorial: Generative adversarial networks. arXiv preprint arXiv:1701.00160, 2016.
[2] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. In ICLR, 2017.
[3] Martin Arjovsky and Le ́on Bottou. Towards principled methods for training generative adversarial networks. In ICLR, 2017.
[4] Martin Arjovsky, Soumith Chintala, and Le ́on Bottou. Wasserstein generative adversarial networks. In ICML, pp. 214–223, 2017.
[5] Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. Pac gan: The power of two samples in generative adversarial networks. arXiv preprint arXiv:1712.04086, 2017.
[6] Zihang Dai, Amjad Almahairi, Philip Bachman, Eduard Hovy, and Aaron Courville. Calibrating energy-based generative adversarial networks. In ICLR, 2017.
[7] Theophilos Cacoullos. Estimation of a multivariate density. Annals of the Institute of Statistical Mathematics, 18(1):179–189, 1966.
[8] Arkadas Ozakin and Alexander G Gray. Submanifold density estimation. In Advances in Neural Information Processing Systems, pp. 1375–1382, 2009.
[9] Alfredo Canziani, Eugenio Culurciello, Adam Paszke. AN ANALYSIS OF DEEP NEURAL NETWORK MODELS FOR PRACTICAL APPLICATIONS. arXiv preprint arXiv:1605.07678, 2017.
[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pp. 2672–2680, 2014.
[11] Atsushi Nitanda and Taiji Suzuki. Gradient layer: Enhancing the convergence of adversarial training for generative models. In AISTATS, pp. 1008–1016, 2018.
[12] Rie Johnson and Tong Zhang. Composite functional gradient learning of generative adversarial models. In ICML, pp. 2376–2384, 2018.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_rJgOIkIKnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice experimental paper (with theory backing) </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SklEEnC5tQ&amp;noteId=rJgOIkIKnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1441 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1441 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this paper, the authors claim that they are able to update the generator better to avoid generator mode collapse and also increase the stability of GANs training by indirectly increasing the entropy of the generator until it matches the entropy of the original data distribution using functional gradient methods.

The paper is interesting and well written. However, there is a lot of work coming out in the field of GANs currently, so I am not able to comment on the novelty of this regularization approach, and I am interested to know how this method performs when compared to other techniques to avoid mode collapse such as feature matching and mini-batch discrimination, etc.  

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">1: The reviewer's evaluation is an educated guess</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJxRuUCz6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you very much for the suggestions and comment. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SklEEnC5tQ&amp;noteId=HJxRuUCz6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1441 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1441 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for the comment.  We believe that the core novelty of our work is in introducing a type of regularization based on a functional gradient.  Because we were able to conduct additional experiment in time, we also conducted feature matching as well and confirmed the superiority of DC-regularization over the method (Table 8). </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>