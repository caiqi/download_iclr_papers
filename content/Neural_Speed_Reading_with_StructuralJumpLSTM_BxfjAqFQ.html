<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Neural Speed Reading with Structural-Jump-LSTM | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Neural Speed Reading with Structural-Jump-LSTM" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1xf9jAqFQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Neural Speed Reading with Structural-Jump-LSTM" />
      <meta name="og:description" content="Recurrent neural networks (RNNs) can model natural language by sequentially ''reading'' input tokens and outputting a distributed representation of each token. Due to the sequential nature of RNNs,..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1xf9jAqFQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Neural Speed Reading with Structural-Jump-LSTM</a> <a class="note_content_pdf" href="/pdf?id=B1xf9jAqFQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019neural,    &#10;title={Neural Speed Reading with Structural-Jump-LSTM},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=B1xf9jAqFQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=B1xf9jAqFQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Recurrent neural networks (RNNs) can model natural language by sequentially ''reading'' input tokens and outputting a distributed representation of each token. Due to the sequential nature of RNNs, inference time is linearly dependent on the input length, and all inputs are read regardless of their importance. Efforts to speed up this inference, known as ''neural speed reading'', either ignore or skim over part of the input. We present Structural-Jump-LSTM: the first neural speed reading model to both skip and jump text during inference. The model consists of a standard LSTM and two agents: one capable of skipping single words when reading, and one capable of exploiting punctuation structure (sub-sentence separators (,:), sentence end symbols (.!?), or end of text markers) to jump ahead after reading a word.
A comprehensive experimental evaluation of our model against all five state-of-the-art neural reading models shows that 
Structural-Jump-LSTM achieves the best overall floating point operations (FLOP) reduction (hence is faster), while keeping the same accuracy or even improving it compared to a vanilla LSTM that reads the whole text.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">natural language processing, speed reading, recurrent neural network, classification</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a new model for neural speed reading that utilizes the inherent punctuation structure of a text to define effective jumping and skipping behavior.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_H1liRmli2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>New incremental work on speed reading with slightly better empirical results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xf9jAqFQ&amp;noteId=H1liRmli2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper514 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper514 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a Structural-Jump-LSTM model to speed up machine reading, which is an extension of the previous speed reading models, such as LSTM-Jump, Skim-LSTM and LSTM-Shuffle. The major difference, as claimed by the authors, is that the proposed model has two agents instead of one. One agent decides whether the next input should be fed into the LSTM (skip) and the other determines whether the model should jump to the next punctuation (jump). The sentence-wise jumping makes the jumping more structural than models like LSTM-Jump, while the word-wise skipping operation has a finer skimming decision. The reinforcement learning algorithm in this paper is also different from LSTM-Jump, where LSTM-Jump uses REINFORCE, while this paper applies actor-critic approach. 

Empirical studies show that Structural-Jump-LSTM is (slightly) better than state-of-the-art methods in terms of both accuracy and speed over most but few datasets. My feeling is that the proposed model should work much better than the previous models in very long texts, which I suggest the author should try on. Otherwise, the performance gain looks marginal and it is thus questionable whether the complicated modeling is necessary. 

I am confused by Figure 1: why are the “yes/no” placed in front of the “skipped”? “Previous LSTM” is confusing as well, which should be “Previous Output/hidden state”.

Minor comment: The LSTM-Jump takes word2vec as the initialization in CBT, while this paper uses GLOVE. I wonder if this results in the performance difference in accuracy. From my experience, GLOVE is usually better than word2vec in most of the tasks. If this effect also applies to CBT, the experiment is not fair.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1gJqdreRm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xf9jAqFQ&amp;noteId=S1gJqdreRm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper514 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper514 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review, questions and suggestions. We address both questions and suggestions below. 

Question1: “Empirical studies show that Structural-Jump-LSTM is (slightly) better than state-of-the-art methods in terms of both accuracy and speed over most but few datasets. My feeling is that the proposed model should work much better than the previous models in very long texts, which I suggest the author should try on. Otherwise, the performance gain looks marginal and it is thus questionable whether the complicated modeling is necessary.”

Answer1: In our experiments we compare our model against the state of the art using more datasets than any other related work. This large selection of datasets includes texts of very different length. On the datasets with the longest texts (IMDB, CBT-CN, CBT-NE, Yelp) we obtain the largest FLOP reductions on 3 out of 4 of them. IMDB, CBT-CN, CBT-NE are also among the datasets where we obtain the lowest reading percentages (only 19.7% to 32.6%). So our model indeed performs very well on long text. However, we also observe that speed reading is very task -dependent, as one of the datasets with short texts (DBPedia) obtains the lowest reading percentage across all datasets (17.5%).
Regarding whether “the complicated modeling is necessary”, we note that our model is not notably more complex than related models, as most related models (except Adaptive-LSTM) implement an agent for making speed-reading decisions. In our setting, we use a simple agent for skipping, followed by a potential decision by the structural jumping agent. This allows to effectively combine the benefits of skipping and jumping. Additionally, in comparison to strong models such as LSTM-Jump and LSTM-shuffle, our model makes parameter tuning notably easier: LSTM-Jump and LSTM-Shuffle both require tuning of 3 model constraint parameters describing the jumping behavior, however these vary significantly from dataset to dataset, and are chosen from a large set of values. In contrast, because our model makes skip and jump decisions dynamically, we do not have the same tuning of model constraints, and as described in Section 4.1 our parameter tuning is relatively stable independent of the dataset.

Question2: ”I am confused by Figure 1: why are the “yes/no” placed in front of the “skipped”? “Previous LSTM” is confusing as well, which should be “Previous Output/hidden state”.”

Answer2: The Yes/No refers to which LSTM state and output is used for the next time step – if the word is skipped, then the previous state and output is used, otherwise the current state and output is used. We have now clarified this in the caption of Figure 1. We have also corrected “Previous LSTM” into “Previous Output/hidden state”. 

Comment 1: ”Minor comment: The LSTM-Jump takes word2vec as the initialization in CBT, while this paper uses GLOVE. I wonder if this results in the performance difference in accuracy. From my experience, GLOVE is usually better than word2vec in most of the tasks. If this effect also applies to CBT, the experiment is not fair.”

Answer3: This question highlights our reason for reporting accuracy difference, as opposed to absolute values, since the accuracy is dependent on the embedding and (most importantly) model architecture. To answer the question, we have re-run our model on CBT-CN and CBT-NE with the word2vec embedding used in LSTM-Jump and report the results below:

CBT-CN                            Acc.     Jump    Read    FLOP-reduction
Vanilla LSTM                   0.506   
Structural-Jump-LSTM  0.526   73.0%  26.8%   4.78x

CBT-NE                            Acc.       Jump    Read     FLOP-reduction
Vanilla LSTM                   0.414
Structural-Jump-LSTM  0.423    59.4%   33.1%    3.82x

The absolute accuracy scores are lower than when using the GLOVE embedding, however the FLOP reductions and accuracy differences are similar to the GLOVE embedding setting (CBT-CN slightly better and CBT-NE slightly worse). If we replaced our original results on CBT-CN and CBT-NE with these new results, it would not change the ranking of the fastest models on those datasets.

We thank the reviewer for the insightful comments. We hope the above clarifications and paper changes related to Figure 1 sufficiently answer the questions and concerns raised by the reviewer. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SylYFwwu2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xf9jAqFQ&amp;noteId=SylYFwwu2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper514 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper514 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a fast-reading method using skip and jump actions. The paper shows that the proposed method is as accurate as LSTM but uses much less computation.

* pros: 
- very fast reading model (?). 

* cons: 
- although the paper is well written, the jump is not described in details. 
- using 'structural-jump' is a little misleading. The model will jump to ".,!" or end of sentence. What is called "structural"? Note that those punctuation marks are not 100% correlated to sentence structure. For example, "He hate fruits such as apples, pears, and oranges." The mode should jump to the end of sentence rather than the first "," when reading "such". 
- maybe the authors should say a little bit about the used computation-cost-reduction method. (I.e. in an appendix). </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1xklFrl07" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xf9jAqFQ&amp;noteId=r1xklFrl07"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper514 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper514 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review, questions and suggestions. We address both questions and suggestions below. 

Comment1: As a positive point the reviewer writes: ”very fast reading model (?).”

Answer1: The overall aim of this work was indeed to create a very fast speed-reading model. However, we would also argue that the paper contains multiple contributions in ways of achieving this goal:
1) As noted by reviewers 1 and 3, the idea of combining skipping and jumping though a multi-agent architecture has not been done previously and empirically provides state-of-the-art speed-reading results. 
2) We provide a more stable way of training the speed reading model compared to strong baselines such as LSTM-Jump and LSTM-Shuffle, which both require selecting 3 parameters describing the model constraints from a very large set of possible values. In contrast, because our model makes skip and jump decisions dynamically, we do not have the same tuning of model constraints, and as described in Section 4.1 our parameter tuning is relatively stable independently of the dataset.

Comment2: “although the paper is well written, the jump is not described in details.”

Answer2: In the first paragraph of Section 3 we describe the idea of both the skip and jump agent. The skip agent can skip a single word, thus not updating the LSTM state. If the word is not skipped, the jump agent makes a decision. In practice, both agents output when to read the next word, as the skip agent can decide to ignore the current word and the jump agent can decide to ignore all words until e.g. the next comma. We have now updated the end of Section 3.1 to better describe how the jumping is made based on the sampled action.

Comment3: “using 'structural-jump' is a little misleading. The model will jump to ".,!" or end of sentence. What is called "structural"? Note that those punctuation marks are not 100% correlated to sentence structure. For example, "He hate fruits such as apples, pears, and oranges." The mode should jump to the end of sentence rather than the first "," when reading "such".”

Answer3: Thank you for pointing this out. By "structure" we indeed refer to "punctuation structure". We have now clarified this point throughout the paper. 

Comment4: “maybe the authors should say a little bit about the used computation-cost-reduction method. (I.e. in an appendix). “

Answer4: The computation-cost-reduction method is inherent in the speed-reading model, since skipped or jumped words correspond to fewer LSTM update computations. To highlight this point, we have explained explicitly in the end of the first paragraph in Section 3, that the speed up is due to the reduced number of LSTM state update computations.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1eAQNnPnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The paper presents a new speed reading model by combined several existing ideas. The idea is novel and the results are good.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xf9jAqFQ&amp;noteId=S1eAQNnPnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper514 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper514 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper presents a novel model for neural speed reading. In this new model, the authors combined several existing ideas in a nice way, namely, the new reader has the ability to skip a word or to jump a sequence of words at once. The reward of the reader is mixed of the final prediction correctness and the amount of text been skipped. The problem is formulated as a reinforcement learning problem. The results compared with the existing techniques on several benchmark datasets show consistently good improvements.

In my view, one important (also a little surprising) finding of the paper is that the reader can make jump choices successfully with the help of punctuations. And, blindly jumping a sequence of words without even lightly read them can still make very good predictions.

The basic idea of the paper, the concepts of skip and jump, and the reinforcement learning formulation are not completely new, but the paper combined them in an effective way. The results show good improvements majorly in FLOPS.

The way of defining state, rewards and value function are not very clear to me. Two value estimates are defined separately for the skip agent and the jump agent. Why not define a common value function for a shared state? Two values will double count the rewards from reading. Also, the state of the jump agent may not capture all available information. For example, how many words until the end of the sentence if you make a jump. Will this make the problem not a MDP? 

Overall, this is a good paper.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rygL7FSlRQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xf9jAqFQ&amp;noteId=rygL7FSlRQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper514 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper514 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review, questions and suggestions. We address both questions and suggestions below. 

Question1: "The basic idea of the paper, the concepts of skip and jump, and the reinforcement learning formulation are not completely new, but the paper combined them in an effective way. The results show good improvements majorly in FLOPS. The way of defining state, rewards and value function are not very clear to me. Two value estimates are defined separately for the skip agent and the jump agent. Why not define a common value function for a shared state? Two values will double count the rewards from reading. [...]"

Answer1: In our model we choose to have a value estimate for each agent, as we posit that reading some high information words can change the state of the LSTM significantly, leading to a different value estimate from the skip agent (which is based on the old LSTM state and the input to the LSTM) and the jump agent (which is based on the updated LSTM state). In principle, if we assume the skip agent can learn how a given word will change the LSTM state, the value estimate from the skip agent could be used for the jump agent, if it is updated to reflect the cost associated with reading the word. 
We have not included in the paper the impact of this on model training and performance explicitly, due to space constraints, but we will investigate it in future work.

Question2: "[...] Also, the state of the jump agent may not capture all available information. For example, how many words until the end of the sentence if you make a jump. Will this make the problem not a MDP?"

Answer2: Whether it is a MDP depends on how we consider the setting when reading the texts. In a streaming setting where each word is continuously arriving, we would not have this information when the decision to jump is made. If we have access to the whole text, we could have access to this information, and our state therefore does not capture all relevant information when making the decision. We have chosen not to use this information, as no other related work uses “future” information when making a decision, but it can potentially give an advantage. 
Similarly to Question1, we have not explicitly extended this discussion in the paper due to space constraints, but we believe it is an interesting idea to try, to see how the policies potentially change when this information is available.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>