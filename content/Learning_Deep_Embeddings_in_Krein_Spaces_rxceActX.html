<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learning Deep Embeddings in Krein Spaces | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learning Deep Embeddings in Krein Spaces" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=r1xce3ActX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learning Deep Embeddings in Krein Spaces" />
      <meta name="og:description" content="The non-linear embedding achieved by a Siamese network is indeed a realization of a Hilbert space, \ie, a metric space with a positive definite inner product.  Krein spaces generalize the notion of..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_r1xce3ActX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning Deep Embeddings in Krein Spaces</a> <a class="note_content_pdf" href="/pdf?id=r1xce3ActX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019learning,    &#10;title={Learning Deep Embeddings in Krein Spaces},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=r1xce3ActX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">The non-linear embedding achieved by a Siamese network is indeed a realization of a Hilbert space, \ie, a metric space with a positive definite inner product.  Krein spaces generalize the notion of Hilbert spaces to geometrical structures with indefinite inner products. As a result, distances and norms in a Krein space can become negative. The negative spectral of an inner product is usually attributed to observation noise, though such a claim has never been fully studied, nor proved. Seeking how Krein spaces can be constructed from data, we propose a simple and innocent-looking modification to Siamese networks, equipping them with the power to realize indefinite inner-products. This provides a data-driven technique to decide whether the negative spectrum of an inner-product is helpful or not. We empirically show that our Krein embeddings outperform Hilbert space embeddings on recognition tasks. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Krein spaces, Deep embedding learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a solution that realizes deep embeddings in Krein spaces.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BJen3lHE27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We do not think this paper appropriate for publication in ICLR for the following reasons.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1xce3ActX&amp;noteId=BJen3lHE27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1100 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1100 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">After reading the paper, we are not sure about the main contributions of this paper. This paper is not on deep embeddings, nor on Krein spaces (as explained below). It seems that the contribution is in Section 2.1, on the explicit rewriting the distance in terms of the eigenvalues. This is from simple linear algebra, with very simple diagonalization of the matrix in the conventional Mahalanobis distance.

In the abstract, introduction and conclusion, the authors emphasize that this work is on the modification to the Siamese network architecture. However, it turns out that this paper is not on these types of networks, and the attempts to provide connections to these networks are too artificial. For example, nothing is said about these networks in the experiments.

It is not clear what is “deep” in this work. The authors did put the word “deep” in expressions like “deep Krein embeddings”, “deep embeddings”, “deep model”, “full deep”, … However, nothing is deep in the used model or embedding. The definition of the distance is not based on a deep formulation.

Section 2 is based on the concept that “a negative distance with a high absolute value will reflect a high similarity between the compared instances”, while Section 4 relies on “the prior assumption that the negative distances are equally important as positive distances”. However, it is not clear how these two concepts are investigated/applied/implemented in the corresponding sections. 

Section 4 ends up with what could be very important in general, and of great importance to understand the use of these distances: “Here,the inputs xi,xj \in R^n are output features from the deep feature extractor”.

On connections with the Krein space, the authors rewrite in (7) the distance as a difference of of two inner products. Based on this observation, they argue that this is a Krein decomposition. However, the Krein decomposition is on decomposition the inner product as a difference of two inner products. Not the distance, nor the squared distance. Otherwise, any distance can be written as a difference of two inner product.

An important part of this paper is on the initialization, which is also the case in the experiments (“we pre-train…”, “For pre-training…”). However, the most important part needs to be on the learning algorithm, and the so-called “data-driven technique”; however, almost nothing is given on this.

The initialization is too simple, relying on some parametric assumptions. Moreover, the initialization uses a simple PCA projection. This may deeply affect the spectral distribution. The authors need to examine in detail the influence of this initialization on the negative embedding space.

In Section 6.1 “Visualization”, we are a bit disappointed how the authors operated the visualization, by subtracting the least distance. By the way, one should not say “largest negative value”, but the absolute one. In general, we do not understand why the proposed method gives “the best discrimination”, since we are not using the labels when computing the distance.

There are many typos:
- In most of the paper (and mostly in the introduction), the authors need to use \citep not \cite.
- “our discussion often requireS”
-“Mahalanobis distaNce”
- L should be bold everywhere because it is a matrix
- “embeddng”
- “between between two vectors”
- “interms”
- “Simalry”
- “Scalling” -&gt; scaling
- correct name “Müller”</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1lR3blJhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Embeddings in Krein space - review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1xce3ActX&amp;noteId=r1lR3blJhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1100 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1100 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">LEARNING DEEP EMBEDDINGS IN KREIN SPACES

Quality: average
Originality: original
Significance: relevant for ICLR
Pros: - interesting idea -- see detailed comments
Cons: limited experiments -- see detailed comments

The underlying idea is interesting and probably novel. Large parts
of the paper are a bit straight forward although sometimes a complicated
described and not always with the reference to existing work (but I am
willing to believe that the authors were not aware of it). Although
the paper is conceptually a nice thing the experiments are very limited
and not yet convincing. In particular it remains rather unclear if the
effort in learning and using an indefinite metric for represent the data
is valuable.

- your approach makes essential use of label information - I would like to see this reflected in the title - e.g. by adding 'supervised'
- I may suggest to add a recent review about indefinite learning, Neural Computation by P. Tino and F.-M. Schleif
  some parts of your intro (and references) could be a bit summarized in this way as well
- 'The negative spectral of an inner product is usually attributed to observation noise, though such a claim has never been fully studied, nor proved.'
  - well and as indicated in the review above and by discussed work from Pekalska / Duin and others it is actually multiple times true that the negative part of the inner
  spectrum contains relevant information. There are e.g. datasets (protein data - see Tino) get very bad classification models if the negativ contributions are
  removed - accordingly they are not just noise but contain valuable information to the problem
- it would be good to add a few sentences at the beginning of the paper to (super brief) review the core idea/concept of siamese networks
- the english is sometimes a bit bulky and it would be good to check it by a native speaker
  e.g ' proposal do not require' -- proposal does not require
  ' unlike the our'
  - follow by a spell-checker 'embeddng', 'interms', 'Simalry'
- 'followed by removing it by flipping' - with flipping the contribution is not 'removed' but mapped to the positive part of the spectrum
- how does your approach compare to a classical embedding of the indefinite kernel matrix into a pseudo euclidean space (see e.g. Tino or Pekalska)?
  -- there you end up also with a vectorial representation of the similarities - with some issues like complexity of the embedding or valid generalization to new test points ...
- I am a bit surprised that you completely skipped the work by Pekalska and Duin -&gt; I suggest to have a look into their work and link a bit to it 
  (there is also a book by Pekalska around this topic)
- in Def 1 - I would call it a squared Mahalanobis distance because it is missing the square root (cmp squared Euclidean distance, where M is an eye matrix)
- 'feature vectors from a non-linear deep model' - obtained in which way?
- just a hint: you may find the work of Michael Biehl on Matrix Learning in prototype networks interesting - your matrix L is called omega or lambda there 
- if I am not misleading there was a paper at AAAI a few year ago about negative locality sensitive hashing where such an embedding is discussed (- not in the
  same way as you do - but related)
- in some way Eq 4 prunes down to a eigenvalue decomposition of your matrix M - in metric learning M could be e.g. the covariance matrix of the data and decomposing
  it into E * V * E' - will give you exactly this
- you allow for quite some flexibility in learning your projection matrix in Eq 5 - how do you make sure that this is not oversimplifying the problem? - E.g. if 
  I place this into a supervised learning context it could easily happen that the mapping fits in perfect alignment to the training data but may be much less
  effective (or even useless) if it is applied on test data
- w.r.t. Remark 1: 'Furthermore, we consider that a negative distance always indi- cates better similarity than a positive or a “0” distance.' -- if this is reflected
  in your model/optimization I would expect that the learned matrix W/Lambda will always aim on negative distances? - shouldnt it?
- 'Here, we show that our distance ...' - this is basically equivalent what is shown already in the book by Pekalska ( I am not sure if it is original proposed/shown by
  them but it least it is there - maybe from 2001 or so)
- 'In other words, the numbers p + and p − are entirely decided by the data' - well yes, but I would slightly reformulate this. The amount of negative spectrum elements (p-)
  and positive spectrum elements (p-) originates within the metric learning optimization and is not pre-specified by the user.
- around Eq 8 a multivariate Gauss (exp) is not shown there ...
- \Sigma_{s,d} - is simply the covariance matrix of the pairwise difference vectors z from all point of the positive class (assuming that the data in (z_i - z_j) are centered to the mean 
  (otherwise this is not a valid covariance matrix formulation in the statistical sense) - so you may expect that your original data X are centered 
	-- I think you could try to make this a bit more clear 
- using this approach you encoded already in the metric initalization the discrimination problem in an explicit way
- I feel a bit uncomfortable with a related work section in sec 5 it really doesnt fit in the flow of your article there
  - I think it should come earlier (after the intro) - it also repeats currently parts of the intro
  and the way to sec 5 - just reorganize
- '(Fine &amp; Scheinberg, 2001)' - why this reference to kernel approaches - why not a text book e.g. by N Cristianini
- typos ref 'Laub &amp; MÃžller' 
- please provide a full crossvalidation e.g. 10 repeated runs and report mean/std-dev. a simple split in 1 train/test set could give just a lucky shot
- the improvements shown in Tab 1 are very minor ! - because you had to do some processing in the metric learning/ parameter decisions it is hard to judge
  whether the tiny benefits are really due to the non-metric behavious. Could you please also provide - data - (not only plots) how far the metric contains now negative spectrum contributions?
  (a measure is give in Tino et al.) - I would give a link to Figure 2 here. Have you done some normalization in Fig 2 - I am very much surprised about the
  scaling?
- in your position I would look for a few more challenging datasets such that the claimed benefit of your method is more clear to observe
- 6.1.1 MDS is a very old and in my view outdated method which should not be used any longer - there are many new more effective visualization methods which also work
  for dissimilarities as input (maybe with the same shift preprocessing) - see e.g. t-SNE. But, there would also be an approach by Laurens v. d. Maaten about 
  non-metric t-SNE or so which you could use straight away. (for some comparison on embedding methods see work by J.A. Lee &amp; Verleysen or A. Gisbrecht, Wiley  
  Fig 1 does not say much and is not discussed a lot
- Some references are incomplete (Hal Daumé III.); Gregory Koch.
- there is a broken reference in 8.1.1 -- Table ?? </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BkeA2nY0jm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The theoretical and experimental aspects of the paper are weak</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1xce3ActX&amp;noteId=BkeA2nY0jm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1100 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1100 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes to learn representations by using Krein inner products which are generalizations of standard inner products in Hilbert spaces. 
A Krein inner product can be formulated as the difference between two inner products, each in a Hilbert space.
Although the paper mentions three main contributions, the contributions of the papers are mainly:
- 1. using Krein distances instead of Hilbert distances to compare examples
- 2. better empirical performance on some datasets

The paper is clear in general, and maybe provides too many details on standard linear algebra (about 3 pages to explain the eigendecomposition of symmetric matrices). 
My main concern is about novelty. The proposed method has already been published in the literature but never been called "representation in a Krein space" (see Novelty point).

1. Motivation:

It is already known in the literature that Hilbert spaces are limited for certain tasks, non-Euclidean distances have then been used to compare examples.
For instance, based on the work of [A], some works have proposed to learn deep representations exploiting hyperbolic distances to represent data with hierarchical structure [B].
The motivation of the superiority of Krein representations is not clear in the paper. In what specific contexts is it better to use Krein spaces?

2. Novelty:

The authors take the example of Mahalanobis-like distance metric learning where a distance function parameterized by a symmetric matrix usually constrained to be positive semi-definite (PSD) is learned. The paper argues that constraining the matrix to be PSD limits the expressiveness of the model as the negative spectrum is also meaningful. 
If the PSD constraint is removed, the problem simply corresponds to learning a symmetric matrix. Indeed, by definition, symmetric matrices have real eigenvalues which can be either non-negative or negative. The PSD constraint enforces the eigenvalues to be non-negative. If the PSD constraint is removed, then the symmetric matrix can be indefinite, which corresponds to the proposed representation in Krein space.

Some papers have already proposed to remove the PSD constraint (e.g. [C], Section 2.3) when learning a distance parameterized by a symmetric matrix, which corresponds to the proposed model. However, they did not sell the relaxation of the PSD constraint as a contribution.

3. Experiments:

The experimental section is weak as it provides a simple experiment on a toy dataset, and quantitative results on 4 datasets only for one optimization problem and one specific network architecture. 
Since the theoretical aspect of the paper is weak, I would expect more comparisons by replacing standard deep metric learning formulations (that use the squared Euclidean distance) with the proposed Krein distance. For instance, the submission cites (Oh Song et al.), and (Schroff et al.) as deep metric learning approaches. How would the proposed Krein distance perform in the contexts considered in those references?

Demonstrating that the Krein distance also outperforms the squared Euclidean distance in that case would strengthen the experimental aspect of the paper.


In conclusion, the theoretical and experimental aspects of the paper are weak.


[A] Gromov, Hyperbolic groups, 1987
[B] Ganea et al, Hyperbolic Neural Networks, NIPS 2018
[C] Guillaumin et al., Is that you? Metric learning approaches for face identification, ICCV 2009</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>