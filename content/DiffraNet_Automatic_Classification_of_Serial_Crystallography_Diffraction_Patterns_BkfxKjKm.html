<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>DiffraNet: Automatic Classification of Serial Crystallography Diffraction Patterns | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="DiffraNet: Automatic Classification of Serial Crystallography Diffraction Patterns" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BkfxKj09Km" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="DiffraNet: Automatic Classification of Serial Crystallography..." />
      <meta name="og:description" content="Serial crystallography is the field of science that studies the structure and properties of crystals via diffraction patterns. In this paper, we introduce a new serial crystallography dataset..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BkfxKj09Km" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>DiffraNet: Automatic Classification of Serial Crystallography Diffraction Patterns</a> <a class="note_content_pdf" href="/pdf?id=BkfxKj09Km" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019diffranet:,    &#10;title={DiffraNet: Automatic Classification of Serial Crystallography Diffraction Patterns},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BkfxKj09Km},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Serial crystallography is the field of science that studies the structure and properties of crystals via diffraction patterns. In this paper, we introduce a new serial crystallography dataset generated through the use of a simulator; the synthetic images are labeled and they are both scalable and accurate. The resulting synthetic dataset is called DiffraNet, and it is composed of 25,000 512x512 grayscale labeled images. We explore several computer vision approaches for classification on DiffraNet such as standard feature extraction algorithms associated with Random Forests and Support Vector Machines but also an end-to-end CNN topology dubbed DeepFreak tailored to work on this new dataset. All implementations are publicly available and have been fine-tuned using off-the-shelf AutoML optimization tools for a fair comparison. Our best model achieves 98.5% accuracy. We believe that the DiffraNet dataset and its classification methods will have in the long term a positive impact in accelerating discoveries in many disciplines, including chemistry, geology, biology, materials science, metallurgy, and physics.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Serial Crystallography, Deep Learning, Image Classification</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We introduce a new synthetic dataset for serial crystallography that can be used to train image classification models and explore computer vision and deep learning approaches to classify them.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_r1gFixco3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Well written and organized paper. Domain concepts presented fairly clearly. The approach and choices of classification algorithms is well articulated and results interesting. Good combination of known algorithms on a purposely built dataset (Originality questionable).</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkfxKj09Km&amp;noteId=r1gFixco3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper414 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper414 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Contribution:
	- Using a known parameters crystallography simulator (X-ray beam, structure being analyzed, environment (crystalline or not)) built a dataset (called DiffraNet) of 25,000 512x512 grayscale labeled images of resulting diffraction images of various materials/structures (crystalline or not) .
	- carried various classification approaches of the dataset (labelled) images in two steps:
		- Feature extraction (Scale Invariant Feature Transform with the Bag-of-Visual-Words approach as local feature extractor, and the Gray-level Co-occurrence Matrix and Local Binary Patterns as global feature extractor) then
		- Classification of the diffraction images is carried with three approaches. Two using images described by extracted features (from the previous feature extraction step) coupled with either random Forests or Support Vector Machines and a third consisting in a Convolution Neural Network (CNN) topology named DeepFreak.
		- The images are classified according to the diffraction patterns they encompass into one of 5 classes: blank, no-crystal, weak, good and strong. The last three describing presence of a crystalline structure.
		- A fine tuning step of the various algorithms was carried using AutoML optimization tools.
All algorithms were off the shelf publicly available implementations and have previously been used for such domain applications (crystallography patterns).
The approach and choices of classification algorithms is well articulated and results interesting.

A few questions though:
•	In what way the diffraction images are ‘synthetic’? Aren’t they actual diffraction images but in a controlled known and controlled setting: set of parameters (beam, structure to analyze)?
o	More like a library of diffraction pattern images for various materials/structures.
•	How many structures were analyzed (Were there 25000 for the 25000 pattern images), one image each?
o	This is to understand  the representability of the samples (structures) analyzed regarding the possible structures (Hundreds of thousands as per paper’s 2.1 ) .
•	What variations for each of the setting variabilities (X Ray beam(flux, beam size, divergence, and bandpass), crystal properties (unit cell, number of cells, and a structure factor table), and the experimental parameters (sources of background noise, detector point-spread, and shadows)) were used? 
o	This is to assess the size of the pattern space.
•	Were any real-life setting obtained pattern samples classified using DiffraNet dataset patterns’ fine-tuned classification algorithms?
o	This is to assess the generalization level of the DiffraNet dataset patterns’ fine-tuned classification algorithms to real-life obtained patterns (relates to the previously stated representability of the samples).
o	If not, your statement “ … we plan to add new images and new classes that are common place in serial crystallography” (in 6. Conclusions) would be an appreciated validation of general usability of your DiffraNet fine–tuned setting.
•	Were all the structures analyzed crystalline? 
o	It’s stated in Figure 2 and Table 6 that 2 classes are either blank or no-crystal but is that a known fact (purposely chosen) or no pattern images for crystalline structures due to inadequate experimental settings to uncover the crystalline nature of the analyzed structure?
•	Were the pattern images pre-processed in any manner before being classified?


Nota: In table 6, use no-crystal class as in Figure 2 for consistency. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BkeHnOh53m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting application domain, but no novelity in terms of methods; For a synthetic dataset, I would like to see the impact to real test data</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkfxKj09Km&amp;noteId=BkeHnOh53m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper414 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper414 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper introduces a purely synthetic dataset for crystallography diffraction patterns. For this very specific application domain, this might be a very welcome approach, however, I feel the paper is not strong enough for ICLR for two reasons:

1. The scope is too narrow for ICLR. Only a limited readership will be interested in this specific problem. Since the contribution is mainly on the dataset level and not on the methodological level, I suggest submitting such an article in venues more focused on the application domain. I can see no contribution which is general enough to be interesting for the broader readership. A new dataset might be of interest if it is a really challenging one where current approaches cannot yield high performance levels while it would be easy for domain experts to recognize.

2. The experiments are only on synthetic data. I agree that synthetic data in general can be very useful, if generated correctly (this has been shown in many works). For a substantial article contribution, one should, however, in general add much more exhaustive experiments. Besides analyzing the behavior on synthetic data, one should perform tests on real data and see the influence of, e.g., pre-training on this synthetic dataset. Furthermore, comparison to pre-training on other datasets should be performed. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJeqFsAajm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The paper described a new open synthetic dataset for serial crystallography and compared three approach to classify these data. The paper is well written, clear and contribute an original dataset. The significance of the work need to be justified further by making comparison to real data. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkfxKj09Km&amp;noteId=SJeqFsAajm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper414 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">24 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper414 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper describes a new open synthetic dataset for serial crystallography generated by a simulator. Three methods are proposed and implemented to demonstrate the classification of these diffraction images. The results from these methods are compared and clearly show the ones achieve high performance. The article structure is clear and is well written. The experiments are carried out in a professional way and statistical analysis is shown. It will be better if the authors can demonstrate how the models obtained from training the synthetic data perform in real scenario. Please also add some discussion on how good the synthetic data simulate the real data. Some image comparison between the synthetic data and real data should be analysed. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJlULFh9n7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>I think that realistic tests are crucial</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkfxKj09Km&amp;noteId=BJlULFh9n7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper414 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018</span><span class="item">ICLR 2019 Conference Paper414 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">You state that it would be better to perform experiments on real scenarios. I would argue that this is crucial to be perfomed. And also it is required to see the influence of domain-shift. How better would be pre-training on this synthetic data compared to pre-training on imageNet, for example.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>