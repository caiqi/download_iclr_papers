<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Nonlinear Channels Aggregation Networks for Deep Action Recognition | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Nonlinear Channels Aggregation Networks for Deep Action Recognition" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rJgdHs05FQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Nonlinear Channels Aggregation Networks for Deep Action Recognition" />
      <meta name="og:description" content="We introduce the concept of channel aggregation in ConvNet architecture, a novel compact representation of CNN features useful for explicitly modeling the nonlinear channels encoding especially..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rJgdHs05FQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nonlinear Channels Aggregation Networks for Deep Action Recognition</a> <a class="note_content_pdf" href="/pdf?id=rJgdHs05FQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019nonlinear,    &#10;title={Nonlinear Channels Aggregation Networks for Deep Action Recognition},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rJgdHs05FQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We introduce the concept of channel aggregation in ConvNet architecture, a novel compact representation of CNN features useful for explicitly modeling the nonlinear channels encoding especially when the new unit is embedded inside of deep architectures for action recognition. The channel aggregation is based on multiple-channels features of ConvNet and aims to be at the spot finding the optical convergence path at fast speed. We name our proposed convolutional architecture “nonlinear channels aggregation networks (NCAN)” and its new layer “nonlinear channels aggregation layer (NCAL)”. We theoretically motivate channels aggregation functions and empirically study their effect on convergence speed and classification accuracy. Another contribution in this work is an efficient and effective implementation of the NCAL, speeding it up orders of magnitude. We evaluate its performance on standard benchmarks UCF101 and HMDB51, and experimental results demonstrate that this formulation not only obtains a fast convergence but stronger generalization capability without sacrificing performance.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">action recognition, convolutional neural network, network training</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">An architecture enables CNN trained on the video sequences converging rapidly </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_r1ei-Desh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nothing particularly new</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJgdHs05FQ&amp;noteId=r1ei-Desh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper103 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper103 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">First the paper is poorly written. The abstract contains the glaring typo "optical convergence", presumably meant to be "optimal convergence" and the last two paragraphs of page 2 are near identical repeats. Besides these egregious errors that could easily be fixed with better proofreading, the text and mathematics are difficult to follow and not precise. For example, in Equation (1), i and j appear both as subscripts and feature map indices.

Second, the approach is not particularly well motivated nor analysed. Why for example use cosine and sine functions? Is this some connection to Fourier domain analysis? Moreover, since neural networks can approximate other functions, is there an equivalent (possibly slower and deeper) multi-layer perceptron equivalent to the proposed aggregation functions?

Last, the experimental results are not very compelling with state-of-the-art on HMBD51 now around 72% and UCF101 now around 95% (Feichtenhofer et al., 2017). The faster convergence time (demonstrated as more rapid decrease in loss as a function of training iteration) is also underwhelming.

All up more work needs to be done in the paper for it to make a valuable contribution.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Skxtoryc37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Poor presentation and weak technical contents.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJgdHs05FQ&amp;noteId=Skxtoryc37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper103 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper103 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: 
This paper presents a non-linear channel-aggregation layer (NCAL) for deep neural networks.
To globally aggregate channel features, NCAL first projects the feature map along the channel dimension into the new feature space of the same channel dimensionality by means of trigonometric functions like discrete cosine transform (DCT).
So-transformed feature map is multiplied with the original one in a point-wise manner (Hadamard Product), and then passed to the next layer in the neural network.
The experimental results on action classification using UCF101 and HMDB51 show that the NCAL slightly boosts the performance.


Comments:
The reviewer regards this paper falls below the border due to the following negatives.

1. Poor presentation
- This paper is written in poor English, containing considerable amount of grammatical errors. The reviewer strongly recommends the authors to proof-read the manuscript before submission.

- As to mathematical presentation, the notations are conflicted and/or confusing; especially, in Eq.(1), the notations of i and j are conflicted and thus it is difficult to understand its mathematical meaning at first glance, though it actually shows a simple "3D" conv. And, some functions that compose the method, such as G and H, are not clearly defined throughout the paper.

- The term of "hand-crafted rule" is found several times in this paper, but it is unclear since the authors do not give any definition nor examples for the term. What does it actually mean?

- It is trivial to show Fig.5 which merely explains trigonometric functions.

2. Technical content
- First of all, the reviewer guesses that the authors might misunderstand the "3D" operations, such as 3D-conv and 3D-pool. In the literature of action recognition, the 3D operations generally work in the spatio-temporal domain (X-Y-T = "3"D); thus, the 3D pooling aggregates features across X-Y-T space and the 3D conv filters are usually applied to the whole feature channels, except for grouped conv. This misunderstanding would be a critical flaw of this work, leading to the incorrect comparison described below.

- It is unclear why the global channel aggregation (3) is necessary; the theoretical reason/motivation is not found in this manuscript. In ConvNet, feature channels encode different characteristics and thus it makes less sense to aggregate those heterogeneous information; note that the 3D-pooling aggregates them along X-Y-T, NOT along channel. And, why do the authors employ trigonometric functions in (3)? Such a transformation is closely related to DCT that maps a feature "sequence" into "frequency" domain. From this perspective, the reviewer has two concerns about the forms (3,4).
 First, extracting frequency characteristics along channel dimension via (3) would be less effective. The frequency information is valuable only for the signals that are sequenced in structured order such as on temporal and/or spatial domain. The CNN feature channels, however, do not exhibit such structural order since we can arbitrarily swap the feature channels by manipulating the filter weights.
 Second, it is not clearly discussed what kind of physical characteristics are extracted through the multiplication of the original feature and the frequency feature in (4). The reviewer does not understand how useful so-combined features are for classification.
 As to the second issue, the multiplication (Hadamard Product) can also be seen in the literature of bilinear model [a]. In that framework, the multiplication form is naturally derived from the low-rank approximation of bilinear weights through introducing trainable weights in contrast to this work that employs the fixed weights (W^1 and W^2).

- The temporal information of the feature vector sequence {V_k} is not considered in this work, though the objective of this paper is "action" recognition as the title says. The channel aggregation along the temporal axis is discussed with thorough experiments in [b].

[a] Jin-Hwa Kim, Kyoung Woon On, Woosang Lim, Jeonghee Kim, Jung-Woo Ha, and Byoung-Tak Zhang. "Hadamard Product for Low-rank Bilinear Pooling". In ICLR2017.
[b] Carreira, J., &amp; Zisserman, A. "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset". In CVPR2017.

3. Performance
- The performance improvement is quite limited, compared to the standard CNN. To convince the readers of the improvement by the method, the authors have to show the statistical significance such as by reporting the standard deviation of test accuracies over three splits in those datasets. And, as mentioned above, the method of "3D convolution" would be wrongly implemented.

- Although the authors state many times that the method makes convergence in training faster, the charts in Fig.3~4 do not support the authors' claim; from those charts, all the methods are converged around 300 iterations. Note that the convergence point can be found when the training loss does not decrease any more, and the training loss value itself is not a proper measure for comparing the performance of the models; we can easily reduce training loss by employing the more complex models, though they poorly work on test samples due to overfitting.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJxMaQHFhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Non linear 'channel aggregation' to improve on 3D convs and various 3D pooling methods. Badly written paper with low novelty and limited and unconvincing empirical evaluation.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJgdHs05FQ&amp;noteId=BJxMaQHFhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper103 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper103 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary
The paper proposes a method for channel aggregation in CNNs for the task of human action 
recognition. The main component proposed is the nonlinear channel aggregation layer (NCAL) which can 
be potentially inserted in any CNN architecture. Empirical results are reported on UCF101 and 
HMDB51.

General comments:
The paper is not very well written and it's hard to fully understand it. From the related works 
section I deduce that the method proposed competes with 3D convs and 3D pooling methods in some way. 
With this assumption my specific points are as follows.
                                              
                                       
Positive                        
- The general direction is somewhat interesting, i.e. the current methods methods do a linear 
  computation over the channels at any intermediate step of the CNN architecture, this paper aims to
  explore more complex nonlinear computations. 
  
  
Negatives:
- The paper is badly written with many hard to appreciate parts e.g.

"We conjecture that there is complex nonlinear relationship among the channels of CNN features. Once
this implicit relationship is explicitly modeled, such accomplishment will facilitate converging
with faster search to the optimal trajectory."
What exactly do you mean by the implicit relationship? How would you get ideas to explicitly model 
it?                                                                                         

"these temporal pooling and variants tend to result in the reduced resolution, which will achieve
coarse classifications"                 
In what sense reduced resolution, eventually everything has to be reduced to a #classes size vector 
giving the probability of the classes. How would you fix this?

- The main equation (3) is not understandable, G is said to be the "linear kernel functions of 
  NCAL", what is that? and what does G(cp) mean, what is p?

- The positioning of the paper is also not very clearly defined, sometimes it argues as being an 
  improvement of 3D pooling, while at others it seems to try and improve frame aggregation
  for video representation. This should also be clearly clarified and discussed.

- The exprimental results are also not very convincing, on UCF101 compared to the baseline accuracy 
  of 79.5 the best NCAL version gets 80 (and this is not the cross-validated value) while on HMDB51 
  it gets 50 cf. std CNN's 49.7


Unfortunately, overall the paper is hard to understand and the empirical evaluation is not 
convincing.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>