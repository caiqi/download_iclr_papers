<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Bayesian Deep Learning via Stochastic Gradient MCMC with a Stochastic Approximation Adaptation | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Bayesian Deep Learning via Stochastic Gradient MCMC with a Stochastic Approximation Adaptation" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=S1grRoR9tQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Bayesian Deep Learning via Stochastic Gradient MCMC with a..." />
      <meta name="og:description" content="We propose a robust Bayesian deep learning algorithm to infer complex posteriors with latent variables. Inspired by dropout, a popular tool for regularization and model ensemble, we assign sparse..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_S1grRoR9tQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Bayesian Deep Learning via Stochastic Gradient MCMC with a Stochastic Approximation Adaptation</a> <a class="note_content_pdf" href="/pdf?id=S1grRoR9tQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 15 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019bayesian,    &#10;title={Bayesian Deep Learning via Stochastic Gradient MCMC with a Stochastic Approximation Adaptation},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=S1grRoR9tQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=S1grRoR9tQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We propose a robust Bayesian deep learning algorithm to infer complex posteriors with latent variables. Inspired by dropout, a popular tool for regularization and model ensemble, we assign sparse priors to the weights in deep neural networks (DNN) in order to achieve automatic “dropout” and avoid over-fitting. By alternatively sampling from posterior distribution through stochastic gradient Markov Chain Monte Carlo (SG-MCMC) and optimizing latent variables via stochastic approximation (SA), the trajectory of the target weights is proved to converge to the true posterior distribution conditioned on optimal latent variables. This ensures a stronger regularization on the over-fitted parameter space and more accurate uncertainty quantification on the decisive variables. Simulations from large-p-small-n regressions showcase the robustness of this method when applied to models with latent variables. Additionally, its application on the convolutional neural networks (CNN) leads to state-of-the-art performance on MNIST and Fashion MNIST datasets and improved resistance to adversarial attacks. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">generalized stochastic approximation, stochastic gradient Markov chain Monte Carlo, adaptive algorithm, EM algorithm, convolutional neural networks, Bayesian inference, sparse prior, spike and slab prior, local trap</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">a robust Bayesian deep learning algorithm to infer complex posteriors with latent variables</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BJeOIPB9nQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Unclear benefits of SG-MCMC with SA and the experiments are not sufficiently convincing</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1grRoR9tQ&amp;noteId=BJeOIPB9nQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper894 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper894 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors describe a new method of posterior sampling with latent variables based on SG-MCMC and stochastic approximation (SA). The new method uses a spike and slab prior on the weights of the deep neural networks to encourage sparsity. Experiments on toy regressions, classification and adversarial attacks demonstrate the superiority over SG-MCMC and EMSV.

Compared to the previous work EMSV (ESM), the novelty of SG-MCMC-SA is replacing the MAP in EMSV by SG-MCMC with stochastic approximation to alleviate the local trap problem in DNNs. However, I did not see why SG-MCMC with SA can achieve this goal. It is known that SG-MCMC methods tend to get trapped in a local optimal [1]. How did SA solve this problem? Besides, it is unclear to me where Eq. 17 uses stochastic approximation. The authors need to explain more about stochastic approximation for the readers who are not familiar with this method. 

Empirical results on a synthetic example, MNIST and FMNIST show that SG-MCMC-SA outperforms the previous methods. However, the improvements of the proposed method are marginal. MNIST and FMNIST are small and easy datasets and it is very hard to tell the effectiveness of SG-MCMC-SA. It would be more convincing to show the empirical results on other datasets, e.g. CIFAR, using some larger architectures. The comparison would be more significant in that case. 

[1]. Zhang, Yizhe, et al. "Stochastic Gradient Monomial Gamma Sampler." arXiv preprint arXiv:1706.01498 (2017).
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJlutyw5am" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply from Author</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1grRoR9tQ&amp;noteId=SJlutyw5am"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper894 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper894 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the valuable comments.

Q1: How does SG-MCMC-SA alleviate the local trap problem in DNNs from replacing the MAP in EMVS by SG-MCMC with stochastic approximation.

A1: The advantage of the SG-MCMC-SA over EMVS in alleviating local traps in DNN training can be explained as follows:

The EMVS algorithm is designed for linear regression models, although the idea can be extended to nonlinear models. However, when extending to nonlinear models, such as DNNs, the M-step will not have a closed-form update anymore. A trivial implementation of the M-step will likely cause a local-trap problem. To tackle this issue, we replace the E-step and the M-step by SG-MCMC with the prior hyperparameters tuned via stochastic approximation. 

During the past two years, there are quite a few papers studying the convergence rate of SGLD, the most basic SG-MCMC algorithm, on non-convex functions. For example, [1] concludes that SGLD is able to escape from shallow local optima in an acceptable training time given a suitable temperature, ruling out local optima that only exist in the empirical risk but are not present in the smooth population risk. The work [2], [3] and [4] show the convergence rate of the global optimization error in expectation; that is, given long enough running time, Bayesian model averaging is guaranteed to achieve better solutions than SGD. 

In a nutshell, the sampling algorithm has a non-convex optimization ability. In SG-MCMC-SA, the introduction of latent variables controls over-specified models through adaptive learning for the prior hyperparameters, which tends to lead to an optimal regularization for the DNN structure and weights. 
This also explains why SG-MCMC-SA can achieve better prediction performance than the existing algorithms.


Q2: It is unclear where Eq.(17) uses the stochastic approximation. The authors need to explain more about stochastic approximation.

A2: Thanks for your suggestions. We updated the Eq.(17) and the section of stochastic approximation in appendix A.2 to show more details. The item $g_{\theta^{(k)}}(\beta^{(k+1)})-\theta^{(k)}$ in Eq.(17) is to construct the biased (unbiased as $k\rightarrow \infty$) estimator $H(\theta, \beta)$ as in Eq.(23) in the updated paper.

Q3: Empirical results show that the improvement of the proposed method is marginal.

A3: Although MNIST is a popular dataset in computer vision which has been highly optimized and Fashion-MNIST is a slightly smaller dataset than CIFAR-10, achieving state-of-the-art performance is never easy. There are no models using an unsophisticated network with less than five layers to achieve more than 99.60\%/ 99.75\%/ 99.79\% accuracy on MNIST and more than 93.01\%/ 94.38\%/ 94.78\% on Fashion-MNIST. The state-of-the-art performance on those two datasets under different settings demonstrates the strength of our proposed algorithm. Due to space limitations, we only reported the state-of-the-art results. The improvement over SG-MCMC is also statistically significant in a two-sample t-test with multiple runs.

We do believe a comprehensive evaluation of our proposed algorithm on larger models like Network-in-Network and larger datasets like CIFAR-10 and Street View House Numbers (SVHN) will be more convincing. We will include more experiment results in the camera-ready version if accepted.

We thank the reviewer for the helpful comments and would appreciate further discussions.


[1] Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of stochastic gradient Langevin dynamics. In Proc. of Conference on Learning Theory (COLT), pages 1980–2022, 2017.

[2] Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic gradient Langevin dynamics: a nonasymptotic analysis. ArXiv e-prints, June 2017.

[3] Pan Xu, Jinghui Chen, Difan Zou, and Quanquan Gu. Global convergence of Langevin dynamics based algorithms for nonconvex optimization. ArXiv e-prints, February 2018.

[4] X. Cheng, N. S. Chatterji, Y. Abbasi-Yadkori, P. L. Bartlett, and M. I. Jordan. Sharp Convergence Rates for Langevin Dynamics in the Nonconvex Setting. ArXiv e-prints, Sep 2018.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ByguuVqt3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting work, but in my view not substantial novelty and significance</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1grRoR9tQ&amp;noteId=ByguuVqt3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper894 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper894 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">TITLE
Bayesian deep learning via stochastic gradient mcmc with a stochastic approximation adaptation

REVIEW SUMMARY
Fairly well written paper on SG-MCMC type inference in neural networks with slab and spike priors. In my view, the originality and significance is limited.

PAPER SUMMARY
The paper develops a method for sampling/optimization of a Bayesian neural network with slab and spike priors on the weights.

QUALITY
I belive the contribution is technically sound (but I have not checked all equations or the proof of Theorem 1). The empirical evaluation is not unreasonable, but also not strongly convincing.

CLARITY
The paper is fairly well written, but grammar and use of English could be slightly improved (not so important).    

ORIGINALITY
The paper builds on existing work on EM-type algorithms for slab and spike models and SG-MCMC for Bayesian inference in neural networks. The novelty of the contribution is limited: The main contribution is the combination of the two methods and some theoretical results. I am not able to judge if there is significant originality in the theoretical results (Theorem 1 + Corr 1+2) but if I am not mistaken it is more or less an application of a known result to this particular setting?

SIGNIFICANCE
While I think the proposed algorithm is reasonable and most likely useful in practice, I am not sure the contribution is substantial enough to gain large interest in the community.  

FURTHER COMMENTS
Figure 2 (d+e) are in my view not so useful for assessing the training/test performance, but I am not even completely sure what the figures shows, as there are no axis labels. I would prefer some results on the loss, perhaps averaged over multiple data sets.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJeHCkv5pX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply from Author </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1grRoR9tQ&amp;noteId=BJeHCkv5pX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper894 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper894 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the useful feedback. We have clarified some important points about our work below -- in particular about the originality of our paper.

Q1: The main contribution is the combination of EMVS and SG-MCMC, the theoretical analysis is just an application, the novelty of the contribution is limited.

A1: There is an essential difference between a naive extension of EMVS to DNN and our proposed SG-MCMC-SA, we emphasize that we are not combining EMVS and SG-MCMC. This is the first variable-selection algorithm in DNN that solves the computational issue and achieves the state-of-the-art performance in classical CV datasets like MNIST and Fashion-MNIST with replicable experiment results. The convergence rate is shown based on loose assumptions, inspired by stochastic approximation theory [1]. 

In addition, the proposed algorithm alleviates the local trap problem for the latent variables when we directly apply EMVS to deep neural networks. For the detailed reason why SG-MCMC-SA alleviates the local trap problem, please refer to our response A1 to reviewer3. If you are interested in the details of stochastic approximation, please refer to our updated section A.2 in the appendix. Briefly speaking, SG-MCMC-SA are coupled processes which mutually reinforce each other: the sampling algorithm has non-convex optimization ability to enhance the robustness of latent variables, the adaptive update of latent variables based on the stochastic approximation theory avoids the radical updates of the latent variables and rigorously equips the sampling algorithm with an optimal regularization.


Q2: The contribution may not be substantial enough to gain large interest in the community

A2: For one thing: variable selection in high dimensional non-linear systems is still an open problem. The three main obstacles are: (i) unknown functional form of the nonlinear system, (ii) high computational cost, and (iii) variable selection consistency [2]. Our approach overcomes the first two difficulties by employing DNN for the universal approximation and stochastic approximation for the computational speedup. It gains great computational efficiency in deep neural networks and paves the way for efficient variable selection in deep neural networks in the future to avoid over-specified models.

For another: as long as dropout is used in the deep learning framework, our proposed SG-MCMC-SA potentially makes more robust inference than the dropout-based method. In addition, it can be generalized to many other adaptive algorithms, especially the EM-based algorithms. The model with spike-and-slab priors is only one example.


Q3. Regarding the figures (d+e)

A3: We appreciate your suggestions. We believe the figures (d+e) are also essential to show the over-fitting problem of the general SGLD based on the over-specified models. These dots in figures (d+e) are response values y given the predictor values X and the estimated $\beta$ from SGLD-SA, SGLD and EMVS respectively, the x-axis represents the different indices of the response values, the y-axis represents the scales of the response values. We already added some description to the newly submitted version to avoid confusion. The estimated response values y from SGLD is close to the true values in the training set (Fig.2(d)), but are far away from them in the testing set (2(e)), indicating the over-fitting problem of SGLD without proper regularization.

We thank the reviewer for the helpful comments and would appreciate further discussions.

[1] Albert Benveniste, Michael Métivier, and Pierre Priouret. Adaptive Algorithms and Stochastic Approximations. Berlin: Springer, 1990.

[2] Liang, F., Li, Q. and Zhou, L.  (2018) Bayesian Neural Networks for Selection of drug sensitive Genes. J. Amer. Statist. Assoc., in press.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HyxiMQrDh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The proposed SGLD-SA algorithm with its convergence properties is interesting</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1grRoR9tQ&amp;noteId=HyxiMQrDh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper894 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper894 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">* The proposed SGLD-SA algorithm, together with its convergence properties, is very interesting. The introduction of step size $w^{k}$ is very similar to the "convex combination rule" in (Zhang &amp; Brand 2017) to guarantee convergence.
  
* It seems that this paper only introduced Bayesian inference in the output layers. It would be more interesting to have a complete Bayesian model for the full network including the inner and activation layers.

* This paper imposed spike-and-slab prior on the weight vector which can yield sparse connectivity. Similar ideas have been explored to compress the model size of deep networks (Lobacheva, Chirkova and Vetrov 2017; Louizos, Ullrich and Welling 2017 ). It would make this paper stronger to compare the sparsification and compression properties with the above work.

* In equation (11) there is a summation from $\beta_{p+1}$ to $\beta_{p+u}$. I wonder where this term comes from, as I thought $\beta$ is a vector of dimension $p$.

Reference:
Zhang, Ziming, and Matthew Brand. "Convergent block coordinate descent for training tikhonov regularized deep neural networks." Advances in Neural Information Processing Systems. 2017.

Lobacheva, Ekaterina, Nadezhda Chirkova, and Dmitry Vetrov. "Bayesian Sparsification of Recurrent Neural Networks." arXiv preprint arXiv:1708.00077 (2017).

Louizos, Christos, Karen Ullrich, and Max Welling. "Bayesian compression for deep learning." Advances in Neural Information Processing Systems. 2017.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJejZgw5pQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply from Author</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1grRoR9tQ&amp;noteId=BJejZgw5pQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper894 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper894 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comments. Please see our clarification below.

Q1: Where does the summation term in Eq. (11) come from? A complete Bayesian model for the full network including the inner and activation layers is better.

A1: We are indeed using a full Bayesian model as mentioned in the last two lines of page 2, we assume that the weights from the layer FC1 follow the spike-and-slab priors, the weights from the other layers follow the standard Gaussian distribution, and the biases follow improper uniform priors. The vector $\beta$ contains all neural network weights, including parameters with spike-and-slab priors, with indexes from 1 to p, and the standard normal priors with indexes p+1 to p+u respectively. The vector $\beta$ is sampled through backpropagation using SG-MCMC. We already made the corresponding changes to the formula (11) to make it clearer. In addition, the convolutional layer and pooling layer are assumed to have infinitely strong priors over their weights [Goodfellow, Bengio, Courville, 2016], which is implemented in the neural network architectures by default. Therefore, we are not only introducing Bayesian inference in the output layers.



Q2: Comparison of our model with others to compress the model size of deep networks

A2: From the regularization perspective, the goal of our algorithm is to replace dropout with a better regularization to avoid over-fitting. Dropout is known to set Gaussian mixture priors to the neurons [2], but the priors are not tuned during the training. Our SG-MCMC-SA algorithm differs from the dropout regularization in treating the priors - it keeps updating the Gaussian mixture priors during posterior inference. Therefore, our experiments only included dropout method and SG-MCMC as the benchmark methods to demonstrate the improvement.

From the variable selection perspective, variable selection in high dimensional non-linear systems is still an open problem. To the best of our knowledge, variable selection in deep neural networks (DNN), known for the enhanced generalization by reducing overfitting, hasn't been able to compete with the state-of-the-art performance on computer vision datasets like MNIST and Fashion-MNIST for various reasons, e.g. high-demanding computation and variable selection consistency. This shows the limitation of the existing variable selection methods in DNN. Our paper is the first variable-selection algorithm that solves the computational issue and achieves the state-of-the-art performance in classical CV datasets like MNIST and Fashion-MNIST with replicable experiment results. This shows the potential of our proposed SG-MCMC-SA to be effective on more challenging datasets like CIFAR10, and the possibility that we don't have to artificially design a too sophisticated network to achieve a good performance.


We thank the reviewer for the helpful comments and would appreciate further discussions.

[1] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016

[2] Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. In Proc. of the Conference on Advances in Neural Information Processing Systems (NIPS), page 1019–1027, December 2016</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>