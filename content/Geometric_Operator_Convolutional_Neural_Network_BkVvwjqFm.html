<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Geometric Operator Convolutional Neural Network | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Geometric Operator Convolutional Neural Network" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BkVvwj0qFm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Geometric Operator Convolutional Neural Network" />
      <meta name="og:description" content="The Convolutional Neural Network (CNN) has been successfully applied in many fields during recent decades; however it lacks the ability to utilize prior domain knowledge when dealing with many..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BkVvwj0qFm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Geometric Operator Convolutional Neural Network</a> <a class="note_content_pdf" href="/pdf?id=BkVvwj0qFm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019geometric,    &#10;title={Geometric Operator Convolutional Neural Network},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BkVvwj0qFm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">The Convolutional Neural Network (CNN) has been successfully applied in many fields during recent decades; however it lacks the ability to utilize prior domain knowledge when dealing with many realistic problems. We present a framework called Geometric Operator Convolutional Neural Network (GO-CNN) that uses domain knowledge, wherein the kernel of the first convolutional layer is replaced with a kernel generated by a geometric operator function. This framework integrates many conventional geometric operators, which allows it to adapt to a diverse range of problems. Under certain conditions, we theoretically analyze the convergence and the bound of the generalization errors between GO-CNNs and common CNNs. Although the geometric operator convolution kernels have fewer trainable parameters than common convolution kernels, the experimental results indicate that GO-CNN performs more accurately than common CNN on CIFAR-10/100. Furthermore, GO-CNN reduces dependence on the amount of training examples and enhances adversarial stability.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Convolutional Neural Network, Geometric Operator, Image Classification, Theoretical Analysis</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Traditional image processing algorithms are combined with Convolutional Neural Networks，a new neural network.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">5 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_ryg81cG52X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Misleading title and not novel enough.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkVvwj0qFm&amp;noteId=ryg81cG52X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper273 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper273 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes to use a gabor and schmid operators as first layer in a CNN
and to do gradient descent to their parameters.
I think that the title is over-selling and that ultimately misleads the reader.

Clarity needs to be improved and the text should be polished, it contains lot 
of repetitions for example.

In the contribution the author mention a new framework, I think it is better to
say it is a different layer for convolutional networks as the computational
paradigm is exactly the same. 

In the related work the authors try to motivate their approach but I am afraid 
in a too hand-wavy manner. For example for gabor it is written that is based 
on human vision. Even if this has been shown, to some degree, by Hubel and Wiesel
why would this work for a CNN that has nothing to do with the human vision?

In Section 3.2.1 it is mentioned that gabor may be an optimal feature extractor,
what does it mean? Optimal with respect to what? Overall the section is too 
vague and does not seem, frankly, to make any good point.

The two operators proposed in the work are indeed differentiable, but use 
periodic functions and divisions that could trigger numerical issues and instabilities
during training. This should be addressed in the text.

I find the many definitions, theorems and corollaries overwhelming considering
what is in the end proposed.

Experiments are too weak and not sufficient to draw any conclusions. 
In 5.1 please do at least 10 runs and report mean and variance.
Adversarial stability is misleading as it has nothing to do with adversarial 
examples but it is rather an ablation study.
The experiment on generalization is using the test set for training to make
use of small amount of data, but this makes things completely not comparable. 
Use rather a random subset of the training set as it is customary to do.

Overall I don't find the motivation of this strong enough, in fact DL research 
seems to move in the opposite direction, and results are not convincing.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">2: Strong rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BkxcvhQtnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review for the paper Geometric Operator Convolutional Neural Network</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkVvwj0qFm&amp;noteId=BkxcvhQtnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper273 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper273 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">I appreciate the responses authors provided to my comments. Many of my main concerns have been addressed there. 

Having said this, I do not consider the contribution of this paper substantial given the fact the a more involved framework (CKN) has been already known. I honestly believe that the findings of the authors are interesting but doubt that we use Gabor features in the first layer of CNNs from now on. 

If the paper is accepted, I kindly encourage the authors to reflect their answers to my comments in their work. For the sake of completeness, I have incorporated my comments and authors responses below.

Good luck

-------------------------------------------------------------------------------------------------------------
I would like to get my head around a few points and appreciate clarifications on 

- To me the title is a bit misleading, especially the word geometric seems a bit far-stretched. can you explain why geometric is essential here?

- In experiments, given that the Gabor filters have less tunable parameters, did the authors keep the number of channels similar to their CNN counterparts or increased the number of channels (Gabor filters) to match the number of tunable parameters in CNNs?

- have authors evaluated their solution on large scale problems, say image classification on image-net?

- what will happen if more than 1 layer of Gabor filters is considered? I am curious to know whether the performance decreases or not

- how is your proposal different from previous studies where learning the parameters of a kernel function (e.g., CKN by Mairal et al.) has been discussed? I am not very much convinced that the proposed approach is brand-new.

- can you explain how you initialize the Gabor filters? Can you also report [min,max] accuracies over say 10runs if random initialization is used?   


&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
Re: Some clarifications  
ICLR 2019 Conference Paper273 Authors
25 Oct 2018ICLR 2019 Conference Paper273 Official CommentReaders:  Everyone
Comment: Q1 feedback: 
  The title of our paper is originated from geometric operators in traditional image process algorithms. Geometric properties in geometric operators, such as symmetry, account for the word formation. The kernels in the first convolutional layer in AlexNet \cite{krizhevsky2012imagenet} show similarity with Gabor operator kernels in vision, and there is redundancy of parameters in CNNs, which inspire us to combine geometric operators in the traditional image process algorithms into CNNs.

Q2 feedback:
  In our experiments, the output channel number of the first convolutional layer keeps the same between common CNN and the corresponding GO-CNN. Therefore, the number of trainable parameters of common CNN is larger than that of the corresponding GO-CNN. There is theoretical guarantee in our article that, under less parameter degrees of freedom, the approximation ability of the Geometric Operator Convolutional Network is not worse than that of common CNN, and the parameter redundancy of common CNN is further verified.

Q3 feedback:
  The main purpose of this article is to validate and analysis the potential of the proposed architecture in it, and experiments on large scale dataset are not considered in this article. We are going to study applications by the proposed architecture on large scale datasets in the future. 

Q4 feedback:
  One of the motivations in this paper is to observe that the visualization of the first layer of the AlexNet convolution kernel has certain geometric characteristics, such as symmetry and volatility, and parameter redundancy. The visualization of other layer convolution kernels does not show obvious geometric characteristics (\cite{krizhevsky2012imagenet} ). In addition, in the MNIST experiment, the experimental effect of replacing the two-layer ordinary convolution kernel with the geometric operator kernel is the same as that of replacing the first-layer convolution kernel. At the same time, in order to reduce the computational complexity and the convenience of theoretical analyses, only the results of replacing one layer are considered in this paper, and the research of replacing multiple layers will be carried out in the future.

Q5 feedback:
  There are many related studies that have similarities with the method presented in this article, which are discussed in the part of related work. As for CKN mentioned, there are at least three differences with the GO-CNN proposed in this article:
1. The number of trainable parameters about convolution kernels is unchanged in CKN comparing to common CNN, however that of the corresponding GO-CNN is smaller.
2. CKN calculates the convolution kernel by approximating the Gaussian kernel with linear functions, and GO-CNN directly learn proper parameters of the geometric operator functions from training samples to get the convolution kernel.
3. CKN uses an energy function as loss for better performance in approximation, and GO-CNN uses directly cross entropy as loss for better performance in classification.
Therefore, GO-CNN proposed in this article is very different from CKN, and other similar studies discussed in the related work section.

Q6 feedback:
  In this article, we use random uniform initializer to initialize the five parameters in Gabor kernel function, wherein \phi_{init} ~ U(-\pi, \pi), \lambda_{init} ~ U(2, 10), \theta_{init} ~ U(0, 2\pi), \sigma_{init} ~ U(0, 2\pi), and \gamma_{init} ~ U(0, 1).
  The model’s accuracy rates over ten experiments on the Cifar-10/100 test set if random initialization is used are listed as following:
1. GO-ResNet18 achieves accuracy of 95.17%±0.13% in Cifar-10 and 77.59%±0.04% in Cifar-100.
2. GO-ResNet34 achieves accuracy of 95.77%±0.14% in Cifar-10 and 78.26%±0.03% in Cifar-100.
3. GO-ResNet50 achieves accuracy of 94.72%±0.08% in Cifar-10 and 79.50%±0.06% in Cifar-100. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJxPrs3uhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Missing many technical details and more thorough evaluation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkVvwj0qFm&amp;noteId=BJxPrs3uhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper273 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper273 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This work replaces first convolutional layers with combination of Gabor and Schmidt filters with learnable parameters. However it does not provide enough technical details which would allow this work to be reproducible and the experimental section does not verify almost any of the design choices which would allow the reader to asses the main factors which lead to the obtained network performance .

I do not believe that this paper is of sufficient technical quality for this conference, thus I would not recommend this paper for acceptance. I believe that the amount of changes required for addressing the following issues would require a resubmission.

Mainly, the text misses many technical details of the work, for example:
- How are the derivatives of the Gabor/Schmidt filter parameters computed? Automatic differentiation? What are the resulting distributions of these parameters, considering that some of them have to be non-negative? (e.g. \sigma for Gabor filters). How does they differ between each other after training? How are they initialised? Are the learning rates the same for each of the GO parameters?
- What is the hit on performance when the filter parameters are initialised by hand and not trained? (e.g. the ones visualised in Figure 2 in appendix, which span the spatial frequency spectrum). What is the performance of a random initialisation?
- What is the processing speed of the network, e.g. training time versus training time of the vanilla CNN?
- What are the confidence intervals for the adversial stability experiment? (Table 3). The difference between the proposed algorithm and vanilla CNN are so small it might be easily a result of for example a favourable draw of the random rotations.
- How does this method performs against other methods addressing the Conv1 approximation with fixed bases [1] or bases with learnable parameters? [2]

Additionally, it is not clear what the proof is proving as it is clear that a CNN has the ability to learn the Gabor/Schmidt filters exactly. However,  for this to hold the other way around and for the universal approximator theorem to hold a sufficient number of predefined filters would have to be used where the sufficient number of filters has no upper bound, thus reducing practicality of this proof. More trivially, without any upper bound on the number of predefined filters, one can simply use any bases (which are by definition injective) for generating the exact CNN filters which lead to a good performance. More interesting would be to show (empirically) that the predefined Gabor/Schmidt bases are more efficient for a sparse coding the manifold of CNN filters with good performance, which would have some practical implications regarding the aptness of these predefined filters.

[1] Yao, Hu, et al. "Gabor feature based convolutional neural network for object recognition in natural scene." Information Science and Control Engineering (ICISCE), 2016 3rd International Conference on. IEEE, 2016.
[2] Qiu, Qiang, et al. "DCFNet: Deep Neural Network with Decomposed Convolutional Filters." arXiv preprint arXiv:1802.04145 (2018).</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJxHVVocjQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Some clarifications</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkVvwj0qFm&amp;noteId=BJxHVVocjQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper273 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">22 Oct 2018</span><span class="item">ICLR 2019 Conference Paper273 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear authors,

I would like to get my head around a few points and appreciate clarifications on 

- To me the title is a bit misleading, especially the word geometric seems a bit far-stretched. can you explain why geometric is essential here?

- In experiments, given that the Gabor filters have less tunable parameters, did the authors keep the number of channels similar to their CNN counterparts or increased the number of channels (Gabor filters) to match the number of tunable parameters in CNNs?

- have authors evaluated their solution on large scale problems, say image classification on image-net?

- what will happen if more than 1 layer of Gabor filters is considered? I am curious to know whether the performance decreases or not

- how is your proposal different from previous studies where learning the parameters of a kernel function (e.g., CKN by Mairal et al.) has been discussed? I am not very much convinced that the proposed approach is brand-new.

- can you explain how you initialize the Gabor filters? Can you also report [min,max] accuracies over say 10runs if random initialization is used?   </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rygyoRlyn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: Some clarifications</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkVvwj0qFm&amp;noteId=rygyoRlyn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper273 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018</span><span class="item">ICLR 2019 Conference Paper273 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Q1 feedback: 
  The title of our paper is originated from geometric operators in traditional image process algorithms. Geometric properties in geometric operators, such as symmetry, account for the word formation. The kernels in the first convolutional layer in AlexNet \cite{krizhevsky2012imagenet} show similarity with Gabor operator kernels in vision, and there is redundancy of parameters in CNNs, which inspire us to combine geometric operators in the traditional image process algorithms into CNNs.

Q2 feedback:
  In our experiments, the output channel number of the first convolutional layer keeps the same between common CNN and the corresponding GO-CNN. Therefore, the number of trainable parameters of common CNN is larger than that of the corresponding GO-CNN. There is theoretical guarantee in our article that, under less parameter degrees of freedom, the approximation ability of the Geometric Operator Convolutional Network is not worse than that of common CNN, and the parameter redundancy of common CNN is further verified.

Q3 feedback:
  The main purpose of this article is to validate and analysis the potential of the proposed architecture in it, and experiments on large scale dataset are not considered in this article. We are going to study applications by the proposed architecture on large scale datasets in the future. 

Q4 feedback:
  One of the motivations in this paper is to observe that the visualization of the first layer of the AlexNet convolution kernel has certain geometric characteristics, such as symmetry and volatility, and parameter redundancy. The visualization of other layer convolution kernels does not show obvious geometric characteristics (\cite{krizhevsky2012imagenet} ). In addition, in the MNIST experiment, the experimental effect of replacing the two-layer ordinary convolution kernel with the geometric operator kernel is the same as that of replacing the first-layer convolution kernel. At the same time, in order to reduce the computational complexity and the convenience of theoretical analyses, only the results of replacing one layer are considered in this paper, and the research of replacing multiple layers will be carried out in the future.

Q5 feedback:
  There are many related studies that have similarities with the method presented in this article, which are discussed in the part of related work. As for CKN mentioned, there are at least three differences with the GO-CNN proposed in this article:
1. The number of trainable parameters about convolution kernels is unchanged in CKN comparing to common CNN, however that of the corresponding GO-CNN is smaller.
2. CKN calculates the convolution kernel by approximating the Gaussian kernel with linear functions, and GO-CNN directly learn proper parameters of the geometric operator functions from training samples to get the convolution kernel.
3. CKN uses an energy function as loss for better performance in approximation, and GO-CNN uses directly cross entropy as loss for better performance in classification.
Therefore, GO-CNN proposed in this article is very different from CKN, and other similar studies discussed in the related work section.

Q6 feedback:
  In this article, we use random uniform initializer to initialize the five parameters in Gabor kernel function, wherein \phi_{init} ~ U(-\pi, \pi), \lambda_{init} ~ U(2, 10), \theta_{init} ~ U(0, 2\pi), \sigma_{init} ~ U(0, 2\pi), and \gamma_{init} ~ U(0, 1).
  The model’s accuracy rates over ten experiments on the Cifar-10/100 test set if random initialization is used are listed as following:
1. GO-ResNet18 achieves accuracy of 95.17%±0.13% in Cifar-10 and 77.59%±0.04% in Cifar-100.
2. GO-ResNet34 achieves accuracy of 95.77%±0.14% in Cifar-10 and 78.26%±0.03% in Cifar-100.
3. GO-ResNet50 achieves accuracy of 94.72%±0.08% in Cifar-10 and 79.50%±0.06% in Cifar-100.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>