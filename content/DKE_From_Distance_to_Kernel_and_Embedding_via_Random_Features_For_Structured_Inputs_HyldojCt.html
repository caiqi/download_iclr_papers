<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>D2KE: From Distance to Kernel and Embedding via Random Features For Structured Inputs | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="D2KE: From Distance to Kernel and Embedding via Random Features For Structured Inputs" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HyldojC9t7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="D2KE: From Distance to Kernel and Embedding via Random Features For..." />
      <meta name="og:description" content="We present a new methodology that constructs a family of \emph{positive definite kernels} from any given dissimilarity measure on structured inputs whose elements are either real-valued time series..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HyldojC9t7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>D2KE: From Distance to Kernel and Embedding via Random Features For Structured Inputs</a> <a class="note_content_pdf" href="/pdf?id=HyldojC9t7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019d2ke:,    &#10;title={D2KE: From Distance to Kernel and Embedding via Random Features For Structured Inputs},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HyldojC9t7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We present a new methodology that constructs a family of \emph{positive definite kernels} from any given dissimilarity measure on structured inputs whose elements are either real-valued time series or discrete structures such as strings, histograms, and graphs. 
Our approach, which we call D2KE (from Distance to Kernel and Embedding), draws from the literature of Random Features.
However, instead of deriving random feature maps from a user-defined kernel to approximate kernel machines, we build a kernel from a random feature map, that we specify given the distance measure. 
We further propose use of a finite number of random objects to produce a random feature embedding of each instance.
We provide a theoretical analysis showing that D2KE enjoys better generalizability than universal Nearest-Neighbor estimates. 
On one hand, D2KE subsumes the widely-used \emph{representative-set method} as a special case, and relates to the well-known \emph{distance substitution kernel} in a limiting case. 
On the other hand, D2KE generalizes existing \emph{Random Features methods} applicable only to vector input representations to complex structured inputs of variable sizes. 
We conduct classification experiments over such disparate domains as time series, strings, and histograms (for texts and images), for which our proposed framework compares favorably to existing distance-based learning methods in terms of both testing accuracy and computational time.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Distance Kernel, Embeddings, Random Features, Structured Inputs</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">From Distance to Kernel and Embedding via Random Features For Structured Inputs</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HJlnryK22m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>a specific way of using distance to define kernel. not clear what's the benefit</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyldojC9t7&amp;noteId=HJlnryK22m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper633 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper633 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposed a new way to define kernel functions using some distance function as random features. The paper also provides some standard analysis of the random feature approach to generalization ability. Overall, the novelty of the paper is low. It is not clear what's the benefit of this approach and it doesn't seems that one can build upon this and lead to new research directions. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SkeI9oq43m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This is an incremental work with insufficient depth and novelty.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyldojC9t7&amp;noteId=SkeI9oq43m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper633 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper633 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a kernel function based on a dissimilarity measure between a pair of instances. A general dissimilarity measure does not necessarily have properties of a metric and the standard transformations from dissimilarities to kernel or similarity functions typically result in indefinite kernel matrices. For example, the two most frequently used transformations are negative double-centering characteristic to multidimensional scaling and the exponentiation of the negative squared dissimilarities between a pair of instances (e.g., exponentiated negative squared geodesic distance defines an indefinite kernel).

The main idea of the paper is to mimic the random Fourier features approximation of the stationary kernels and use dissimilarities as basis function. In particular, the proposed kernel function based on dissimilarities can be written as

k(x,x')=\int \phi(w, x) \phi(w, x') p(w) dw,

where \phi is a dissimilarity function and p(w) is a probability density function. This is the same format of the kernel function as the one considered in [1-3], with \phi chosen to be different from the cosine basis function. While the focus in [1-3] was on stationary kernels, their theoretical derivations and presentation was designed for general basis functions. Having this in mind, all the theoretical derivations by the authors are readily obtained from [1-3] and I fail to see any novelty here. For example, the result in Proposition 2 follows directly from [1, Claim 1] or [2].

For the proposed kernel, I also fail to see a significant difference compared to [4] where dissimilarities are used as features. In fact, for a large number of dissimilarity functions/features and l1 penalty on the linear model the approach by [4] retrieves the proposed approximate kernel with the `optimal' density function. An experiment along these lines was, for instance, reported in [5].

In the related work part, the authors also mention previous approaches for learning with indefinite kernels and that they suffer from the non-convexity of the optimization problem for risk minimization. A recent approach builds on [6] and alleviates this shortcoming with a non-convex problem for which a globally optimal solution can be found in polynomial time (e.g., see [7]).

In the experiments, the most appropriate baseline would be the approach from [5] and yet the approach is not even listed in the related work section.

[1] A. Rahimi and B. Recht (NIPS 2008). Random features for large-scale kernel machines.
[2] A. Rahimi and B. Recht (IEEE 2008). Uniform approximation of functions with random bases.
[3] A. Rahimi and B. Recht (NIPS 2009). Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning.
[4] T. Graepel, R. Herbrich, P. Bollmann-Sdorra, and K. Obermayer (NIPS 1999). Classification on pairwise proximity data.
[5] I. Alabdulmohsin, X. Gao, and X.Z. Zhang (PMLR 2015). Support vector machines with indefinite kernels.
[6] C.S. Ong, X. Mary, S. Canu, and A. Smola (ICML 2004). Learning with non-positive kernels.
[7] D. Oglic and T. Gaertner (ICML 2018). Learning in Reproducing Kernel Krein Spaces.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJeLZ-e1hQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>D2KE - review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyldojC9t7&amp;noteId=HJeLZ-e1hQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper633 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper633 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">D2KE: From Distance to Kernel and Embedding

Quality: average
Originality: original
Significance: relevant for ICLR
Pros: - interesting idea -- see detailed comments
Cons: some technical issues, validity of the results not clear -- see detailed comments

I have seen this paper already at ICML as a review and I am happy to see
that the authors have improved the paper. Although the core idea is interesting
and novel the paper raises still a number of questions (see below). Beside
of some technical bits, I am (again) unhappy with the experimental evaluation
- in particular there are strange mismatches between the results reported in the
ICML submission and this one. The method looks now better in the shown results
- basically be removing results from ICML on another method and some additional datasets.

Further some information is not provided (parametrization,
standard deviation of the results, significance of the differences).
Some results are changed unexpectedly and there is still no comparison to some more
challenging datasets as provided by other authors from the field. 

comments
- the paper has a number of typos in the references - please carefully
  check and also complete missing information
- I understand that you have to promote your approach but looking
  on the title your objective is to go from distances to kernels
  --&gt; although you go a different way (which is fine) you can
  also do this directly by using concepts as proposed by Gisbrecht et al.
  'Metric and non-metric proximity transformations at linear costs', Neurocomputing
  which you should at least take into account
- to sum up a bit your related work part you should refer to a recent review
  by Tino et. al about indefinite learning, neural computation
- 'A line of work has therefore focused on estimating a positive-definite (PD) ...' 
  - yes and by combining the approach from the Gisbrecht paper with the one
  of Loosli you can have e.g. an SVM in the Krein space without restriction to a single Hilbert space and
  with a simple and clear out of sample extension to test data. So basically
  there are ways to stay with an indefinite kernel (or non-metric dissimilarity), 
  not loosing any performance and having no need to put it into a vector space. 
  --&gt; You could point to this option in your introduction
- And as mentioned by a number of other authors (Pekalska, Tino, ...) there maybe
  good reasons for not going to a PD kernel - because one may still loose information
- 'distance kernel learning,' - this is a bit a mismatch - you either have a kernel
  or a distance and you may define a distance based on a kernel ...
- 'This type of kernel, however, results in a diagonal-dominance problem, where the diagonal entries of the kernel Gram matrix 
   are orders of magnitude larger than the off-diagonal entries,' -- well yes, but could 
  this not be solved by a renormalization of the kernel matrix (assuming that the diagonal elements
  are at least positive) such that finally all diagonals are 1?
- Eq 4 is not a so new idea. Many authors have already plugged distances into an exp to make
  it a similarity - but this is changing the data representation (this is widely discussed
  by pekalska). And if d is non-metric the Eq for may not even provide the 'correct' mathematical
  formulation.  That may not be a problem - e.g. by asking - can I get any kind of
  similar kernel to my given dissimilarities (I think there is old work around this by 
  G. Wahba et al.), e.g. by learning a proxy kernel such that the similarities of the kernel
  are close to the dissimilarities. One may also more directly deal with the dissimilarities
  see e.g. work of Yiming Ying about learning with dissimilarities (or so). 
  But if I would like to keep the classical formulation that from an inner product based on K
  I can define a dissimilarity - and this should be the same like the original one which I used
  to get K the Eq 4 may not be so nice (--&gt; see double centering in the book of Pekalska)
- ', our kernel in Equation (5) is always PD by' - Eq 5 or Eq 4 - later on Eq 5 is never used again
  - cmp Algo 1 - caption/title
- if I understand correct in Alg 1 l(x,y) - refers to a loss function (?) and y to a label - not 
  introduced anywhere - but then your approach is supervised - the classical RFF are unsupervised
  (and hence more generic) - beside of this I may not have any label information if I go from a 
  dissimilarity to similarities --&gt; if this is a restriction of your method you should reflect this
  in adding e.g. ' ... in supervised learning' in the title 
- 'Since most distance measures are computationally demanding, having quadratic complexity, we adapted or implemented
  C-MEX programs for them; other codes were written in Matlab.' - software implementation details are
  not relevant here - please remove 
- please provide the full crossvalidation information including mean/std-dev, aligned with a significance test
 
- 'For our new method D2KE, since we generate random samples from the distribution, we can use as many as needed to
   achieve performance close to an exact kernel. We report the best number in the range R = [4, 4096] 
   (typically the larger R is, the better the accuracy).' - if so I will hope you tuned your R on the training data
  and the reported values are from the test data! - please clarify
- the other methods may also have meta-parameters e.g. C in the case of classical SVM - which values are used
  and how are they obtained
- classical DTW is not defined for multiple timeseries - what did you use
- your result tables have changed compared to the ICML paper (also one method GDK_LDE by Pekalska has vanished)
  please explain ! In particular also the runtime of RSM has nonlinearly changed between the two papers and
  the overall results are now a bit more in favour of your method. (Table 1)
  The dataset 'mnist-str4' has vanished as well. For the image data you also have a strange change - in the
  ICML paper GDK_LED was best and now - because this method does not show up anymore your approach looks to be
  the best. As the runtime change so much between the two submissions I am wondering why you still report them.
  Basically the O-notation is telling that your method is linear (for reasonable large N and not to large R) 
  and that most other have O(N^2).

  To ensure reproducability of your results I ask you to provide the respective codes e.g. on github (can be done anonymous)

- Repeating myself from the last review: 
 	there is a lot of work addressing that making a kernel psd may not be good idea - you provide experiments
  	for a small number of data where your kernel is now psd but what is with the other data (where e.g. in
  	Pekalska and followers it was shown that making them psd is bad ... ) - is your approach solving
  	this - or do we end with an approach which is not very performant (in accuracy) for the hard/crucial datasets?
	

- why do you not use some of the datasets provided by Pekalska (simbad EU project - still in the web) 
  (some maybe also be found by Loosli) to have a more realistic comparison
- once more the number of datasets used in the evaluation is not particular large
- still a few typos 'dissimilairty' --&gt; spell checker
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>