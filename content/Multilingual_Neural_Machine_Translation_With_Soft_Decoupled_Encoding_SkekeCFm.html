<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Multilingual Neural Machine Translation With Soft Decoupled Encoding | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Multilingual Neural Machine Translation With Soft Decoupled Encoding" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Skeke3C5Fm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Multilingual Neural Machine Translation With Soft Decoupled Encoding" />
      <meta name="og:description" content="Multilingual training of neural machine translation (NMT) systems has led to impressive accuracy improvements on low-resource languages. However, there are still significant challenges in..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Skeke3C5Fm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Multilingual Neural Machine Translation With Soft Decoupled Encoding</a> <a class="note_content_pdf" href="/pdf?id=Skeke3C5Fm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019multilingual,    &#10;title={Multilingual Neural Machine Translation With Soft Decoupled Encoding},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Skeke3C5Fm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=Skeke3C5Fm" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Multilingual training of neural machine translation (NMT) systems has led to impressive accuracy improvements on low-resource languages. However, there are still significant challenges in efficiently learning word representations in the face of paucity of data. In this paper, we propose Soft Decoupled Encoding (SDE), a multilingual lexicon encoding framework specifically designed to share lexical-level information intelligently without requiring heuristic preprocessing such as pre-segmenting the data. SDE represents a word by its spelling through a character encoding, and its semantic meaning through a latent embedding space shared by all languages. Experiments on a standard dataset of four low-resource languages show consistent improvements over strong multilingual NMT baselines, with gains of up to 2 BLEU on one of the tested languages, achieving the new state-of-the-art on all four language pairs.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Hkxv-rD2p7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Some revisions to the paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skeke3C5Fm&amp;noteId=Hkxv-rD2p7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1038 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1038 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank all the reviewers for your careful comments and great suggestions. Based on your comments and some of our further experiments, we just uploaded a new version of the paper with the following updates:

1) We fixed some typos, made the terminology consistent, and clarified the paper based on the comments.

2) We added two visualization sections in the appendix. One about attention over latent embedding space, and the other about word vectors.

3) We updated the sub-sep baseline result (row 3 in Table 3 and Figure 4). We were looking at the results and found that for the sub-sep baseline only, we used a different subword vocabulary for one of the three random seed runs. After the update, the baseline of bel decreased, while slk improved, aze and glg stay about the same. In general, the updated number doesn’t change any of our conclusions in the paper; SDE still outperforms the strong baseline on all four languages, bringing gains of up to 2.5 BLEU (for the bel-rus dataset). 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ryeWrj7ETQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting and clean idea for multilingual lexicon sharing. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skeke3C5Fm&amp;noteId=ryeWrj7ETQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1038 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1038 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Overall:
This paper proposed soft decoupled encoding (SDE), a special multilingual lexicon encoding framework which can share lexical-level information without requiring heuristic preprocessing. Experiments for low-resource languages show consistent improvements over strong multilingual NMT baselines.

General Comments:
To me this paper is very interesting and is nicely summarized and combined previous efforts in two separated directions for sharing multilingual lexicons: based on the surface similarity (how the word is spelled, e.g. subword/char-level models), and based on latent semantic similarity (e.g. Gu et.al. 2018). However, in terms of the proposed architecture, it seems to lack some novelty. Also, more experiments are essential for justification.

I have some questions:
(1) One of the motivation proposed by Gu et.al. 2018 is that spelling based sharing sometimes is difficult/impossible to get (e.g. distinct languages such as French and Korean), but monolingual data is relatively easy to obtain. Some languages such as Chinese is not even “spelling” based. Will distinct languages still fit in the proposed SDE? In my point of view, it will break the “query” vector to attention to the semantic embeddings.
(2) How to decide the number of core semantic concepts (S) in the latent semantic embeddings? Is this matrix jointly trained in multilingual setting?
(3) Is the latent semantic embeddings really storing concepts for all the languages? Say would you pick words in different languages with similar meanings, will the they naturally get similar attention weights? In other words, do multiple languages including very low resource languages learn to naturally align together to the semantic embeddings during multilingual training? I am a bit doubtful especially for the low resource languages.
(4) It seems that the language specific transformation does not always help. Is it because there is not enough data to learn this matrix well?
(5) During multilingual training, how you balance the number of examples for low and high resource languages?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BylOnrwnT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to the reviewer</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skeke3C5Fm&amp;noteId=BylOnrwnT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1038 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1038 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the comments and insightful questions. Here are some response for the questions:


1) The reviewer is correct that SDE is mainly tailored for multilingual training of related languages with reasonably overlapping character vocabularies. 
a) We choose to focus on related language because multilingual training is most effective when the data from different languages have similar probability distribution. In Table 2 of Gu et al (2018) paper, Korean, the most distant language from other related languages, has the lowest BLEU score with multilingual training, as well as the least improvement even after adding the universal encoder. In fact, it is not clear from their experiments that whether the multilingual training can even outperform training with Korean data only. In Table 2 of Neubig &amp; Hu 2018, they also found that bilingual training of two highly related languages is comparable to or even better than training with all languages. 

b) We can still use SDE even if two languages don’t overlap strongly in their character vocabularies. In this case, although the character n-gram embeddings cannot be shared directly between languages, the latent embedding space may still be shared between languages. SDE might behave more like the encoder in Gu et al (2018), but it still has the advantages that (1) it does not need to use word pieces and (2) it can capture morphology etc. within each language through the character n-gram embedding.

c) For languages that do not have natural word boundary, such as Chinese, it is standard to first perform word segmentation and then encode each word with its character embedding using SDE. This might be helpful for co-training of Chinese and Japanese since both languages share part of the character vocabulary. 



2) Size and training of latent embedding space: The latent embedding matrix is trained jointly with the whole model. We set the number of latent word embedding to 10,000 based on the performance on development set for bel-rus dataset. We started some new experiments varying the latent embedding size of 5000, 10,000, and 15,000 for azetur and belrus dataset, and here are the results on the test set: for azetur, the test BLEU scores are 12.43, 11.66, 10.65 respectively, while the sub-sep baseline is 10.42; for belrus, the test BLEU scores are 18.74, 19.03, 17.76 respectively, while the sub-sep baseline is 17.14. Therefore, SDE out-performs the sub-sep baseline with all three different latent embedding sizes. Moreover, SDE with a relatively small latent embedding size can usually achieve good performance.




3) Functionality of latent embedding space: The reviewer raised a very interesting question on the kind of information that the latent embedding space stores. While in general neural models are difficult to interpret concretely (even attention in NMT models does not perfectly correspond to word alignments), we can still hypothesize about the functionality of each part of the model especially from the ablation studies. For example, in Table 4 we show that removing the latent word embedding can harm the performance of SDE. Moreover, in Appendix A4, we also added a visualization of the KL divergence of the attention over the latent embedding space between word pairs from two related languages. We found that  similar words from two related languages tend to have similar attention distribution over the latent embedding space.



4) Data size and language specific transform: We think that the reviewer proposed an interesting hypothesis that data size might relate to the effectiveness of the language specific transform. Moreover, we think that language specific transform might be more helpful for divergent languages. In Figure 3 of our paper, we find that bel-rus language pair, which benefits the most from language specific transform, has the most divergent vocabularies.



5) Balance of HRL and LRL: For all our experiments, we just used the training data as it is and did not balance the training size of HRL and LRL data. In fact, Neubig &amp; Hu 2018 found that balanced sampling does not have significant effect on multilingual training.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1lOhvAt27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Well-motivated problem and reasonable solution. Desire a few more experiments and clarifications.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skeke3C5Fm&amp;noteId=H1lOhvAt27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1038 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1038 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper focuses on the problem of word representations in multilingual NMT system. The idea of multilingual NMT is to share data among multiple language pairs. Crucially this requires some way to tie the parameters of words from different languages, and one popular method is to share subword units among languages. The problem is that subword units in different languages may not be semantically equivalent, and many semantically-equivalent concepts are not represented by the same subwords. This paper proposes an alternative way to share word representation, in particular by proposing a common set of "semantic" concept vectors across languages which are then folded into the word representations via attention. 

The problem is well-motivated and the proposed solution is reasonable. Previous works such as (Gu et. al. 2018) have been motivated in a similar fashion, and the proposed solution seems to outperform it on the TED dataset of Qi et. al. 2018. 

The experiments are informative. The main open questions I have are:

(a) Varying the latent embedding size. It seems like only 10,000 is tried. Since this is the main contribution of the work, it will be desirable to see results for different sizes. Is the method sensitive to this hyperparameter? Also suggestions on how to pick the right number based on vocabulary size, sentence size, or other language/corpus characteristics will be helpful. 

(b) What do the latent embeddings look like? Intuitively will they be very different from those from Gu et. al. 2018 because you are using words rather than subwords as the lexical unit? 

(c) The explanation for why your model outperforms Gu et. al. 2018 seems insufficient -- it would be helpful to provide more empirical evidence in the ablation studies in order really understand why your method, which is similar to some extent, is so much better. 

The paper is generally clear. Here are few suggestions for improvement:

- Table 1: Please explain lex unit, embedding, encoding in detail. For example, it is not clear what is joint-Lookup vs. pretrain-Lookup. It can be inferred if one knows the previous works, but to be self-contained, I would recommend moving this table and section to Related Works and explaining the differences more exactly.

- Sec 4.2: Explain the motivation for examining the three different lexical units. 

- Table 3: "Model = Lookup (ours)" was confusing. Do you mean "our implementation of Neubig &amp; Hu 2018? Or ours=SDE? I think the former?

- Are the word representions in Eq 4 defined for each word type or word token? In other words, for the same word "puppy" in two different sentences in the training data, do they have the same attention and thus the same e_SDE(w)? You do not have different attentions depending on the sentence, correct? I think so, but please clarify. (Actually, Figure 2 has a LSTM which implies a sentential context, so this was what caused the potential confusion). 

- There are some inconsistencies in the terms: e.g. latent semantic embedding vs latent word embedding. Lexical embedding vs Character embedding. This makes it a bit harder to line up Sec 4.4 results with Sec 3.2 methods. 

- Minor spelling mistakes. e.g. dependant -&gt; dependent. Please double-check for others. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1xR3Ss-pQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to the reviewer comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skeke3C5Fm&amp;noteId=S1xR3Ss-pQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1038 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1038 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the insightful questions and suggestions. We have performed additional experiments and will add more experimental results and revise the final version. Here are some partial response to address the questions:

(a) Varying the embedding size: In our first few experimental runs, we varied the latent embedding size of 5000, 10,000, and 15,000 for the bel-rus dataset. We found that SDE with embedding size of 10000 performs the best, so we just fixed the number for all the rest of the experiments. We started some new experiments varying the latent embedding size of 5000, 10,000, and 15,000 for azetur and belrus dataset, and here are the results on the test set: for azetur, the test BLEU scores are 12.43, 11.66, 10.65 respectively, while the sub-sep baseline is 10.42; for belrus, the test BLEU scores are 18.74, 19.03, 17.76 respectively, while the sub-sep baseline is 17.14. Therefore, SDE out-performs the sub-sep baseline with all three different latent embedding sizes. Moreover, SDE with a relatively small latent embedding size can usually achieve good performance. We will update the paper accordingly.

(b) What do the embeddings look like?: The reviewer raised a reasonable point that our latent embedding might look very different from Gu et al.. This is certainly true because of the following differences between SDE and the encoder in Gu et al.: 1) SDE uses words as lexical units, while Gu et. al. uses word pieces; 2) SDE constructs a character-ngram embedding that can effectively capture the similarity in spelling of words in related languages (see appendix A3), while Gu et. al requires standard pre-trained word embedding; 3)  SDE adds back the lexical character n-gram embedding to the latent semantic embedding to create the final word embedding, while Gu et. al. only uses the latent embedding. Therefore, the latent embedding space in SDE has a very different functionality than the one in Gu et. al. The latent embedding itself is difficult to visualize, mainly because there is not a one-to-one correspondence between the vocabulary and the latent embedding, but we will try to visualize the word vectors at different stages in the SDE encoding process and will update once we have some results.

(c) Explanation for improvement over Gu et al.: We think the gains are from two main differences: 1) SDE has a character n-gram embedding that can capture the character overlap of similar words in different languages, while the standard word embedding in Gu et. al. cannot leverage this information. This is particularly important in the case of languages with similar spelling, like the ones we used in our experiments; 2) SDE has a residual connection that adds the lexical character n-gram embedding back to the latent semantic embedding, while Gu et. al. only has the latent embedding. In Table 4., we show that the performance of SDE is significantly harmed by removing the residual connection of the character n-gram embedding. In fact, the results after removing character n-gram embedding (row 4 in Table 4) are somewhat comparable to the results of Gu et. al. (row 5 in Table 3). Another advantage of SDE is that it can directly use words as lexical units, while the model in Gu et. al. uses subwords. As the ablation experiments in Table 5 suggest, using subwords as lexical units in general is not as good as using words directly with SDE. 


We are very grateful for the careful comments about the presentation of the paper. We will address these comments in the updated version of the paper:
1. In Table 3, (ours) means our implementation of Neubig &amp; Hu 2018.

2. For the same word “puppy” in two different sentences of the same language, we will have the same e_SDE(w). The LSTM in Figure 2 is meant to represent the encoder RNN. We will make this clear. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkg0svOopQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for the clarification</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skeke3C5Fm&amp;noteId=rkg0svOopQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1038 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1038 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the clarification. 

(a) Nice! This is good to know. I would also suggesting testing the limits -- making the embedding so small or so big such that results degrade. This will give a better sense of the sensitivity. 

(b) Visual inspection might be difficult but perhaps you can include some kind of kNN of of the same words/subwords. If it's not possible to find any trends, then it's ok, I understand. I wouldn't cherry pick too much. 

(c) This is a very nice explanation. Please put it in the paper if you can, so differences with Gu et. al. is clear. By the way, I hadn't realized the residual connections were so important. I think standardizing some of the terminology in different parts of the paper would make it clearer. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_SJe-3psYnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting word representation model with good ablation experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skeke3C5Fm&amp;noteId=SJe-3psYnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1038 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1038 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents an approach to creating word representations that operate at both the sub-word level and generalise across languages. The paper presents soft decoupled encoding as a method to learn word representations from weighted bags of character-n grams, a language specific transformation layer, and a "latent semantic embedding" layer. The experiments are conducted over low-resource languages from the multilingual TED corpus. The experiments show consistent improvements compared to existing approaches to training translation models with sub-word representations. The ablation studies in Section 4.4 are informative about the relative importance of different parts of the proposed model.

Can you comment on how your model is related to the character-level CNN of Lee et al. (TACL 2017)?

In the experiments, do you co-train the LRLs with the HRLs? This wasn't completely clear to me from the paper. In Section 4.2 you use phrases like "concatenated bilingual data" but I couldn't find an explicit statement that you were co-training on both language pairs.

What does it mean for the latent embedding to have a size of 10,000? Does that mean that W_s is a 10,000 x D matrix?

Is Eq (4) actually a residual connection, as per He et al. (CVPR 2016)? It looks more like a skip connection to me.

Why do you not present results for all languages in Section 4.6?

What is the total number of parameters in the SDE section of the encoder? The paper states that you encode 1--5 character n-grams, and presumably the larger the value of N, the sparser the data, and the larger the number of parameters that you need to estimate.

For which other tasks do you think this model would be useful?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJecB8iba7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to the reviewer comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skeke3C5Fm&amp;noteId=HJecB8iba7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1038 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018 (modified: 09 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1038 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the comments and insightful questions:

1. Comparison to Lee et al.: SDE share some of the motivations with Lee et. al.: 1) both models don’t require segmenting into subwords; 2) both methods can leverage the similarity in spelling of words from different languages. SDE also has several advantages over Lee et. al. : 1) SDE still use words as lexical units, while Lee et. al. use characters. Therefore, the fully character-based NMT model in Lee et. al. needs to operate on much longer sequence than SDE, potentially making it much slower than word-level NMT; 2) The model in Lee et. al. needs a large number of other hyperparameters, like the number of kernels, kernel size, size of max pooling stride, while for SDE we only need to decide the size of the latent embedding space. While we did not directly compare in this paper, we have previously found that it is quite difficult to get good results with fully character-based models due to this necessity of careful hyper-parameter tuning and long experimental time. We can try to add a comparison if the reviewer thinks it would be informative, but we likely won’t be able to do so by the end of the rebuttal period.

2. Joint training?: Yes we do co-train the LRL with the HRL. We will clarify this in the final version.

3. Latent embedding size?: Yes the latent embedding size of 10,000 means the W_s matrix is 10,000*D

4. Residual connection?: In equation 4, c_i(w) can be seen as the input x, and e_latent(w) is a function that takes in input x. e_SDE(w) is the sum of the function of x and the input x, which we believe follows the definition of residual connection in He et. al.

5. Why not other languages?: To use all HRL data, we need to co-train the LRL with all of the HRL data, so for each LRL, the time it takes to converge is much longer. Therefore we only presented two experiments here to show that SDE has more desirable performance than the sep-sub baseline when using data from languages that are less related to the LRL. We can probably add these to the final version of the paper, however, if we start experiments now. 

6. Number of parameters: For the experiments in the paper, SDE NMT model has around 22M parameters, and the sub-sep NMT model has 18M parameters. The extra parameters are from the character n-gram embedding. Moreover, we show in Appendix A2 that  SDE still outperforms the sub-sep baseline when using a smaller character n-gram vocabulary.

7.Other tasks: SDE is a model-agnostic word encoding framework. First, it can easily integrate with other neural machine translation models, such as the Transformer model. Second, it can be used together with any neural models that need to encode language data. For example, we can use SDE to encode multilingual data for named entity recognition or other natural language analysis tasks for low-resource languages.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>