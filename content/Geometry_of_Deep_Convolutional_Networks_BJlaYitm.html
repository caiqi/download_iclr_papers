<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Geometry of Deep Convolutional Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Geometry of Deep Convolutional Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJlaYi05tm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Geometry of Deep Convolutional Networks" />
      <meta name="og:description" content=" We give a formal procedure for computing preimages of convolutional&#10;    network outputs using the dual basis defined from the set of&#10;    hyperplanes associated with the layers of the network. We point..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJlaYi05tm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Geometry of Deep Convolutional Networks</a> <a class="note_content_pdf" href="/pdf?id=BJlaYi05tm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 14 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019geometry,    &#10;title={Geometry of Deep Convolutional Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BJlaYi05tm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value"> We give a formal procedure for computing preimages of convolutional
  network outputs using the dual basis defined from the set of
  hyperplanes associated with the layers of the network. We point out
  the special symmetry associated with arrangements of hyperplanes of
  convolutional networks that take the form of regular
  multidimensional polyhedral cones. We discuss  the efficiency of of
  large number of layers of nested cones that result from incremental
  small size convolutions in order to give a good compromise between
  efficient contraction of data to low dimensions and shaping of
  preimage manifolds. We demonstrate how a specific network flattens a
  non linear input manifold to an affine output manifold and discuss
  it's relevance to understanding classification properties of deep
  networks.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">convolutional networks, geometry</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Analysis of deep convolutional networks in terms of associated arrangement of hyperplanes</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BJxOEuA3pX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting idea but result is not significant</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlaYi05tm&amp;noteId=BJxOEuA3pX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper485 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper485 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper considers the problem of computing preimages of convolutional network outputs. The main technology is a change of basis: expressing an arrangement of d generic hyperplanes in \R^d in the basis givens by their rays (section 3). The paper focuses on arrangements coming from circulant matrices (section 4), and found that the expression of such arrangements in this basis is regular, and for a particular example in \R^3, showed that they could completely understand the ReLu map (section 5). 

The problem is definitely interesting, and very difficult. However, the contribution is minimal. Firstly, the reduction to circulant matrices is a huge step and needs serious justification. The paper justified its restriction to circulant network via this claim: (Section 4):
\begin{quote}
We will exploit the fact that convolutional
matrices are in most respects asymptotically equivalent to those of circulant matrices where each new row is a one element cyclic shift of the previous.
\end{quote}
This needs to be made precise to be useful. In what respect? What does asymptotically equivalent mean? Without a serious justification here, analyzing circulant matrix is too narrow a scope. 

Nest codes: the punchline seems to be that the network contracts data, which is not new. The results are from the analysis of a *single* example in 3 variables - how far does this extend? 

In short, this paper has very little concrete mathematical or computational contribution. 

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">2: Strong rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkxO1-AR3Q" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlaYi05tm&amp;noteId=rkxO1-AR3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper485 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1gCvZ09h7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting work, but not significant enough</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlaYi05tm&amp;noteId=B1gCvZ09h7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper485 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper485 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies the geometry of convolutional networks with ReLU activation functions. It provides some insights into the way neural networks process data. However, the overall quality of this paper is relatively low. 

Pros:

This paper introduces a method to compute the preimages of a neural network layer using dual bases defined by the affine mappings of the network. The authors also relate the number of layers in convolutional neural network to the contraction of data. Some interesting phenomena are illustrated using examples. Although not very rigorous, the conclusion of this paper gives some insights on the geometry of convolutional neural networks.

Cons:

The paper is not written very clearly. First of all, the analysis of the paper is based on the assumption that each layer of the neural network has exactly d (the dimension of the input) nodes. However it seems that this assumption is not mentioned anywhere in this paper. Some of the notations are misleading. For example, in Equation (20) d is redefined as a parameter of an affine transform. There are also grammar errors and missing punctuation which I suggest the authors should carefully fix. 

The contribution of this paper is not very significant. No results in this paper can be formulated into rigorous theorem. As previously mentioned, the proposed method to compute preimages only works when all the layers have same amount of nodes, which is not the case in practice. Most of the conclusions the authors make are vague and lack impact. The authors do not convincingly demonstrate how the proposed preimage calculation method can be applied to practically useful network structures. The claim that nested cones efficiently contract input data is not very convincing either, and looks not very relevant to the preimage calculation part of the paper. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJgwGtwuhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Difficult to read, and I am unsure about relevance</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlaYi05tm&amp;noteId=rJgwGtwuhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper485 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper485 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
The paper is concerned with finding the preimage of of the features extracted by a series of convolution layers with ReLU activations. It is argued that these can be described through a series of nested polyhedral cones. This seem like a reasonable observation, but I am unsure why it is a particularly interesting observation. In the end, what does this tell me about the behavior of convolution layers?

I found the paper very difficult to read; not so much because of the topic, but due the writing. Too often I simply do not understand individual sentences, and I am left to guess the intended meaning. In such scenarios, I cannot recommend acceptance.

Details (I am not including all my confusions here, but just a selection to keep review time manageable):
*) References to figures appear to be incorrect. I think the authors accidentally refer to section numbers instead of figure numbers, but I am constantly left to guess which figure I should look at.

*) Sentences often miss periods, and words are capitalized within sentences.

*) It is chosen not to take subsampling or max pooling into account during the analysis. I can understand skipping max pooling, but why skip subsampling? Isn't that just a linear projection, which wouldn't affect the analysis?

*) On page 2: "Cones associated with successive layers are then in general partly nested inside their predecessor." I don't quite understand what is meant by "partly nested"; I get the intuition that two cones can intersect, but if the wording is supposed to mean something more specific, then I would like a more specific definition of "partly nested".

*) On page 2: "These hyperplanes divides the input space into a maximum of 2^d number of different cells with the maximum attained if all hyperplanes cut through the input space which we take as the non negative orthant of the d-dimensional Euclidean space R_+^d." I am not sure I understand this sentence, e.g. how can a hyperplane not cut through the input space? Should I read this as the input space is the non negative orthant? Why is that the input space? Is this based on the assumption of non negative pixel intensities? If so, why not include a maximal intensity such that the input space is a box?

*) On  page 3: i is used as a general index as well as the negative half space index (i vs i_k). This is a bit confusing, so it might be good to rename one of the indices.

*) On page 3, what does it mean for the set e_i to be "complete"? Do you mean that span({e_i}) = R^d ?

*) It would be good to be more explicit about the term "dual basis" -- dual to what?

*) On page 4: "We now define the positive direction of the vector e_i as that associated with the negative side of the plane Î _i" (an example of a sentence missing a period, btw). What is the negative side of a plane?

*) On page 4: "We will exploit the fact that convolutional matrices are in most respects asymptotically equivalent to those of circulant matrices where each new row is a one element cyclic shift of the previous." (previous sentence miss a period, btw) and "...this approximation will be negligible." I strongly miss either an explanation of this or a citation. As it stands, these are claims not facts.

*) On page 5: "The domain of the input that is mapped to output domain is just quite trivial planar manifolds." I don't understand this sentence.

*) On page 8: "The convolutional part of a deep network can therefore be seen as a component in a metric learning system where the purpose of the training will be to create domains in input space associated with different classes that are mapped to separate low dimensional outputs as a preprocessing for the final fully connected layers that will make possible efficient separation of classes." I don't understand how this conclusion is related to the analysis of the paper. I do not disagree with the quoted statement, but I miss how this is related to the analysis regarding nested cones.

*) On page 9: Just below the figure caption appears an "as" that appear to have not relation to any other text in the paper.

*) Citation 13: The authors name is misspelled, it should be "Roweis".
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>