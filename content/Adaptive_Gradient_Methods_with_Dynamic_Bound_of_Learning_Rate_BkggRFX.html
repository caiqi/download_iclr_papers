<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Adaptive Gradient Methods with Dynamic Bound of Learning Rate | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Adaptive Gradient Methods with Dynamic Bound of Learning Rate" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Bkg3g2R9FX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Adaptive Gradient Methods with Dynamic Bound of Learning Rate" />
      <meta name="og:description" content="Adaptive optimization methods such as AdaGrad, RMSProp and Adam have been proposed to achieve a rapid training process with an element-wise scaling term on learning rates. &#10;  Though prevailing, they..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Bkg3g2R9FX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Adaptive Gradient Methods with Dynamic Bound of Learning Rate</a> <a class="note_content_pdf" href="/pdf?id=Bkg3g2R9FX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019adaptive,    &#10;title={Adaptive Gradient Methods with Dynamic Bound of Learning Rate},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Bkg3g2R9FX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Adaptive optimization methods such as AdaGrad, RMSProp and Adam have been proposed to achieve a rapid training process with an element-wise scaling term on learning rates. 
Though prevailing, they are observed to generalize poorly compared with SGD or even fail to converge due to unstable and extreme learning rates. 
Recent work has put forward some algorithms such as AMSGrad to tackle this issue but they failed to achieve considerable improvement over existing methods. 
In our paper, we demonstrate that extreme learning rates can lead to poor performance.
We provide new variants of Adam and AMSGrad, called AdaBound and AMSBound respectively, which employ dynamic bounds on learning rates to achieve a gradual and smooth transition from adaptive methods to SGD and give a theoretical proof of convergence.
We further conduct experiments on various popular tasks and models, which is often insufficient in previous work.
Experimental results show that new variants can eliminate the generalization gap between adaptive methods and SGD and maintain higher learning speed early in training at the same time.
Moreover, they can bring significant improvement over their prototypes, especially on complex deep networks.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Optimization, Adam, Generalization</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Novel variants of optimization methods that combine the benefits of both adaptive and non-adaptive methods.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">12 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SJef8P3thQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review of "Adaptive Gradient Methods with Dynamic Bound of Learning Rate"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkg3g2R9FX&amp;noteId=SJef8P3thQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1111 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1111 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents new variants of ADAM and AMSGrad that bound the gradients above and below to avoid potential negative effects on generalization of excessively large and small gradients; and the paper demonstrates the effectiveness on a few commonly used machine learning test cases.  The paper also presents detailed proofs that there exists a convex optimization problem for which the ADAM regret does not converge to zero.

This paper is very well written and easy to read.  For that I thank the authors for their hard word.  I also believe that their approach to bound is well structured in that it converges to SGD in the infinite limit and allows the algorithm to get teh best of both worlds - faster convergence and better generalization.  The authors' experimental results support the value of their proposed algorithms.  In sum, this is an important result that I believe will be of interest to a wide audience at ICLR.

The proofs in the paper, although impressive, are not very compelling for the point that the authors want to get across.  That fact that such cases of poor performance can exists, says nothing about the average performance of the algorithms, which is practice is what really matters.

The paper could be improved by including more and larger data sets.  For example, the authors ran on CIFAR-10.  They could have done CIFAR-100, for example, to get more believable results.

The authors add a useful section on notation, but go on to abuse it a bit.  This could be improved.  Specifically, they use an "i" subscript to indicate the i-th coordinate of a vector and then in the Table 1 sum over t using i as a subscript.  Also, superscript on vectors are said to element-wise powers.  If so, why is a diag() operation required?  Either make the outproduct explicit, or get rid of the diag().</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BkeFPweF3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkg3g2R9FX&amp;noteId=BkeFPweF3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1111 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1111 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors introduce AdaBound, a method that starts off as Adam but eventually transitions to SGD. The motivation is to benefit from the rapid training process of Adam in the beginning and the improved convergence of SGD at the end. The authors do so by clipping the weight updates of Adam in a dynamic way. They show numerical results and theoretical guarantees. The numerical results are presented on CIFAR-10 and PTB while the theoretical results are shown on assumptions similar to AMSGrad (&amp; using similar proof strategies). As it stands, I have some foundational concerns about the paper and believe that it needs significant improvement before it can be published. I request the authors to please let me know if I misunderstood any aspect of the algorithm, I will adjust my rating promptly. I detail my key criticisms below:

- I'm somewhat confused by the formulation of \eta_u and \eta_l. The way it is set up (end of Section 4), the final learning rate for the algorithm converges to 0.1 as t goes to infinity. In the Appendix, the authors show results also with final convergence to 1. Are the results coincidental with the fact that SGD works well with those learning rates? It is a bit odd that we indirectly encode the final learning rate of the algorithm into the \eta s. 

- Am I correct in saying that with t=100 (i.e., the 100th iteration), the \eta s constrain the learning rates to be in a tight bound around 0.1? If beta=0.9, then \eta_l(1) = 0.1 - 0.1 / (0.1*100+1) = 0.091. After t=1000 iterations, \eta_l becomes 0.099. Again, are the good results coincidental with the fact that SGD with learning rate 0.1 works well for this setup? In the scheme of the 200 epochs of training (equaling almost 100-150k iterations), if \eta s are almost 0.099 / 0.10099, for over 99% of the training, we're only doing SGD with learning rate 0.1. 

- Along the same lines, what learning rates on the grid were chosen for each of the problems? Does the setup still work if SGD needs a small step size and we still have \eta converge to 1? A VGG-11 without batch normalization typically needs a smaller learning rate than usual; could you try the algorithms on that? 

- Can the authors plot the evolution of learning rate of the algorithm over time? You could pick the min/median/max of the learning rates and plot them against epochs in the same way as accuracy.This would be a good meta-result to show how gradual the transition from Adam to SGD is. 
 
- The core observation of extreme learning rates and the proposal of clipping the updates is not novel; Keskar and Socher (which the authors cite for other claims) motivates their setup with the same idea (Section 2 of their paper). I feel that the authors should clarify what they are proposing as novel. Is it correct that a careful theoretical analysis of this framework is what stands as the authors' major contribution?

- Can you try experimenting with/suggesting trajectories for \eta which converge to SGD stepsize more slower? 

- Similarly, can you suggest ways to automate the choice for the \eta^\star? It seems that the 0.1 in the numerator is an additional hyperparameter that still might need tuning? 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkg0-SM-3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice experiments, but theory does not reflect any benefit</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkg3g2R9FX&amp;noteId=rkg0-SM-3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1111 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">27 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1111 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">*Summary :
The paper explores variants of popular adaptive optimization methods.
The idea is to clip the magnitude of the gradients from above and below in order to prevent too aggressive/conservative updates.
The authors provide regret bound to this algorithm in the online convex setting and perform several illustrative experiments.


*Significance:
-There is not much novelty in Theorems 1,2,3 since similar results already appeared in Reddi et al.

-Also, the theoretical part does not demonstrate the benefit of the clipping idea. Concretely, the regret bounds seem to be similar to the bounds of AMSBound.
Ideally, I would like to see an analysis that discusses a situation where AdaGrad/AMSBound fail or perfrom really bad, yet the clipped versions do well.

-The experimental part on the other hand is impressive, and the results illustrate the usefulness of the clipping idea.

*Clarity:
The idea and motivation are very clear and so are the experiments.


*Presentation:
The presentation is mostly good.

Summary of review:
The paper suggests a simple idea to avoid extreme behaviour of the learning rate in standard adaptive methods. The theory is not so satisfying, since it does not illustrate the benefit of the method over standard adaptive methods. The experiments are more thorough and illustrate the applicability of the method.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Skx8lvomim" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A few comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkg3g2R9FX&amp;noteId=Skx8lvomim"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1111 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear authors,

Interesting work! My coauthors and I have been suffered from the poor generalization of Adam in many of our productions for a long time. We have to use SGD for better performance but I do HATE fine-tuning hyperparameters of SGD again and again!

I noticed that there have been many new proposed optimizers claiming they are better than Adam. I once tried some of them and was disappointed to find that they can bring nothing improvement but more hyperparameters! I doubt that the more and more complicated design of optimizers is not a right way and there must be a simple way to build an optimizer as fast as Adam while as good as SGD.

Thatâ€™s why this paper really attracts me. The idea of gradually transforming Adam to SGD is really simple but looks intuitive and reasonable. It makes sense to me. The algorithm is also well-presented. I am surprised that you also provide convincing proofs about the algorithm --- I had thought you would just construct some empirical studies w/o theoretical analysis.

I have a few questions about the paper and personal thoughts of the future work. I hope they will be useful to the authors. Feel free to leave them as is if they are not correct. :D

- Besides rapid training, the smoothness of the learning curve is another advantage of adaptive methods. Personally, I think it might be more important. When trying to train new models, we often do not know whether it can converge in advance. A common approach is training few epochs and making a preliminary decision of what to do next based on the trend of learning curve in the early stage. The sharp fluctuation of loss is common when using SGD, which makes it hard to estimate the trend of learning curve quickly. Are your framework able to keep this strength of Adam? What is your take on this?

- I tried AdaBound on CIFAR-10 by myself. It is interesting that I have used simpler bound functions (linear functions and piecewise constant function) and still got very good performance. As you also mentioned that the convergence speed of bound functions is not very important, I suggest you may choose simpler ones (Occamâ€™s Razor).

- I am thinking about if we may use a way like in Keskar et al. (2017) to determine the final step size automatically. I didnâ€™t think through this carefully of whether it is possible. What is your opinions?

Thanks in advance for your time and I hope this paper get accepted!</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkg7oagJn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Some answers and responses</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkg3g2R9FX&amp;noteId=rkg7oagJn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1111 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1111 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Sorry for the late response. It's been a bit busy in the past few days. Thanks for your comments and we present our responses below.

1. We didn't pay much attention to the smoothness of the learning curve before and the analysis mainly focuses on training speed w.r.t. the advantage of adaptive methods. But, actually, we also mentioned that the learning curve of our framework is smoother than that of SGD in the experiment on PTB (in para. 1, section 5.4, page 8). We would agree with your opinion that the smoothness is also important. We are pleased to add some more discussion on this point in the next revision.

2. That's interesting. It is a good engineering question that what particular bound function is best (simplicity, efficiency, effectiveness etc.) in production. As for this paper, it is more about to show the potential of a novel framework and stimulate others' research. It would be a direction of future work to investigate whether there is a simpler bound function that guarantees the performance.

3. I am afraid that the method in Keskar et al. (2017) seems not able to be applied to our algorithm directly. Introducing automation is meaningful. But it is not a very easy task, IMO. We may think about this point carefully in the future work.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Hklkd3A25X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarification about ADAGRAD and generalization of your method</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkg3g2R9FX&amp;noteId=Hklkd3A25X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1111 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi,

i have three main questions for you. It would be great if you could help clarify them.

1. You mention  the following about ADAGRAD along with ADAM and RMSPROP - "they are observed to generalize poorly compared with SGD or even fail to converge due to unstable and extreme learning rates.". As far as I am aware the issue with the convergence analysis of exponentiated squared gradient averaging algorithms like ADAM and RMSPROP do not extend to ADAGRAD. So, ADAGRAD is indeed guaranteed to converge given the right assumptions. In the rest of the paper, the experiments and arguments mainly consist of ADAM and not adaptive methods in general. So I think the distinction between adaptive methods in general and adaptive methods like ADAM and RMSPROP with respect to convergence guarantees should be made clearer.

2. I am not sure I understand but could you please clarify how AMSGRAD helps in the generalization of ADAM. From my understanding, it only solved the convergence issue by ensuring that the problematic quantity in the proof is PSD.

3. You mention "Experimental results show that new variants can eliminate the generalization gap
between adaptive methods and SGD". Given that the paper only contains a few empirical results (on some important and common tasks) and no theoretical proof in that respect, I find it to be a misleading statement. The experiments in Wilson et. al(2017) give proper evidence of the gap between SGD and Adaptive methods in overparameterized settings. To show that this method overcomes it, I think you need a stronger argument than what you have shown.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Byg-RFWA5X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Part1 of Clarifications</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkg3g2R9FX&amp;noteId=Byg-RFWA5X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1111 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1111 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your interests.

I respond point by point below.

&gt;&gt;&gt; "they are observed to generalize poorly compared with SGD or even fail to converge due to unstable and extreme learning rates." As far as I am aware the issue with the convergence analysis of exponentiated squared gradient averaging algorithms like ADAM and RMSPROP do not extend to ADAGRAD. So, ADAGRAD is indeed guaranteed to converge given the right assumptions.

A more precise expression of that sentence should be "Adam, RMSprop, AdaGrad, and other adaptive methods are observed to generalize poorly ..., and, some of them (i.e. Adam) even fail to converge ...", which is summarized from [1][2][3] and Section 3 in our paper. We didn't notice the original sentence might be misunderstood. We will use a more precise way to summarize the phenomenon. However, although AdaGrad is theoretically guaranteed to converge, it is well-accepted that in practice the convergence is too slow at the end of training due to its accumulation of second order momentum. As we usually use a limited number of epochs or limited time in a training job, it may fail to achieve the "theoretical convergence". Therefore, maybe we can say "may hard to converge" to summarize. :D

&gt;&gt;&gt; In the rest of the paper, the experiments and arguments mainly consist of ADAM and not adaptive methods in general. So I think the distinction between adaptive methods in general and adaptive methods like ADAM and RMSPROP with respect to convergence guarantees should be made clearer.

The main purpose of this paper is to introduce a novel framework that can combine the advantages of adaptive methods and SGD(M). The framework applies to Adam as well as AdaGrad and other adaptive methods. As mentioned above, the weaknesses of adaptive methods are in common and combining with SGD can help overcome the problems. Therefore, we don't think it is necessary to distinguish particular adaptive methods everywhere in the paper. We run experiments mainly on Adam because of its popularity. According to your comments, we would consider adding more experiments on other adaptive methods like AdaGrad.

&gt;&gt;&gt; I am not sure I understand but could you please clarify how AMSGRAD helps in the generalization of ADAM. From my understanding, it only solved the convergence issue by ensuring that the problematic quantity in the proof is PSD.

I guess we understand "generalization" differently. If you regard "generalization error", in a narrow sense, as how large the gap between training and testing error is, then I agree that AMSGrad only solves the convergence issue. But broadly speaking, "generalization error" is a measure of how accurate of a method on unseen data (see <a href="https://en.wikipedia.org/wiki/Generalization_error)." target="_blank" rel="nofollow">https://en.wikipedia.org/wiki/Generalization_error).</a> It depends on not only handling overfitting but the convergence results on training data. Therefore, attempts on solving convergence issue can also help the generalization in a broad sense.

&gt;&gt;&gt; The experiments in Wilson et. al(2017) give proper evidence of the gap between SGD and Adaptive methods in overparameterized settings. To show that this method overcomes it, I think you need a stronger argument than what you have shown.

We would first argue that the experiments in Wilson et al. (2017), including a few common tasks in CV and NLP, are not much different to ours and that in other recent similiar works. While, their artifactual example before the experiment section does use a overparameterized settings, but they never claim it is the main cause of poor generalization. It is a necessary but not sufficient condition. Indeed, the poor generalizaion is mainly caused by the propery of the carefully constructed particular task. In other words, it is highly problem-dependent. The actual statement of Wilson et al. (2017) is 

** When a problem has multiple global minima, different algorithms can find entirely different solutions when initialized from the same point. In addition, we construct an example where adaptive gradient methods find a solution which has worse out-of-sample error than SGD. **

Therefore, no one can affirm there are no examples that adaptive methods find a better solution than SGD. The above are just examples and there are infinite exmaples. We don't think it is meaningful to show our framework can perform well on that particular one, even though it is not hard.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1glD5ZAcm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Part2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkg3g2R9FX&amp;noteId=H1glD5ZAcm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1111 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1111 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt;&gt;&gt; You mention "Experimental results show that new variants can eliminate the generalization gap between adaptive methods and SGD". Given that the paper only contains a few empirical results (on some important and common tasks) and no theoretical proof in that respect, I find it to be a misleading statement.

Honestly, maybe I don't exactly get your point. We said "experimental results show", not "theoretical proof show" or something like that. If you said "your experiments are not enough", I could understand and may add some additional experiments on other tasks if reasonable. But we don't think "no theoretical proof in that respect" is a valid point to criticize that the statement is misleading or overclaiming.

In addition, it should be mentioned that our understanding of the generalization behavior of deep neural networks is still very shallow by now. It is a big challenge of investigating from theoretical aspects. A summary of recent achievements can be found here (<a href="http://ruder.io/deep-learning-optimization-2017/)," target="_blank" rel="nofollow">http://ruder.io/deep-learning-optimization-2017/),</a> and we can see their theoretical analysis is still under strong or particular assumptions. That's why most similar works on optimizers tend to use experiments to support their arguments.

As for the richness of our experiments, in our paper the tasks include several popular ones on CV and NLP area; the models include simple perceptron, deep CNN models and RNN models. We give a brief comparison to some recent works for a fair judgment. 

- [1] does not propose novel algorithms or frameworks as we do. Their main contribution is empirically showing that the minima found by adaptive learning rate methods perform generally worse compared to those found by SGD, and providing some possible causes. The richness of experiments of ours is similar to theirs. Personally, the amount of experiments in this work is an average level among similar works as far as I know.
- The experiments in [2] are very limited, as the authors also state the experiments are "preliminary".
- [3] conducts more experiments than other similar works. But there is no theoretical analysis, which is important in such kinds of works.
- [4] (posted on arXiv and also a submission to ICLR19) only conducts experiments on image classification tasks. As it is known that the gap between Adam and SGD on this task is notable, while on some NLP tasks like machine translation, well-turned Adam may even outperform SGD ([6]), it is not enough to just test on this single task.
- The experiments in [5] (posted on arXiv and also a submission to ICLR19) are even more limited than that of [2], only a toy model on MINST.

Therefore, we argue that our experiments have already shown the potential of our proposed framework. Future papers by other researchers are a more appropriate home for additional experiments on other tasks. We think publicizing now with the set of baselines that we have already included so as to stimulate others' research is more effective than us delaying publication and presentation of this work.

-----
[1] Wilson, A.C., Roelofs, R., Stern, M., Srebro, N., &amp; Recht, B. (2017). The Marginal Value of Adaptive Gradient Methods in Machine Learning. NIPS.
[2] Sashank J.R., Satyen K., &amp; Sanjiv K. (2018). On the Convergence of Adam and Beyond. ICLR.
[3] Keskar, N.S., &amp; Socher, R. (2017). Improving Generalization Performance by Switching from Adam to SGD. CoRR, abs/1712.07628.
[4] Chen, J., &amp; Gu, Q. (2018). Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks. CoRR, abs/1806.06763.
[5] Chen, X., Liu, S., Sun, R., &amp; Hong, M. (2018). On the Convergence of A Class of Adam-Type Algorithms for Non-Convex Optimization. CoRR, abs/1808.02941.
[6] Denkowski, M.J., &amp; Neubig, G. (2017). Stronger Baselines for Trustable Results in Neural Machine Translation. NMT@ACL.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_r1lJqtvjcm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Some questions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkg3g2R9FX&amp;noteId=r1lJqtvjcm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Hyesst_Wu1" class="profile-link">Hyesst Wu</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1111 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, thanks for the nice paper. The way of combining the adaptive methods and SGD proposed in the paper is really interesting, while I guess I find some small typos or mistakes. They are all minor and do not much affect the understand of the paper, but I think a clarification on them would be fine.

First, the upper bound function at the end of Section 4 and Appendix G does not converge to 0.1. I believe it is a typo: there is a redundant "1" at the denominator and the correct expression should be $0.1 + \frac{0.1}{(1-\beta)t}$. Also, I guess you miss the subscripts of $\beta$ in the functions in Section 4. Maybe it should be $\beta_1$ or $\beta_2$, I guess.

Second, how many layers do you use in DenseNet? You provide the source code you used for DenseNet, and it is DenseNet-121 in the code. However, I suggest mentioning the number of layers directly in the paper. It is an important hyperparameter of deep CNN network.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SklPiVVncm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarification</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkg3g2R9FX&amp;noteId=SklPiVVncm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1111 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1111 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi Hyesst,

Thanks for your interests. 

1. You are absolutely right! Thanks for your correction! It should be $\beta_1$ in the upper bound function at the end of Section 4.

2. Yes, we used DenseNet-121. We will add this information in the next revision.

Thank you very much for your comments and suggestions.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1goTjL5qX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>About the code</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkg3g2R9FX&amp;noteId=S1goTjL5qX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1111 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi!  I am interested in the algorithm you proposed and want to have a try on my researches. Could you provide an implementation of the algorithm? Or, if it is not convenient in the review period, could you give a brief instruction of how to implement it? 
Good luck and hope your paper can be accepted. :-)</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HkxJNxD55X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your interests</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkg3g2R9FX&amp;noteId=HkxJNxD55X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1111 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1111 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your interests!

Honestly, the code is a little bit messy currently. We are cleaning up the code for releasing it these days.
If you can't wait to have a try, it is easy to implement the algorithm by making some minor changes on the optimizers in PyTorch. Take AdaBound/AMSBound as an example, we just modify the source code of Adam (<a href="https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py)." target="_blank" rel="nofollow">https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py).</a> Specifically, we use torch.clamp(x, l, r) function, which can constrain x between l and r element-wisely, to perform the clip operation mentioned in the paper. You can also make similar changes to other optimizers such as AdaDelta and RMSprop.

The codes for the experiments in the paper, as mentioned in the footnote on page 6, are obtained from https://github.com/kuangliu/pytorch-cifar and https://github.com/salesforce/awd-lstm-lm.

We would be happy if you can share your results on your own researches using our methods.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>