<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Skgge3R9FQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Controlling Over-generalization and its Effect on Adversarial..." />
      <meta name="og:description" content="Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Skgge3R9FQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation</a> <a class="note_content_pdf" href="/pdf?id=Skgge3R9FQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019controlling,    &#10;title={Controlling Over-generalization and its Effect on Adversarial Examples Detection and Generation},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Skgge3R9FQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=Skgge3R9FQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Convolutional Neural Networks (CNNs) significantly improve the state-of-the-art for many applications, especially in computer vision. However, CNNs still suffer from a tendency to confidently classify out-distribution samples from unknown classes into pre-defined known classes. Further, they are also vulnerable to adversarial examples. We are relating these two issues through the tendency of CNNs to over-generalize for areas of the input space not covered well by the training set. We show that a CNN augmented with an extra output class can act as a simple yet effective end-to-end model for controlling over-generalization. As an appropriate training set for the extra class, we introduce two resources that are computationally efficient to obtain: a representative natural out-distribution set and interpolated in-distribution samples. To help select a representative natural out-distribution set among available ones, we propose a simple measurement to assess an out-distribution set's fitness. We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. Finally, we show that generation of white-box adversarial attacks using our proposed augmented CNN can become harder, as the attack algorithms have to get around the rejection regions when generating actual adversaries.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Convolutional Neural Networks, Adversarial Instances, Out-distribution Samples, Rejection Option, Over-generalization</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">17 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SkevWvSj6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>On the originality/novelty of the paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skgge3R9FQ&amp;noteId=SkevWvSj6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1046 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1046 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The originality of our paper is not on making use of a dustbin class (OOV). As mentioned by the reviewers, this has been proposed not only for NLP and vision tasks but also for adversaries detection (Gross et. al, 2017, Hosseini et. al, 2017) and out-distribution identification (Yu et. al. 2017). 

Our main goal is rather to demonstrate the relationship between over-generalization induced by naive CNN and its sensitivity to **both** adversarial examples and out-distribution samples. To this end, we used Augmented CNN (A-CNN) as a tool for controlling over-generalization. Indeed, to our knowledge, we are the first to demonstrate this relationship, i.e. over-generalization and sensitivity to adversaries (as well as out-distribution samples), in an extensive experimental setup. Thus, the originality of our paper does not lie on re-proposing A-CNNs, but we used it as a tool to show that mitigating the effect of over-generalization on the development of more secure neural networks (e.g. CNN) in the presence of adversaries and out-distribution sets. 

The immediate natural key question is how to acquire the training samples for the extra class (from a nearly infinite number of out-distribution samples) in order to cover properly the over-generalized regions induced by a naive CNN. Instead of synthesizing artificial out-distribution samples using a hard-to-train generator (Lee. et al ICLR 2018, Jin et. al NIPS 2017, Yu et. al IJCAI 2017 ), we rather propose the use of a novel measurement for selecting a representative out-distribution dataset among the readily accessible natural ones for training an effective A-CNN. This not only maintains the accuracy on in-distribution samples but also results in identifying both adversaries and unseen out-distribution sets simultaneously.

Reference:
- Grosse, Kathrin, et al. "On the (statistical) detection of adversarial examples." arXiv preprint arXiv:1702.06280 (2017).

-Hosseini, Hossein, et al. "Blocking transferability of adversarial examples in black-box learning systems." arXiv preprint arXiv:1703.04318 (2017).

-Jin, Long, Justin Lazarow, and Zhuowen Tu. "Introspective classification with convolutional nets." Advances in Neural Information Processing Systems. 2017.

-Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training confidence-calibrated classifiers for detectingout-of-distribution samples. ICLR2018.

- Yu, Yang, et al. "Open-category classification by adversarial sample generation." Proceeding of the 26-th International Joint Conference on Artificial Intelligence (2017).





</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Skl-BtQq6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting research direction but not good enough</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skgge3R9FQ&amp;noteId=Skl-BtQq6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1046 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1046 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposed to add an additional label for detecting OOD samples and adversarial examples in CNN models. This research direction seems interesting, however, the idea of using an extra label for OODs is not new and was previously explored in different domains. I would expect the describe how their method is different, and keep the research from that point.
Additionally, there are several claims in this paper which I'm not convinced are true, such as the over-generalization of CNNs, the choice of OODs (recent studies have shown NNs are not well calibrated, so using softmax as the confidence might not be the best idea), etc.
Reg. the results, did the authors compare their method to existing adv. example detection methods, such as Ma, Xingjun, et al. ICLR (2018) "Characterizing adversarial subspaces using local intrinsic dimensionality." ? or some other method? 
Moreover, in Table 2. I'm not sure what should I conclude from the "Naive Model Error" on OOD samples.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1eqf5OA3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The work can be improved</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skgge3R9FQ&amp;noteId=H1eqf5OA3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1046 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1046 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The idea of having a separate class for out-distribution is a very interesting idea but unfortunately previously explored. In fact, in machine learning and NLP there is the OOV class which sometimes people in computer vision also use. Some of the claims in the paper can be further substantiated or explored. For example in abstract there is a simple claim that is presented too strong: We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries. This claim is bigger than just CNNs and needs to be studied in a theoretical framework not an empirical one. Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax. In general it is very unlikely that you will be able to choose every variation of out-distribution cases. Much easier if you just try to solve the problem using a set of n Sigmoids (n total number of classes) and consider each output a probability distribution. 

However, the studies in this paper are still valuable and I strongly recommend continuing on the same direction. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJeC2_Hspm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>More clarification </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skgge3R9FQ&amp;noteId=BJeC2_Hspm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1046 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1046 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We appreciate the reviewer 1 for his/her feedback on our paper. 

The reviewer mentioned: “In general it is very unlikely that you will be able to choose every variation of out-distribution cases”:
Actually, for training A-CNN (Augmented CNN), we did not train it on every variation of out-distribution cases, rather, we recognize a single representative out-distribution set among the available ones according to our measurement. Then using it for training A-CNN with the aim of effectively controlling over-generalization.


The reviewer mentioned: “ Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax”:
We would be appreciated if the reviewer could provide us with the references that showing using only sigmoid could control such a challenging problem of adversaries. 
Please note we did not aim to devise a method that is able to reject all adversaries. Rather, we attempted to show that a CNN with less over-generalization is able to reject some of the adversaries while correctly classifies many of the remainder, particularly non-transferable attacks. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1gMR8h6h7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Too many hidden assumptions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skgge3R9FQ&amp;noteId=H1gMR8h6h7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1046 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1046 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper propose to incorporate an additional class for adversarial and out-distribution samples in CNNs. The paper propose to incorporate natural out-distribution images and interpolated images to the additional class, but the problem of selecting the out-distribution images is itself an important problem. The paper presents a very simple approaches for selecting the out-distribution images that relies on many hidden assumptions on the images source or the base classier, and the interpolation mechanism is also too simple and there is the implicit assumption of low complexity images. There exists more principled approaches for selecting out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors.
In summary, the quality of the paper is poor and the originality of the work is low. The paper is easily readable.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SylJ-xIopQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Asking for more explanations</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skgge3R9FQ&amp;noteId=SylJ-xIopQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1046 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1046 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We are thankful for the reviewer 2 to provide us with his/her feedback,

The reviewer mentioned: "the interpolation mechanism is also too simple":
We would like to highlight that despite the simplicity of interpolated samples, there has been demonstrated the effectiveness of using such samples on developing more regularized and generalized neural networks (Zhang et al, 2018) as well as on making them more secure (Zhao et. al. 2018).  Thus, we believe that simplicity does not necessarily lead to ineffectiveness. 


The reviewer mentioned “many hidden assumptions on the images source or the base classier”:
As this statement is not clear for us, we would appreciate if the reviewer could elaborate more on it. The only assumption we made is on the fact that the out-distribution samples should be statistically and semantically different than the in-distribution samples. Then among such out-distribution sets, we propose a measurement for identifying the most representative one among those available. 

The reviewer stated "There exists more principled approaches for *selecting* out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors":
While there are the approaches that aim to detect out-distribution sets, they have not been designed for the selection purposes as we do. By mentioning this statement, if the reviewer means the missing of some principled approaches like ODIN in our comparisons, we would like to inform the inclusion of ODIN results in the revised version of the paper. 

Reference:
- Zhao, Jake, and Kyunghyun Cho. "Retrieval-Augmented Convolutional Neural Networks for Improved Robustness against Adversarial Examples." arXiv preprint arXiv:1802.09502 (2018).

- Zhang, H., Cisse, M., Dauphin, Y. N., &amp; Lopez-Paz, D. (2017). mixup: Beyond empirical risk minimization. ICLR 2018 (arXiv preprint arXiv:1710.09412).
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rygaTnElhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Rethink the Interpolated Instances</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skgge3R9FQ&amp;noteId=rygaTnElhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">26 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1046 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The interpolated  samples $ {x}' = \alpha x_i + (1 − \alpha ) x_j $  are not necessarily around the decision boundaries. 

Because the interpolation operation is performed in the input space, but not in the high-level feature space.  Assume that the last convolution layer is f(x), $ f({x}') $ is not equal $ f(\alpha x_i + (1 − \alpha ) x_j) $ in fact.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rylwp4Sm67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skgge3R9FQ&amp;noteId=rylwp4Sm67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1046 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1046 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">As you mentioned, we agree that the interpolated samples are not necessarily located on (around) the decision boundaries. There are some reliable approaches for providing an exact solution for this problem, such as DeepFool, that are able to find the samples (adversaries) located around the decision boundaries, but these are computationally expensive. 

Zhang et. al (2017) also have considered the interpolated samples (in input space) generated with \alpha=0.5 from the pairs of samples (x_i, x_j) selected from different classes are placed around some virtual decision boundaries as their labels are regarded as the average of their true labels (one-hot vectors i.e. 0.5 y_i + 0.5 y_j ). By regarding such interpolated samples on some virtual decision boundaries, we labeled them as dustbin class. 

Zhang, H., Cisse, M., Dauphin, Y. N., &amp; Lopez-Paz, D. (2017). mixup: Beyond empirical risk minimization. ICLR 2018 (arXiv preprint arXiv:1710.09412).
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkxFFJ-jsX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Some questions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skgge3R9FQ&amp;noteId=rkxFFJ-jsX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">22 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1046 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt;&gt; the fooling classification regions (spanned by the adversary direction and one of its orthogonal random directions) of the naive CNNs are occupied by dustbin regions  (in Section 3.1)

It doesn't mean that  the fooling classification regions of the Augmented CNNs are occupied by dustbin regions. Could you plot several church-windows where the x-axis of each window is the adversary direction achieved by FGS or DeepFool using  the Augmented CNNs? 

For Section 3.2, have you tried the targeted adversarial attacks to skip over some regions assigned to dustbin class ?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1ghn977a7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>dustbin regions in white-box adversaries directions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skgge3R9FQ&amp;noteId=H1ghn977a7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1046 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1046 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Question 1 (fooling regions vs dustbin): From Figure 6, it can be observed that moving in the white-box adversaries directions can end up in the dustbin regions. This can show that some of the fooling regions are assigned to the dustbin label in augmented CNNs.

Question 2 (about Sec. 3.2): To generate T-FGS adversaries, we discard the dustbin class as possible fooling target. Adversaries are then generated with a specific epsilon and within a fixed number of iterations (same values as for generating black-box adversaries, see Table 5 of Appendix). To skip over the dustbin regions, the number of iterations for finding adversaries should be increased, which in turn will increase the amount of perturbation.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1lmogO4j7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Details on Section 2.2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skgge3R9FQ&amp;noteId=B1lmogO4j7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1046 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, could you describe the details on how to find the nearest neighbor of x_i in the feature space of a CNN on Section 2.2 ? Is it expensive computationally for large-scale datasets ?

And is the last layer referring to the previous layer of the logit layer?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SygmR_X7pX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Computational complexity</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skgge3R9FQ&amp;noteId=SygmR_X7pX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1046 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1046 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">As mentioned in the paper, the instances are mapped into the feature space generated by the last convolutional layer. We use this space to identify the nearest neighbor using an Euclidean distance, i.e., $min_x’ \|\phi(x) - \phi(x’)\|_2$, where $\phi(x)$ is the representation of $x$ in the feature space. As the dimensionality in the feature space is significantly lower than the original pixel space, the computational complexity of finding nearest neighbors in the feature space is lower than that of the pixel space. By using a proper data structure (e.g., k-d tree, which is implemented in scikit-learn), the complexity time of finding the nearest neighbor can be further reduced, compared to that of a naive k-NN implementation. Moreover, using GPUs and parallel computations, it has been shown the computational time for k-NN can be diminished significantly (Johnson et. al., 2017).

Reference: Johnson, Jeff, Matthijs Douze, and Hervé Jégou. "Billion-scale similarity search with GPUs." arXiv preprint arXiv:1702.08734 (2017)</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ByelE-bMjX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Missing baselines</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skgge3R9FQ&amp;noteId=ByelE-bMjX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Oct 2018 (modified: 22 Oct 2018)</span><span class="item">ICLR 2019 Conference Paper1046 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">You are missing too many baselines. 

 I could not see why  out-distribution detection methods can be considered as baselines in the paper.

out-distribution detection:

- baseline: [1]  (Hendrycks &amp; Gimpel, 2016)
- ODIN: [2] (Liang et al., 2017)
- Bayesian Neural Network: [3] (Gal, 2016)
- LearningConfidence: [4] (DeVries &amp; Taylor, 2018)

Similarly,  I could not see why adversarial examples detection methods can be considered as baselines in the paper.

 adversarial examples detection:

- KD+PU: [5]
- LID: [6]


[1] A baseline for detecting misclassified and out-of-distribution examples in neural networks.
[2] Enhancing the reliability of out-of-distribution image detection in neural networks
[3] Uncertainty in deep learning
[4] Learning confidence for out-of-distribution detection in neural networks
[5] Detecting adversarial samples from artifacts.
[6] Characterizing adversarial subspaces using local intrinsic dimensionality. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkelT3a3c7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Confusing the over-generalized regions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skgge3R9FQ&amp;noteId=rkelT3a3c7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1046 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, thanks for the nice paper. 

This paper makes a claim:
&gt;&gt;such that  the samples drawn from many over-generalized regions including a wide-range of out-distribution samples and various types of adversaries are mapped to this “dustbin” sub-manifold.

Your theory seems to assume that adversarial examples exist in the over-generalized regions but not in the in-distribution region, am I right? What is the relationship between the over-generalized regions and the in-distribution regions?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkgfQcOksQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>To respond the anonymous commenter</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skgge3R9FQ&amp;noteId=rkgfQcOksQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1046 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1046 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We greatly thank the anonymous commenter, 
Through statistical testing, Grosse et al. [1] demonstrated that adversarial examples are indeed drawn from a distribution different from the distribution of original (in-distribution) data of a given task. Moreover, the over-generalized (i.e. out-distribution) regions contain the samples statistically/semantically different from in-distribution ones. Thus, adversarial examples can be treated as out-distribution samples. We show that if the over-generalized regions can be effectively reduced, then the risk of being fooled by adversaries can be significantly reduced.

In other words, by training an augmented CNN with the dustbin class containing only representative natural out-distribution samples, we tend to refine the frontiers (boundaries) of our model regarding out-distribution regions, and as such improving how adversarial samples are processed, by either correctly classifying these samples or classifying them as dustbin (equivalent to rejection).

Reference: Grosse, Kathrin, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. "On the (statistical) detection of adversarial examples." arXiv preprint arXiv:1702.06280 (2017)</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1evvljJsQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Adversarial examples are not drawn from a different distribution</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skgge3R9FQ&amp;noteId=S1evvljJsQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1046 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">It should be noted that the work of Grosse et al. was refuted by <a href="https://arxiv.org/abs/1705.07263" target="_blank" rel="nofollow">https://arxiv.org/abs/1705.07263</a> which shows that it is only adversarial examples generated with fixed and very simple attack algorithms are statistically detectable as different. However, any recent optimization-based attack (PGD/C&amp;W) will generate adversarial examples that are not statistically different from clean examples as far as the detection Grosse et al. scheme will find.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkeqla1No7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our response </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skgge3R9FQ&amp;noteId=Bkeqla1No7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1046 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1046 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The incapacity of Grosse et al. approach to detect some well-crafted white-box attacks does not warrant that many of the black-box adversaries, which typically includes more perturbations than their white-box counterparts, are not statistically different from the problem distribution (i.e. in-distribution). For example, for creating transferable black-box C&amp;W adversaries, the amount of perturbations required increases (Carlini and Wagner, 2017).  

Moreover, note that we also exhibit in Table 1 that training an augmented CNN on *only I-FGS* (similar to the approach of Grosse et al.) cannot make it robust to other types of (black-box) adversaries like C&amp;W and DeepFool. Plus, this augmented CNN (trained on I-FGS) is also unable to identify out-distribution samples (Table 3 in Appendix). This may be happened as I-FGS only cover partially the over-generalized regions (Fig. 1 (b)). 

Finally, we are not claiming that our augmented CNN is an ultimate solution for all possible kinds of adversaries. But, through extensive experiments, we shed some light on how controlling over-generalization effectively can positively influence the development of CNNs that are more robust in the presence of *both* natural out-distribution samples and some strong and highly transferable (black-box) attacks.

Reference :
1) Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. IEEE Symposium on Security and Privacy (SP), 2017.
2) Grosse, Kathrin, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. "On the (statistical) detection of adversarial examples." arXiv preprint arXiv:1702.06280 (2017)



</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>