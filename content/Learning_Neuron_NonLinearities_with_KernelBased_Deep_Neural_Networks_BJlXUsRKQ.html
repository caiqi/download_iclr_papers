<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learning Neuron Non-Linearities with Kernel-Based Deep Neural Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learning Neuron Non-Linearities with Kernel-Based Deep Neural Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJlXUsR5KQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learning Neuron Non-Linearities with Kernel-Based Deep Neural Networks" />
      <meta name="og:description" content="The effectiveness of deep neural architectures has been widely supported in terms of both experimental and foundational principles. There is also clear evidence that the activation function (e.g...." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJlXUsR5KQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning Neuron Non-Linearities with Kernel-Based Deep Neural Networks</a> <a class="note_content_pdf" href="/pdf?id=BJlXUsR5KQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019learning,    &#10;title={Learning Neuron Non-Linearities with Kernel-Based Deep Neural Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BJlXUsR5KQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">The effectiveness of deep neural architectures has been widely supported in terms of both experimental and foundational principles. There is also clear evidence that the activation function (e.g. the rectiﬁer and the LSTM units) plays a crucial role in the complexity of learning. Based on this remark, this paper discusses an optimal selection of the neuron non-linearity in a functional framework that is inspired from classic regularization arguments. A representation theorem is given which indicates that the best activation function is a kernel expansion in the training set, that can be effectively approximated over an opportune set of points modeling 1-D clusters. The idea can be naturally extended to recurrent networks, where the expressiveness of kernel-based activation functions turns out to be a crucial ingredient to capture long-term dependencies. We give experimental evidence of this property by a set of challenging experiments, where we compare the results with neural architectures based on state of the art LSTM cells.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Activation functions, Kernel methods, Recurrent networks</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_S1xA6on62m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>LEARNING NEURON NON-LINEARITIES WITH KERNEL-BASED DEEP NEURAL NETWORKS</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlXUsR5KQ&amp;noteId=S1xA6on62m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper161 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper161 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper investigates the problem of designing the activation functions of neural networks with focus on recurrent architectures. The authors frame such problem as learning the activation functions in the space of square integrable functions by adding a regularization term penalizing the differential properties of candidate functions. In particular, the authors observe that this strategy is related to some well-established approaches to select activation functions such as ReLUs. 


The paper has some typos and some passages are hard to read/interpret. The write-up needs to be improved significantly.  

While some of the observations reported by the authors are interesting it is in general hard to evaluate the contributions of the paper. In particular the discussion of Sec. 2 is very informal, although ti describes the key technical observations used in the paper to devise the model (Sec. 3) that is then evaluated in the experiments (Sec. 4). In particular, it is unclear whether the authors are describing some known results - in which case they should add references - or original contributions - in which case they should report their results with more mathematical rigour. Indeed, in the abstract, the authors state that a representation theorem is given, but in the text they provide only an informal discussion of such result. 

Overall, it is hard to agree with the authors' conclusion that "the KBRN architecture exhibits an ideal computational structure to deal with classic problems of capturing long-term dependencies": the theoretical discussion does not provide sufficient evidence in this sense.

Some minor points: 

Confusing notation: why were the alpha^k replaced with the \chi^k between Sec. 2 and Sec. 3?

Unclear motivation for some design choices. For instance 1) the justification given by the authors to neglect the linear terms from both g(x) and k(x) in Sec. 3 is unclear. 2) why was the \ell_1 norm used as penalty for the regularizer R(\chi) in Sec.3? One could argue that \ell_1 is used to encourage sparse solutions, but the authors should explain why sparsity is desirable in this setting. 

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJl3u3O5nX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Unclear presentation and weak results.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlXUsR5KQ&amp;noteId=rJl3u3O5nX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper161 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper161 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: the authors propose a method for learning activation functions in neural networks using kernels. Each activation function is modeled as a weighted set of kernels, where (as I understand it) the weights are learned simultaneously with the linear weights in the network. The authors apply their method to learn the XOR function and to a simple sequence memory task.

My main concern with the paper is that the presentation is very hard to follow. The motivation, background, and contributions of this paper are all mixed in the abstract and introduction, making it hard to understand what is the current state of learned activations, what this paper introduces, and how the work in this paper relates to prior work. Instead, I found the presentation in Scardapane et al much clearer (that paper has a clear separation of background, related work, and their contributions). By contrast, this paper has one large paragraph in the introduction that muddles together multiple threads of thought, making it hard to digest. Before the reader has had time to digest the main ideas, the paper launches into a highly technical description of the method, without a clear high level explanation of what the main technique is. A lot of the mathematical notation introduced in Sec2 is not clearly motivated.

My other main concern is with the results. The paper solves two simple tasks with their method, and it is unclear what their kernel methods really buy them. For the XOR problem, it is such a simple task that it is hard to judge the method (it seems like complex activation functions are not required to solve the task, and it is not clear what including them gets you). For the sequence memory task, it seems unfair to compare their results to recurrent networks with a *single* hidden unit. If you have networks with 2 or 4 or 8 hidden units, do they solve the task? These experiments do not shed much light on the advantages of using kernel activations in recurrent networks.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkgv-Z-I2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning Neuron Non-Linearities with Kernel-Based Deep Neural Networks</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlXUsR5KQ&amp;noteId=rkgv-Z-I2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper161 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper161 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The scope of the paper is interesting: to additionally learn the nonlinear activation function of the neuron.

The insights provided in section 2 with eqs (2)-(5) are interesting and naturally build on the previous work of Poggio &amp; Girosi (1990) and Smola (1998). I found this a nice new insight and the strongest part of the paper. It is e.g. revealing to see to which P and L the rectifier nonlinearity is corresponding.

On the other hand I also have a number of suggestions for further improvement:

- Section 1: related to the overall function to be learned, the authors state "this general problem has been already solved". I think this statement is not completely correct, because depending on the choice of the stabilizer one obtains different optimal representations (e.g. Gaussian RBF or thin plate splines) as explained in Poggio &amp; Girosi (1990). The theory does not tell what the best stabilizer is.

Additional relevant work that would be good to mention at this point, in the area of kernel methods, is e.g. learning the kernel.

- It seems that no other existing work on deep kernel machines has been mentioned in the paper, while in the conclusions the authors state "In this paper we have introduced Kernel-based deep neural networks". 

- Related to the training set T_N the notation e^kappa is not explained. It is not clear how this is related to eq (1).

- It would be good to comment on the difference between (3)(4) and Poggio &amp; Girosi (1990).

- unnumbered eq after (5): are there multiple solutions to the problem (non-convex)? 

- The explanation of the recurrent network at the end of section 2 is too limited. Moreover, LSTM is not just a neuron nonlinearity, but a recurrent network with a particular structure. To which P and L would LSTM correspond?

- Fig.2: some of the nonlinearities look quite complicated and some of them are oscillatory (is this desirable? it reminds us of overfitting). Often one is interested in activation functions with a "simple shape" like sigmoid, tanh, relu. A more complicated nonlinearity may reduce the interpretability of the model.

- The examples given are rather conceptual (though nice) examples of the proposed method. However, no comparisons with other methods have been made yet in terms of generalization performance, e.g. on a few standard classification benchmark data sets, in comparison with other deep or shallow models.

A possible drawback of the proposed method might be (or maybe not) that additional unknown parameters need to be learned, which could possibly lead to worse generalization. It might be good to further investigate this.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>