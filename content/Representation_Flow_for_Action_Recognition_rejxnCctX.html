<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Representation Flow for Action Recognition | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Representation Flow for Action Recognition" />
        <meta name="citation_author" content="AJ Piergiovanni" />
        <meta name="citation_author" content="Michael S. Ryoo" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=r1ejxnCctX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Representation Flow for Action Recognition" />
      <meta name="og:description" content="In this paper, we propose a convolutional layer inspired by optical flow algorithms to learn motion representations. Our representation flow layer is a fully-differentiable layer designed to..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_r1ejxnCctX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Representation Flow for Action Recognition</a> <a class="note_content_pdf" href="/pdf?id=r1ejxnCctX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=ajpiergi%40indiana.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="ajpiergi@indiana.edu">AJ Piergiovanni</a>, <a href="/profile?email=mryoo%40indiana.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="mryoo@indiana.edu">Michael S. Ryoo</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=r1ejxnCctX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">In this paper, we propose a convolutional layer inspired by optical flow algorithms to learn motion representations. Our representation flow layer is a fully-differentiable layer designed to capture the `flow' of any representation channel within a convolutional neural network for action recognition. Its parameters for iterative flow optimization are learned in an end-to-end fashion together with the other model parameters, maximizing the action recognition performance. Furthermore, we newly introduce the concept of learning `flow of flow' representations by stacking multiple representation flow layers. We conducted extensive experimental evaluations, confirming its advantages over previous recognition models using traditional optical flows in both computational speed and performance.
</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">15 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rkxGG9-pTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1ejxnCctX&amp;noteId=rkxGG9-pTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1107 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1107 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1le8eD92m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good results, but method unconvincing</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1ejxnCctX&amp;noteId=H1le8eD92m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1107 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1107 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
This paper presents an end-to-end learnable deep network for action recognition that estimates "optical flow" within the neural network layers. The "flow" is quoted as it may not be the optical flow in traditional sense over image frames, but could be the "flow" over the intermediate channels of a layer from the two frames.  The paper further extends this idea to compute the flow of flow. For the flow computation, the paper uses the standard TV-L1 scheme, but its steps are being done within the network and thus could be back propagated. Finally, the entire framework is trained on a loss defined over action classification. Experiments are presented on Kinetics and HMDB datasets, and show good performance.

Strengths:  I think the main strength of this paper is its competitive performances in Table 9 against RGB only methods. There are also various analysis presented on several architectural choices for the fusion with the flow layer.

Weaknesses: 
1) I think the main weakness of this paper is its lack of any significant novelty, or inadequate coverage of state of the art in deep learning based flow estimation. For example, there is prior work such as FlowNet or FlowNet2.0 that could compute optical flow in an end-to-end manner. This paper does not cite these prior works and in the context of which the novelty as claimed in the paper is not substantial.

2) Another important weakness is the lack of any convincing argument on why it is a good idea to compute/apply an optical flow algorithm on the intermediate feature maps of a network? How is the fundamental assumption of flow -- the brightness constancy -- applicable to such intermediate layers? 

3) Further, while the idea of proposing the deep variant of flow was to avoid the computational expense of an otherwise out-of-the-box flow algorithm, the paper ultimately has to resort to several heuristic workarounds such as low-resolution of the inputs, or reducing the iterations of flow optimization, etc. to make the model practical. The flow estimation of the feature maps also require trimming down the number of channels in a layer. 

4) It is surprising to see that even after such heuristic workarounds to compute an approximate flow, the final performance of the model is compelling; which needs more analysis to understand where precisely is the proposed scheme gaining in performance even with these approximations in comparison to an otherwise accurate external optical flow scheme?

5) Finally, given that the paper proposes to optimize the model against the action classification loss, it is unclear to me if the proposed flow layers are in fact learning anything related to flow, or why should they learn flow, or for that matter, if the proposed iterative scheme is even necessary? If learning flow is important, shouldn't there be some intermediate objective that ensures that the flow layers do learn flow, using a suitable loss? For example, as in the FlowNet2.0 paper. 

Minor comments: 
1) The section 3.1 on optical flow methods could be improved to be more intuitive. For example, it is unclear what \rho_c is capturing, and Eq.(2) seems to be just |I_2-I_1|.  What precisely are the roles of \lambda, \theta etc? How is the second term \lambda |I_1-I_2| useful in (7) ? Isn't it a constant?

2) More details and discussions need to be added to the comparisons against state of the art in Table 9. Why are the HMDB results missing from the middle column? 

Overall, I think the paper has some interesting results, however the method is unconvincing/unclear why it should work nor its novelty commendable.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SklkiR0naX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Errors/misunderstandings in the review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1ejxnCctX&amp;noteId=SklkiR0naX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1107 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1107 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We first want to question whether the reviewer is not confusing this paper with another paper he/she is reviewing. Are we talking about the same paper?

The reviewer argues that we are not citing FlowNet, but we clearly did so in the paper. We are even experimentally comparing our method with more advanced extensions of FlowNet such as ActionFlowNet and TVNet (which already perform better than FlowNet2.0 itself on action recognition) to show the superiority of our method, but the reviewer does not seem to mention them at all. The reviewer argues that our method has "heuristic workarounds such as low-resolution of the inputs, or reducing the iterations of flow optimization, ...", but all these were to run more detailed experiments more efficiently, which has little to do with the proposed approach itself. As stated in the paper, we run our final comparisons using the standard resolution of 224x224 images. Further, reducing the number of iterations is not a “heuristic workaround” but an advantage of learning allowing the flow representation to be learned end-to-end (see Tables 2 and 3).

The reviewer also seems to be confused that our ultimate goal is in computing accurate optical flows and calls our approach "approximations" to compute them, but no, we are not interested in whether our flows maintain accurate 'pixel' motion or not. We are interested in learning motion flow 'representation' that eventually benefits action recognition more; we confirm that our representation flow helps the action recognition more than using external optical flows (Tables 5-9). This is an action recognition paper not optical flow computation paper; everything is optimized for the purpose of doing better action recognition.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Skln_11a67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>minor comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1ejxnCctX&amp;noteId=Skln_11a67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1107 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1107 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Regarding the minor comments:
1) \rho_c is the constant part of \rho (pre-computed for efficiency, rather than computing each iteration). \rho captures the motion residual between the two frames, based on the current flow estimation.

\lambda, \theta, and \tau are described just after Eq. 5 based on their definition in the TVL-1 paper. \lambda |I_1-I_2| in Eq. 7 is incorrect, it should be \lambda|\grad I_1 * u + I_1-I_2|, which we will correct.

2) The missing results are due to those papers not including results training on HMDB from ImageNet.

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ryx9DkkppX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The iterative optimization is needed</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1ejxnCctX&amp;noteId=ryx9DkkppX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1107 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1107 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">5) We are training the model to classify actions. Thus the flow layers may not be computing flow, but as they are designed to follow the TVL-1 optical flow algorithm, and since they take sequential inputs, they are computing latent flow-like motion representation. Further, as shown in Table 3, the iterative method is certainly necessary, there is ~15% increase in performance by using multiple iterations.

Further, we outperform ActionFlowNet which tries to jointly recognize action and learn flow using a loss function.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rylZBy1aTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Further analysis?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1ejxnCctX&amp;noteId=rylZBy1aTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1107 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1107 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">4) What further analysis would you like to see? We have already compared many aspects to show where our method gains performance (see tables 1-5). To summarize our findings, we find that learning the divergence kernels and hyperparameters, computing the flow after the third residual block, and using 20 iterations performs quite well and each aspect is important.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJemXkk6pm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The low-resolution was only to enable running of many experiments, not a "heuristic workaround"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1ejxnCctX&amp;noteId=BJemXkk6pm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1107 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1107 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">3)  We used low-resolution inputs mainly to reduce the cost of training time of the video CNNs for our experiments, and this choice has less to do with the approach itself we are presenting. In order to run the many experiments in the paper, we had to use smaller datasets and smaller images to train even the baseline RGB-only CNNs in a reasonable time. Further, we used the standard 224x224 size images in the experiments in Tables 8 and 9.

Additionally, the finding that fewer iterations are needed to compute flow with learning is consistent with the findings in TVNet (Fan et al. 2018). Traditional optical flow algorithms work only on 1 channel, using a greyscale image. Reducing the number of channels in a feature map to 32 is still many more channels than previously used.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BkxP1JkTa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Brightness consistency assumption for CNNs</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1ejxnCctX&amp;noteId=BkxP1JkTa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1107 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1107 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">2) The brightness consistency is applicable to feature maps. This allows us to capture consistency of the feature's value instead of capturing the brightness/intensity of a pixel at a location. The same assumption hold because CNNs produce roughly the same feature value for the same object as it moves. We will clarify this in the paper.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1x6TARha7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We do cite FlowNet (and more related to learning flow and actions)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1ejxnCctX&amp;noteId=S1x6TARha7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1107 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1107 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1) Although the reviewer claims that we are missing the citation to the FlowNet (Dosovitskiy et al., 2015), in fact, we are already citing it as well as other more advanced works computing optical flows in the related work section. Please read once more.

+ In the paper (Table 8), we also explicitly compare our method with more recent extensions of the FlowNet (i.e., ActionFlowNet and TVNet), which have been confirmed to perform superior to the vanilla FlowNet and FlowNet2.0. Note that ActionFlowNet performs much better than FlowNet and TVNet reported performances far superior to FlowNet2.0.

+ All the previous works computed the flows directly from images, not representations. For instance, FlowNet is a CNN trained to produce optical flow, which can be used as input to another CNN to prediction actions. This is quite different from our approach, where an flow-like representation is computed within the CNN and learned to directly classify the action.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_r1eUEUb53Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting approach, but novel contributions seem to be few and poorly justified over Sun et al's work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1ejxnCctX&amp;noteId=r1eUEUb53Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1107 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1107 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a convolutional layer, inspired by optical flow calculation, to calculate the flow between feature maps. Given two feature maps (t, t+1) at layer L in the network, the flow layer calculates the flow between these
temporally-consecutive feature maps using TV-L1 optical flow algorithm. The parameters of the flow algorithm are learnt as opposed to being hand selected. This representation does not require pre-computation of flow, and thus is more
computationally efficient compared to approaches that fuse two streams (e.g. 2SCNN). The paper is well-written. The motivation is strong, there is enough experimental evidence in the paper that this works. The idea of calculating the
shift (i.e the flow) of the feature maps that are optimised for action recognition is interesting. On the other end, I have several reservations/criticisms to the presentation and conclusions of the paper:

1. The paper dismisses other efforts that make similar assumptions as irrelevant in the related work. Sun et al (2018) also only use RGB input, propose a feature representation layer inspired by flow calculations and perform impressively on HMDB outperforming this paper’s results. Dismissing this work in the comparison, and only comparing to Fan et al, seems slightly misleading experimentally. The architectural formulation of Sun et al is slightly different in that flow of features is computed at multiple depths of the RGB network--this seems to be a more expressive approach than the proposed one. I understand that the authors add iterative optimisation, but the contribution in the paper seems to claim similar contribution to Sun et al in proposing the flow layer.

2. The fact that these feature maps are only optimised for the flow (and not to perform recognition separately) seems slightly strange. In the current architecture there’s an assumption that the RGB features are already optimised for the problem, if I understood correctly. I had expected an architecture that optimises the RGB features separately from optimising the flow. It is not clear how the network is trained, is the RGB network first trained and then the flow representation layer? Or is it trained in a single pass backpropagating gradients through the flow layer?

3. I found the structure of the ablation study to be very difficult to follow. Every bold title can be understood solely but is difficult to link to what’s before/after. The usage of a variety of dataset flavours: mini-Kinetics, lowRes-HMDB, HMDB, pre-training, etc makes it difficult to follow the argument.

4. The motivation for flow-of-flow whilst interesting seems to suffer from the same flaw as computing flow-of-flow using TV-L1: the method makes the brightness constancy assumption which will only hold when computing flow between features of constant velocity. The use of an intermediate convolutional layer between the stacked flow layers (flow-conv-flow) achieves
superior results to the flow layer alone, there is no theoretical motivation or explanation for why this would work/solve the brightness-constancy assumption issues in computing flow-of-flow.

5. Flow-of-flow which is pushed from the abstract of the paper as novel, fails to produce valuable results towards the end. Given this is a primary contribution, it is not clear what conclusions to make about this.

6. It has been shown experimentally that calculating flow after the third resNet block produces the best results, but it is not clear why this is the case. The number 3 seems to be magical and little explanation is given.

Minor comments:

1. Fig 1 appears to be missing the additional convolutional layer remapping from 2C' channels back to C channels described in the text
2. Feature maps are mapped to [0, 255] help numerical stability, but no explanation for why this improves numerical stability is given.
3. Providing standard deviation metrics for runtime performance in table 9 would give more confidence in the method's superior runtime performance
4. I Would like to see a study of how changing the number of input channel to the flow layer C' affects performance.
5. The name MiniKinetics has already been taking and using it to refer to a different subset of the Kinetics dataset is confusing for readers who might mistakenly compare methods based on these results.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1lq3TC3TX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarification for comparison to Sun et al. (OFF)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1ejxnCctX&amp;noteId=r1lq3TC3TX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1107 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1107 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1) As we described in another reply (<a href="https://openreview.net/forum?id=r1ejxnCctX&amp;noteId=rJlA1Qu5cm" target="_blank" rel="nofollow">https://openreview.net/forum?id=r1ejxnCctX&amp;noteId=rJlA1Qu5cm</a> ), there are several key differences between Sun et al. (OFF) and our method.

We revised our paper to more explicitly compare our approach against OFF. This not only includes conceptual comparison but also includes direct experimental comparison showing the advantage of our approach: the advantage is clear in both recognition accuracy and  computation time. Please check.

First, OFF does not compute any flow representation like we do, but just uses dx, dy, and dt gradients as the features. Secondly, while Sun et al. only uses RGB input for some experiments, they still require explicit computation of traditional optical flow and learning of a two-stream approach to obtain results comparable to the state of the arts (which is exactly what we are trying to avoid).  As stated on page 8 of Sun et al., “Our results are obtained by assembling the scores from RGB, OFF(RGB), optical flow and their corresponding version of OFF(optical flow)” and based on their GitHub code, OFF requires multiple streams to achieve competitive performance. They apply their method on top of (i) RGB, (ii) RGB difference, and (iii) optical flow inputs, requiring three separate base networks, and 3 additional OFF networks. I.e., two networks per-stream and up to 3 streams. This is significantly more than the single network we train.

The Sun et al. paper does compute motion multiple times, which is similar to our method computing flow-of-flow. We addressed questions regarding this paper in another comment, and we will add the experimental results and clarifications to the paper.

2) Our feature maps are not optimized to compute flow at any point. The layer is designed to compute flow based on optical flow algorithms, but all the parameters are learned to maximize recognition. This is one network, with a layer that computes the flow (see Fig. 1). It is trained all at once, back-propagating through the flow layer.

3) We will revise and clarify the experimental section to make it easier to follow.

4) The brightness consistency assumption for CNNs could be interpreted as "feature intensity consistency" as a CNN will produce a similar value for sequential frames, shifted by the motion. Adding the conv layer between the flow layers allows the network to produce feature values more suitable for computing flow, rather than directly using the output of the flow layer. We will add a justification to the flow-of-flow.

5) The flow-of-flow gives strong performance when used with 2D CNNs, as 2D CNNs capture no temporal information. When combined with a 3D or (2+1)D CNN, it still benefits, but less so because some temporal information has been captured with the temporal convolution.

6) We believe that computing flow after the 3rd residual block provides the best performance as it provides the best trade-off between high resolution feature maps that still display motion in sequential frames, and abstracting RGB information into feature maps (as we described in the "Where to compute flow" section).


Minor comments:
1) It isn't explicitly drawn, but is part of the green block. We can add an explicit illustration of the channel reduction.

2) We found that CNN feature maps provided values mostly between 0 and 1 (after ReLU), with some larger values. The TVL1 algorithm and hyperparameters were selected for images in [0,255]. We found that when using the feature map values, after a few iterations, they became very small and unstable. When normalizing to [0,255], the values were much more stable after each iteration. We will clarify this in the paper.

3) We will add standard deviations to the runtimes.

4) We can add experiments comparing the number of channels. Note that we use 32, significantly more than optical flow algorithms that work with a single-channel, greyscale image.

5) We will change the naming of mini-kinetics in this paper to avoid that confusion.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJgxEirLn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Need more experiments to justify FoF and comparison to previous work </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1ejxnCctX&amp;noteId=rJgxEirLn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1107 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1107 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a "representation flow layer" for action recognition, to learn hidden motion representation in a CNN end-to-end. The authors also come up with a concept termed "flow-of-flow". The paper is interesting, but I have several concerns.

1. The concept of 'flow-of-flow' needs more justification. As shown in Table 5, flow-of-flow performs much worse than a single flow layer. After adding a conv layer in between, the performance gets better. I assume this is because the estimated flow are noisy, directly compute flow on them again making the result worse. It is the same as applying TVL1 twice. Adding the extra conv layer can help smoothing the feature map. So how about justify another configuration "flow-conv"? I just want to see if 'flow-of-flow' makes sense. 

Actually, as pointed out by authors, flow-of-flow usually performs worse due to inconsistent optical flow results. I think the reason it gets a little bit higher accuracy here is because over-fitting. 

So one more experiment of only "flow-conv", will help making the paper solid. 

2. The caption of figure 5 need more clarification. Is it TVL1 twice and FoF? or simply TVL1 and flow? Because subfig (b) and (c) do not align well. I am assuming this is computing the flow twice. 

3. I have several concerns for Table 9. 

(1) The authors do not provide results on HMDB dataset (2nd column). Is it because the performance is inferior to R(2+1)D or simply have no time before the deadline? This result is important, so please complete the table. 

(2) I don't see obvious advantage of the proposed approach over R(2+1)D and S3D. Especially for S3D, the performance is similar but S3D is faster. So the contribution of the paper is limited.  

(3) If we compare the proposed approach to I3D, I3D is still better. Besides, there are several recent work reporting better performance on these two datasets. For example, ARTNet (Appearance-and-Relation Networks for Video Classification, CVPR18) report a 78.7 score on Kinetics. 

(4) The authors report their performance for 2D CNN and 2+1D CNN, but what about 3D CNN? It is also important to show. 

4. Since another goal of this paper is to improve efficiency, so I want to see a comparison, at least a discussion, to two previous literature. These two literature are the first work to propose to learn optical flow inside a CNN for action recognition. In these two literature, the authors incorporate FlowNet-like architectures to compute optical flow and feed them as input to the temporal stream. Because the FlowNet-like architecture can be quite shallow, their efficiency (fps) is very high, and the performance is on par with state-of-the-art.

(1) Yi Zhu, et al, Hidden Two-Stream Convolutional Networks for Action Recognition
(2) Laura Sevilla-Lara, et al, On the Integration of Optical Flow and Action Recognition


In conclusion, 

(1) In terms of novelty, the flow layer is adapted from TVNet, and the FoF concept need more clarification. 
(2) In terms of performance, there is no obvious advantage over previous work like S3D and R(2+1)D. And the experiments right now seem not complete. 

Hence, I recommend a initial rating of 5. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJgfHaR36m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Justification of FoF</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1ejxnCctX&amp;noteId=BJgfHaR36m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1107 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1107 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1) We will better justify why we believe the flow-conv-flow method works. It is due to smoothing the feature map and producing feature maps better suited for computing a flow representation. What do you mean by adding the "flow-conv" configuration?  If you mean a representation flow layer followed by a conv layer, that is our standard setting. We will make this more clear in the paper. (Why do you suspect overfitting? Our observations are consistent across datasets and base models, which implies we aren't overfitting.)

2) We will clarify the caption for figure 5. It is the RGB image, the TVL1 produced optical flow, and the output of our flow layer applied to the RGB image. This figure illustrates that our method produces comparable flows to TVL1 on RGB inputs.

3)

(1) In table 9, the missing results are due to the papers not reporting those settings (e.g., the (2+1)D paper only reported HMDB results when first pretraining on kinetics). It wouldn't be fair to complete the table ourselves as our training settings or implementations may be different from those in the paper and comparisons across columns may not be accurate. We will add the results for our method in all settings.

(2) RGB-S3D is faster because it doesn't use any flow representation. However, our approach does outperform it, while only being ~100ms slower.

(3) I3D is only better when using two-streams. However, I3D used 64 GPUs to train, and we do not have access to enough GPUs to train I3D. We compared on smaller 3D CNNs, and showed our performance benefitted, but we are unable to do so using I3D.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Bkl7nVrOcm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>What is the key difference between yours and the OFF</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1ejxnCctX&amp;noteId=Bkl7nVrOcm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1107 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, I've found your idea is quite close to the one illustrated in the paper:
Optical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition, Shuyang Sun, Zhanghui Kuang, Lu Sheng, Wanli Ouyang, Wei Zhang, CVPR 2018. 
as both of you apply the sobel operator and frame-by-frame difference for action recognition. May I ask what is the key difference between yours and theirs?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJlA1Qu5cm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>There are several key differences</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1ejxnCctX&amp;noteId=rJlA1Qu5cm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1107 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1107 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">There are several key differences between our work and the Optical Flow Guided Feature (OFF). 

1) The main conceptual difference is that OFF is not about flow learning or flow computation. The OFF paper is using the optical flow formulation only to justify and motivate that dF/dx, dF/dy (computed using Sobel filters), and dF/dt (frame difference) are good features to use (where F is a particular feature map). They do not learn to compute explicit flows, which is in contrast to our paper. There is no iterative optimization either, which we found to be a quite important process.

2) Although the OFF paper claims that they only need RGB as an input in the abstract, as stated on page 8 (“Our results are obtained by assembling the scores from RGB, OFF(RGB), optical flow and their corresponding version of OFF(optical flow)”) and based on their GitHub code, OFF requires multiple streams to achieve competitive performance. They apply their method on top of (i) RGB, (ii) RGB difference, and (iii) optical flow inputs, requiring three separate base networks, and 3 additional OFF networks. I.e., two networks per-stream and up to 3 streams. This is significantly more than the single network we train. Further, we do not use optical flow as input, as it is expensive to compute.

3) Experimentally, OFF also performs worse than ours. Using the code provided by the authors as well as our own implementation, on HMDB, OFF gave 55.4% (our implementation) and 55.9% (their code) on the RGB-only 16×224×224 input setting. The difference is mostly due to different base architectures (ResNet vs. BN-Inception). This is only a slight increase over the baseline RGB performance, which is 53.4%, and significantly lower than our model’s performance of 65.4%. On Mini-Kinetics, the baseline RGB was 55.2%, OFF was 55.6% and ours was 61.1%. We will update Table 8 with these results.


Further, combining OFF with video models using temporal convolution (i.e., 3D CNNs or (2+1)D CNNs) gains very little additional information. We implemented the OFF method on top of a 3D ResNet-18 on Mini-Kinetics and it provided 54.8%, which is essentially the same as the baseline 3D CNN which provided 54.6% accuracy. However, our representation flow was able to benefit even 3D CNNs (54.6 -&gt; 59.7%).

We will update the paper with these new results and further clarify the differences.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>