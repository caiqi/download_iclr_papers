<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>An Automatic Operation Batching Strategy for the Backward Propagation of Neural Networks Having Dynamic Computation Graphs | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="An Automatic Operation Batching Strategy for the Backward Propagation of Neural Networks Having Dynamic Computation Graphs" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SkxXwo0qYm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="An Automatic Operation Batching Strategy for the Backward..." />
      <meta name="og:description" content="Organizing the same operations in the computation graph of a neural network into batches is one of the important methods to improve the speed of training deep learning models and applications since..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SkxXwo0qYm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An Automatic Operation Batching Strategy for the Backward Propagation of Neural Networks Having Dynamic Computation Graphs</a> <a class="note_content_pdf" href="/pdf?id=SkxXwo0qYm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019an,    &#10;title={An Automatic Operation Batching Strategy for the Backward Propagation of Neural Networks Having Dynamic Computation Graphs},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SkxXwo0qYm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Organizing the same operations in the computation graph of a neural network into batches is one of the important methods to improve the speed of training deep learning models and applications since it helps to execute operations with the same type in parallel and to make full use of the available hardware resources. This batching task is usually done by the developers manually and it becomes more dif- ficult when the neural networks have dynamic computation graphs because of the input data with varying structures or the dynamic flow control. Several automatic batching strategies were proposed and integrated into some deep learning toolkits so that the programmers don’t have to be responsible for this task. These strategies, however, will miss some important opportunities to group the operations in the backward propagation of training neural networks. In this paper, we proposed a strategy which provides more efficient automatic batching and brings benefits to the memory access in the backward propagation. We also test our strategy on a variety of benchmarks with dynamic computation graphs. The result shows that it really brings further improvements in the training speed by cooperating with the existing automatic strategies.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Automatic Operation Batching, Dynamic Computation Graphs</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SJxssjhchm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Batching for back propagation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkxXwo0qYm&amp;noteId=SJxssjhchm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper249 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper249 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Batching of similar and independent operations in a neural network computation graph is a common way to improve efficiency through computational parallelism. Optimization is often applied to the computation graph by grouping independent operations into batches that can be computed in parallel. 
Existing techniques typically optimizes the feed forward computation and the backward computation follows the same grouping as the feed forward computation, which may not be optimal. In particular, the authors argue that a separate batching strategy should be applied to the back propagation computation to further improve the efficiency and showed that a recurrent network can benefit from such an optimization. The proposed solution is an automatic batching strategy that work for dynamic computation graphs.

Pros:

- The results (for both CPU and GPU) show that the proposed method improves on top of two existing batching strategies (by depth and by agenda) across three different tasks. 

Cons:

- The feed forward and backward (gradient) computation can be viewed as a single computation graph. As such, would applying optimization to this graph achieve the same thing? Some clarifications/discussions will be helpful.

- It is unclear why the proposed method is better for dynamic computation graph.  The benchmark results on CPU show that the proposed method is worse than the baseline for Tree-LSTM.  Will be useful to also do a similar profiling for the cases where the proposed method did not help.

   
  </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hyghugeq3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Straightforward method, need more analysis.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkxXwo0qYm&amp;noteId=Hyghugeq3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper249 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper249 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposed a just-in-time optimization method of neural network calculation on dynamic computation graphs. The method focused on batching summation of gradients on the backward calculation which was performed independently in conventional toolkits, and experiments on 3 LSTM tasks showed that in several settings the proposed method improved the speed of backward computation.

The proposed method is straightforward and reasonable in terms of improving the speed of the backward computation. Authors discussed the proposed method on only the neural network toolkits with a dynamic computation strategy, but this kind of optimization can be applied to any existing toolkits even which has a non-dynamic strategy. This point looks a kind of misleading of the discussion on the paper.

The paper provided a detailed analysis of time consumption on only a success-case (Table 4). Unfortunately, Table 2 and 3 showed that the proposed method does not have a global effectiveness and suggest a necessity for a further discussion about when to use the proposed method. Since this discussion can surely be conducted by comparing analyses of success and failure-cases, authors should provide analyses of all experiments.

A conceivable weakness of the method may be the increase of memory consumption. If the toolkit plan to perform batch operations for summations of gradients, it needs to store all available gradients about each use of the corresponding variables. If the variable has a large shape and is used very frequently (e.g., variables in the softmax layer), the amount of total memory consumed by its gradient tends to be a serious problem. The non-batching strategy can mitigate this problem by discarding gradient information as soon as it is propagated back to all preceding nodes. The paper does not provide any information about memory consumption but it is important to discuss this kind of perspective.

Others:
In Table 2 and 3, please provide the ratio of speeds which are more reasonable to judge the real improvement rather than the one-zero decision (showed as up/down arrows).
In Table 4, why the time of the forward propagation slightly increased?
You should write a full list of authors of the DyNet paper that the official README provided:
<a href="https://github.com/clab/dynet/blob/master/README.md" target="_blank" rel="nofollow">https://github.com/clab/dynet/blob/master/README.md</a></span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJxJTFhd2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice idea, but a limited study presented with lack of clarity</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkxXwo0qYm&amp;noteId=BJxJTFhd2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper249 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper249 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value"># Summary

This work describes a shortcoming in existing dynamic batching strategies, namely that they operate only on the forward pass while some operations can be batched only in the backward pass. For example, the gradient of the transition matrix in a RNN consists of the sum of partial derivatives over each time step; the terms of this sum and the summation can be batched into a single matrix multiplication. The authors implement this batching strategy in DyNet and show empirically that it can lead to decent (0-25%) speedups.

# Quality

The proposed technique comes with a trade-off which is not discussed in the paper: Delaying computations until several can be batched together can increase peak memory usage. In particular, the memory requirements of a RNN would increase from O(T) to O(2T) since each forward and backward state must now be stored. (In fact, the authors use a separately allocated contiguous block of memory that they copy the states and gradients into, which would bring this to O(3T) or O(4T) memory complexity.)

A second observation that should have been made is that the potential for speedups depends on the batch size, hidden state size, and number of time steps (or tree depth). Given a small batch size and large hidden state, the batching method effectively replaces a series of outer products with a single matrix multiplication. One would expect good speedups in this scenario. On the other hand, for a large batch size with a small hidden state, the dynamic batching strategy effectively replaces a series of inner products with a single larger inner product, which would be far less beneficial. The experiments in this work use relatively small batch sizes (64), which gives little insight about whether the proposed method would lead to speedups in a wide range of models (for example, batches of 512 are common in some NLP applications).

Some smaller comments:

* Multi-threading on a multicore architecture does not necessarily imply that operations are performed sequentially.
* Input sequences in NLP are not always sentences given as sequences of words.
* The argument that padding always leads to unnecessary computation is overly simplistic; the added control flow and branching required to perform irregular computation can often make it slower than doing regular computation plus masking (additionally, sparse kernels are often memory bandwidth bound, leading to different performance properties).
* The authors say that operations of the same "type" can be batched together, but don't specify what "type" means. I assume the type is defined by both the operation as well as the shapes of its inputs and outputs?
* No distinction is made between different ways of batching and their performance characteristics. Two matrix-vector multiplications gemv(X, y1) and gemv(X, y2) can be efficiently batched as gemm(X, [y1 y2]') which reduces the number of times X needs to be loaded into working memory. This is not the case when batching distinct inputs such as gemv(X1, y1) and gemv(X2, y2). On the other hand, gemv(X1, y1) + gemv(X2, y2) can be efficiently batched as gemv([X1 X2], [y1', y2']'), reducing the number of memory accesses in the output buffer.
* Why perform 3 runs and report the fastest speed? Why not report the range, or better yet, perform more runs and report confidence intervals.

# Clarity

The writing in this paper needs significant improvement. In terms of structure, the introduction (section 1) and background (section 2) are very repetitive. The third, fourth, fifth and sixth paragraph of the introduction are effectively repeated in full in sections 2.1, 2.2, 2.3 and 3.1 respectively. On the other hand, the inclusion of table 1 at the beginning puts the reader on the wrong foot thinking that this paper will consider NMT models, whereas the paper only deals with POS tagging and sentiment analysis.

The text contains grammatical errors ("days even weeks", "The parallel computing helps"), tautological definitions ("batching [...] means organizing the same operations of computation graphs into batches", "padding, which is to pad the input sequences"), unclear use of language ("cooperating with the existing strategies"), and typographical mistakes (multiple citations are separately parenthesized). Overall, the lack of clarity inhibits the understanding of the paper.

# Originality and significance

The central contribution of this paper is relatively straightforward in retrospect, but can certainly be beneficial for the training of some particular models. I am no expert in the literature, but the authors' claim that they are the first ones to consider this technique seems justified. The paper has no reference to code, so it is hard to judge how easy it would be for practitioners to use the suggested technique.

# Summary

Pros:

  * Useful dynamic batching trick that can lead to speedups
  * Empirical evaluation compares to two existing techniques and breaks down individual components of runtime

Cons:

  * No critical look at the disadvantages of this technique such as applicability to larger batch sizes and memory usage
  * Some questionable statements and assumptions
  * Lack of formalization and clear definitions
  * Paper reads long-drawn-out, subpar writing hurts readability</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>