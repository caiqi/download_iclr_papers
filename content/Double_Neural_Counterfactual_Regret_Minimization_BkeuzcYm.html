<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Double Neural Counterfactual Regret Minimization | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Double Neural Counterfactual Regret Minimization" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Bkeuz20cYm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Double Neural Counterfactual Regret Minimization" />
      <meta name="og:description" content="Counterfactual regret minimization (CRF) is a fundamental and effective technique for solving imperfect information games. However, the original CRF algorithm only works for discrete state and..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Bkeuz20cYm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Double Neural Counterfactual Regret Minimization</a> <a class="note_content_pdf" href="/pdf?id=Bkeuz20cYm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019double,    &#10;title={Double Neural Counterfactual Regret Minimization},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Bkeuz20cYm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Counterfactual regret minimization (CRF) is a fundamental and effective technique for solving imperfect information games. However, the original CRF algorithm only works for discrete state and action spaces, and the resulting strategy is maintained as a tabular representation. Such tabular representation limits the method from being directly applied to large games and continuing to improve from a poor strategy profile. In this paper, we propose a double neural representation for the Imperfect Information Games, where one neural network represents the cumulative regret, and the other represents the average strategy. Furthermore, we adopt the counterfactual regret minimization algorithm to optimize this double neural representation. To make neural learning efficient, we also developed several novel techniques including a robust sampling method, mini-batch  Monte Carlo counterfactual regret minimization (MCCFR) and Monte Carlo counterfactual regret minimization plus (MCCFR+) which may be of independent interests. Experimentally, we demonstrate that the proposed double neural algorithm converges significantly better than the reinforcement learning counterpart. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Counterfactual Regret Minimization, Imperfect Information game</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We proposed a double neural CFR which can match the performance of tabular based CFR and opens up the possibility for a purely neural approach to directly solve large imperfect information game.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Hklezu2Z6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Isn’t it hard to learn cumulative quantities in a neural net?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkeuz20cYm&amp;noteId=Hklezu2Z6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1280 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1280 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a neural net implementation of counterfactual regret minimization where 2 networks are learnt, one for estimating the cumulative regret (used to derive the immediate policy) and the other one for estimating a cumulative mixture policy. In addition the authors also propose an original MC sampling strategy which generalize outcome and external sampling strategies.

The paper is interesting and easy to read. My main concern is about the feasibility of using a neural networks to learn cumulative quantities. 

The problem of learning cumulative quantities in a neural net is that we need two types of samples: 
- the positive examples: samples from which we train our network to predict its own value plus the new quantity, 
but also:
- the negative examples: samples from which we should train the network to predict 0, or any desired initial value.

However in the approach proposed here, the negative examples are missing. So the network is not trained to predict 0 (or any initial values) for a newly encountered state. And since neural networks generalize (very well...) to states that have not been sampled yet, the network would predict an arbitrary values in states that are visited for the first time. For example the network predicting the cumulative regret may generalize to large values at newly visited states, instead of predicting a value close to 0. The resulting policy can be arbitrarily different from an exploratory (close to uniform) policy, which would be required to minimize regret from a newly visited state.  Then, even if that state is visited frequently in the future, this error in prediction will never be corrected because the target cumulative regret depends on the previous prediction. So there is no guarantee this algorithm will minimise the overall regret. 
This is a well known problem for exploration (regret minimization) in reinforcement learning as well (see e.g. the work on pseudo-counts [Bellemare et al., 2016, Unifying Count-Based Exploration and Intrinsic Motivation] as one possible approach based on learning a density model). 

Here, maybe a way to alleviate this problem would be to generate negative samples (where the network would be trained to predict low cumulative values) by following a different (possibly more exploratory) policy. 


Other comments:
- It does not seem necessary to predict cumulative mixture policies (ASN network). One could train a mixture policy network to directly predict the current policy along trajectories generated by MC. Since the samples would be generated according to the current policy \sigma_t, any information nodes I_i would be sampled proportionally to \pi^{\sigma^t}_i(I_i), which is the same probability as in the definition of the mixture policy (4). This would remove the need to learn a cumulative quantity.
- It would help to have a discussion about how to implement (7), for example do you use a target network to keep the target value R_t+r_t fixed for several steps?
- It is not clear how the initialisation (10) is implemented. Since you assume the number of information nodes is large, you cannot minimize the l2 loss over all states. Do you assume you generate states by following some policy? Which policy? 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJllmKgjn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Appears to be a solid advance in NN applied to IIG, but I'm not an expert in this area</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkeuz20cYm&amp;noteId=BJllmKgjn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1280 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1280 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a pair of LSTM networks, one of which estimates the current strategy at iteration t+1 and the other estimates the average strategy after t iterations. By using these networks within a CFR framework, the authors manage to avoid huge memory requirements traditionally needed to save cumulative regret and average strategy values for all information sets across many iterations.  
The neural networks are trained via a novel sampling method with lower variance/memory-requirements that outcome/external sampling, and are amendable to continual improvement by warm-starting the networks based on cloned tabular regret values.

Overall, the paper is well-written with clear definitions/explanations plus  comprehensive ablation-analyses throughout, and thus constitutes a nice addition to the recent literature on leveraging neural networks for IIG.  

I did not find many flaws to point out, except I believe the paper could benefit from more extensive  comparisons in Figure 4A against other IIG methods such as Deep Stack, as well as comparing on much larger IIG settings with many more states to see how the neural CFR methods hold up in the regime where they are most needed.

Typo:  "care algorithm design" -&gt; "careful algorithm design"
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BkxiXMw927" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review: Interesting application of function approximation to CFR but requires larger experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkeuz20cYm&amp;noteId=BkxiXMw927"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1280 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1280 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">========= Summary =========

The authors propose "Double Neural CFR", which uses neural network function approximation in place of the tabular update in CFR. CFR is the leading method for finding equilibria in imperfect information games. However it is typically employed with a tabular policy, limiting its applicability large games. Typically, hand-crafted abstractions are employed for games that are too large for exact tabular solutions. Function approximation could remove the necessity for hand-crafted abstractions and allow CFR to scale to larger problems.

The DN-CFR algorithm roughly consists of:
- start with an arbitrary vmodel_0, smodel_0
for t = 0,1,2,3:
  - collect a "batch" of (infoset I, immediate counterfactual value v_I) samples by traversal against vmodel_t (as well as I, strategy samples)
  - train a new network vmodel_{t+1} on this "batch" with y=(v_I + vmodel_t(I)) and MSE loss
  - similarly for (I, strategy)
- return smodel_t

The authors also propose a novel MC samping strategy this is a mixture between outcome and external sampling.

DN-CFR is evaluated on two games: a variant of Leduc hold-em with stack size 5, and one-card poker with 5 cards. If I understand correctly, these games have &lt;10,000 and &lt;100 infosets, respectively. The authors show that DN-CFR achieves similar convergence rates to tabular CFR, and outperform NFSP variants.

========== Comments ========

The authors are exploring an important problem that is of great interest to the IIG community. Their application of NN function approximation is reasonable and mostly theoretically well-grounded (but see below), I think it's on the right track. However, the games that are used for evaluation are very small, in fact I believe they have fewer states than the number of parameters in their network (the number of network parameters is not provided but I assume &gt;1000). As a result, the NN is not providing any compression or generalization, and I would expect that the network can memorize the training set data exactly, i.e. predict the exact mean counterfactual value for each infoset over the data. If that's true, then DN-CFR is essentially exactly replicating tabular CFR (the approximation serves no purpose). 

As a result, in my opinion this work fails to address the important challenges for function approximation in CFR, namely:

- Can function approximation allow for *generalization across infosets* in order to reduce sample complexity of CFR (i.e. an unsupervised abstraction)? Are specific network architectures required for good generalization?
- The magnitude of counterfactual regrets in the support of the equilibrium decays to zero relative to dominated actions. Are NN models capable to estimating the regrets accurately enough to converge to a good strategy?
- Are optimization methods able to deal with the high variance in large IIGs?
- Since each successive approximation is being trained from the previous NN, does this cause errors to accumulate?
- How do approximation errors accumulate across CFR iterations?
- Is minimizing MSE loss sufficient to approximate the strategy well? (since the mapping from regrets -&gt; strategy is non-linear)

I believe there is also a theoretical problem with this algorithm. In Eq. 8, they minimize the loss of CF value predictions *over the distribution of infosets in the last CFR step ("batch")*. However, this distribution may change between CFR iterations, for example if the strat_t folds 2-3 on the preflop then flop infosets with 2-3 hole cards will never be observed on iteration t+1. As a result, the NN loss will only be minimized over infosets observed in the last iteration - so the network will "forget" the regrets for all other infosets. I think this issue does not arise in these toy games because all infosets are observed at each iteration, but this is certainly not the case in real games. 
There are a number of ways that this issue could be addressed (e.g. train on historical infosets rather than the current batch, etc.) These would need to be explored.

I would recommend that the authors evaluate on more complex games to answer the important questions stated above and resubmit. I think this would also make an excellent workshop submission in its current state as it contains many interesting ideas.

Detailed comments:

"... the original CFR only works for discrete stand and action spaces...": Are the authors implying that DN-CFR addresses this limitation? 
"Moravk" -&gt; "Moravcik"
"...these methods do not explicitly take into account the hidden information in a game..." Could you clarify? Is your point that these methods operate on the normal form rather than extensive form game?
"care algorithm design" -&gt; "careful algorithm design"
The paragraph starting "In standard RNN..." should be reduced or moved to appendix. The exact NN architecture is not central to the ideas, and there are no experimental comparison with other architectures so we have no evidence that the architecture is relevant.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SkgWej4oYX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Lemma 2 = Lemma 1 of from MCCFR paper?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkeuz20cYm&amp;noteId=SkgWej4oYX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Marc_Lanctot1" class="profile-link">Marc Lanctot</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018</span><span class="item">ICLR 2019 Conference Paper1280 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">One more thing. Is your Lemma 2 any different than Lemma 1 from Lanctot et al. 2009? If not, that fact should be cited somewhere as well (that it's a restatement of the main MCCFR lemma).</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJeKcZdsYX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to the question: Lemma 2 = Lemma 1 of from MCCFR paper?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkeuz20cYm&amp;noteId=SJeKcZdsYX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1280 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018</span><span class="item">ICLR 2019 Conference Paper1280 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your suggestion. The Lemma 2 is the same with the Lemma 1 from Lanctot et al. 2009, which is cited in the background. This Lemma will be used to prove the unbiased estimation of counterfactual value for mini-batch MCCFR. We will cite Lemma 1 from Lanctot et al. 2009 in the revised version.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1lHz-hstQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Reply about Lemma 2 citation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkeuz20cYm&amp;noteId=B1lHz-hstQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Marc_Lanctot1" class="profile-link">Marc Lanctot</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018</span><span class="item">ICLR 2019 Conference Paper1280 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Great, thanks. The citation in the background makes sense. However, I recommend you also add "(Lemma 1 of Lanctot et al. 2009)" after "Proof: " in Section E.2. As currently written, the casual reader might misinterpret the proof in E.2 as novel.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ByxhZ6Pot7" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkeuz20cYm&amp;noteId=ByxhZ6Pot7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper1280 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1g5hL4jK7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Neat paper!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkeuz20cYm&amp;noteId=H1g5hL4jK7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Marc_Lanctot1" class="profile-link">Marc Lanctot</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018</span><span class="item">ICLR 2019 Conference Paper1280 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I have not read it fully, but this paper looks super interesting!

Just wanted to quickly mention that you should be citing the Regression CFR paper (Waugh et al., 2015, "Solving Games with Functional Regret Estimation", AAAI) as it is clearly related work. It was the first to propose function approximation in CFR. Like one of the parts of your double network, it proposed building a regressor to predict cumulative regrets. Here is the link: <a href="https://arxiv.org/abs/1411.7974" target="_blank" rel="nofollow">https://arxiv.org/abs/1411.7974</a></span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1ginbdjY7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to the question: missing reference "Solving Games with Functional Regret Estimation"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkeuz20cYm&amp;noteId=B1ginbdjY7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1280 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018</span><span class="item">ICLR 2019 Conference Paper1280 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for pointing out the missing reference, we will cite the paper in the revised version.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1ex5awoF7" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkeuz20cYm&amp;noteId=r1ex5awoF7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper1280 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>