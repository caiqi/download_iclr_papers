<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Detecting Adversarial Examples Via Neural Fingerprinting | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Detecting Adversarial Examples Via Neural Fingerprinting" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SJekyhCctQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Detecting Adversarial Examples Via Neural Fingerprinting" />
      <meta name="og:description" content="Deep neural networks are vulnerable to adversarial examples: input data that has been manipulated to cause dramatic model output errors. To defend against such attacks, we propose..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SJekyhCctQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Detecting Adversarial Examples Via Neural Fingerprinting</a> <a class="note_content_pdf" href="/pdf?id=SJekyhCctQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019detecting,    &#10;title={Detecting Adversarial Examples Via Neural Fingerprinting},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SJekyhCctQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Deep neural networks are vulnerable to adversarial examples: input data that has been manipulated to cause dramatic model output errors. To defend against such attacks, we propose NeuralFingerprinting: a simple, yet effective method to detect adversarial examples that verifies whether model behavior is consistent with a set of fingerprints. These fingerprints are encoded into the model response during training and are inspired by the use of biometric and cryptographic signatures. In contrast to previous defenses, our method does not rely on knowledge of the adversary and can scale to large networks and input data. The benefits of our method are that 1) it is fast, 2) it is prohibitively expensive for an attacker to reverse-engineer which fingerprints were used, and 3) it does not assume knowledge of the adversary. In this work, we 1) theoretically analyze NeuralFingerprinting for linear models and 2) show that NeuralFingerprinting significantly improves on state-of-the-art detection mechanisms for deep neural networks, by detecting the strongest known adversarial attacks with 98-100% AUC-ROC scores on the MNIST, CIFAR-10 and MiniImagenet (20 classes) datasets.  In particular, we consider several threat models, including the most conservative one in which the attacker has full knowledge of the defender's strategy. In all settings, the detection accuracy of NeuralFingerprinting generalizes well to unseen test-data and is robust over a wide range of hyperparameters.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Adversarial Attacks, Deep Neural Networks</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Novel technique for detecting adversarial examples -- robust across gradient-based and gradient-free attacks, AUC-ROC &gt;95%</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">27 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_r1e6HZUHaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Summary and general comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=r1e6HZUHaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper947 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper947 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank all reviewers for their thoughtful comments and reviews. We would like to briefly summarize and highlight our contributions here, and address specifics in individual comments to the reviewers.

Some merits of our contribution we would like to highlight:

1. NFP is a blackbox defense, i.e. it is attack agnostic. To the best of our knowledge, this is the first such defense that does not use adversarial examples to learn to distinguish between real and adversarial images, but rather relies on the training data alone and still manages to achieve near perfect detection in the experiments described. 

2. NFP does not require extensive fine-tuning to be effective. Indeed, our experiments show that NFP is effective against the strongest state-of-the-art attacks over a wide range of hyperparameters.

Despite the lack of formal guarantees, we present an abundant amount of empirical evidence that our method is robust against a full-range of the strongest available attacks in the literature. In sum, NFP serves as a strong baseline for developing stronger attacks. In turn, this can eventually lead to better defenses.

3. NFP is a promising step towards the problem of detecting and defending against  adversarial attacks with unbounded perturbations (we test SPSA, FGSM with eps=0.25 for CIFAR-10) -- particularly, when the fingerprint is not known to the adversary (for e.g., consider the challenge <a href="https://github.com/google/unrestricted-adversarial-examples)." target="_blank" rel="nofollow">https://github.com/google/unrestricted-adversarial-examples).</a></span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Syg8JVYkTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=Syg8JVYkTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper947 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper947 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a new technique for detecting adversarial examples by introducing "fingerprints" into the landscape while training, and exploiting the fingerprints at test time to detect adversarial examples. The idea is novel and the paper is well-written, but concerns about gradient masking prevent me from recommending acceptance just yet.

Positives: 
The paper is extremely well-written, and the approach is clear and presented well. The authors also clearly put significant effort into the evaluation, and accurately/consistently describe threat models that they consider. The approach is also clearly novel, and is interesting. 

Concerns:
My biggest concern is that this detection mechanism masks gradients in its loss function. The two reasons I strongly believe this is the case are (a) Figure 5 and (b) the authors themselves state that their loss is highly non-convex and that no gradient-based method may be able to find a solution. This, however, does not guarantee robustness (see [1] for why such “unfriendly” landscapes can usually be circumvented)

Some concrete evaluation concerns and experiments that the authors can run to alleviate them:
- Figure 5 shows adversarial robustness even against eta = 0.25---at this value of epsilon, the attacker should be able to create realistic images of other classes (even without PGD), so this suggests that the loss is somehow making examples hard to find rather than removing them. The authors should address this issue.
- Showing that at a sufficiently high eta attacks start to succeed is also useful
- Running SPSA, CarliniL2-FP, and PGD for *many* more iterations and using *many* more steps for binary search (right now it looks like the binary search is looking in a space of size 10^6 with 10 steps, which only has a granularity of about 5k, which means you never see any value &lt; 5k in a bisection search, which casts into doubt all of these results)
- The AUC should monotonically degrade with eta (this is another indication the attacks might not be running for long enough)
- The method does not seem to be specific to L-infinity constraint. To this end, a version of Figure 5 in the L2 case would be extremely useful in understanding the detection method.

I also apologize if some of these concerns about gradient masking seem unsubstantiated; that said, I tried to run the code given in the paper, but got several OOM and other errors (utils modules not found, and PyTorch deprecations), even on a machine with 8 12GB-memory GPUs. If the authors can provide instructions for running the code I will be happy to test it and alleviate some of my own concerns. I also tried to reimplement the approach, but did not manage to finish before the review deadline. If I am able to reimplement the approach I will update my review accordingly.

Some smaller comments on the paper:
A consolidated set of tables for attack parameters in an appendix is needed
- Page 4 last paragraph line 4 find the subset that “satisfies” instead of “satisfy”
- Page 5 paragraph 1 line 1 defender,for needs a space before for
- Page 5 paragraph right before theorem 1 last line Here, for detection needs a , after detection 
- Page 7 paragraph 1 line 2 (2 hidden layers the 2 should be written two
- Page 7 second last paragraph line 2 “is chosen” instead of “are chosen”
- Page 7 last paragraph line 2 “across attacks” needs a , after 
- Page 9 table 6 label line 2 “does not shown” should be “show” instead of “shown” 
- Page 9 last line “measure of robustness” remove "of"

[1] <a href="https://arxiv.org/pdf/1802.00420.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1802.00420.pdf</a></span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJx4wGIHpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to review (part 1)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=HJx4wGIHpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper947 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018 (modified: 12 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper947 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We sincerely thank the reviewer for taking time to implement the paper and run our experimental code,  we believe that openness of any defense is a key necessary step in the evaluation process and we will do our best to support your efforts. 
 
While we (and possibly, the reviewer) run experiments with a large number of iterations, with large number of steps for the bisection search (and for PGD, *many* randomized restarts), to encourage discussion we are posting an initial response addressing the reviewer’s concerns.

*** Regarding gradient masking:

First, we would like to address your main concern, i.e., gradient masking.

Other than the fact that FGSM and SPSA fail with large eta, NeuralFP does not show any other signs of gradient masking. The reference [1] mentions a number of features of gradient masking. We note that NeuralFP behaves *contrary* to defenses that rely on gradient masking in many ways: it is 

1) robust to black-box attacks, SPSA attack and adaptive attacks, and 
2) single step attacks do worse than multi-step attacks.

This indicates that NeuralFP may not rely on gradient-masking. With the current set of available attacks it seems hard to conclude (beyond reasonable doubt) about NeuralFP’s reliance on gradient masking, one way or another. 

“ The two reasons I strongly believe this is the case are (a) Figure 5 and (b) the authors themselves state that their loss is highly non-convex and that no gradient-based method may be able to find a solution. This, however, does not guarantee robustness (see [1] for why such “unfriendly” landscapes can usually be circumvented)”

First we note that the function L_{fp} is continuous and differentiable with respect to the logits. Also, please note all of the attacks used in [1] are indeed gradient based. 

Second, [1] (See section 4) introduces three strategies to traverse “unfriendly” landscapes: 
  1. BPDA
  2. Expectation over transformation
  3. Reparameterization

However, we note that none of these techniques seem directly applicable to our defense: 
a) BPDA is applicable in settings where gradient information is masked with either non-differentiable operations or optimization procedures that are likely to cause the gradients to not be readily available. Our loss is continuous and differentiable. This leads us to believe BPDA is not relevant to this setting.
b) Expectation over transformation -- This mechanism is not relevant as NeuralFP involves no randomization during detection.
c) Reparameterization -- There is no reparameterization involved in the defense. The network *directly* acts on the image space. 

Phenomena such as label leaking (<a href="https://openreview.net/forum?id=BJm4T4Kgx)" target="_blank" rel="nofollow">https://openreview.net/forum?id=BJm4T4Kgx)</a> are not likely because we do not train using any information about adversarial examples. 

However, we understand that it might be possible that there exist attacks that break our defense since we do not provide formal guarantees. We have conducted experiments to extensively evaluate our defense, and are continuing to work on evaluating the defense in more detail. 

In this light, NeuralFP sheds light on possible weak spots in several of the attacks we evaluated against. This implies that either NeuralFP is indeed strong and/or we need stronger attacks for performing accurate evaluations -- both situations being beneficial to the adversarial ML community.  We are happy to summarize these discussion points in the final paper.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJgi7QLHpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Review (part 2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=SJgi7QLHpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper947 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018 (modified: 14 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper947 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">*** Extra experiments: 

To alleviate your concerns regarding attacks with more iterations, we are running more experiments as suggested. Note that our previous experiments were already extensive -- as noted by the reviewer, we have put in significant effort into our evaluation.

For 112 unseen CIFAR-10 test points we are running: 
1. CW-fp with 20,000 iterations and bisection search with 25 steps for gamma-1 and 12 steps for gamma-2 (roughly 6,000,000 gradient steps per point!).

2. Adaptive-PGD attack (similar to the other adaptive attacks) with 
a) 1,000 steps and 1 randomized restart with 6 bisection steps, eps = 16.0/255.0, eps_iter=0.005 (same parameters as <a href="https://arxiv.org/pdf/1807.10272.pdf)" target="_blank" rel="nofollow">https://arxiv.org/pdf/1807.10272.pdf)</a>
b) 50 steps and 50 randomized restarts with 6 bisection steps, eps = 16.0/255.0, eps_iter=0.005 
We believe 6 bisection steps suffice here because we are searching for gamma in the range (0.01,10) -- at gamma&gt;10, misclassification starts to fail for a significant fraction of the samples. The fact that increasing gamma causes misclassification to fail indicates that the gradients from L_{fp} are likely not exploding or vanishing. 

3. We ran SPSA with 1,000 iterations and 20 bisection steps (with default parameters for eps=0.05, lr=0.01 and delta=0.01). The AUC-ROC is 99.97%. We are running experiments with 
1. lr=0.005 and delta=0.005 to see if that leads to better performance. 
2. eps=0.25, 1000 iterations, lr=0.01 and delta=0.01 to see if SPSA degrades accuracy at larger eta.

Once we have all the numbers, we will compile the results and update our paper and the page. If you would like to suggest other relevant experiments, please let us know. 

*** Specific comments: 

-- “Figure 5 shows adversarial robustness even against eta = 0.25….. hard to find rather than removing them. The authors should address this issue.”

We have not seen any paper that has been able to generate “realistic” images (for e.g. starting with a ‘2’ and ending up with a ‘6’) using any of the attacks we test against. It would be very useful if you could point us to related work that has been successful in doing so. We believe traversing the manifold of natural images in a high-dimensional spaces is an inherently challenging optimization problem (assuming a reasonable compute budget). There is no guarantee that any of the current attack techniques we are aware of will find other realistic images in a computationally tractable way (e.g., within a reasonable number of iterations/random restarts), starting from the vicinity of one image. 

-- “-Showing that at a sufficiently high eta attacks start to succeed is also useful”

This makes sense in the context of robust prediction, where for large perturbations you can turn realistic images into noise where classification makes no sense. The functioning of our method is entirely different to robust prediction methods, since noise is unlikely to pass through NeuralFP. One way to definitely fool NeuralFP is to generate images from the true data-distribution, but as discussed, generating “realistic” images from the true data-distribution is not necessarily an easy task.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJehrXIBpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to review (part 3)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=rJehrXIBpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper947 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper947 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">-- “- Running SPSA, CarliniL2-FP, and PGD for *many* more iterations and using *many* …… these results)”

*** Regarding Bisection Search: 
We use a starting point of 1e-2 (&lt;5k) for gamma (please see our hyper-parameters at the end of the public discussion) during the bisection search. Please note that this same bisection search is used in the original CW-L2 attack. Further, larger the gamma, more preference is given to minimizing L_{fp}. 


*** Regarding SPSA: 

We directly inquired with the authors of (<a href="https://arxiv.org/abs/1802.05666)" target="_blank" rel="nofollow">https://arxiv.org/abs/1802.05666)</a> about choosing the optimal Lagrange parameter to use for the SPSA attack. We were informed that the Lagrange coefficients for their experiments were hand-picked loosely based on the values of the losses. They further recommended that the simplest thing to do, in the context of their paper, is to just optimize J = C * min(1, J_adv) + L_CNN (for large C). This will cause the optimizer to ensure the image is misclassified, but stop optimizing the misclassification loss once it is already misclassified, and then focus on the likelihood loss. In summary, we believe the Lagrange parameter itself does not affect the results to that great an extent, and a coarse search should suffice. However, we are doing a finer search now as detailed at the end of our comment. 

*** Regarding CarliniL2-FP: 

Note that the loss in CW-fp has a similar form as that used in the SPSA paper (the loss in the SPSA paper was modelled after the loss from the CW attack), hence we believe that the optimization of L_{CW} and L_fp is not too sensitive to the exact Lagrange coefficients; a coarse search should suffice in this case as well. In the paper where the CW-loss function was introduced, the authors argue that the coefficient matters the most in producing the smaller adversarial perturbation. However, as suggested by the reviewer, we are running experiments with an increased number of bisection steps and iterations to be sure of the result. 

*** Regarding PGD:
We are currently running PGD with a large number of iterations and restarts as discussed above. We will post the results shortly. 

-- “The AUC should monotonically degrade with eta (this is another indication the attacks might not be running for long enough)”

We agree that with large eta this statement holds true for robust prediction, but it is not obvious that this should indeed be the case for *detection* with NeuralFP, as explained earlier. Large eta can be used to generate noisy images, but it is not entirely obvious that one would be able to generate realistic images from the true distribution by simply allowing for a large eta using the current attack schemes in a computationally tractable manner. 

--  “The method does not seem to be specific to L-infinity constraint. To this end, a version of Figure 5 in the L2 case would be extremely useful in understanding the detection method.”

Indeed, our method is not specific to any adversarial attack. We test against the JSMA attack, which is L-0 based and we have near perfect detection. The CW attack attempts to minimize the L-2 perturbation and we cannot vary the L-2 perturbation as a hyperparameter. Do you recommend any specific L2 based attacks to test-against?

[1] https://arxiv.org/pdf/1802.00420.pdf
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rye8BOm8aX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response (part 1)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=rye8BOm8aX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper947 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper947 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the in-depth reply to the review. Although it will take me some time to fully read and respond to the rebuttal, there are some crucial points that I think may inform the authors' revisions:

- R.e. papers that turn 2s into 6s etc, see this paper: <a href="https://arxiv.org/abs/1805.12152," target="_blank" rel="nofollow">https://arxiv.org/abs/1805.12152,</a> which I believe may be useful.

- R.e. high eta leading to failures: this should actually be true both for detection and prediction. For example, at eta = 1.0, an attack that selects the nearest neighbor from another class in the training set should suffice to get 0% detection. In the same vein, an attack that simply uses a generative model of some kind to find images of a class while minimizing distance to an original image should suffice to find high-eta examples of other images that are not adversarially perturbed in the first place. 

In any case, if extending Figure 5 to eta = 1.0 still yields a high success rate at detection with the current attacks, this indicates that it is a failure of current optimization methods, rather than a true detection mechanism, that might be generating this success, so this would be useful. It is then unclear if this inability is due to computational hardness, or simply an extremely non-convex landscape that requires "tricks" to be able to navigate.

- PGD using L2-normalized gradients is an L2 attack which does not minimize l2 distance (i.e. taking gradient steps (step size) * grad / ||grad||. (In the same vein, the authors should also verify that the L-inf PGD attacks they are using normalize the gradient via sign, i.e. (step size) * sign(grad).)

Concretely, it would be very helpful for the authors to show:
- Figure 5 for Linf and L2 (using PGD [this means sign(grad) and grad/||grad|| respectively] in both cases), extending from eta=0 to eta=1.

I will read and address the rest of the points directly, but just wanted to respond to these in order to give reasonable time for experimentation. Thanks again for the detailed response, and the overall thoroughness. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryxH6LfhaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to response (part 1)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=ryxH6LfhaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper947 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper947 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the quick reply and the heads up about the experiments.

“- R.e. papers that turn 2s into 6s etc, see this paper:     <a href="https://arxiv.org/abs/1805.12152," target="_blank" rel="nofollow">https://arxiv.org/abs/1805.12152,</a> which I believe may be useful.”


Thanks for this interesting reference. We respectfully note that the adversarial  examples illustrated in the paper can clearly be deciphered to be altered and unnatural images even with the human eye (e.g. Figure 3 in the paper). This by itself is not convincing evidence that PGD/SPSA can generate “realistic” images from the “true” distribution.

“- R.e. high eta leading to failures: this should actually be true both for detection and prediction. For example, at eta = 1.0, an attack that selects the nearest neighbor from another class in the training set should suffice to get 0% detection. In the same vein, an attack that simply uses a generative model of some kind to find images of a class while minimizing distance to an original image should suffice to find high-eta examples of other images that are not adversarially perturbed in the first place.”

We agree that “realistic” unbounded attacks undoubtedly exist, e.g., images from the training dataset. We would like to clarify that most of our earlier comments with regards to finding realistic images in the rebuttal were in regard to the attacks that we use to  evaluate NeuralFP, such as SPSA/PGD/FGSM/CW. 

We would like to emphasize that our contribution focuses on detecting *bounded* attacks from the literature. Note that we do *not* evaluate or claim robust detection of (yet unpublished) unbounded attacks, e.g., based on generative models such as GANs (e.g., Song et al, NIPS 2018  https://arxiv.org/abs/1805.07894). Although we plan to evaluate on detection of unbounded (generative) attacks shortly, the scope of the current paper is limited to bounded adversarial attacks.

Furthermore, for bounded adversarial attacks (e.g., eta&lt;0.25), robust prediction is certainly a more difficult task than detection. For instance, Schmidt et al, NIPS 2018 (https://arxiv.org/abs/1804.11285) argue that robust prediction requires significantly larger amounts of data.

“In any case, if extending Figure 5 to eta = 1.0 still yields a high success rate at detection with the current attacks, this indicates that it is a failure of current optimization methods, rather than a true detection mechanism, that might be generating this success, so this would be useful."


We agree that failure at eta=1.0 would indicate flaws in the evaluated attacks, e.g., their inability might be due to the computationally hardness or might need improved optimization algorithms. We will try to address your suggestion to evaluate up to eta=1.0. However, we note that is computationally demanding.

Note that for each eta, the PGD attack evaluation (with the bisection search  and 1000 iterations) for 112 samples is expected to take 4-5 days on a single GPU. We will try to run as many cases as possible before the revision dates, but we have a limited compute budget and time constraints.

Furthermore, although evaluating at large eta is interesting, note that we do *not* claim NeuralFP can detect all (unbounded) adversarial attacks. However, given its excellent detection performance on both bounded gradient and non-gradient based attacks, we do believe it is a very strong baseline. As such, NeuralFP highlights the need for better attacks. 
     
Also, as we discussed in our previous replies, we do not believe our experimental results show that NeuralFP provides robustness merely through gradient masking. We feel that failure of the attacks at eta=1.0 also would not provide conclusive evidence of gradient masking, but rather a failure of the current attacks. However we are running experiments with larger eta-s and will update the manuscript/the page shortly.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkgOSwznaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to response (part 2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=BkgOSwznaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper947 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper947 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">“It is then unclear if this inability is due to computational hardness, or simply an extremely non-convex landscape that requires "tricks" to be able to navigate.”
      
We feel that this an extremely interesting question, and we have extensively shown that the current set of optimization methods used to effectively evaluate adversarial attacks do not convincingly answer this question. Further (theoretical) analysis of this question is an exciting direction for future research.

"- PGD using L2-normalized gradients is an L2 attack which does not minimize l2 distance (i.e. taking gradient steps (step size) * grad / ||grad||. (In the same vein, the authors should also verify that the L-inf PGD attacks they are using normalize the gradient via sign, i.e. (step size) * sign(grad).)
      
Concretely, it would be very helpful for the authors to show:
- Figure 5 for Linf and L2 (using PGD [this means sign(grad) and grad/||grad|| respectively] in both cases), extending from eta=0 to eta=1."

*** Attack normalization. 
We use the default Cleverhans implementation for the SPSA/PGD attacks and (<a href="https://github.com/carlini/nn_robust_attacks)" target="_blank" rel="nofollow">https://github.com/carlini/nn_robust_attacks)</a>  for the CW attack. We did verify that Cleverhans normalizes the gradient via sign for PGD. 

*** Experiments and compute budget.
Thanks for the suggestions for new experiments. Running PGD (both versions) with the above mentioned #iterations/bisection steps, at a coarseness of \delta eta = 0.05, requires about 40 GPUs for 5 days, which equates to ~$10000 of AWS credits. Since we work in an academic setting, this is beyond our compute budget. 

We will try PGD with the normalized L2 norm and do our best within the allowed time frame and our financial constraints. However, we do feel our current extensive experiments already stand on their own as strong positive validation of NeuralFP.

*** Additional tests for gradient masking
We are also working on an additional test for gradient masking with a large set of random-points described in [1]. We will update the page and our manuscript with these results shortly.

Please let us know if you need any additional technical help with your evaluation and if you have other questions.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1gxPy_ham" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=S1gxPy_ham"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper947 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper947 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the reply. I am looking forward to the results of the other experiments (PGD &gt;1000 iters, other experiments from the initial rebuttal).

I understand if running the Figure 5 experiment again as suggested above would be too costly, but would it be possible to simply try the attack only with eps=1.0 for l-infinity, eps=10.0 for l2, and unbounded PGD for l2?

Thanks again for the response; please let me know when the additional results (the standard ones from the initial rebuttal) are complete.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJlVieO2Tm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarification -- step-size</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=rJlVieO2Tm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper947 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper947 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Minor clarification: Do you have a preferred step-size for the bounded and unbounded l2 PGD attacks? I am struggling to find literature/code that recommends a good step-size.

Thanks again for the help and feedback!</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HklGS-_n67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Hyperparameters</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=HklGS-_n67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper947 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper947 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Ideally these could be found via grid search. Under computational constraints, however, I would try something like epsilon=0.5 for the l2 attacks (with l2 normalization).

Good luck and thanks for the responsiveness.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_Hyx_7PKyT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Help Running Our Code</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=Hyx_7PKyT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper947 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper947 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Before, we address your other issues we would like to immediately get you to be able to run our code. We run our code on a machine with 2 GPUs, so the OOM error is quite surprising. 

Could you please let us know if you have access to AWS instances? We have been able to run our experiments on the AWS DeepLearning AMI. I can make a public AMI and share it with you if that's easier. 


I am able to run our code for MNIST on an Ubuntu 16.04 machine with a 16GB RAM (no GPU). The commands I am running on the Ubuntu machine are as follows:
1. cd into the directory with run.sh after extracting the files from the tar-ball
2. Run the command 'export PYTHONPATH=$(pwd):$PYTHONPATH'
3.  Run the command './run.sh mnist train attack eval nogrid 50 0.1 10'

Also, this runs with python 2.7 and we have not tested with python 3 or above.

We have tested our code on multiple linux platforms with the following specific software versions:

&gt;&gt;&gt; torch.__version__
'0.3.0.post4'

&gt;&gt;&gt; torchvision.__version__
'0.2.0'

&gt;&gt;&gt; tensorflow 
'1.4.0'

&gt;&gt;&gt; keras
'2.1.0'

We are actively currently working on running PGD (with random restarts) and SPSA with many more iterations. We will provide detailed answers to your questions, and do our best to get you running with the code ASAP! Engineering the whole project is considerable effort, we strongly recommend trying to run our scripts which we have shared. 

</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJxcwoK1am" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=BJxcwoK1am"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper947 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper947 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks! I have started a thread which is only visible by the authors, in order to keep the page limited to discussion about the content of the paper rather than technical help.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJxOmdee6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Please don't count this against the paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=SJxOmdee6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Nicholas_Carlini1" class="profile-link">Nicholas Carlini</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper947 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">A quick unsolicited comment for reviewer 1: please don't penalize the authors for any difficulties you have in running their code. This is one of two adversarial example defense paper at ICLR this year that has released code with their paper (I believe), and I would hate to see them penalized for it.

I fear one of the main reasons authors don't release code is that it's currently seen only as a potential liability (what if someone breaks my model faster because I released the code? what if someone tries to run it and it doesn't work? etc) rather than a strength (now people can reproduce my work/verify my claims/etc).

Eventually I'd hope that providing code with papers becomes so common that criticizing a paper because the code is hard to work with will be a valid critique. But I don't think we're there yet. The fact that the authors released the code at all is commendable.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkehwiggpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks! We are committed to openness about our defense! </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=Bkehwiggpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper947 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper947 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have shared one set of well performing model weights, the defense parameters in the link below (and also, some adversarial examples we generated as well). 

<a href="https://www.dropbox.com/sh/0ow62124skbzy1y/AABOHL0Y0EeAdEmPfHItpOt_a?dl=0" target="_blank" rel="nofollow">https://www.dropbox.com/sh/0ow62124skbzy1y/AABOHL0Y0EeAdEmPfHItpOt_a?dl=0</a> 


Thank you for the comment!  We are actively working with the reviewer to get them to be able to run our code, and will support anyone else who wishes to do the same during the review process.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SkgPOKglam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>I am not counting this against the paper in the slightest</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=SkgPOKglam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper947 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper947 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">As your comment mentions, it is commendable that the authors released their code at all, and I actually counted this in favor of the paper in my review. I only said that I couldn't run their code in order to justify the fact that my concerns about gradient masking were not substantiated via my own testing (ideally, rather than suggesting that the defense masks gradients, I could have run some evaluations myself and assessed this). I am now privately communicating with the authors in order to get the code up and running.

Again, for me the fact that the code is released (and in general, the authors' commitment to openness and reproducibility) is impressive and definitely contributed positively towards my impression of the paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_Bkg6qVsa2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Innovative perturbation-based learning strategy leads to very impressive performance in adversarial detection</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=Bkg6qVsa2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper947 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper947 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a method for the detection of adversarial examples via what the authors term "neural fingerprinting" (NeuralFP). Essentially, a reference collection of perturbations are applied to the training data so as to learn the effects on the classification decision. The premise here is that on average, normal examples from a common class would have similar changes in the classification decision when reference perturbations are applied, whereas adversarial examples (particularly those off the local submanifold) may have a markedly different set of changes from what was expected for the targeted class. These reference perturbations as well as the anticipated output perturbations together form the "fingerprints".

To measure the difference between observed outputs and fingerprints, the average (squared?) Euclidean distance is used. Given a fixed set of input fingerprints (presumably chosen so as to provide coverage of the range of possible perturbation directions), the authors use the distance formula as a regression loss ("fingerprint loss") to train the choice of output fingerprints. Although the authors do not explicitly state it this way, this secondary training objective encourages a K-Means style clustering of output perturbations where the output fingerprints serve as cluster representatives. 

This learning formulation is to my mind is both very innovative and extremely effective, as demonstrated by the authors' experimental results. Their experiments show superlative performance (near perfect detection!) against essentially the full range of state-of-the-art attacks. They give careful attention to the mode of attack, and show excellent performance even for adaptive white-box attacks, in which existing attack methods are given the opportunity to minimize the fingerprint loss.

The presentation of the paper is excellent - clear, well-motivated, and detailed, with careful attention given to experimental concerns such as the choice of perturbation directions (the recommendation is to choose them at random), and the number of fingerprints to pick.

Overall, the reported results are so good, and the approach so convincing, that one wonders what the weaknesses of the approach might be (if any). Questions that do come to mind are:
* Can an adversarial strategy can be developed that could execute a successful attack while minimizing the fingerprint loss. 
* Another issue is whether the NeuralFP would work on more challenging data sets where the classes are highly fragmented - at what rate would the benefits of NeuralFP fade as the classification performance degrades? 
* What happens to performance if the perturbation directions are chosen so as to better conform with the local sub-manifolds... would fewer perturbations be required? (It would seem that reducing the number of perturbations needed could have a significant effect on training time.)

Overall, this is a very strong and important result, fully deserving of acceptance.

P.S. Two sets of typos that need attention:
* In Equation 3, the Euclidean norm is taken. In Equation 5, the squared Euclidean norm is taken. Presumably, one of these is a typo. Which?
* In the definition of delta-min and delta-max in the first paragraph of Section 2.2, y-hat should be w-hat.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">9: Top 15% of accepted papers, strong accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkgs9ELr6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=rkgs9ELr6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper947 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper947 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the comments and the feedback, we are glad you found our contributions interesting and promising. 

Addressing your questions:

-- “Can an adversarial strategy be developed that could execute a successful attack while minimizing the fingerprint loss.”

This is an interesting question, if one can indeed develop a new attack to break NeuralFP. We are actively investigating this, and we believe that NeuralFP will result in better attacks being formulated and eventually, better defenses.

-- “Another issue is whether the NeuralFP would work on more challenging data sets where the classes are highly fragmented - at what rate would the benefits of NeuralFP fade as the classification performance degrades?”

From the few experiments we report on MiniImagenet-20, we see good detection rates on FGSM and BIM. Also, when moving from MNIST to CIFAR-10, the classification performance degrades quite a bit, while NeuralFP’s performance nearly stays the same. We think these are promising signs that NeuralFP would be beneficial even on fragmented data-sets where classification is harder. However, conclusively answering this question concretely would need a thorough investigation.

-- “What happens to performance if the perturbation directions are chosen so as to better conform with the local sub-manifolds... would fewer perturbations be required? (It would seem that reducing the number of perturbations needed could have a significant effect on training time.)”

The increased training time (and memory) is indeed one of the biggest drawbacks of our approach, and the suggested idea of using a more principled approach for choosing the perturbation directions could indeed result in faster training, and possibly, better detection of outliers. We believe this is the immediate next step towards making NeuralFP more accessible on more complex tasks like ImageNet.

Thanks for pointing out the connection to K-means clustering, that is an insightful interpretation. We will consider this and see if it sheds more light on our approach, and helps us improve NeuralFP.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1eAQp4a27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Suggestions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=B1eAQp4a27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018</span><span class="item">ICLR 2019 Conference Paper947 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, I have some suggestions for the sanity check.

1) For all attacks, what happens if an adversary only considers \gamma L_{fp}? (i.e., an adversary does not make an input to be misclassified, but only want to make it a fake.)

2) How is the detection rate on Gaussian noise ~ N(0, \eta^2)? </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1x7tfDT27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Suggestions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=B1x7tfDT27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper947 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018</span><span class="item">ICLR 2019 Conference Paper947 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the suggestions for sanity checks. We will get back in more detail on this, but some initial comments are below:

1. During our initial tests we have tried this for FGSM. It turns out that the perturbations become nearly indistinguishable  (AUC-ROC ~~0.5-0.6) but the adversarial success rate drops to 0. We will try this for the other attacks (and FGSM again to confirm) and get back shortly. 

Attacking with only \gamma*L_{fp} is loosely equivalent to choosing a very large coefficient for L_fp (at which point the misclassification starts to fail).  

2. We have tried detecting random images (every pixel drawn uniformly) and the AUC-ROC was ~~100%. We will get back on the detection of gaussian noise with varying eta shortly. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rylIKYfn2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>On the adaptive CW attack</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=rylIKYfn2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper947 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper947 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The proposed method seems to be appropriate and the performance is well investigated and compared with several competitors. 

One question is that why the proposed method can be protective against the adaptive CW attack. In the public discussion, the authors mention that the defense works successfully because the landscape of the fingerprinting loss is non-convex and no gradient method is guaranteed to find a suitable solution. If this is correct, did you repeatedly try the gradient-based attack with changing random seeds? By doing so, the attack might work successfully with a certain probability.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1g_CLIBp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=B1g_CLIBp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper947 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper947 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your feedback. As detailed in our response to Reviewer 1, we are working on new experiments with randomized restarts for PGD with *many* iterations. We will report back with the numbers for our experiments with randomized restarts soon. Please see our response to Reviewer 1 for a detailed discussion with regard to the details of our new experiments and specific comments on gradient masking.	

Even though our loss is non-convex, the fingerprint loss L_{fp} is differentiable and continuous, and it is not obvious to us that any of the attacks we study are likely to succeed against NeuralFP, even with a larger distortion budget. We believe this is because finding “realistic” images in a high-dimensional space starting from the vicinity of the current image may not be an easy task . 

Besides the experiments with randomized restarts, which we are currently running, if you have further specific suggestions to improve the paper and the evaluation, we would be glad to hear them.

We would like to also point out that the Carlini-Wagner attack (<a href="https://arxiv.org/abs/1608.04644)" target="_blank" rel="nofollow">https://arxiv.org/abs/1608.04644)</a> was compared with that of exhaustive search using a solver-based approach (https://arxiv.org/abs/1709.10207). It was shown that the Carlini-Wagner attack does not necessarily succeed in finding the optimal attack. Note that exhaustive search comes with tremendous computation costs (the problem is NP-hard!). This implies that there is no guarantee that the CW attack does indeed find the global minima, and finding the global optima through exhaustive search might be intractable.

Also, we would like to note again that the SPSA attack and black-box attacks have been shown to overcome defenses that mask gradients. However, NeuralFP performs quite well against these attacks.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_r1edIXVBhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Evaluation questions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=r1edIXVBhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018</span><span class="item">ICLR 2019 Conference Paper947 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">In Figure 5, it appears that AUC is uncorrelated with the magnitude of the adversarial perturbation. If you increase the perturbation budget to something much larger does the AUC begin to decrease? (Also: what is the dataset for this figure? MNIST?)

The adaptive CW attack is unbounded, and should always eventually succeed. That is, for any fixed decision threshold that could be used for detection, the attack should always eventually produce an adversarial example that can fool the detector by introducing a very large perturbation. However, it looks like you claim robustness even at very small thresholds (e.g., a 80+% TPR @ 0%FPR). Do you know what is going on here?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1eZuarS3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Addressing evaluation questions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=B1eZuarS3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper947 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018</span><span class="item">ICLR 2019 Conference Paper947 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the questions, and catching the missing detail in Figure 5. 

“In Figure 5, it appears that AUC is uncorrelated with the magnitude of the adversarial perturbation. If you increase the perturbation budget to something much larger does the AUC begin to decrease? (Also: what is the dataset for this figure? MNIST?)”

The dataset for Figure 5 is CIFAR-10 — we will include this detail in the paper. 0.25 is already a significantly large perturbation, since our pixel values in the input images are scaled from [-0.5,0.5]. We will run experiments with larger perturbations and report the numbers for this shortly.

“The adaptive CW attack is unbounded, and should always eventually succeed. That is, for any fixed decision threshold that could be used for detection, the attack should always eventually produce an adversarial example that can fool the detector by introducing a very large perturbation. However, it looks like you claim robustness even at very small thresholds (e.g., a 80+% TPR @ 0%FPR). Do you know what is going on here?”

The adaptive CW attack is not always guaranteed to work for an arbitrary detection rule for an arbitrary point. For example, the adaptive CW attacks formulated in [1,2] do not succeed. However, we do note that the adaptive CW attack fails in [1] due to gradient obfuscation and [2] has not been tested extensively for gradient obfuscation. 

For the adaptive CW attacks we consider, we do not set up a fixed threshold, instead we attempt to misclassify the point and minimize the fingerprint loss corresponding to the *true class*. From all the successful attacks that produce a misclassification during the iterations, we choose the one with the smallest fingerprint loss. 

Also, we would like to clarify the intuition: At smaller thresholds it is actually harder for the attacker to fool the detector. This is because the fingerprint loss under smaller thresholds admits smaller regions while larger thresholds admit larger regions.

The fingerprints provide a characterization of the data-distribution and reject regions away from the data-distribution corresponding to the training data . It is not obvious if 
1) there are points that successfully produce a misclassification and have a small fingerprint loss (corresponding to the true label), and 
2) if the CW attack will find it. 

For instance, consider Figure 3 — there exist no “adversarial regions” that produce a misclassification and have a small fingerprint loss corresponding to the true label. We conjecture that the networks from our experiments on the image dataset (MNIST, CIFAR-10…) show similar behavior — the “adversarial regions” producing both a misclassification and a small fingerprint loss do not exist/are either quite small and difficult to find. 

We have also considered a variant where the adaptive CW attacker can determine both the target-class for the label-misclassification and the fingerprint loss to be minimized — constrained to both being the same. This attack performed worse than the variant we report in the paper. 
  
Our source code (link provided in the paper) includes the implementation for the adaptive CW attack discussed in the paper. If you would like to suggest other variants that are interesting, we would be happy to try them out. 

[1] <a href="https://openreview.net/forum?id=B1gJ1L2aW" target="_blank" rel="nofollow">https://openreview.net/forum?id=B1gJ1L2aW</a> (ICLR’18)
[2] https://arxiv.org/abs/1807.03888 (NIPS’18)
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJeJ4hUr2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Concerned about gradient masking</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=HJeJ4hUr2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018</span><span class="item">ICLR 2019 Conference Paper947 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">It's concerning to me that the attack still is failing even at very large epsilons of 64/255 on CIFAR-10. No prior defense has achieved robustness at even 16/255. It might be useful to verify that the attack does eventually succeed if epsilon is allowed to be 128/255. (Not because this is an actual valid attack -- but just to make sure the attack is functioning correctly.)

&gt; It is not obvious if 
&gt; 1) there are points that successfully produce a misclassification and have a small fingerprint loss (corresponding to the true label), and 

These points definitely exist: we know that there exists at least one image correctly classified as each class, so given an unlimited perturbation budget, there will always exist an "adversarial example" with unbounded distortion.

&gt; 2) if the CW attack will find it. 

It may not. But clearly an optimal attack should find it, and so if detection rate is not 0% then we know the attack is performing sub-optimally.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Skldi0wrhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Addressing comments on gradient masking</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=Skldi0wrhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper947 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018</span><span class="item">ICLR 2019 Conference Paper947 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt;It's concerning to me that the attack still is failing even at very large epsilons of 64/255 on CIFAR-10. No prior defense has achieved 
&gt;robustness at even 16/255. It might be useful to verify that the attack does &gt;eventually succeed if epsilon is allowed to be 128/255.  
&gt;(Not because this is an actual valid attack -- but just 
&gt;to make sure the attack is functioning correctly.)

 The optimization problem of finding adversarial examples that both misclassify and have a low-fingerprint loss is non-convex, and no gradient method is guaranteed to find a suitable solution (i.e. good local minimum). Though we do not provide *formal* guarantees, it is not obvious if a “computationally tractable functional” attack exists that with increasing epsilon will fool neural fingerprinting. We have evaluated with a variety of currently known attacks, and find that neural-fingerprinting is robust across these attacks. 

To evaluate if the defense is simply based on gradient masking, we evaluate both with the adaptive-SPSA attack and black-box adversarial examples, and observe that the detection AUCs do not degrade. Please note that SPSA is gradient-free. 

Further, we do notice a small degradation in AUCs for the adaptive BIM-b and CW-L2, which suggests that the adaptive attacks are not entirely dysfunctional -- which would have likely been the case if the defense was working solely based on masked gradients. 

Another sign against gradient masking is that one step attacks do strictly worse than iterative attacks, unlike [1] cited above. 

&gt;&gt; It is not obvious if 
&gt;&gt;1) there are points that successfully produce a misclassification and have a small fingerprint loss (corresponding to the true label), and 

&gt;These points definitely exist: we know that there exists at least one image correctly classified as each class, 
&gt;so given an unlimited perturbation budget, there will always exist an "adversarial example" with 
&gt;unbounded distortion.


Thank you for pointing this out, we agree that with unbounded perturbations, such examples exist. For instance, an unbounded “successful attack” could transform a natural image of a dog to a natural image of a cat. But, coming up with an attack that finds such "successful" unbounded transformations  is likely hard. 

An intuition for why finding such “successful” unbounded transformation is likely hard might be that real images are thought to occupy a very low-dimensional subspace in the high-dimensional input space, and so the space of “good” perturbations might be a very small fraction of the space of all allowed perturbations that a CW attacker searches over.

&gt;&gt; 2) if the CW attack will find it. 

&gt;It may not. But clearly an optimal attack should find it, and so if detection rate is not 0% then we know the 
&gt;attack is performing sub-optimally.

As stated earlier, an optimal attack may be intractable. For reference, in our experiments for the CW-attack, we used the following settings:

L2_BINARY_SEARCH_STEPS_1 = 9 (corresponding to gamma-1)  
L2_BINARY_SEARCH_STEPS_2 = 5 (corresponding to gamma-2) 
L2_MAX_ITERATIONS = 1000    
L2_ABORT_EARLY = True      
L2_LEARNING_RATE = 1e-2     
L2_TARGETED = True          
L2_CONFIDENCE = 0           
L2_INITIAL_CONST = 1e-3    (gamma-1 initial value)
L2_INITIAL_CONST_2 = 0.1   (gamma-2 initial value)

Meanwhile, we will run experiments with larger L2_MAX_ITERATIONS to see if there is a significant degradation in AUC-ROC. We welcome suggestions from you (and other readers) for better settings or modifications to consider.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJeg3ZP8hm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>SPSA parameters</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJekyhCctQ&amp;noteId=BJeg3ZP8hm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper947 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018</span><span class="item">ICLR 2019 Conference Paper947 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Some additional information for the OP and the interested reader:

The SPSA attack is designed for when the gradients do not point in useful directions -- essentially, to check for gradient obfuscation. Note that our method uses no information about the adversarial examples or the attack mechanism during train, so things like label leaking cannot even occur. For SPSA, we use the cleverhans implementation and the hyper-parameters we test against are:
delta=0.01
learning rate=0.01
iterations=100

(Same as the parameters from the SPSA [3] paper used to attack the CIFAR-10 dataset). 

[3] <a href="https://arxiv.org/abs/1802.05666" target="_blank" rel="nofollow">https://arxiv.org/abs/1802.05666</a> (ICML'18)</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>