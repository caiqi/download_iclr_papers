<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Meta-Learning for Contextual Bandit Exploration | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Meta-Learning for Contextual Bandit Exploration" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rJxug2R9Km" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Meta-Learning for Contextual Bandit Exploration" />
      <meta name="og:description" content="We describe MÊLÉE, a meta-learning algorithm for learning a good exploration policy in the interactive contextual bandit setting. Here, an algorithm must take actions based on contexts, and learn..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rJxug2R9Km" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Meta-Learning for Contextual Bandit Exploration</a> <a class="note_content_pdf" href="/pdf?id=rJxug2R9Km" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019meta-learning,    &#10;title={Meta-Learning for Contextual Bandit Exploration},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rJxug2R9Km},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We describe MÊLÉE, a meta-learning algorithm for learning a good exploration policy in the interactive contextual bandit setting. Here, an algorithm must take actions based on contexts, and learn based only on a reward signal from the action taken, thereby generating an exploration/exploitation trade-off. MÊLÉE addresses this trade-off by learning a good exploration strategy based on offline synthetic tasks, on which it can simulate the contextual bandit setting. Based on these simulations, MÊLÉE uses an imitation learning strategy to learn a good exploration policy that can then be applied to true contextual bandit tasks at test time. We compare MÊLÉE to seven strong baseline contextual bandit algorithms on a set of three hundred real-world datasets, on which it outperforms alternatives in most settings, especially when differences in rewards are large. Finally, we demonstrate the importance of having a rich feature representation for learning how to explore.
</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">meta-learning, bandits, exploration, imitation learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We present a meta-learning algorithm, MÊLÉE, for learning a good exploration function in the interactive contextual bandit setting.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rkeSHBI937" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Decent idea with very good validation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxug2R9Km&amp;noteId=rkeSHBI937"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1093 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1093 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes to train exploration policies for contextual bandit problems through imitation learning on synthetic data-sets. An exploration policy takes the decision of choosing an action on each time-step (balancing explore/exploit) based on the history, the confidences of taking different actions suggested by a policy optimizer (bet expert policy given the history). The idea in this paper is to generate many multi-class supervised learning data-sets and sun an imitation learning algorithm for training a good exploration policy. I think this is a novel idea and I have not seen this before. Moreover some intuitive features for training the exploration policy, like the historical counts of the arms, the time-step, arms rewards variances are used on top the the confidence scores from the policy optimizer. It is shown empirically that these extra features add value. 

Overall I think this is a well-written paper with very thorough experimentation. The results are also promising. It would be interesting to gain some insights from the learnt policy, in order to improve hand-designed policies. For example, in a few data-sets it would be interesting to see whether the learnt policy is similar to epsilon greedy in the early stages and switches to greedy after a point, or which of the hand-designed strategies like bagging/cover is the learnt policy most similar to in terms of choice of actions, however I am not sure how such an analysis can be done.  It would also be fair to discuss the offline training time and online run-time of the algorithm with respect to others.  Also, I think the paper should provide a brief introduction to imitation learning, as it is commonly not known in the bandit community. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HkgsDh8wp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxug2R9Km&amp;noteId=HkgsDh8wp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1093 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1093 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for recognizing the contribution of our method. We answer each of the improvement points below.

1) Analysis for Learnt Policy:
======================
 We agree that it’d be interesting to analyze and gain more insight from the learnt policy to design better exploration algorithms for contextual bandits, however, it’s not clear how to perform this analysis. One possibility is to track the exploitation / exploration decisions made by the learnt policy over-time. We can also compute feature importance estimates or perform an ablation study for the features used by MELEE. Similarity between MELEE and other exploration algorithms in terms of the selected action could also be analyzed. However, these results could be highly dependent on the underlying dataset properties.

2) Offline training time and online runtime:
=====================================
 In our experiments, the online runtime for MELEE was similar to epsilon-greedy &amp; exponentiated gradient epsilon-greedy. MELEE was faster than both Cover (which requires a bag of policies) and LinUCB (which requires an inversion for the estimated covariance matrix. Offline training for MELEE requires more time for generating the synthetic data and running the imitation learning algorithm. We trained the model used in our experiments for approximately one day. We will provide exact statistics about the training time and the online runtime performance for MELEE in the final version for this work.

3) Introduction to Imitation Learning: 
================================
We thank the reviewer for highlighting this issue. We will include a more detailed introduction for imitation learning in the final version for this work.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1xpvoeO3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Overall, given the novelty of the idea and the good results, I am inclined to accept, with major modifications.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxug2R9Km&amp;noteId=S1xpvoeO3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1093 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1093 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a new method (Melee) to explore on contextual bandits. It uses a supervised full information data set to evaluate (using counterfactual estimation) and select (using imitation learning) a proper exploration strategy. This exploration strategy is then used to augment an e-greedy contextual bandit algorithm.

The novelty is that the exploration strategy is learned from the data, as opposed to being engineered to minimize regret. The edge of Melee stems from the expected improvement for choosing an action against the standard bandit optimization recommendation.

Pros:
- using data to learn exploration strategy in tis manner is a novel idea for bandits
- good experimental results
- well written paper

Cons:
- Practical impact may be minimal. This setting is seldom encountered in reality.
- No comparison with Thompson sampling bandits, which also use data in devising an exploration strategy. I suggest authors compare to better suited bandits and exploration strategies, beyond basic e-greedy and UCB.
- Article assumes knowledge of imitation learning. which is not a given in bandit literature. I suggest a simple explanation or sketch of the imitation algorithm.
- Theoretical guarantees questionable. Theorem 1 talks about "no-regret algorithm". you then extend this notion and claim "if we can achieve low regret .... then ....". It is unclear to me how this theorem allows you to make such claim. A low regret is &gt; no-regret, and hence a bound on no-regret may not generalize to low regret.
- May want to add noise to augmentation data, to judge robustness of method.

Overall, given the novelty of the idea and the good results, I am inclined to accept, with major modifications. Improvements of the method and analysis are likely to follow. Given the flaws though, I am not fighting for this paper.

Minor comments:
sec 2.1: you may want to explain why you require reward to be [0,1]
Alg 1: explain Val and rho in algorithm.
sec 2.3: what is "ergo". Also, you may want to refer to f as "function" and to pi as "policy". referring to f as policy may be confusing (even though it is a policy). For example: "(line 8) on which it trains a new policy"
End of 2.4: "as discussed in 2.4" should be "in 2.3"
sec 3.3: why is epsilon=0 the best? is it because synthetic data has no noise? This result surprises me.




</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJeF96IDpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxug2R9Km&amp;noteId=SJeF96IDpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1093 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1093 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the detailed review and insightful comments. We clarify several points below.
 
1) Practical Impact: 
=================

We request a clarification from AnonReviewer3 for why they think the practical impact may be minimal. The setting we study in the paper is the standard contextual bandit setting encountered in reality. The algorithm learns an exploration policy for balancing exploration with exploitation in contextual bandits, a fundamental issue addressed by any contextual bandit algorithm. We stress that our algorithms doesn’t assume access to fully supervised datasets at runtime, we rely on synthetic fully supervised datasets only for offline training. These datasets are generated synthetically and doesn’t require labelling effort from annotators at runtime. 

2) Comparison with Thompson Sampling:
====================================

 We compared MELEE to seven other exploration algorithms. Many of these algorithms does indeed use data in devising an exploration strategy. For example LinUCB, Cover, and Cover-NU all leverage information from the observed data to balance exploration and exploitation. Exponentiated Gradient epsilon-greedy as well uses the observed data to select the best epsilon for exploration. For completeness, we will add Thompson Sampling in our comparison. 

3) Knowledge of imitation learning: 
===============================

We thank the reviewer for highlighting this issue. We will include a more detailed introduction for imitation learning in the final version for this work.

4) Theoretical guarantees no-regret vs low-regret: 
===========================================

What we mean by low regret in this statement is the low average regret epsilon-class-hat: the average regression regret for each policy π-n), not the no-regret LEARN procedure in Alg 1 - line 16. We’ll rephrase this to make this distinction clear in the final draft for this paper. 

5) Noise in augmentation data: 
===========================

We include noise in the augmentation datasets used for training MELEE. These datasets are generated synthetically and the details for the data generation process is highlighted in Appendix B. We generate 2D datasets by first sampling a random variable representing the Bayes classification error. The Bayes error is sampled uniformly from the interval 0.0 to 0.5. This Bayes error controls for the amount of noise in the dataset.


6) Minor comments: 
==================

The authors thank the reviewer for highlighting these issues. We’ll take all of these comments into account in the final version.

7) Why we require reward to be [0,1] in Alg 1: 
=======================================

We’ll add a clarification for why we require bounded rewards. Theoretically, this is required to ensure the no-regret bound in theorem 1. Empirically,  for multi-class contextual bandit classification problems, we use a reward of one for the correct action, and the reward of zero for all other incorrect actions.

8) Why is epsilon=0 the best? 
==========================

Empirically, MELEE doesn’t require the added extra exploration on top of the learned exploration strategy, and at runtime the best performance was achieved when we set the additional exploration parameter \mu to 0 . At training time the synthetic datasets we used are not noise-free. As described in point (5) of this response, we control the amount of noise in the training dataset via the Bayes error parameter. Bietti et. al. observed a similar behavior for the same datasets we used in our experiments. For epsilon-greedy exploration, the best performance was achieved when setting epsilon to zero. They attribute this to the diversity of the context vectors in these datasets. 

9) Major Modifications: 
====================

We assume the “major modifications” are the issues highlighted in the “cons” section of the review. We kindly request a clarification about any other major modifications the reviewer thinks should be necessary. 

References:

Alberto Bietti, Alekh Agarwal, and John Langford. A Contextual Bandit Bake-off. working paper or preprint, May 2018. URL <a href="https://hal.inria.fr/hal-01708310." target="_blank" rel="nofollow">https://hal.inria.fr/hal-01708310.</a>
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJluMESy2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper investigates a problem that does not correspond to any real problem.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxug2R9Km&amp;noteId=HJluMESy2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1093 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1093 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper investigates a meta-learning approach for the contextual bandit problem. The goal is to learn a generic exploration policy from datasets, and then to apply the exploration policy to contextual bandit tasks. The authors have adapted an algorithm proposed for imitation learning (Ross &amp; Bagnell 2014) to their setting. Some theoretical guarantees straightforwardly extracted from (Ross &amp; Bagnell 2014) and from (Kakade et al 2008) are presented. Experiments are done on 300 supervised datasets.

Major concerns:

1 This paper investigates a problem that does not correspond to the real problem: how to take advantage of a plenty of logs generated by a known stochastic policy (or worst unknown deterministic policy) for the same (or a close) contextual bandit task? 
Most of companies have this problem. I do not know a single use case, in which we have some full information datasets, which are representative of contextual bandit tasks to be performed. If the full information datasets does not correspond to the contextual bandit tasks, it is not possible to learn something useful for the contextual bandit task. 

2 The experimental validation is not convincing.

The experiments are done on datasets, which are mostly binary classification datasets. In this case, the exploration task is easy. May be it is the reason why the exploration parameter \mu or \epsilon = 0 provides the best results for MELEE or \epsilon-greedy?

The baselines are not strong. The only tested contextual bandit algorithm is LinUCB. However a diagonal approximation of the covariance matrix is used when the dimension exceeds 150. In this case LinUCB is not efficient. There are a lot of contextual bandit algorithms that scale with the dimension.


3 The theoretical guarantees are not convincing. 

The result of Theorem 1 is a weak result. A linear regret against the expected reward of the best policy is usually considered as a loosely result. Theorem 2 shows that there is no theoretical gain of the use of the proposed algorithm: the upper bound of the expected number of mistakes obtained when Banditron is used in MELEE is upper than the one of Banditron alone.

Minor concerns:

The algorithms are not well written. POLOPT function has sometimes one parameter, sometimes two and sometimes three parameters. The algorithm 1 is described in section 2, while one of the inputs of the algorithm 1 (feature extractor function) is described in section 3.1. The algorithm 1 seems to return all the N exploration policies. The choice of the returned policy has to be described.

In contextual bandits, the exploration policy is not handcrafted. The contextual bandit algorithms are designed to be optimal or near optimal in worst case: they are generic algorithms.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Sklm6yPP67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxug2R9Km&amp;noteId=Sklm6yPP67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1093 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1093 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the provided feedback. Please find our response below:

1) Relevance to Real Problems:
===========================
  
We believe that there is a fundamental misunderstanding in this point of the review regarding the experimental setup we study on our paper. 

We want to stress that we don’t assume access to full information datasets that are representative of the contextual bandit task to be performed. As mentioned on the abstract &amp; section 3.1 of the paper, MELEE uses offline synthetic datasets during the training phase. Contrary to the assertion in the review, these synthetic full information datasets are quite different from the task dependent contextual bandit dataset. 

These synthetic datasets are very diverse and broad in their complexity, and exploration strategies learned on these datasets does indeed generalize to real contextual bandit datasets as we have verified both empirically and theoretically. The context vectors for these synthetic datasets are quite different in structure from the real contextual bandit task at hand, for which we don’t assume access to any sort of full information data. We learn a dataset independent exploration policy from these synthetic datasets, and use meta-features that can generalize across different datasets to learn how to explore in realistic contextual bandit settings. 

We describe how we generate these synthetic datasets in appendix B. We generate 2D datasets by first sampling a random variable representing the Bayes classification error. The Bayes error is sampled uniformly from the interval 0.0 to 0.5. This Bayes error controls for the amount of noise in the dataset. 


2) Experimental Validation: 
=======================

It’s not true that we only compare to the LinUCB exploration algorithm. We compare to seven other contextual bandit exploration algorithm these algorithms are (Section 3.3): Epsilon greedy, Exponentiated Gradient Epsilon Greedy, Tau-first exploration, LinUCB, Cover, and Cover-Nu. Many of these algorithms does indeed use data in devising an exploration strategy. For example LinUCB, Cover, and Cover-NU all leverage information from the observed data to balance exploration and exploitation. 

3) Theorem 1 and sublinear Regret:  
==============================

This is a really good observation. The regret bounded by Theorem 1 is dependent on the term epsilon-hat-class (i.e. the average regression regret for each policy π-n). Sublinear regret is still achievable whenever this term decreases over the time horizon T. For any reasonable underlying learning algorithm, we expect this term to be decreasing at a rate of T^-a (e.g. a:½), putting this together, the sublinear regret will still be achievable. 


4) Theorem 2 and expected number of mistakes:
=========================================  

The theoretical gain is still guaranteed because it’s never the case that the upper bound of the expected number of mistakes obtained when Banditron is used in MELEE is larger than the one of Banditron alone. This follows directly from the edge assumption we make, as Eγt ≥ 0, and Γ ≤ 1. 

5) Minor concerns:  
=================

We thank the reviewer for highlighting these concerns. The authors appreciate the reviewer’s suggestions for improving the overall exposure of the paper. In order to make it easier for reviewers’ to track the changes we kept the structure largely consistent with the original submission, but we’ll take all of these comments into account in the final version. 

6) Hand-crafted Exploration:
=========================

With “hand-crafted” exploration algorithms we meant “not learned”, we agree that this terminology is not accurate and we will remove it in the final version. 

7) Returned Policy:
=================

Theoretically Algorithm 1 averages between the set of learned N policies. In practice, it’s typical that the final policy leads to a better performance empirically. In our experiments, Algorithm 1 returns the final N-th policy. We’ll describe the return policy explicitly in the paper and fix the notation for POLOPT. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>