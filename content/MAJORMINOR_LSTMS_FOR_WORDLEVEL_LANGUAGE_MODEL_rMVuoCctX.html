<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>MAJOR-MINOR LSTMS FOR WORD-LEVEL LANGUAGE MODEL | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="MAJOR-MINOR LSTMS FOR WORD-LEVEL LANGUAGE MODEL" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=r1MVuoCctX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="MAJOR-MINOR LSTMS FOR WORD-LEVEL LANGUAGE MODEL" />
      <meta name="og:description" content="As a widely-accepted evaluation criterion, complexity has attracted more and more attention in the design of language models. The parameter count is a proxy for complexity, which is often reported..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_r1MVuoCctX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>MAJOR-MINOR LSTMS FOR WORD-LEVEL LANGUAGE MODEL</a> <a class="note_content_pdf" href="/pdf?id=r1MVuoCctX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019major-minor,    &#10;title={MAJOR-MINOR LSTMS FOR WORD-LEVEL LANGUAGE MODEL},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=r1MVuoCctX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">As a widely-accepted evaluation criterion, complexity has attracted more and more attention in the design of language models. The parameter count is a proxy for complexity, which is often reported and compared in research papers. In general, more parameters means better model performance, but higher complexity. Therefore, reconciling the contradiction between the complexity and the model performance is necessary. In this paper, we propose a simple method to make use of model parameters more effectively, so that the LSTM-based language models can reach better results without the cost of increasing parameters. The method constructs another small-scale LSTM with a part of parameters originally belonging to the vanilla LSTM in each layer, whose output can assist the next layer in processing the output of the vanilla LSTM. We name these two LSTMs Major Minor LSTMs. In experiments, we demonstrate the language model with Major Minor LSTMs surpasses the existing state-of-the-art model on Penn Treebank and WikiText-2 with fewer parameters.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Language model, LSTM, Deep Learning, NLP</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HylfyL5l67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>About Skip Connection</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1MVuoCctX&amp;noteId=HylfyL5l67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper345 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper345 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Skip-connections are indeed well-known. But our “skip connection” is very different from these existing ones. As far as we know, existing skip connections (highway and ResNet) are mainly designed to ease gradient-based training of neural network (i.e. Highway Networks, ResNet, and RHN) or increase architecture complexity (NAS). In general, these skip connections were applied in deep network and achieved good results, because they can benefit the optimization of the models. 
But in our model, “skip connection” is just a “coincidence”. We have analyzed that the effectiveness of our model is because the Minor LSTM generates an auxiliary feature vector for the Major LSTM layer, so that the next layer can better process the output of the current Major LSTM layer (in Section 3.3). In addition, we have stated that because the LSTM is a recursive model, there is a high “consistency” between different features in its output vector. So if we want to generate an auxiliary feature vector for Major LSTM output vector, we should leverage another model (the Minor LSTM), rather than simply increase the dimension of Major LSTM. As for the input of Minor LSTM, we have compared the output of last layer and the original embedding of the sequence. Finally, we consider that it is entirely possible to generate enough good auxiliary feature vector for each Major LSTM with fewer parameters (the dimension of word embedding is often smaller than the output of last layer), we chose the original embedding of the sequence. In fact, our model has only three LSTM layers, and we do not think that it suffers the same training problems like those deep networks. The choice of the skip connection is not the focus of our consideration, and the purpose we choose the skip connection is essentially different from reference papers.
In other hand, the existing models with skip connection have no concept of major and minor at all, the branch of skip connection and the main branch has the same status in the model design. In Figure2 of <a href="https://arxiv.org/pdf/1505.00387.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1505.00387.pdf</a> (Highway networks), we can see that Most of the outputs stay constant over many layers forming a pattern of stripes. This indicates that the branch of can play an important role. In contrast, our model clearly defines the importance of the two branches, the skip connection can only play a supporting role, and account for a small proportion. In our previous experiments, if the ratio of the Major LSTM and Minor LSTM was excessive, the model performance would be worse. However, other skip connection models will not face this problem.   
Therefore, just by form, we do not deny that the Minor LSTM is a skip connection, and the improvement of our model performance is due to the use of “skip connection”. We do not agree that simple similarities in form can erase the meaning and novelty of our work. 
In addition, we will modify the deficiencies and errors in our description of the paper, and upload a new version in time.

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ByxMuNqxpQ" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1MVuoCctX&amp;noteId=ByxMuNqxpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper345 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJgpV8XK3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Problems of clarity , novelty and significance</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1MVuoCctX&amp;noteId=HJgpV8XK3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper345 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper345 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes an architecture for language modeling composed of a bigger and smaller LSTM. The authors propose to add skip connections directly from the input layer to the layers of the "major" lstm. The skip-connections are computed using the small LSTM. The authors claim to reach better performance with smaller number of parameters in two language modeling tasks (PTB and WT2).

I had a hard time reading and understanding the paper. The clarity and style of the writing should be improved to better stress the significance of the underlying motivation. Increasing the efficiency of learning algorithms in terms of the number of parameters is clearly an important endeavour. However, the novelty of the architecture (skip-connections are well-known in the context of language modeling, e.g. , Melis et al. (2017) for a recent account) as well as the reduction in the number of overall parameters are limited enough to make the marginal gains observed in the reported results insufficient for publication.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJg4MSVZpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>About Skip Connection</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1MVuoCctX&amp;noteId=HJg4MSVZpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper345 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper345 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for  your reading and valuable comments. Since our use of skip connection has been stronly questioned, we specifically give our opinions in the official comment "About Skip Connection". As for the other issues you mentioned, we will modify and refine them in the next version. We are looking forward to your reply.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1x1N17mnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>More efficient spending of the parameters in LSTM language models</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1MVuoCctX&amp;noteId=H1x1N17mnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper345 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper345 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=H1x1N17mnX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper suggests more efficient spending of the parameters in LSTM language models: instead of increasing the hidden layer size of the LSTM, the authors suggest building a small assistant LSTM and its output is concatenated with the output of the main LSTM.

The approach reminds me the SCRN Model of Mikolov et al. (2015). I don't claim it is the same, as you have an independent LSTM, but there are some similarities: in both cases an assisting small layer is added, and in both cases one needs to choose an appropriate size of this assisting layer. It would be interesting to see if your Minor LSTM plays the same role as the context state in SCRN, e.g. does the hidden state in Minor LSTM change slower than that of the Major LSTM? If yes, then we could say that the Minor LSTM focuses more on what is being talked about (context).

Experimental evaluation of the idea seems adequate. However, my feeling is that the recent optimization and regularization techniques (such as those used in AWD-LSTM) may hide the effect of your idea. If possible, could you please evaluate your approach under less recent regularization, e.g. variational dropout with tied weights as in Inan et al. (2016).

Minor comment on "more parameters means better model performance" in the abstract: If by model performance we understand performance on held-out data, then I would say that "more parameters means more flexible model".

Note after reading other reviews: I agree with Reviewer1 and Reviewer2 that the use of skip-connections (Melis et al., 2017; Zilly et al., 2016) is one of the main reasons why the model achieves its performance (Table 5). Without skip-connections the proposed Major-Minor LSTM is practically on par with the AWD-LSTM-MoS of Yang et al. (2017) on PTB and only 0.57 perplexity units better on WT-2.

References
- Inan, H., Khosravi, K. and Socher, R., 2016. Tying word vectors and word classifiers: A loss framework for language modeling. arXiv preprint arXiv:1611.01462.
- Melis, G., Dyer, C. and Blunsom, P., 2017. On the state of the art of evaluation in neural language models. arXiv preprint arXiv:1707.05589.
- Mikolov, T., Joulin, A., Chopra, S., Mathieu, M. and Ranzato, M.A., 2014. Learning longer memory in recurrent neural networks. arXiv preprint arXiv:1412.7753.
- Yang, Z., Dai, Z., Salakhutdinov, R. and Cohen, W.W., 2017. Breaking the softmax bottleneck: A high-rank RNN language model. arXiv preprint arXiv:1711.03953.
- Zilly, J.G., Srivastava, R.K., Koutník, J. and Schmidhuber, J., 2016. Recurrent highway networks. arXiv preprint arXiv:1607.03474.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hklauq4WpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>About Skip Connection</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1MVuoCctX&amp;noteId=Hklauq4WpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper345 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper345 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for  your reading and valuable comments. Since our use of skip connection has been stronly questioned, we specifically give our opinions in the official comment "About Skip Connection". As for the other issues you mentioned, we will modify and refine them in the next version. We are looking forward to your reply.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJlrgBPyn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Missing comparison - what about highway/skip connections?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1MVuoCctX&amp;noteId=BJlrgBPyn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper345 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper345 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors introduce an language modeling architecture that introduces so-called "minor-LSTMs" computed directly on top of the input in addition to each network layers input. This approach seems hardly novel. considering approaches like skip/highway connections, e.g. (Zilly et al., 2016, <a href="https://arxiv.org/abs/1607.03474)." target="_blank" rel="nofollow">https://arxiv.org/abs/1607.03474).</a> Even if the proposed approach varies this theme in some way, a comparison to these strongly related approaches would be due.

Also, the presented approach could also be viewed as a kind of factorization, cf. (Kuchaiev et al., 2017, https://arxiv.org/abs/1703.10722).

Sec. 3.3: it remains totally unclear to me, how the authors reach the recursive equations (9) and (10) and how they relate to Eq. (8) mention before.

In the results tables on p. 6, at some point, perplexity numbers are given with a second position after the decimal point. I strongly doubt that these differences in perplexity are significant, especially on these small tasks.

The authors should take more care w.r.t. the notation, which is not consistent throughout the paper (e.g. x_j vs. x_t, y_j vs. y_t) and should be introduced properly.

Finally, the mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications. Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches. It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1xtnMSWaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>About Skip Connection</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1MVuoCctX&amp;noteId=S1xtnMSWaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper345 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper345 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for  your reading and valuable comments. Since our use of skip connection has been stronly questioned, we specifically give our opinions in the official comment "About Skip Connection". As for the other issues you mentioned, we will modify and refine them in the next version. We are looking forward to your reply.
In addition, about the reference paper (Kuchaiev et al., 2017, <a href="https://arxiv.org/abs/1703.10722)," target="_blank" rel="nofollow">https://arxiv.org/abs/1703.10722),</a> we have to state that we have different basic idea. our original intentions are both to improve the performance without increasing model complexity (parameters). The Factorized LSTM approximated a matrix W in LSTM cell as W=W1*W2,  so a n*m-dimenaional matrix can be decomposed into a n*r and a r*m matrices (r &lt; n, r&lt;m). In this way the rank of W must less than or equal to r. But in our opinion, due to the diversity of the language, the matrices in LSTM should be high-rank, like in (Breaking the softmax bottleneck: A high-rank RNN language model. arXiv preprint arXiv:1711.03953). So we don't think that this method is suitable for out point of view, even if this method can save more parameters than our model. We argue that parameters should be save without significantly affecting the performance of Major LSTM.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJlDGeXjim" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Missing reference</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1MVuoCctX&amp;noteId=HJlDGeXjim"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">22 Oct 2018</span><span class="item">ICLR 2019 Conference Paper345 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi,

Here is a NIPS 2018 paper which achieves lower perplexity on PTB and WT2 datasets compared to AWD-LSTM-MoS. 

<a href="https://arxiv.org/pdf/1809.06858.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1809.06858.pdf</a></span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkx6uCIjsm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Missing reference</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1MVuoCctX&amp;noteId=Bkx6uCIjsm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper345 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">22 Oct 2018</span><span class="item">ICLR 2019 Conference Paper345 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for pointing out the paper. 1) The reference paper proposed an improvement on word embedding layer (while ours  on LSTM layer), which does not have much comparability with our model. 2) We will refer it as parallel work in our paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Bkx3paHis7" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1MVuoCctX&amp;noteId=Bkx3paHis7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper345 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>