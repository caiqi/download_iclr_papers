<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learning Exploration Policies for Navigation | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learning Exploration Policies for Navigation" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SyMWn05F7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learning Exploration Policies for Navigation" />
      <meta name="og:description" content="Numerous past works have tackled the problem of task-driven navigation. But, how to effectively explore a new environment to enable a variety of down-stream tasks has received much less attention...." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SyMWn05F7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning Exploration Policies for Navigation</a> <a class="note_content_pdf" href="/pdf?id=SyMWn05F7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019learning,    &#10;title={Learning Exploration Policies for Navigation},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SyMWn05F7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=SyMWn05F7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Numerous past works have tackled the problem of task-driven navigation. But, how to effectively explore a new environment to enable a variety of down-stream tasks has received much less attention. In this work, we study how agents can autonomously explore realistic and complex 3D environments without the context of task-rewards. We propose a learning-based approach and investigate different policy architectures, reward functions, and training paradigms. We find that use of policies with spatial memory that are bootstrapped with imitation learning and finally finetuned with coverage rewards derived purely from on-board sensors can be effective at exploring novel environments. We show that our learned exploration policies can explore better than classical approaches based on geometry alone and generic learning-based exploration techniques. Finally, we also show how such task-agnostic exploration can be used for down-stream tasks. Videos are available at <a href="https://sites.google.com/view/exploration-for-nav/." target="_blank" rel="nofollow">https://sites.google.com/view/exploration-for-nav/.</a></span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Exploration, navigation, reinforcement learning</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Skx8ACqjp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response Overview</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyMWn05F7&amp;noteId=Skx8ACqjp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1147 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1147 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewers for their comments and suggestions. We are glad that the reviewers found:
        (a) our paper to tackle an important and clearly motivated problem (R1, R3)
        (b) our approach to be a great idea (R2), a good addition to the literature (R3) and not-complicated (R1).
        (c) our paper to be “well-executed”, with “various ablations”, and “comparisons to … commendably a classical SLAM baseline” (R2)
        (d) our paper to be well-written (R1, R3), and well-explained (R2).
We have answered *ALL* questions that the reviewers posed by providing additional experimental comparisons, pointing to relevant existing experiments and providing clarifications. Hopefully, this clarifies some of the misunderstandings that R1 has about our paper. Additional experiments have been added to Appendix C of the updated PDF. We will incorporate these experiments and other suggestions in camera-ready upon acceptance.

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1epUX832m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>No significant novelty, lack of experimental evaluations, missing technical details</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyMWn05F7&amp;noteId=S1epUX832m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1147 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1147 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a method for learning how to explore environments. The paper mentions that the “exploration task” that is defined in this paper can be used for improving the well-known navigation tasks. For solving this task, a reward function a network architecture that uses RGBD images + reconstructed map + imitation learning + PPO is designed.

&lt;&lt;Pros&gt;&gt;

-The paper is well-written (except for a few typos).
-The overall approach is simple and does not have much complications. 
-The underling idea and motivation is clearly narrated in the intro and abstract and the paper has a easy-to-understand flow.  

&lt;&lt;Cons&gt;&gt;

**The technical novelty is not significant**

-This paper does not provide significant technical novelty. It is a combination of known prior methods: imitation learning + ppo (prior RL work). The presented exploration task is not properly justified as to how it could be useful for the navigation task. The reconstruction of maps for solving the navigation problem is a well-explored problem in prior SLAM and 3D reconstruction methods. Overall the novelty of the approach and the proposed problem is incremental. 

**The paper has major short comings in the experimental section. The presented experiments do not support the main claim of the paper which is improving the performance in the well-known navigation task. Major baselines are missing. Also, the provided results are not convincing in doing the right comparison with the baselines. **

-Experimental details are missing. The major experimental evaluations (Fig. 2 and Fig. 3) are based on the m^2 coverage after k steps and the plots are cut at 1000 steps. What are the statistical properties of the 3D houses used for training and testing? E.g what is their area in m^2? How big is each step in meters?  Why are the graphs cut at 1000 steps? How would different methods converge after more than 1000 steps, e.g. 2000 steps? I would like to see how would the different methods converge after larger number of steps? How long would each step take in terms of time? How could these numbers convey the significance of the proposed method in a real would problem settings? 

-The experiments do not convey if learning has significantly resulted in improved exploration. Consider a simple baseline that follows a similar approach as explained in the paper for constructing the occupancy map using the depth sensor. A non-learning agent could use this map at each step to make a greedy choice about its next action which greedily maximizes the coverage gain based on its current belief of the map. While the performance of random policy is shown in Fig.2 the performance of this greedy baseline is a better representative of the lower bound of the performance on the proposed task and problem setup.

-What is the performance of a learning-based method that only performs collision avoidance? Collision avoidance methods tend to implicitly learn to do a good map coverage. This simple baseline can show a tangible lower bound of a learning-based approach that does not rely on map.

-The major promise of the paper is that the proposed exploration task can improve navigation. However, the navigation experiment does not compare the proposed method with any of prior works in navigation. There is a huge list of prior methods for navigation some of which are cited in the “learning for Navigation” section of the related works and the comparison provided in Fig. 4 is incomplete compared to the state-of-the-arts in navigation. For example, while the curiosity driven approach is compared for the exploration, the more related curiosity based navigation method which uses both “exploration strategy” and “imitation learning” : “Pathak, Deepak, et al. "Zero-shot visual imitation." International Conference on Learning Representations. 2018.
“ is missed in navigation comparison. The aforementioned paper is also missed in the references.  

-Algorithmic-wise, it would make the argument of the paper clearer if results were conducted by running different exploration strategies for navigation to see if running RL with a good exploration strategy could solve the exploration challenge of the navigation problem without needing an explicit exploration stage (similar to the proposed method) which first explores and constructs the map and then does navigation by planning.

-The navigation problem as explained in section is solved based on planning approach that uses a reconstructed map. This is a fairly conventional approach that SLAM based methods use. Therefore, comparison with a SLAM method that constructs the map and then does navigation would be necessary. 


** Technical details are missing or not explained clearly**

- Section 3.1 does not clearly explain the map construction. It seems that the constructed map is just a 2D reconstruction of the space (and not 3D) using the depth sensor which does not need transformation of the 3D point cloud. What is the exact 3D transformation that you have done using the intrinsic camera parameters? This section mentions that there can be error in such map reconstruction because of robot noise but alignment is not needed because the proposed learning method provides robustness against miss-alignment. How is this justified? Why not using the known loop closure techniques in SLAM? 

-The technical details about the incorporated imitation learning method are missing. What imitation learning method is used? How is the policy trained during the imitation learning phase? 

-Last paragraph of intro mentions that the proposed method uses 3D information efficiently for doing exploration. The point of this sentence is unclear. What 3D information is used efficiently in the paper? Isn’t it only 2.5D (information obtained by depth sensor) used in the proposed method?

**Presentation can be improved**

-The left and right plots of the Figure 3 contains lots of repetitions which brings in confusion in comparing the performance of runs with different settings. These two plots should be presented in a single plot. 

- Interpretation of “green vs white vs black” in the reconstructed maps is left to the reader in Fig. 1. 

- Last line in page 5: there is no need for reiteration. It is already clear.

**Missing references**

-Since the paper is about learning to explore, discussion about “exploration techniques in RL” is recommended to be added in at least the related work section. 

-A big list papers for 3D map reconstruction is missing. Since the proposed method relies on a map reconstruction, those papers are relevant to this work and can potentially be used for comparison (as explained above). It is highly recommended that relevant prior 3D map reconstruction papers be added to the related work sections. 


</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SygDhGjoT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Additional Experiments, Pointers to Existing Experiments and Clarifications (1)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyMWn05F7&amp;noteId=SygDhGjoT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1147 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1147 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank R1 for their comments. R1’s primary concerns are about novelty and missing empirical comparison. These perhaps stem from some misunderstandings about our paper as some requested comparisons are either irrelevant or stronger comparisons are already presented in the paper.  Therefore, we urge the reviewer to take a second look at the paper in light of the rebuttal.

1. Novelty: In this paper, we learn policies for exploring novel 3D environments (Section 3 through Section 4.2), and show that exploration data, gathered by executing our learned exploration policies, improves performance at downstream navigation tasks (Section 4.3). To the best of our knowledge, this is the first work that studies learned exploration policies for navigation, systematically compares them to classical and learning-based baselines, and shows the effectiveness of exploration data for downstream tasks. In doing so, we adopt existing learning techniques (imitation learning + reinforcement learning), and map building techniques. Our novelties are orthogonal to these aspects:
      (a) Problem formulation: Framing exploration as a learning problem, and showing the utility of exploration data for downstream tasks.
      (b) Map based policy architectures and reward functions. Classical SLAM based approaches indeed produce maps but: (a) it still needs a policy for exploration during the map-building phase; (b) does not solve navigation rather uses geometric analysis for path planning. Our approach focuses on (a) and unlike heuristic approaches used in SLAM, we use a learning-based approach.
      (c) We also show maps can also be used for learning effective policies, and for computing reward signals.
      (d) Use of IL + RL to optimize our policy, as opposed to pure RL that is typically used.

2. Comparison with other exploration approaches: 
a) Simple Greedy Baseline: We experimented with the suggested one-step greedy policy. Here we virtually simulate all possible actions that the agent can take, and compute the gain in coverage. We then execute the action that results in the maximum gain in coverage. At 1000 steps such a policy only covers 40m^2, as opposed to our policies that cover up to 125 m^2. This is not surprising as the policy gets stuck inside local regions of full coverage. No action leads to any increase in coverage and the agents move back and forth. The full performance plot is provided in Fig C3(a) in the updated PDF. 

Note, in the paper, we have provided a more compelling comparison point to classical exploration approaches: frontier-based method. Reviewer seems to have missed this comparison as R1 still asks for comparisons to classical approaches.

b) Collision Avoiding Policy: A policy that purely avoids collisions has a degenerate solution of the agent staying in-place, resulting in negligible coverage (Fig C4(a)). We also tried a more sophisticated version, where the agent moves straight unless a collision happens (Fig C3(a)), at which point it randomly rotates (by angle between 0 and 2pi), and continues to move straight. To help the policy further, we used ground truth collision-checking. This policy covers 75m^2, still much lower than our performance (125m^2).</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkgYymsoTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Additional Experiments, Pointers to Existing Experiments and Clarifications (2) </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyMWn05F7&amp;noteId=BkgYymsoTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1147 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1147 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">3. Comparison with other learning-based navigation works: First, we do not study a specific navigation task, but instead our contribution is a task-independent exploration policy. We do however show that exploration helps in downstream navigation tasks (Section 4.3). We use well-established Classical Path Planning, the simplest navigation algorithm for doing these experiments. This was a conscious choice so as to not-conflate quality of learned navigation policy with the quality of our learned exploration policy. Our contribution is orthogonal to navigation task itself and therefore our approach can be used in conjunction with any navigation approach. For example, the exploration data by running our policy can be used ‘as is’ with Savinov et al’s state-of-the-art SPTM approach [A]. SPTM otherwise requires a human to demonstrate the environment, which is impractical in real-world scenarios. 

R1 suggests we should compare to Pathak et al’s “Zero-shot visual imitation” (ZSVI) as it uses “exploration strategies” and “imitation learning” for navigation. While they indeed use both terms (exploration and imitation), the context and usage is completely different.
      (a) *Exploration for Imitation (ZSVI) vs. Imitation for Exploration (Ours)*
           In ZSVI, exploration is used in training to collect trajectories and imitation is used in testing to follow a path. On the other hand, ours is completely the opposite. We use imitation in training to learn how to explore at test time. Again we emphasize: ZSVI does not run any explicit exploration policy during testing.
      (b) This leads to completely different behavior of two algorithms. The time/distance range in ZSVI is much smaller as compared to ours. Either the goal is in the same room or they need a lot of waypoint images to solve the navigation task.

In order to show a comparison to “RL with a good exploration … without explicit exploration“, we have implemented navigation on top of Curiosity Driven Exploration using Self-Supervision. As shown in Appendix C.6 (will be added to Sec 4.3),  the comparison is in our favor. 

4. More Experimental Details: We have added additional details in Appendix C. We have included:
a) Stats and floor-plans of houses used for training and testing (Appendix C1).
b) Coverage plots for when we run the agent for 2000 steps (Appendix C4, Fig C3). Conclusions are the same as for the original 1000 steps plots as presented in the paper.
c) Agent details. Step size is 0.25m forward motion, 9 degree rotations (already provided in the paper). Real world performance depends on how fast a robot is. A turtlebot-2 can move at a peak speed of 0.65 m/s, if that’s what you were looking for.

5. More Technical Details: 
a) We have added details about map construction in Appendix C2. Yes, we can use known-loop closure techniques in SLAM, though there may still be error and we wanted to show that learning is robust to it (Fig 2 (center), video on website).
b) Imitation learning details are in Appendix C5.
c) 3D Information: Yes, you are right depth images only give 2.5D information, however, we integrate information from different views, to obtain a more complete sense of the environment than given by a single depth image. 3D information can also be extracted from RGB images, see [B] and numerous others for example.

We will incorporate your suggestions on presentation in the final version.

[A] Semi-parametric Topological Memory for Navigation Nikolay Savinov, Alexey Dosovitskiy, Vladlen Koltun. ICLR 2018.
[B] Factoring Shape, Pose, and Layout from the 2D Image of a 3D Scene Shubham Tulsiani, Saurabh Gupta, David Fouhey, Alexei A. Efros, Jitendra Malik. CVPR 2018.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_S1eK3wbjnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good use of mapping for exploration</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyMWn05F7&amp;noteId=S1eK3wbjnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1147 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1147 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This is a well explained and well executed paper on using classical SLAM-like 2D maps for helping a standard Deep RL navigation agent (convnet + LSTM) explore efficiently an environment and without the need for extrinsinc rewards. The agent relies on 3 convnets, one processing RGB images, one the image of a coarse map in egocentric referential, and one of the image of a fine-grained map in egocentric referential (using pre-trained ResNet-18 convnets). Features produced by the convnets are fed into a recurrent policy trained using PPO. Two rewards are used: the increase in the map's coverage and an obstacle avoidance penalty. The agent is further bootstrapped through imitation learning in a goal-driven task executed by a human controlling the agent. The authors analyze the behavior of the navigation algorithm by various ablations, a baseline consisting of Pathak's (2017) Intrinsic Curiosity Module-based navigation and, commendably, a classical SLAM baseline with path planning to empty, unexplored spaces.

Using an explicit map is a great idea but the authors need to acknowledge how hand-engineered all this is, when comparing it to actual end-to-end methods. First, the map reconstruction is done by back-projections of a depth image (using known projective geometry parameters) onto a 3D point cloud, then by slicing it to get a 2D map, accumulated over time using nearly perfect odometry. SLAM was an extremely hard problem to start with, and it took decades and particle filters to get to the quality of the images shown in this paper as obvious. Normally, there is drift and catastrophic map errors, whereas the videos show a nearly perfect map reconstruction. Is the motion model of the agent unrealistic? Would this ever work out of the box on a robot in a real world? The authors brush off the need for bundle adjustment, saying that the convnet can handle noisy local maps. Second, how do you get and maintain such nice ego-centric maps? Compared to other end-to-end work on learning how to map (see Wayne et al. or Zhang et al. or Parisotto et al., referred to later in the paper), it looks like the authors took a giant shortcut. All this SLAM apparatus should be learned!

One crucial baseline that is missing is that of explicit extrinsic rewards encouraging exploration. These rewards merely scatter reward-yielding objects throughout the environment; over the course of an episode, an object reward that is picked does not re-appear until the next exploration episode, meaning that the agent needs to cover the whole space to forage for rewards. Examples of such rewards have been published in Mnih et al. (2016) "Asynchronous methods for deep reinforcement learning" and are implemented in DeepMind Lab (Beatie et al., 2016). Such an extrinsic reward would be directly related to the increase of coverage.

A second point of discussion that is missing is that of the collision avoidance penalty: roboticists working on SLAM know well that they need to keep their robot away from plain-texture walls, otherwise the image processing cannot pick useful features for visual odometry, image matching or ICP. What happens if that penalty is dropped in this navigation agent?

Finally, the authors mention the Neural Map paper but do not discuss Zhang et al. (2017) "Neural SLAM" or Wayne et al. (2018) "Unsupervised Predictive Memory in a Goal-Directed Agent", where a differentiable memory is used to store map information over the course of an episode and can store information relative to the agent's position and objects' / obstacles' positions as well.

Minor remark: the word "finally" is repeated twice at the end of the introduction.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkgdffoi6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Additional Experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyMWn05F7&amp;noteId=rkgdffoi6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1147 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1147 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your comments and suggestions. We address your specific concerns below:

1. Explicit mapping is hand-engineering. We acknowledge (and will explicitly state in the paper) that using occupancy map as the policy input is based on domain/task knowledge. Using the occupancy map gives the agent a better representation of long-horizon memory and show great improvement compared to the policy without the map as input. We do agree ego-motion estimation in real-world might be noisy. To handle that we performed experiments with noise and show that our model seems robust (See video on the website, Fig 4b in the paper).  

With regard to end-to-end approaches, approaches like Zhang et al. (2017) uses a differentiable map structure to mimic the SLAM techniques. These works are orthogonal to our effort on exploration. Indeed, our exploration policy can benefit from their learned maps instead of only using reconstructed occupancy map. We also believe our current approach provides a strong baseline for future end-to-end versions.

2. Explicit environment rewards for exploration: We agree that the use of reward yielding objects throughout the environment will lead to a very similar outcome as our approach. The key distinction is that our approach instruments the agent (with a depth sensor) as opposed to instrumenting the environment. This makes our proposed formulation more amenable to being trained and deployed in the real world: all we need is an RGB-D sensor. This is a big advantage over spreading reward yielding objects that disappear as the agents arrive at those locations, which is almost impractical in the real world. With this key distinction being said, we did do several experiments where our policy is trained with external rewards. The performance is shown in Fig C4(c) in Appendix C4. The results show that our coverage map reward is much more effective than external rewards generated by reward-yielding objects. Our method covers 125m^2 on average while even 4 reward yielding objects per square meter is 91m^2.

3. Role of collision avoidance penalty: We added the performance of the agent trained with our policy but with only coverage reward (no collision penalty) in Fig C4(b) in Appendix C4. We observe that adding collision penalty indeed helps improve performance slightly (125m^2 with penalty as opposed to 120m^2 without penalty). Thus, our policy explores well even without explicit collision avoidance penalty.

We will add more references to the related work and improve the writing as you suggested in the final version of the paper.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkgGSK0Mhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>good paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyMWn05F7&amp;noteId=rkgGSK0Mhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1147 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1147 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes learning exploration policies for navigation. The problem is motivated well. The learning is conducted using reinforcement learning, bootstrapped by imitation learning. Notably, RL is done using sensor-derived intrinsic rewards, rather than extrinsic rewards provided by the environment. The results are good.

I like this paper a lot. It addresses an important problem. It is written well. The approach is not surprising but is reasonable and is a good addition to the literature.

One reservation is that the method relies on an oracle for state estimation. In some experiments, synthetic noise is added, but this is not a realistic noise model and the underlying data still comes from an oracle that would not be available in real-world deployment. I recommend that the authors do one of the following: (a) use a real (monocular, stereo, or visual-inertial) odometry system for state estimation, or (b) acknowledge clearly that the presented method relies on unrealistic oracle odometry.

Even with this reservation, I support accepting the paper.

Minor: In Section 3.4, "existing a room" -&gt; "exiting a room"</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJxWrMsipX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We agree, will Incorporate Feedback into Manuscript</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyMWn05F7&amp;noteId=rJxWrMsipX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1147 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1147 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comments and suggestions. We acknowledge that most of our evaluation is in the perfect odometry setting which is unrealistic. We experimented with a reasonable noise model that compounds over time within the episode, but we admit it may not be very realistic. We will prominently note both these points in the final version of the paper upon acceptance.

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>