<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Simple Black-box Adversarial Attacks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Simple Black-box Adversarial Attacks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rJeZS3RcYm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Simple Black-box Adversarial Attacks" />
      <meta name="og:description" content="The construction of adversarial images is a search problem in high dimensions within a small region around a target image. The goal is to find an imperceptibly modified image that is misclassified..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rJeZS3RcYm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Simple Black-box Adversarial Attacks</a> <a class="note_content_pdf" href="/pdf?id=rJeZS3RcYm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019simple,    &#10;title={Simple Black-box Adversarial Attacks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rJeZS3RcYm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=rJeZS3RcYm" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">The construction of adversarial images is a search problem in high dimensions within a small region around a target image. The goal is to find an imperceptibly modified image that is misclassified by a target model. In the black-box setting, only sporadic feedback is provided through occasional model evaluations. In this paper we provide a new algorithm whose search strategy is based on an intriguingly simple iterative principle: We randomly pick a low frequency component of the discrete cosine transform (DCT) and either add or subtract it to the target image. Model evaluations are only required to identify whether an operation decreases the adversarial loss. Despite its simplicity, the proposed method can be used for targeted and untargeted attacks --- resulting in previously unprecedented query efficiency in both settings. We require a median of 600 black-box model queries (ResNet-50) to produce an adversarial ImageNet image, and we successfully attack Google Cloud Vision with 2500 median queries, averaging to a cost of only $3 per image. We argue that our proposed algorithm should serve as a strong baseline for future adversarial black-box attacks, in particular because it is extremely fast and can be implemented in less than 20 lines of PyTorch code. </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HygHF_6spX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Revision</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJeZS3RcYm&amp;noteId=HygHF_6spX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1514 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1514 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have uploaded a revision with the following changes:

- Significantly improved attack when using the standard basis (SimBA). In particular, Table 1 and Figures 2-4 were updated.
- Supplementary material containing 10 additional sample images for Google Cloud Vision attack.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkxkTpcJTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>simple algorithm, intriguing message</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJeZS3RcYm&amp;noteId=rkxkTpcJTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1514 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1514 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper demonstrates that a simple greedy random search algorithm in DCT space based on score feedback is able to synthesize adversarial examples with quite good query efficiency.  The algorithm is demonstrated on ImageNet  with three common architectures, showing much higher efficiency when sampling from the DCT basis. The algorithm is also shown to outperform state of the art attacks in terms of query count. Finally, a successful attack is demonstrated on Google Cloud Vision.

While not particularly heavy on technicalities, this work does make a couple intriguing points, namely that adversarial attacks can potentially be quite easy to perform due to the inherent nature of high-dimensional classification, and that the space is in which the search is perform might be more important than the sophistication of the search itself. I interpret the proposal not so much as a claim to a state-of-the-art algorithm (even though the results are impressive) but as a very reasonable baseline in the evaluation of attack efficiency -- one might even wonder why it has not been common practice thus far to evaluate against such kinds of algorithms by default. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BklIEDTs6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: simple algorithm, intriguing message</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJeZS3RcYm&amp;noteId=BklIEDTs6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1514 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1514 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review. We are equally surprised that the SimBA attack has not been discovered earlier - and that it can even outperform far more sophisticated approaches. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SJeaN4LihX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>simple and effective blackbox attack based on random directions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJeZS3RcYm&amp;noteId=SJeaN4LihX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1514 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1514 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a simple and effective black box adversarial attack on sota deep nets for image classification tasks. It is based on randomly picking a low frequency component of the DC Transform.  It is claimed to be most efficient when compared to the sota methods in terms of number of queries required for the attack. It is shown that a median of 600 queries for resnet-50 for imagenet dataset, and 2500 for google cloud vision. Due to its simplicity, it is also claimed that the attack is quite simple to implement in code. The paper presents a detailed analysis of their attack in pixel and DCT space, targeted vs untargeted attack, comparison over different architecture such as Densenet, resnet, and inception.

Though the work is quite important and presents a simple and effective baseline black box attack. My concern is primarily on the novelty and originality of the idea, as it is mainly based on the work of Guo etal 2018, which this paper says is the motivation behind their work. So, it is not clear what is the contribution of this paper, as a similar study seems to have been carried out in that paper as well. The authors do not clearly give the relative comparison wrt Guo etal 2018.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkxPHvTiaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: simple and effective blackbox attack based on random directions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJeZS3RcYm&amp;noteId=SkxPHvTiaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1514 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1514 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Comparison with Guo et al. 18: 
There may have been a misunderstanding. We do compare directly to the exact method by Guo et al. It is algorithm “LFBA” in Figure 2 and Table 1. LFBA stands for “Low Frequency Boundary Attack”, which is the terminology used by Guo et al.

Novelty over Guo et al. 18: 
While black-box adversarial examples in DCT space has certainly been studied in Guo et al.,
the core component that makes SimBA drastically more efficient than other black-box attacks is that we take numerous small steps in random orthonormal directions, whether that be random pixel perturbations or the DCT basis. The fact that SimBA (without DCT) is already very competitive compared to all previous untargeted and targeted black-box attacks supports this claim. We consider this insight important to be shared with the community, because it shows that the problem of adversarial attacks may be much simpler than most of us had assumed. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SkesQhRO2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting black-box adversarial attack using DCT basis; performance evaluation on targeted attack is insufficient and threat model is inconsistent (unfair comparison)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJeZS3RcYm&amp;noteId=SkesQhRO2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1514 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1514 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposed a simple query-efficient "score-based" black-box attack based on iteratively perturbing an input image with a direction randomly sampled (w/o replacement) from a set of orthonormal bases. In particular, the authors proposed the use of low-frequency parts of DCT (discrete cosine transformation) as in (Guo 2018) to perform this task. Experimental results on ImageNet and three different classification models demonstrate the query efficiency of the proposed method -- able to achieve high attack success rate within fewer query budgets, where the visual distortion has an L2 norm threshold set to be 10. The authors also demonstrate an untargeted score-based black-box attack on Google CloudVision API.

While the results seem promising, there are several issues that may potentially weaken the query-efficient claims made in this paper, especially due to the lack of sufficient attack comparisons (on smaller datasets) and inconsistent threat models when compared to existing works. My main concerns are summarized as follows.

1. Unfair comparison due to inconsistent threat models (knowledge known to an attacker): the proposed method (simBA) is a "score-based" black-box attack, not a "decision-based" black-box attack. The proposed method assumes knowing the prediction likelihood (or prediction score) as the model output when performing black-box attacks, whereas the compared methods in black-box settings, such as Opt Attack and Boundary Attack, are "decision-based" attack that assumes only knowing the top-1 prediction label. Therefore, the query count comparison is meaningless and unfair, since these two methods require far less information from the model.

On the other hand, ZOO/AutoZOOM is a score-based attack. But ZOO can achieve a very low L2 distortion due to its coordinate descent nature. A fair comparison is to set the same L2 distortion for all score-based methods, and compare the median/avg query counts of each image to reach the same L2 distortion. The comparison to Opt-Attack / Boundary attack makes sense only if the proposed method (simBA) can also perform decision-based attack. Nonetheless, the query count to same-distortion comparison argument still holds. The authors should specify whether simBA can apply to the decision-only attack scenario. If so, how to implement and what is the performance?

Lastly, the QL attack (Ilyas 2018) can perform both score-based and decision-based attacks. So the authors should make the query comparison (to same L2 distortion) as well. According to a recent report (Table-1, NES column) in <a href="https://arxiv.org/pdf/1807.07978.pdf," target="_blank" rel="nofollow">https://arxiv.org/pdf/1807.07978.pdf,</a> the QL-attack has a comparable performance in terms of query counts as reported in this paper.

2. More experiments on targeted black-box attacks: While untargeted attacks on Imagenet is a relatively easy task, I was a bit skeptical on the attacking performance of simBA in targeted attacks - since the selection of low-frequency bases directly limits the search space of adversarial examples, as opposed to arbitrary random directions adopted in QL-attack, Boundary-attack, and Opt-attack. It is also not clear how the target label is chosen in the targeted attack experiment.
I suggest including two more experiments to validate the function of simBA: (i) compare the performance of least-likely targeted attack (ii) show results on smaller datasets such as Cifar-10. As pointed out by the authors, Imagenet has too many image dimensions and make it more vulnerable to attack. Showing attacking results on smaller datasets can properly justify the value of the proposed attack, rather than the benefit from high dimensionality.


3. Novelty relative to LFBA (Guo) should be better differentiated: The idea of using DCT is originated from the LFBA paper. Since in that paper the authors also leveraged low-frequency DCT to perform black-box attacks, it is not clear to me what makes the proposed method perform better than the LFBA paper. The novelty and difference between this paper and the LFBA paper should be addressed.

4. The Google Cloud Vision API attack is not too appealing - the tree label is still there and the trees are obviously present in the picture, while I appreciate the effect of removing the original top-3 labels. Can the authors show another set of non-trivial (more surprising) and targeted-attack experiments? Or simply do the same experiment using the same image (men snowing -&gt; dog)  as in the QL-attack.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bklu_v6spQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: interesting black-box adversarial attack using DCT basis; performance evaluation on targeted attack is insufficient and threat model is inconsistent (unfair comparison)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJeZS3RcYm&amp;noteId=Bklu_v6spQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1514 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1514 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Our detailed response is below. We are happy to include the additional results and comparisons that you ask for, however we do want to emphasize that we think it is generally inappropriate to ask for a comparison with a concurrent submission to the same conference [Ilyas et al. 2018]. 

Detailed response:
We believe there may have been several possible misunderstandings that resulted in R2’s concerns.

1. We agree that the threat model varies across the various baselines. However, our inclusion of boundary attack and Opt attack are not meant to diminish their results, but rather to be comprehensive in our evaluation of prior work. As for other score-based attacks, although it is true that ZOO can sometimes achieve a very low L2 distortion, it also suffers from an order of magnitude higher failure rate (11.1% rather than &lt;2% of SimBA) and requires several orders of magnitude more queries (192,000!!! rather than 1,232 for SimBA-DCT or 1,665 SimBA). In the preprint <a href="https://arxiv.org/pdf/1807.07978.pdf," target="_blank" rel="nofollow">https://arxiv.org/pdf/1807.07978.pdf,</a> the QL-attack is comparable to our attack in terms of query efficiency but at a high failure rate of 41.7%!! We believe that our evaluation is as fair to the other baselines as possible by achieving a lower L2 distortion than all methods other than ZOO and maintaining a success rate close to 100%, while requiring far fewer queries. 

2. We performed targeted attack against random classes, similar to Tu et al. 2018 (https://arxiv.org/abs/1805.11770) and Cheng et al. 2018 (https://arxiv.org/pdf/1807.04457.pdf). We tested our attack against the least likely class as well and found that our method is less efficient but remains very competitive. More precisely, the average query count for SimBA increases from 7,899 to 12,256, while the average query count for SimBA-DCT increases from 9,275 to 17,272.

We also tested our method on CIFAR-10 (targeted attack against the least likely class) and found that restricting to the low frequency basis does not affect the attack’s efficiency. SimBA achieves an average query count of 522 with average L2 norm = 1.41, while SimBA-DCT achieves an average query count of 606 with average L2 norm = 1.60. Both attacks are successful 100% of the time and are very competitive with state-of-the-art attack algorithms such as AutoZOOM. While using low frequency perturbations does not improve the attack for CIFAR-10, it does not hinder the attack’s efficiency either. As for the comment regarding limiting search space, Guo et al. have found that restricting to the low frequency subspace does not hinder adversarial optimality, which is empirically demonstrated in both theirs and our work.

3. The basic SimBA attack uses axis-aligned directions rather than the DCT basis when picking random directions of descent, and provides the majority of the query efficiency compared to other methods (see Table 1). The choice of the DCT basis further improves our attack in the untargeted case and demonstrates that it can generalize to other orthonormal bases, but is not crucial. In this regard, our paper differs significantly from the work of Guo et al.

4. Since our attack does not begin with an image of the adversarial class, it is not designed for targeted attacks on GCV. However, as GCV is the most widely used real world image classification platform for black-box adversarial attacks, we would like to demonstrate the efficacy of our method despite this limitation. Thus, we chose removing the top 3 original classes as a reasonable objective. In comparison with the QL-attack, the query efficiency of our method allows substantially more adversarial images to be created within the same cost budget, and our work is the first to show aggregate statistics for attacking a deployed machine learning service. We included 10 additional random samples in the Supplementary Material. For a non-trivial example, the second image in the Supplementary Material shows a case where a set of camera instruments is misclassified as a weapon after perturbation.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryxurJRsTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarification on NES attack performance (aka QL attack)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJeZS3RcYm&amp;noteId=ryxurJRsTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1514 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1514 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I appreciate the authors' prompt response. But the authors apparently misunderstood my comment 1. I am NOT asking to compare with the new method proposed in an unpublished work (Ilyas 2018 - <a href="https://arxiv.org/pdf/1807.07978.pdf)." target="_blank" rel="nofollow">https://arxiv.org/pdf/1807.07978.pdf).</a> Rather, I am asking the performance comparison with NES attack (it is called QL attack in this paper), which is a published paper at ICML 2018 (https://arxiv.org/pdf/1804.08598.pdf). I did explicitly point out the "NES column" in the table of the unpublished work because it's basically the same setting to be compared with the results presented in this paper. I am very surprised to see the response that "we think it is generally inappropriate to ask for a comparison with a concurrent submission to the same conference", as QL attack was already compared (but in a different setting) in this paper.

</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJeUWhL3TX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: Clarification on NES attack performance (aka QL attack)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJeZS3RcYm&amp;noteId=rJeUWhL3TX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1514 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1514 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We understand that the NES attack is the same as the QL-attack evaluated in our paper. We would like to point out that the evaluation in Ilyas et al. 2018 (<a href="https://arxiv.org/pdf/1807.07978.pdf)" target="_blank" rel="nofollow">https://arxiv.org/pdf/1807.07978.pdf)</a> does not compromise our claim of unprecedented efficiency. Although QL-attack can achieve average query count close to that of our method, its failure rate is extremely high -- 41.7% -- whereas SimBA and SimBA-DCT achieve failure rate of &lt;2% within around the same number of queries.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>