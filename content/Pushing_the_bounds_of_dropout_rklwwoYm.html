<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Pushing the bounds of dropout | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Pushing the bounds of dropout" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rklwwo05Ym" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Pushing the bounds of dropout" />
      <meta name="og:description" content="We show that dropout training is best understood as performing MAP estimation concurrently for a family of conditional models whose objectives are themselves lower bounded by the original dropout..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rklwwo05Ym" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Pushing the bounds of dropout</a> <a class="note_content_pdf" href="/pdf?id=rklwwo05Ym" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019pushing,    &#10;title={Pushing the bounds of dropout},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rklwwo05Ym},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We show that dropout training is best understood as performing MAP estimation concurrently for a family of conditional models whose objectives are themselves lower bounded by the original dropout objective. This discovery allows us to pick any model from this family after training, which leads to a substantial improvement on regularisation-heavy language modelling. The family includes models that compute a power mean over the sampled dropout masks, and their less stochastic subvariants with tighter and higher lower bounds than the fully stochastic dropout objective. We argue that since the deterministic subvariant's bound is equal to its objective, and the highest amongst these models, the predominant view of it as a good approximation to MC averaging is misleading. Rather, deterministic dropout is the best available approximation to the true objective.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Bkg0Rq9Vam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A special interpretation of Dropout. Unfortunately, not convincing.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklwwo05Ym&amp;noteId=Bkg0Rq9Vam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper269 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper269 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Different from an existing variational dropout method which used variational inference to explain Dropout, this paper proposes to interpret Dropout from the MAP perspective. More specifically, the authors utilize the Jensen inequality to develop a lower bound for log-posterior, which is used as training objective for dropout. They then exploit the power mean to develop the conditional power mean model family, which provide additional flexibility for evaluation during validation.
Even though the way how the proposed method is analyzed/generalized is interesting, the proposed method is not convincing, but I am not absolutely sure. Besides the paper is hard to follow, some other concerns are listed below.
(1) “…the original/usual dropout objective” and “the dropout rate” are not defined in the paper, even though they appear many times in the paper.
(2) In the last paragraph of Sec. 2, the authors argue that utilizing their MAP objective “sidestep any questions about whether variational inference makes sense.” However, the presented MAP lower bound has its own problem, since it is derived using the Jensen inequality.  For example, as shown in Appendix C, the equality becomes true only when p(w|\Theta) is a delta function.
(3) How to tune the hyperparameters (alpha, lambda) of the extended dropout family in practice?
(4) The current experiments might be weak. Additional experiments on popular image datasets are recommended.

Minors:
(1) In Eq. (3), is p(w_r|\Theta) of the second formula identical to p(w|\Theta) of the third formula?
(2) In the second row below Eq. (6), E_w p(w|\Theta) p(y|x,w) is a typo.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryerzjWvpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: A special interpretation of Dropout. Unfortunately, not convincing. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklwwo05Ym&amp;noteId=ryerzjWvpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper269 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper269 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Main criticism is that the method not convincing. Does that refer to the theoretical or empirical results or both? What are the areas we should improve?

(1) We'll address this oversight in an upcoming revision.

(2) It is true that all objectives derived in the paper are lower bounds and so is the variational bound. They are also the same. Our goal with the MAP construction is not to have an exact objective but to avoid the question whether it makes sense to use a variational approach purely for regularizing a model (see the beginning of section 2.1 and Osband, 2018).

(3) Alpha and lambda were tuned with a simple grid search (not unlike table 4). We will clarify this in the paper.

(4) In our experiments, imbalance of class label distribution is shown to be the crucial for the Monte-Carlo evaluation method to perform better than deterministic evaluation (see table 2, especially the MNIST results). Thus there is no reason to expect that large image datasets - on which dropout regularization is not a big win to start with - would benefit from our method.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJgOl3ChnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting paper but still lack of novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklwwo05Ym&amp;noteId=rJgOl3ChnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper269 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper269 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper point that the dropout in training is equivalent to MAP estimate of hierarchical models when the prior distribution of weights, \Theta, is a zero mean Gaussian. Based on that observation the authors propose several different evaluation methods for dropout. The experimental results show that the proposed evaluation methods improved the performance of language models.

Are there any experimental results of the proposed evaluation methods for another type of data beyond language modeling?

Do the term "deterministic dropout" in the last sentence of the first paragraph on page 1 and the one in Sec 3 (the first bullet) refer to the same thing? 

Minor: gaussian -&gt; Gaussian</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJlGHsZDTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: Interesting paper but still lack of novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklwwo05Ym&amp;noteId=HJlGHsZDTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper269 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper269 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We feel that the paper has plenty of novelty. Here is a list of our contributions:
- theoretical: dropout training optimizes a common lower bound for a family of models
- theoretical: in the absence of validation data, we should choose the deterministic model, because it has the highest and tightest lower bound
- theoretical: evaluation-time model selection (or lazy hyperparameter tuning)
- empirical: demonstrating that one important side effect of MC dropout is smoothing (i.e. tweaking the softmax temperature)
- empirical: improved language modelling results (even using weights of heavily tuned models)

Currently, we only have language modelling results. Preliminary experiments on MNIST indicated no benefit to our method. In section 4, we provide an analysis and argue that class imbalance is a crucial factor.

Yes, deterministic dropout refers to the same thing at both places.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Sye5tvB53X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Try to provide new perspectives on the problem. But they are not sound. No convincing approach. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklwwo05Ym&amp;noteId=Sye5tvB53X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper269 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper269 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies the problem of making predictions with a model trained using dropout. Authors try to provide a theoretical foundation for using dropout when making predictions. For this purpose, they show that when using dropout training we are maximizing a common lower bound on the objectives of a family of models, including most of the previously used methods for prediction with dropout. 

I find that the paper addresses a relevant problem and try to apply a novel approach. But, in general, I find the paper is not easy to follow and to grasp the main ideas. 

Here I detail my main concerns:


1. This is one of my main concerns. The contraposition between the geometric and the average model. I don't like this contraposition. The average model is just the standard marginalization operation over the weights, $p(y|x) = \int p(y|x,w)p(w|\Theta)dw$. This is the natural solution for the prediction problem to the problem if we accept the generative model given in Eq (3). 

In the case of the variational dropout, we depart from the same generative model, but we employ an approximation. It is the variational approximation the one that induces the geometric mean provided in eq (6). I.e. if we want to compute the posterior over the label y* for a sample x*, after training, we should compute the associated lower bound
$\ln p(y*|x*) &gt;= E_q[\ln p(y*|x*,w)] - KL(q|p)$
In this case, q(w) = p(w|\Theta), as stated Eq (3) and in the corresponding equation provided in page 2 (the q(w) is not learnt because it only depends on the dropout rate, while the $\Theta$ are learnt by maximum log-likelihood and do not have a $q$ associated).  This gives rise to the geometric mean approximation provided in Eq (6).  I.e. the geometric mean prediction is simply the result of using a variational approximation at prediction time.   

My problem here is that authors employ convoluted arguments to introduce this geometric mean prediction and the average prediction, without making the connection discussed above. 

3. Section 3.3 and 3.4 introduces new arguments for modifying the dropout rate (and the alpha) parameter at test time. But, again, I find the arguments convoluted. We consider the dropout rate a hyper-parameter of the model, the standard learning theory tells us to fix the parameters with the training data and evaluate them later when making predictions. Why should we use different dropout rates at training and testing? Authors arguments about the tightness of the bound of Eq (8) and Eq(9). are not convincing to me. 

So, I don't find authors provide convincing answers to the raised questions at the beginning of the paper about the use of dropout when making predictions. 

Minor comments:

1. The generative model for Variational dropout is the same than the generative model for the "conditional model", eq. (3). 

2. In Eq. (7) authors are defining the weighted power mean. I think it would be clearer to directly introduce the weighted power mean instead of the standard power mean in Section 3.2.

3. Section 3.3. I find some parts are difficult to understand. "suppose we pick a base model from the power mean family and have a continuum of subvariants with gradually reduced variance in their predictions but the same expectation." Later, I can understand authors are referring to the possibility of reducing the dropout rate. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJgktsbDpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: Try to provide new perspectives on the problem. But they are not sound. No convincing approach.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklwwo05Ym&amp;noteId=rJgktsbDpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper269 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper269 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank reviewer #1 for their comments.

1. We agree that marginalizing out the weights is the natural thing to do if we accept the generative model. However, due to the training objective being a lower bound on the true objectives of the proposed family of models why would we prefer the arithmetic model to the others? This one of the main points of the paper.

 As to the variational approximation leading to the geometric mean prediction, they are closely related but the variational approximation lacks the normalizing constant Z present in eq 6. We acknowledge this relationship by saying that "oftentimes the geometric average (GMC) is used, because of its close relationship to the loss".

3. There is no disagreement with standard learning theory, in a sense we propose a shortcut: there is no need to train the model for different alpha, lambda choices since these hyperparameters only affect evaluation. This is due to the training objective being a lower bound on the true objectives of the proposed family of models.

It is unclear what is unconvincing about the tightness of the bound argument. In any case, our intention there is to argue that in the absence of validation data, the most natural evaluation method is the deterministic one. However, if there is validation data available, then the tightness of the bound argument is not necessary: tuning alpha, lambda only requires the common lower bound.


In general, regarding the paper not being convincing, sound and the arguments being convoluted, we would like to improve both content and presentation. To that end, we ask for more detailed feedback on these points, if possible.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>