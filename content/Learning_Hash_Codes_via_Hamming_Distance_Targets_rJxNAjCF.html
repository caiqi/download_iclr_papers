<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learning Hash Codes via Hamming Distance Targets | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learning Hash Codes via Hamming Distance Targets" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rJxNAjC5F7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learning Hash Codes via Hamming Distance Targets" />
      <meta name="og:description" content="We present a powerful new loss function and training scheme for learning binary hash codes with any differentiable model and similarity function.&#10;  Our loss function improves over prior methods by..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rJxNAjC5F7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning Hash Codes via Hamming Distance Targets</a> <a class="note_content_pdf" href="/pdf?id=rJxNAjC5F7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019learning,    &#10;title={Learning Hash Codes via Hamming Distance Targets},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rJxNAjC5F7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=rJxNAjC5F7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We present a powerful new loss function and training scheme for learning binary hash codes with any differentiable model and similarity function.
Our loss function improves over prior methods by using log likelihood loss on top of an accurate approximation for the probability that two inputs fall within a Hamming distance target.
Our novel training scheme obtains a good estimate of the true gradient by better sampling inputs and evaluating loss terms between all pairs of inputs in each minibatch.
To fully leverage the resulting hashes, we use multi-indexing.
We demonstrate that these techniques provide large improvements to a similarity search tasks.
We report the best results to date on competitive information retrieval tasks for Imagenet and SIFT 1M, improving recall from 73% to 85% and reducing query cost by a factor of 2-8, respectively.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">information retrieval, learning to hash, cbir</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We present a new loss function for training any differentiable model to hash that can vastly improve recall and lookup speed.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">10 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BJlMlHIJ67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting intuition but still far from a real-world solution</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxNAjC5F7&amp;noteId=BJlMlHIJ67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper885 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper885 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper is about learning to hash. The basic idea is motivated by the intuition: given points z_i and z_j on the hypersphere, the angle between the two points is arccos(z_i \dot z_j), while the probability that a random bit differs between them is arccos(z_i \dot z_j)/\pi. This leads to a nice formulation of learning Hamming Distance Target (HDT), although the optimization procedure requires every input has a similar neighbor in the batch.

The minor issue of this paper is that the writing should be polished. There are numerous typos in paper citing (e.g., Norouzi et al in the 3rd page is missing the reference; Figure 3.2 in the 7th page should be Figure 3; and a number of small typos). But I believe these issues could be fixed easily.

The major issue is how we should evaluate a learning to hash paper with nice intuition but not convincing results. Below are my concerns of the proposed approach.

1. Learning to hash (including the HDT in this paper) and product quantization (PQ) are not based on the same scenario,  so it is unfair to claim hashing method outperforms PQ.

Most learning to hash methods requires two things in the following:
a) the query samples
b) similar/dissimilar samples (or we can call them neighbors and non-neighbor) to the query

PQ does not require a) and b). As a result, in PQ based systems, a query can be compared with codewords using Euclidean distance, without mapping to a hash code. This is important especially for novel queries, because if the system does not see similar samples during training, it will probably fail to map such samples to good hash codes. 

Such advantage of PQ (or other related quantization methods) is important for real-world systems, however, not obvious in a controlled experiment setting. As shown in the paper, HDT assumes the queries will be similar in the training and testing stages, and benefits from this restricted setting. But I believe such assumption may not hold in real systems. 

2. It is not clear to me that how scalable the proposed method is.

I hope section 1.2 can give analysis on both **space** and time complexity of Algorithm 2. It will be more intuitive to show how many ms it will take to search a billion scale dataset. Currently I am not convinced how scalable the proposed algorithm is. 

3. Implementation details
In page 5, it is not clear how the hyper parameters \lamda, \lamda_w and p_0 are selected and how sensitive the performance is. I am also interested in the comparison with [Johnson Dooze Jegou 2017] “Billion-scale similarity search with GPUs”.

4. Missing literature
I think one important recent paper is “Multiscale quantization for fast similarity search” NIP 2017


To summarize, I like the idea of this paper but I feel there are still gap between the current draft and real working system. I wish the submission could be improved in the future.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1gyNgoyTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Addressing your concerns</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxNAjC5F7&amp;noteId=r1gyNgoyTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper885 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper885 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your thoughtful review. We have used HDT in production systems with datasets of over 10 Billion rows and achieved average retrieval times of &lt;2ms, so it is quite practical. We can clear up most of your concerns easily:

1. The comparison against PQ is fair because both are provided only the "train" set from the SIFT1M dataset; neither receives the query samples. As described in section 3.2, we defined similarity within the train set by each elements 10 nearest neighbors in the train set, but this is not additional information.

PQ also "learns" a codebook, relying on the assumption that training and testing datasets will be similar. There may be cases where it better handles novel data points, but HDT performed much better on a common benchmark nonetheless.

2. As mentioned, we have used HDT with datasets of over 10 Billion rows and achieved average retrieval times of &lt;2ms. Expected memory usage for kNN is simply O(k) by streaming through query results. Expected space to store the indexed dataset is O((r+1)Nn), where r is the Hamming radius and n is the number of bits per hash. Since r is practically no more than ~5, and hash bit size is small, this is not much of a concern.

3. We plot different values of \lambda in Figure 4 to demonstrate how the recall/performance tradeoff varies, so that should address part of your question. p_0 is simply chosen such to extrapolate values of likelihood less that 10^{-100}. As \lambda_w is simply a regularization term, we did think it warranted an entire table or plot to compare.

How would you recommend comparing against “Billion-scale similarity search with GPUs”? We are familiar with this work, but its main feature and results are for exact kNN with powerful computing resources, so there is no interesting recall/performance comparison to draw.

4. We have just included this in the latest draft.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkxP4RvF3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Solid idea in the learning to hash area, needs further development</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxNAjC5F7&amp;noteId=rkxP4RvF3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper885 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper885 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: This paper contributes to the area of learning to hash. The goal is to take high-dimensional vectors in R^n resulting from an embedding and map them to binary codewords with the goal of similar vectors being mapped to close codewords (in Hamming distance). The authors introduce a loss function for this problem that's based on angles between points on the hypersphere, relying on the intuition that angles corresponds to the number of times needed to cross to the other side of the hypersphere in each coordinate. This is approximately the Hamming distance under a simple quantization scheme. The loss function itself forces similar points together and dissimilar points apart by matching the Hamming distance to the binomial CDF of the angle quantization. They also suggest a batching scheme that enforces the presence of both similar and dissimilar matches. To confirm the utility of this loss function, the authors empirically verify similarity on ImageNet and SIFT.

Strengths: The main idea, to match up angles between points on the hypersphere and Hamming distance is pretty clever. The loss function itself seems generally useful.

Weaknesses: First, I thought the paper was pretty difficult to understand without a lot of background from previous papers. For the most part the authors don't actually state what the input/output/goals are, leaving it implied from the context, which is tough for the reader. The overall organization isn't great. The paper doesn't contain any theory even for simplified or toy cases (which actually seems potentially tractable here); there is only simple intuition. I think that is fine, but then the empirical results should be extensive, and unfortunately they are not. 

Verdict: I think this work contains a great main idea and could become quite a good paper in the future, but the work required to illustrate and demonstrate the idea is not fully there yet. 


Comments and Questions:

- Why do you actually need the embedded points y to be on the unit hypersphere? You could compute distances between points at different radii. The results probably shouldn't change much.

- There's at least a few other papers that use a similar idea, for example 
Gong et al "Angular Quantization-based Binary Codes for Fast Similarity Search" at NIPS 2012. Would be good to discuss the differences.

- The experimental section seems very limited for an empirical paper. There's at least a few confusing details, noted below:

- The experimental results for ImageNet comparing against other models are directly taken from those reported by Lu et al. That's fine, but it does mean that it's hard to make comparisons against *any* other paper than the Lu paper. For example, if the selected ImageNet classes are different, then the results of the comparison may well be different. I checked the HashNet paper (Cao et al. 2017), and it papers that their own reported numbers for ImageNet are better than those of the Lu et al paper. That is, I see 0.5059 0.6306 0.6835 for 16/32/64 bit codewords vs Lu's result of 0.442 0.606 0.684, which is quoted in this paper. What's causing this difference? It would probably be a bit less convenient but ultimately better if the results for comparison were reproduced by the authors, and possibly on a different class split compared to the single Lu paper.

- The comparison against PQ should also consider more recent works of the same flavor as PQ, which themselves outperform PQ. For example, "Cartesian k-means" by Norouzi and Fleet, or "Approximate search with quantized sparse representations" by Jain et al. These papers also use the SIFT dataset for their experimental result, so it would be great to compare against them.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1ljGMwA3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Addressing your concerns</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxNAjC5F7&amp;noteId=H1ljGMwA3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper885 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper885 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your thoughtful review. We can clear up most of your concerns easily:
* Unfortunately different benchmarks in this field use different optimization criteria, so out of input/output/goals, only output can be clearly defined (and we define it to be a binary hash code). A given problem may choose any specific input, such an image, audio clip, or vector. The goals vary wildly, and our benchmarks used MAP and recall vs. query cost. We will work to make this clearer in the final draft.
* The datasets we compare against are common choices, and we believe they are sufficient (and our % improvements large enough) for the results to be clear.

- Good question. The advantage of comparing the angle between points (rather than Euclidean distance) is that we are able to build a good statistical approximation of the conditional distribution of Hamming distances. Any such model must assume some distribution for the embedding. We believe a uniform distribution on the hypersphere is most natural, since the binarized hash codes depend only on the direction of the embedding point, not its magnitude.
- While Gong et al. also work with a hypersphere, their approach is very different, choosing a projection that optimizes binarization loss, rather than log likelihood of falling within a desired Hamming distance, which we argue better represents the true optimization goal. We will mention them in our final version.
- 
- A public commenter already mentioned this, so please refer to that chain. Cao et al. updated those figures later, after Lu et al. published their paper. We will include the updated figures in our final version. Lu et al.'s results simply compare against Cao et al.'s reported results. Both papers use the same subsets of ImageNet and are directly comparable. We believe using the same dataset splits as previous authors makes our results stronger, showing that we did not cherry-pick the dataset.
- This is a great suggestion.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ByxcPXfOim" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Is it fair to compare the proposed algorithm to existing hashing algorithms and PQ</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxNAjC5F7&amp;noteId=ByxcPXfOim"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper885 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper885 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposed a new hashing algorithm with a new loss function. A multi-indexing scheme is adopted for search.  There is one key issue: in general hashing is not good at multi-indexing search for vector-based search in the Euclidean distance or Cosine similarity. The advantage of hashing is reducing the code size and thus memory cost, but it is still not as good as quantization=based approach. 

Here are comments about the experiments.
(1) Table 1: do other algorithms also use multi-indexing or simply linear scan?
(2)  Figure 4: HDT-E is better than PQ. It is not understandable. Something important is missing. How is the search conducted for PQ? Is multi-indexing used? It is also strange to compare the recall in terms of #(distance comparisons). 

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1l19KUA27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Yes</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxNAjC5F7&amp;noteId=S1l19KUA27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper885 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper885 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">"There is one key issue: in general hashing is not good at multi-indexing search for vector-based search in the Euclidean distance or Cosine similarity.": All multi-index search relies on hash codes, so we are not quite sure what you mean. You may argue that multi-indexing has underperformed on Euclidean or Cosine similarity tasks in the past, but it should be clear from our abstract that our approach (HDT) refutes that.

"The advantage of hashing is reducing the code size and thus memory cost, but it is still not as good as quantization=based approach." - Quantization approaches also create hash codes; to quote Jegou et. al.'s Product Quantization paper, "Formally, a quantizer is a function q mapping a D- dimensional vector x ∈ RD to a vector q(x) ∈ C = {ci;i ∈ I}, where the index set I is from now on assumed to be finite: I = 0...k − 1." Again, as stated in our abstract abstract, our method outperforms Product Quantization on its own benchmark. 

(1). The MAP@1000 criterion is defined based on the top 1000 results by Hamming distance (section 3.1). This is achieved for all models by a linear scan.
(2). The product quantization search is as defined in Jegou et. al.; it does not use multi-indexing. Our HDT-E does use multi-indexing. Recall is not the number of distance comparisons; to quote section 3.2, "Metrics used are SIFT 1M recall@100 vs. number of distance comparisons, a measure of query cost".</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJgytH0atQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Some concerns regarding ImageNet-100 results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxNAjC5F7&amp;noteId=rJgytH0atQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Sep 2018</span><span class="item">ICLR 2019 Conference Paper885 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear authors, I have some concerns for the ImageNet-100 results in your paper:

(1)  Results from other methods such as the popular HashNet are not the same as reported in the original paper (for example 16 bits mAP@1000)

(2) HashNet model use pretrained AlexNet whereas your model use pretrained ResNet 50. While I agree that ResNet is a much better choice, the choice of the model is very important for the final performance. For fair comparison, you should include results from AlexNet.

(3)  Resutls for ImageNet mAP@1000 goes down as number of bits increases, this does not seem right to me.

(4)  What about multi-label datasets such as MS COCO or NUS-Wide? 

Thank you.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Sye0dUFAKX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Addressing your ImageNet-100 concerns</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxNAjC5F7&amp;noteId=Sye0dUFAKX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper885 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Oct 2018</span><span class="item">ICLR 2019 Conference Paper885 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We tried to be as consistent as possible with the literature, and are currently working on adding more comparison datasets (i.e., MS COCO and NUS-Wide).

(1) As we mentioned, our comparison numbers are drawn from the Lu et. al.'s DBR paper. We will switch to use the better of Cao et. al.'s results and Lu et. al.'s.

(2) We chose ResNet 50 since there were already a few models being used in the literature, like DBR-v3 using InceptionV3 (a larger model with higher ImageNet accuracy than ResNet 50). We understand your concern, though, and will update with AlexNet results as soon as possible.

(3) We address this thoroughly at the end of section 3.1.

(4) Our method works for mutli-label datasets as well, since we can use any similarity matrix. As mentioned, we are working on adding more datasets.

Thank you</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HkxzzDtL9Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Inception V3 will be fine</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxNAjC5F7&amp;noteId=HkxzzDtL9Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Oct 2018</span><span class="item">ICLR 2019 Conference Paper885 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your response. I think you should at least include results trained on Inception V3 since this is the one you are mainly comparing to. AlexNet may not be necessary if results on Inception V3 is even better. The choice of ResNet V2 50 just seems odd. No other baseline papers currently use it. Add why not the original ResNet or ResNet-101? It makes it look like a hand picked model. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJxr2gvHom" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Inception V3 results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxNAjC5F7&amp;noteId=rJxr2gvHom"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper885 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Oct 2018</span><span class="item">ICLR 2019 Conference Paper885 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I have run the Inception V3 benchmarks:

16 bits: 85.3% MAP
32 bits: 86.1% MAP
64 bits: 85.1% MAP

As expected, this increased our MAP slightly, making our main result slightly more impressive.
We are still working on adding Alexnet benchmarks.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>