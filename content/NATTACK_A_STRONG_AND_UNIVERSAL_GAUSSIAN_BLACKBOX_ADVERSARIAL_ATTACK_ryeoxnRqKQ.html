<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ryeoxnRqKQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK" />
      <meta name="og:description" content="Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ryeoxnRqKQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK</a> <a class="note_content_pdf" href="/pdf?id=ryeoxnRqKQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019nattack:,    &#10;title={NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=ryeoxnRqKQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of ten recently published defense methods (and greater than 90% for the other four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">adversarial attack, black-box, evolutional strategy, policy gradient</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">20 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_r1lSC_3T2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>original</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryeoxnRqKQ&amp;noteId=r1lSC_3T2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1106 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1106 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this paper, authors propose a "universal" Gaussian balck-box adversarial attack.
Original and well-written (although there are a few grammar mistakes that would require some revision) and structured. Having followed the comments and discussion I am convinced that the proposed method is state of the art and interesting enough fro ICLR.
To the best of my knowledge, the study is technically sound.
It fairly accounts for recent literature in the field.
Experiments are convincing.
One thing I am not so convinced about is the naming of the evaluation curve as "a new ROC curve". I understand the appeal of pairing the proposed evaluation curve with the ROC curve but, beyond an arguable resemblance, they have no much in common, really.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BkeM4gQq2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Questions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryeoxnRqKQ&amp;noteId=BkeM4gQq2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1106 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, it looks very interesting.

However, I have a few questions.

(1) Could you specify the threat model? For example, I could not find what substitute models are used to generate adversarial examples. What black-box setting did you use?

(2) I think you don't actually evaluate your attack on Madry et al. (2018). THERM-ADV did not technically use PGD adversarial examples described in Madry et al. (2018), but use LS-PGA examples described in Buckman et al. (2018). In addition, Athalye et al. (2018) argued that THERM-ADV is significantly weaker than Madry et al. (2018) since it is trained against the LS-PGA attacks.  Therefore, The argument in your paper, "Athalye et al. (2018) find that the adversarial robust training (Madry et al., 2018) can significantly improve the defense strength of THERM." may be wrong.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1xPOxN527" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Regarding thermometer encoding</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryeoxnRqKQ&amp;noteId=B1xPOxN527"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Nicholas_Carlini1" class="profile-link">Nicholas Carlini</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1106 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The statement as it is written is technically correct. Thermometer encoding by itself is no more robust than a standard neural network. Adding adversarial training to thermometer encoding confers some amount of robustness, but less than standard adversarial training.

So whether or not adversarial training can "significantly improve the defense strength of THERM" depends I guess on your definition of "significantly". In Athalye et al. (2018) we find this difference to be ~20% at eps=8 and ~40% at eps=4. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1gc1YNc3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarification</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryeoxnRqKQ&amp;noteId=S1gc1YNc3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1106 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your comment!

I clarify my concern more clear. I was trying to say that THERM-ADV of Buckman et al. (2018)  should not be cited as Madry et al. (2018) for evaluation since standard adversarial training, Madry et al. (2018), is more robust than THERM-ADV. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BylxbaE9nX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Answers to (1) &amp; (2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryeoxnRqKQ&amp;noteId=BylxbaE9nX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1106 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1106 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Regarding (2), thank @Nicholas for the catch! We will cite both (Buckman et al., 2018) and (Madry et al., 2018) in the revised paper. It is worth noting that vanilla PGD does not apply to Therm. As a result, the LS-PGA enhanced Therm-Adv is probably one of the best one can do in order to apply Madry et al. (2018)'s defense principle to Therm. 

Regarding (1), you may consider the Gaussian distribution along with the steps 1--4 in the paper as the threat model. We do not use any substitute network in our approach. The black-box setting: We query a black-box network by an input and obtain its output probability vector.  This setting is as standard as many existing works'.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkleNxHqhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Vanilla PGD training</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryeoxnRqKQ&amp;noteId=SkleNxHqhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1106 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the answer. I still have a question. Vanilla PGD training is more robust than Therm-Adv.
For (2), why do you not use vanilla PGD training instead of Therm-Adv? </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1ekDzBc2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to "Vanilla PGD training"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryeoxnRqKQ&amp;noteId=S1ekDzBc2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1106 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1106 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Since Therm discretizes the input, it prevents one from simply applying the gradient projection in PGD, not mentioning that the gradients are estimated through other methods like BPDA or DGA (Buckman et al., 2018). Did you mean that DGA is a more faithful application of (Madry et al., 2018)'s defense to Therm than LS-PGA? However, DGA has been shown a weak attack so it likely cannot lead to strong defense (e.g., called Therm-Adv-DGA) either.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1eCOHrqhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>standard adversarial training</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryeoxnRqKQ&amp;noteId=H1eCOHrqhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1106 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">You don't have to use the Thermometer encoding anymore. I mean the pure vanilla PGD training which Athalye et al. (2018) were unable to defeat.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJe1uvS52Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Will do</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryeoxnRqKQ&amp;noteId=HJe1uvS52Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1106 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1106 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Oops, I misunderstood your earlier question.. We are running experiments against the vanilla PGD defended CNN. As Athalye et al. (2018) did not release this very strong model, we had to train it ourselves. Actually, we will ask them for that model by email now.. Stay tuned please.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByeeVjH92m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Public code</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryeoxnRqKQ&amp;noteId=ByeeVjH92m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1106 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">FYI, Madry et al. released publicly available pre-trained weights.

<a href="https://github.com/MadryLab/mnist_challenge" target="_blank" rel="nofollow">https://github.com/MadryLab/mnist_challenge</a>
https://github.com/MadryLab/cifar10_challenge</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SklRyhS5nQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Got it</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryeoxnRqKQ&amp;noteId=SklRyhS5nQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1106 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1106 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Got it. Thanks for the pointers! </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_S1g3Hhj_hm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryeoxnRqKQ&amp;noteId=S1g3Hhj_hm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1106 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1106 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: In this paper the authors discuss a black-box method to learn
adversarial inputs to DNNs which are "close" to some nominal example
but nevertheless get misclassified. The algorithm essentially tries to
learn the mean of a joint Gaussian distribution over image
perturbations so that the perturbed image has high likelihood of being
misclassified. The method takes the form of zero-th order gradient
updates on an objective measuring to what degree the perturbed example
is misclassified. The authors test their method against 10 recent DNN
defense mechanisms, which showed higher attack-success rates than
other methods. Additionally the authors looked at transferrability of
the learned adversarial examples.

Feedback: As noted before, this paper shares many similarities with

[1] "Black-box Adversarial Attacks with Limited Queries and Information" (<a href="https://arxiv.org/abs/1804.08598)" target="_blank" rel="nofollow">https://arxiv.org/abs/1804.08598)</a>

and the authors have responded to those similarities in two follow-ups. I have reviewed these results and their 
method does appear to improve over [1]. However, I am still reluctant to admit these additions to the original submission, 
mainly because dropping [1] in the original submission seems to be a fairly major omission of one of the most relevant competitors out there. In its current form, the apparent redundancies distract significantly from the paper, and to remedy this, the paper would have to change significantly in order to relate it properly to [1] clear is needed. I'd be curious on the ACs thoughts on this. 

I appreciate the authors' claim that their method can breach many of the popular defense methods out there, but we 
also see that many of the percentages in Figure 1 converge  to 1. On the one hand this suggests that all defense methods 
are in some sense equally bad, but on the other, it could also just reflect on the fact that the thresholds are chosen 
"too large". I understand that many of the thresholds were inherited from previous work, but it would nevertheless help if the authors showed some example adversarial images to help baseline this Figure.  </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SyepS_mP3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good evaluation but important prior work was missed which substantially reduces novelty and makes a major rewrite necessary</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryeoxnRqKQ&amp;noteId=SyepS_mP3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1106 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1106 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this work the authors use a score-based adversarial attack (based on the natural evolution strategy (NES)) to successfully attack a multitude of defended networks, with success rates rivalling the best gradient-based attacks.

As confirmed by the authors in a detailed and very open response to a question of mine, the attack introduced here is actually equivalent to [1]. While the attack itself is not novel (which will require a major revision of the manuscript), the authors point out the following contributions over [1]:

* Attack experiments here go way beyond Ilyas et al. in terms of Lp metrics, different defense models, different datasets and transferability.
* Different motivation/derivation of NES.
* Concept of adversarial distributions.
* Regression network for good initialization.
* Introduction of accuracy-iterations plots.

My main concerns are as follows:
* The review of the prior literature, in particular on score-based and decision-based defences (the latter of which are not even mentioned), is very limited and is framed wrongly. In particular, the statement “However, existing black-box attacks are weaker than their white-box counterparts” is simply not true: as an example, the most prominent decision-based attack [2] rivals white-box attacks on vanilla DNNs as well as defended networks [3].
* The concept of adversarial distributions is not new but is common in the literature of real-world adversarials that are robust to transformations and perturbations (like gaussian noise), check for example [4]. In [4] the concept of _Expectation Over Transformation (EOT)_ is introduced, which is basically the generalised concept of the expectation over gaussian perturbations introduced in this work.
* While I like the idea of accuracy-iterations plots, the idea is not new, see e.g. the accuracy-iterations plot in [2] (sample-based, Figure 6), the loss-iterations plot in [5] or the accuracy-distortion plots in [3]. However, I agree that these type of visualisation or metric is not as widespread as it should be.

Hence, in summary the main contribution of the paper is the application of NES against different defence models, datasets and Lp metrics as well as the use of a regression network for initialisation. Along this second point it would be great if the authors would be able to demonstrate substantial gains in the accuracy-query metric. In any case, in the light of previous literature a major revision of the manuscript will be necessary.

[1] Ilyas et al. (2018) “Black-box Adversarial Attacks with Limited Queries and Information” (<a href="https://arxiv.org/abs/1804.08598)" target="_blank" rel="nofollow">https://arxiv.org/abs/1804.08598)</a> 
[2] Brendel et al. (2018) “Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models” (https://arxiv.org/abs/1712.04248)
[3] Schott et al. (2018) “Towards the first adversarially robust neural network model on MNIST” (https://arxiv.org/abs/1805.09190)
[4] Athalye et al. (2017) “Synthesizing Robust Adversarial Examples” (https://arxiv.org/pdf/1707.07397.pdf)
[5] Madry et al. {2017) “Towards Deep Learning Models Resistant to Adversarial Attacks” (https://arxiv.org/pdf/1706.06083.pdf)</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HkxngDtniX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Comparison with state-of-the-art</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryeoxnRqKQ&amp;noteId=HkxngDtniX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1106 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">23 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1106 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Could the authors elaborate as to how this attack differs from [1]? As far as I can see this work uses the same gradient estimate with Gaussian bases.

[1] "Black-box Adversarial Attacks with Limited Queries and Information" (<a href="https://arxiv.org/abs/1804.08598)" target="_blank" rel="nofollow">https://arxiv.org/abs/1804.08598)</a></span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJg9cjD6o7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Differences from "Black-box Adversarial Attacks with Limited Queries and Information"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryeoxnRqKQ&amp;noteId=HJg9cjD6o7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1106 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">24 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1106 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">That’s a great catch. Thank you very much! We should have read the paper before… It is intriguing (and yet disappointing for us) to see that a similar approach has been proposed (Ilyas et al. 2018) by also resorting to the natural evolution strategy (NES), but it is not surprising. After all, derivative-free methods, such as NES, REINFORCE, and the zero-th order algorithms, are a natural choice for the blackbox attack. 

While we mainly attack up to 10 recently published defense methods by the proposed approach, Ilyas et al. (2018) focus on attacking a vanilla neural network under the constraints of limited queries and information (e.g., top k entries as opposed to the full output vector). 

On the algorithmic aspect, both ours and Ilyas et al. (2018)’s employ NES as the optimization algorithm. However, we arrive at it via different routes and for different purposes. We assume a probabilistic generation process of the adversarial examples (Steps 1–4, Section 2), which finds an adversarial example by a one-step addition to the input. In contrast, Ilyas et al. (2018)’s modeling assumption is that an adversarial example can be found by PGD, which iteratively updates the original input with a small learning rate until it becomes adversarial. To this end, we use NES to estimate the parameters of the distribution, while Ilyas et al. (2018) use NES to replace the true (stochastic) gradients in PGD. We contend that, due to the non-differentiable clip and projection operations and the fairly large Gaussian covariance, NES is *not* an efficient (and possibly a biased) estimator of the true gradients — we are running experiments to empirically verify if this is true or not.  

It is a conceptual change from the traditional attack methods (e.g., PGD) to the way of modeling the adversarial examples by a distribution. This change may enable some exciting future works. For instance, we can draw samples from the distribution to characterize the adversarial boundaries, efficiently do adversarial training, etc.

Another notable difference from (Ilyas et al. 2018)’s is that we train a regression neural network to find a good initialization for NES. Experiments verify the benefit of this regression network. 

On the experimental aspect, we attack the recently proposed defense methods following the protocols set up in the original papers. As a result, we experiment with both CIFAR10 and ImageNet, both the $\ell_2$ and $\ell_infty$ distances, and different types of defenses (e.g., input randomization and discretization, ensembeling, denoising, etc.). In contrast, Ilyas et al. (2018) experiment with ImageNet with an $\ell_\infty$ distance. In addition, we examine the adversarial examples’ transferabilities across different defense methods. Unlike the findings about the transferability across vanilla neural networks, our results indicate several unique characteristics of the transferability of our adversarial examples for the defended neural networks (cf. Section 3.3). Finally, we plot the curves of the attack success rates versus the iteration numbers, a new evaluation scheme which is complementary to the final attack success rates. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkxNNaW_hX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Experimental comparison with "Black-box Adversarial Attacks with Limited Queries and Information" </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryeoxnRqKQ&amp;noteId=BkxNNaW_hX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1106 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1106 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">With the open-source code released by Ilyas et al. (2018), we have evaluated their method on attacking three defense methods: SAP and Therm for CIFAR10 and Randomization for ImageNet. The results (success rate vs. number of optimization iterations) are shown in the tables below. We have also tested larger sample size and higher number of iterations for NES, and yet the results remain about the same. 

Ilyas A, Engstrom L, Athalye A, Lin J. Black-box Adversarial Attacks with Limited Queries and Information. arXiv preprint arXiv:1804.08598. 2018 Apr 23.

The inferior attacking results of (Ilyas et al., 2018) verify our conjecture above, i.e., due to the non-differentiable clip and projection operations and the fairly large Gaussian covariance, NES is *not* an efficient (and possibly a biased) estimator of the true gradients of PGD. As a result, NES is not able to approach PGD’s strong attack  performance. 

Table 1: Success rate on attacking SAP (CIFAR10)
# of iterations           30       90       150      210       270     300      360      400
Ours                          45.13  96.21  99.00   99.54   99.81   100      100      100
Ilyas et al. (2018)'s  33.36  34.51  36.03   37.36   37.36   37.36   37.36   37.36

Table 2: Success rate on attacking Therm (CIFAR10)
# of iterations            30       90       150      210       270     300       360     400
Ours                          67.38   96.38  98.92   99.53   99.74   99.89   100     100
Ilyas et al. (2018)'s  59.22   83.32  83.82   84.32   85.33   85.33   85.33   85.33

Table 3: Success rate on attacking Randomization (ImageNet)
# of iterations          30       90       150      210       270    300    360    400
Ours                         21.54  78.58  90.02   95.41   95.5    95.5   95.5   95.5
Ilyas et al. (2018)'s  3.33    4.56    6.77     8.5       8.5      8.5     8.5     8.5
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkx94ECt2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Please specify the exact differences</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryeoxnRqKQ&amp;noteId=Bkx94ECt2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1106 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1106 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I appreciate the additional experiments, thanks! Could you specify exactly what the difference between NES and your attack is? If I understand you correctly, then the difference is (1) you are using a smaller standard deviation for sampling and (2) you don't perform clipping. However, the standard deviation is merely a hyperparameter of NES and should be tuned for optimal attack efficiency. Second, the clipping is necessary in any real world scenario where you don't have full access to the model but can only query it with images, right?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkgZeTW5hX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarification</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryeoxnRqKQ&amp;noteId=BkgZeTW5hX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1106 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1106 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for asking. We will clarify our previous responses (mainly the paragraph below) by answering three of your questions.

----------------------------------------
On the algorithmic aspect, both ours and Ilyas et al. (2018)’s employ NES as the optimization algorithm. However, we arrive at it via different routes and for different purposes. We assume a probabilistic generation process of the adversarial examples (Steps 1–4, Section 2), which finds an adversarial example by a one-step addition to the input. In contrast, Ilyas et al. (2018)’s modeling assumption is that an adversarial example can be found by PGD, which iteratively updates the original input with a small learning rate until it becomes adversarial. To this end, we use NES to estimate the parameters of the distribution, while Ilyas et al. (2018) use NES to replace the true (stochastic) gradients in PGD. We contend that, due to the non-differentiable clip and projection operations and the fairly large Gaussian covariance, NES is *not* an efficient (and possibly a biased) estimator of the true gradients — we are running experiments to empirically verify if this is true or not.  
--------------------------------------------------------------

== Q1: specify exactly what the difference between NES and your attack is? ==

Using our notation, the pseudo code below sketches our algorithm and Ilyas et al. (2018)’s.

Ours, which searches for the Gaussian from which more than one adversarial examples can be generated.
Iterate until convergence:
1. Draw a sample {\epsilon} from the normal distribution
2. Transform it to a sample of Gaussian by {z=\theta + \sigma * \epsilon}
3. Generate current adversarial examples from {z} by steps 1–4
4. Compute the losses {J(z)}
5. Compute the search gradients {g} by equation (5)
6. Update the Gaussian mean: \theta = \theta - r * g 
Return \theta

Ilyas et al. (2018)’s, which searches for a single adversarial example.
Iterate until convergence:
1. Draw a sample {e} from the normal distribution
2. Transform it to a sample of zero-mean Gaussian by {z=0 + \sigma * \epsilon}
3. Generate current adversarial examples by {x + z} and {x - z}
4. Compute the losses {J(z)}
5. Compute the search gradients {g} by equation (5)
6. x = Projection(x - r * sign(g))
Return x


The differences start from the second line, where we transform the normal sample to a sample of the Gaussian N(\theta, sigma^2) while Ilyas et al. (2018) transform it following a zero-mean Gaussian N(\theta, sigma^2). 

Line 3: The difference is on how to generate the adversarial examples. 

Line 4: Slightly different loss functions are used in the two methods. This is not vital. 

Line 5 is the same for the two methods. 

Line 6: While we update the Gaussian mean by a gradient descent step, Ilyas et al. (2018) update the adversarial example by PGD.

== Q2: a smaller standard deviation for sampling ==
By using the same setting for the NES component of our algorithm and Ilyas et al. (2018)’s, including the same sample size and standard deviation, we obtain the comparison results below. Ours still performs better. We will complete the experiments with all the defense methods studied in our paper. 

Table 1: Success rate on attacking Randomization (ImageNet)
# of iterations          30       90       150      210       270    300     360     400
ours                         21.54  78.58  90.02   95.41   95.5    95.5    95.5    95.5
Ilyas et al. (2018)'s  20.5    46.37  53.33   53.33   53.33  53.33  53.33  53.33 


== Q3: you don't perform clipping ==
We did perform clipping in steps 1--4 of the paper, where we generate adversarial examples from a Gaussian distribution. In contrast, Ilyas et al. (2018)’s performs the clipping of gradients due its employment of PGD attack. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_B1e2hLFnoX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Difference to state of the art</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryeoxnRqKQ&amp;noteId=B1e2hLFnoX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1106 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">23 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1106 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Could the authors elaborate as to how this attack differs from [1]? As far as I can see this work uses the same gradient estimate with Gaussian bases.

[1] "Black-box Adversarial Attacks with Limited Queries and Information" (<a href="https://arxiv.org/abs/1804.08598)" target="_blank" rel="nofollow">https://arxiv.org/abs/1804.08598)</a></span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rklrRiqY5Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Code released: https://github.com/gaussian-attack/Nattack</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryeoxnRqKQ&amp;noteId=rklrRiqY5Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1106 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Code released: <a href="https://github.com/gaussian-attack/Nattack" target="_blank" rel="nofollow">https://github.com/gaussian-attack/Nattack</a></span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>