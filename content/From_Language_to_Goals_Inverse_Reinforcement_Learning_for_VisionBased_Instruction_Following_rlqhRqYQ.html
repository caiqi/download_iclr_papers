<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=r1lq1hRqYQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="From Language to Goals: Inverse Reinforcement Learning for..." />
      <meta name="og:description" content="Reinforcement learning is a promising framework for solving control problems, but its use in practical situations is hampered by the fact that reward functions are often difficult to engineer...." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_r1lq1hRqYQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following</a> <a class="note_content_pdf" href="/pdf?id=r1lq1hRqYQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019from,    &#10;title={From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=r1lq1hRqYQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=r1lq1hRqYQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Reinforcement learning is a promising framework for solving control problems, but its use in practical situations is hampered by the fact that reward functions are often difficult to engineer. Specifying goals and tasks for autonomous machines, such as robots, is a significant challenge: conventionally, reward functions and goal states have been used to communicate objectives. But people can communicate objectives to each other simply by describing or demonstrating them. How can we build learning algorithms that will allow us to tell machines what we want them to do? In this work, we investigate the problem of grounding language commands as reward functions using inverse reinforcement learning, and argue that language-conditioned rewards are more transferable than language-conditioned policies to new environments. We propose language-conditioned reward learning (LC-RL), which grounds language commands as a reward function represented by a deep neural network. We demonstrate that our model learns rewards that transfer to novel tasks and environments on realistic, high-dimensional visual environments with natural language commands, whereas directly learning a language-conditioned policy leads to poor performance.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">inverse reinforcement learning, language grounding, instruction following, language-based learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We ground language commands in a high-dimensional visual environment by learning language-conditioned rewards using inverse reinforcement learning.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SkgNkygonm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Limited technical merit and significance</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lq1hRqYQ&amp;noteId=SkgNkygonm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1007 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1007 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper applies IRL to the cases of multiple tasks/environments and multimodal input features involving natural language (text) and vision (images). It is interesting to see the better performance of their proposed approaches with language-conditioned rewards over language-conditioned policies. The paper is written well.

I view the technical contributions of this work to be at best incremental; it does not seem to address any significant technical challenge to be able to integrate the various known tools in their work. I am not able to learn as much as i would have liked from this paper.

Considering the use of deep learning that can handle highly complex images and text, the practical significance of this work can be considerably improved by grounding their work in real-world context and/or larger-scale environments/tasks, as opposed to simulated environments in this paper. See, for example,

M. Wulfmeier, D. Rao, D. Z. Wang, P. Ondruska, and I. Posner, Large-scale cost function learning for path planning using deep inverse reinforcement learning, The International Journal of Robotics Research, 2017. 

The authors say that "The work of MacGlashan et al. (2015) requires an extensively hand-designed, symbolic reward function class, whereas we use generic, differentiable function approximators that can handle arbitrary observations, including raw images." What then is the implication on how their proposed IRL algorithm is designed differently? How would the algorithm of MacGlashan et al. (2015) empirically perform as compared to the authors' proposed approach?



Minor issues

Page 2: a comparison to in Section 6 to as an oracle?
Page 3: What is rho_0?
Page 7: In order compare against?
Page 7: and and indicator on?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJx-ECNJRQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lq1hRqYQ&amp;noteId=rJx-ECNJRQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1007 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1007 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the review. We have responded to the concerns raised below by adding a comparison to another recent work that learns from raw images, clarifying the complexity of our task, and clarifying the contributions. We would appreciate it if the reviewer could revisit their review and let us know if any other concerns remain.

“What then is the implication on how their proposed IRL algorithm is designed differently? How would the algorithm of MacGlashan et al. (2015) empirically perform as compared to the authors' proposed approach?”
We added a comparison to another work (Bahdanau et al. (2018)) in Table 1 in Section 6.2, and to a modified version of GAIL (Ho et. al 2016), to provide a point of comparison. To our knowledge, this approach is closest in terms of problem statement, in that it also operates on raw RGB images. We unfortunately cannot compare to MacGlashan et al., due to a difference in the basic problem assumptions. The work of MacGlashan et al. require predefined  notions of “colors”, “objects”, “rooms” as well as semantic relationships between entities such as “roomIsRed” or “objectInRoom” to be given to the algorithm. In contrast, our method operates directly from raw text and image observations, making it easier to apply in practice without domain-specific engineering. We believe MacGlashan et. al is incomparable as it operates on significantly stronger assumptions, but we hope the baselines we have included are sufficient.

“I view the technical contributions of this work to be at best incremental; it does not seem to address any significant technical challenge to be able to integrate the various known tools in their work. I am not able to learn as much as i would have liked from this paper.”
As noted by R3,  the main technical challenge we address is to show the effectiveness of transferring reward over transferring policies across environments. Most works in IRL train and test within the same environment and do not concern themselves with generalization across environments. We provide a conceptually simple approach to a well-studied and important problem (language-based instruction following) that is relatively hyperparameter stable (we have included an ablation study over architectures) and easy to train. To our knowledge no previous works have applied deep inverse reinforcement learning to instruction-following tasks. 

“Considering the use of deep learning... larger-scale environments/tasks, as opposed to simulated environments in this paper.”

Pertaining to the scale of our dataset, it contains 1413 tasks, and we use 10 demonstrations per task (14130 demonstrations total). Each task on average contains ~1200 states (for a total of ~1.7m states). As a comparison “Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments” Anderson et. al. uses  7,189 demonstrations. Wulfmeier et. al reports 25,000 demonstrations used, but we could not find the number of states in their tasks.

SUNCG is a commonly used dataset because it contains relatively realistic scenes. It is not a trivial or toy simulated task, but is commonly used for 3D visual navigation research in complex scenes (for example, FollowNet, Shah et. al 2018,  MINOS, Savva et. al. 2017). We believe that our tasks based on SUNCG are more realistic than many prior works on instruction following.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkxKz3BtnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting paper, but results are not very convincing.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lq1hRqYQ&amp;noteId=rkxKz3BtnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1007 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1007 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Paper Summary: This paper studies the inverse reinforcement learning problem for language-based navigation. Given the panorama image as its observation, language embedding as its goal, a deep neural network architecture is proposed to predict the reward function from the input observation and goal. Maximum causal entropy IRL has been adopted to learn such language-conditioned reward function. This paper used the SUNCG environment for experiments and designed two tasks (navigation and pick-and-place) for evaluation.

==
Novelty &amp; Significance:
This paper studies a very interesting topic in reinforcement learning and the problem has potential usage when training robot agent in the real world.

==
Quality:
Overall, reviewer feels that the experimental results are not very strong. Some of the points are not clearly presented. 

Firstly, is not very clear whether the argument made in the abstract “directly learning a language-conditioned policy leads to a poor performance” is justified or not. Please clarify this point in the rebuttal.

Secondly, Table 1 and Table 2 can only be treated as ablation studies. The “reward-regression” is not a baseline but more about a oracle model. 
Is it possible to compare against some recent work such as Tung et al 2018 or Bahdanau et al 2018? Otherwise, it is not very clear whether the proposed approach is the state-of-the-art or not.

Thirdly, using the panorama image as observation seems not a practical choice. Is it possible to provide some ablation studies or discussions on the performance over number of views? 

Finally, the architecture design is not well-justified. Why not using pre-trained image classifiers (or DQN encoder) as feature extractor (or finetune the model from pre-trained image classifier)? The actual resolution (32 x 24 x 3) in the paper looks a bit unusual. 

One more thing, the url provided in the paper directs to an empty project page. 

If these concerns can be addressed in the rebuttal, reviewer is happy to re-evaluate (e.g., raise scores) this work.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Sygm9a4y0m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We have included additional experiments as requested</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lq1hRqYQ&amp;noteId=Sygm9a4y0m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1007 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1007 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the detailed review. We have run additional experiments as requested, and respond to the concerns raised below.

“Firstly, is not very clear whether the argument made in the abstract  “directly learning a language-conditioned policy leads to a poor performance” is justified or not “
This claim is supported by our experiments against the behavioral cloning baseline. We have also added an additional comparison against DAgger (table 1, section 6.2), which is the strategy used by several previous works. For this baseline, we regress directly onto the optimal actions at every single state (which we compute using an oracle), and thus we believe this is an upper-bound on performance for a policy-based method. This baseline is stronger than supervised learning from language and demonstration pairs, as used by papers mentioned in the related work, and for methods such as DAgger and active learning, because every single state in the environment is part of the training set.

“Is it possible to compare against some recent work such as Tung et al 2018 or Bahdanau et al 2018? Otherwise, it is not very clear whether the proposed approach is the state-of-the-art or not.”
We have included a baseline against Bahdanau et. al. 2018 and a variant which uses an exact policy solver, as the original Bahdanau paper uses an RL solver as an inner-loop for policy optimization, rather than planning/dynamic programming. The results are shown in Table 1 in Section 6.2. Our method achieves substantially better performance compared to Bahdanau et. al 2018 in this comparison.

“Thirdly, using the panorama image as observation seems not a practical choice. Is it possible to provide some ablation studies or discussions on the performance over number of views? “
We have updated section 6.1 to include discussion of the choice of panoramic images.  In our experiments, we did notice a significant degradation in performance without the panorama observations, because the agent can sometimes pick/place up objects without seeing the goal location, which confuses the reward regressor. In practice, however, a panorama image can be obtained on a robot by having multiple cameras mounted on a robot (which many robots already have). Due to time constraints, we did not have a chance to run this ablation study, but we will include it in the final version.

“Finally, the architecture design is not well-justified. Why not using pre-trained image classifiers (or DQN encoder) as feature extractor (or finetune the model from pre-trained image classifier)?”
We have updated appendix A to show an ablation study over architectures. We did not find that changing the architecture led to significant performance differences, and thus we believe the architecture is not the main bottleneck in performance. We decided to pick a simple architecture which at the same time performed well.

“The actual resolution (32 x 24 x 3) in the paper looks a bit unusual. “
We did not see a performance difference using different resolutions (we tried 32x24 and 64x64) so we used smaller images for computational speed. We have included our numbers in the appendix, section A. Our resolution was based on parameters used in previous work on a similar environment (Shah et. al 2018).
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ByxaK7Xt2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Comparison to Imitation Learning (not just naive BC)!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lq1hRqYQ&amp;noteId=ByxaK7Xt2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1007 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1007 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:

This paper proposes learning reward functions via inverse reinforcement learning (IRL) for vision-based instruction following tasks like "go to the cup". The agent receives the language instruction (generated via grammar templates) and a set of four images (corresponding to four cardinal directions) from virtual cameras mounted on the agent as input at every time step and its aim is either to 1. navigate to the goal location (navigation task) or 2. move an object from one place to another (pick task). 

The really interesting part in this paper is learning reward functions such that they generalize across different tasks and environments (e.g. indoor home layouts). This differentiates it from the standard IRL setting where reward functions are learnt and then policies optimized on this reward function on the *same* environment. 

In order to generalize across tasks and environments a slight modification to the max-ent IRL gradient equations are made: 1. Similar to meta-learning the gradient is taken with respect to multiple tasks (in a sampling-based manner) and 2. Making the reward function a function of not just states and actions but also language context. The overall algorithm (Algorithm 1) is simple and the critical step of computing an optimal policy to compute the IRL gradient is done by assuming that one has access to full state and dynamics and essentially running a planner on the MDP. This assumption is not unreasonable since in a simulator one has access to the full dynamics and can hence one can compute the optimal trajectories by planning. 

Experiments are presented on the SUNCG dataset of indoor environments. Two baselines are presented: One using behavior cloning (BC) and an oracle baseline which simply regresses to the ground truth reward function which is expected to be an upper bound of performance. Then DQN is used (with and without reward shaping) using the learnt reward functions to learn policies which are shown to have better performance on different tasks. 

Comments and Questions:

- The paper is generally well-written and easy to understand. Thanks!

- The idea of using IRL to learn generalizable reward functions to learn policies so as to aid transfer between environments in such vision-language navigation tasks is interesting and clearly shows benefits to behavior cloning.

- One of my main concerns (and an interesting question that this paper naturally raises) is how does this approach compare to imitation learning (not vanilla behavior cloning which is straight-up supervised learning and has been theoretically and empirically shown to have worse performance due to distribution shifts. See Ross and Bagnell, 2011, Ross, Gordon, Bagnell 2012 (DAgger, Ross and Bagnell 2014 (AggreVate), Chang et al., 2015 (LOLS), etc). If the same budget of 10 demonstrations per environment is used via DAgger (where say each iteration of DAgger gets say 2 or 3 demonstrations until the budget is exhausted) how does it compare? Note online version of DAgger has already been used in similar settings in "Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments" by Anderson et al, CVPR 2018. The main difference from their setting is that this paper considers higher level tasks instead of taking as input low-level turn-by-turn language inputs. 

- The following papers are relevant and should be cited and discussed:
"Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments" by Anderson et al, CVPR 2018.

"Embodied Question Answering", Das et al, CVPR 2018.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkgTwpVkCm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your constructive comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lq1hRqYQ&amp;noteId=BkgTwpVkCm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1007 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1007 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comments and pointing out additional related work. We agree that the main contribution of the paper was to demonstrate the effectiveness of reward learning and its ability to transfer better across visually complex environments than other imitation learning approaches. We have also included a baseline against DAgger and additional baselines against GAIL-based approaches in table 1, section 6.2.

“One of my main concerns …. is how does this approach compare to imitation learning”
Our original behavioral cloning baseline (table 1, section 6.2) runs supervised learning to match the optimal policy (computed via planning/DP) at all states, and not from demonstrations. We believe this provides an upper bound on the performance of any policy-based method, as it is receiving expert advice at every state (in contrast with DAgger-based approaches which only receive this at queried states), and thus distribution shift is not a large concern. Perhaps the name "behavioral cloning" - we have relabeled this baseline as “optimal policy cloning”.

We have updated the related work section to include additional discussion of the works the reviewer has mentioned mentioned. Anderson et. al , as mentioned, concentrates on lower level tasks and policy-based learning. We believe our policy-learning baseline is the strongest baseline we could make for that class of methods and thus provides an accurate comparison, since imitation learning methods do not typically assume access to optimal actions at each state. This is unrealistic, but we believe it provides an upper bound on the performance of any direct imitation method. Das et. al. is also certainly related although they consider a different task based on question-answering. Both of these methods also adopt policy-learning approaches, rather than learning rewards.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>