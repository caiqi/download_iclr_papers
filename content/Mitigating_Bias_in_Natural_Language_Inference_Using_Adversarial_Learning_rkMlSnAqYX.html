<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Mitigating Bias in Natural Language Inference Using Adversarial Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Mitigating Bias in Natural Language Inference Using Adversarial Learning" />
        <meta name="citation_author" content="Yonatan Belinkov" />
        <meta name="citation_author" content="Adam Poliak" />
        <meta name="citation_author" content="Stuart M. Shieber" />
        <meta name="citation_author" content="Benjamin Van Durme" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rkMlSnAqYX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Mitigating Bias in Natural Language Inference Using Adversarial..." />
      <meta name="og:description" content="Recognizing the relationship between two texts is an important aspect of natural language understanding (NLU), and a variety of neural network models have been proposed for solving NLU tasks...." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rkMlSnAqYX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Mitigating Bias in Natural Language Inference Using Adversarial Learning</a> <a class="note_content_pdf" href="/pdf?id=rkMlSnAqYX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=belinkov%40seas.harvard.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="belinkov@seas.harvard.edu">Yonatan Belinkov</a>, <a href="/profile?email=azpoliak%40cs.jhu.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="azpoliak@cs.jhu.edu">Adam Poliak</a>, <a href="/profile?email=shieber%40seas.harvard.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="shieber@seas.harvard.edu">Stuart M. Shieber</a>, <a href="/profile?email=vandurme%40cs.jhu.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="vandurme@cs.jhu.edu">Benjamin Van Durme</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Recognizing the relationship between two texts is an important aspect of natural language understanding (NLU), and a variety of neural network models have been proposed for solving NLU tasks. Unfortunately, recent work showed that the datasets these models are trained on often contain biases that allow models to achieve non-trivial performance without possibly learning the relationship between the two texts. We propose a framework for building robust models by using adversarial learning to encourage models to learn latent, bias-free representations. We test our approach in a Natural Language Inference (NLI) scenario, and show that our adversarially-trained models learn robust representations that ignore known dataset-specific biases. Our experiments demonstrate that our models are more robust to new NLI datasets. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">natural language inference, adversarial learning, bias, artifacts</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Adversarial learning methods encourage NLI models to ignore dataset-specific biases and help models transfer across datasets.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rkerAh4SpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkMlSnAqYX&amp;noteId=rkerAh4SpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1512 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1512 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HkebNhkSpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Potentially withdrawing submission from ICLR later today</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkMlSnAqYX&amp;noteId=HkebNhkSpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1512 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1512 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank the reviewers for the time they put into reviewing our work and their thoughtful feedback. Later today, we are planning to withdraw our paper from ICLR to allow more time to improve our work and presentation based on insightful comments and suggestions from the reviewers. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ByxND1Sq2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A great attempt but not clear experiment and unsatisfactory impact yet</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkMlSnAqYX&amp;noteId=ByxND1Sq2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1512 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1512 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes two adversarial learning frameworks to make a neural entailment more robust against the annotation artifacts or biases. It is clearly written and easy to understand. Especially, I am very impressed by the survey on previous works which covers a lot of recent studies on biases in NLI dataset and robust NLI models. And, this leads to a clear motivation of the work and a well-defined problem setting. 
However, weakness of this paper is the lack of detailed analysis in the experiment and unclear impact of it. Here are more specific reasons for that. 

# unclear evidence for the generalization of the model 
Authors mention a possibility of generalization of the proposed techniques to other tasks, but without any empirical evidence. Would you please conduct a simple experiment on synthetic data which are artificially generated by a certain degree of bias you set. Then, as the bias degree increases, the proposed method needs to show linear improvements against the bias. If this is true, I would believe the generalization capability of the proposed model. Otherwise, given the ambiguous observation from the absolute accuracy improvements between the baseline and the proposed model, I can’t believe it so. 

# lack of explanation for the relevance to active/transfer learning
Removing the bias in the training data seems to be relevant to active learning setting which finds informative points from the training data in general ML. Have you found any theoretical relevance of your adversarial training model with active learning? Otherwise, at least authors need to refer some recent studies. How is the adversarial training of two objectives analogous to the sampling mechanisms in active learning?

# lack of further analysis of between bias and performance
You propose an architecture that can remove the bias of data. However, I can’t find any following analysis between the degree of biases and the improvements in performance. For example, at least authors need to show how much bias each of the target NLI datasets have. What happened if the dataset has no bias? What parts of the proposed model can handle/control this? Such a lack of details in the analysis makes me difficult to believe the robustness of the model. For instance, in the last paragraph of Section 6, the authors didn’t provide a clear effect of hyperparameters yet. 

# transfer setting is not an answer to the hypothesis
The most tricky part to understand is the Experiment section. The accuracy difference between the baseline and the proposed adversarial model doesn’t give the answer for the question of the authors, whether the proposed architecture actually removes the (hypothesis) bias in NLI dataset. Do authors perform the same experiment with the training and testing data from the same type of dataset for each? If then, how much improvement does the model achieve? How does your transfer setting -- training only with the SNLI dataset and testing on other datasets -- provide an evidence of your model that actually removes the data bias in NLI dataset and appropriately deals with them in the testing set? I don’t get quite convinced how transfer setting could be a practical solution for this problem. What is your clear conclusion here? By removing the bias in the dataset, do you actually make a better performing model than the baselines or only show that the two proposed architectures could possibly remove a few biases without a clear logic behind it? In order to better answer the original hypothesis you made, in my opinion, you need a clear dataset that does not contain any biases or some of the biases with a certain degree that you can control, and then show the correlation between the degree and the improvement (i.e., accuracy or whatever). 

# little or ambiguous impact of the result
The accuracy difference in Table 1 seems to be very minor and difficult to understand it due to lack of additional information about (1) how much bias each target dataset has, (2) a statistical test, and (3) real examples where the adversarial model can only answer. The absolute accuracy scores don’t include much information to provide any scientific observation of your original hypothesis. Especially, please provide a few examples where the hypothesis biases are actually resolved by the given architecture.

# some questions on the model design
The double/single classifier designs only penalize the hidden representation of the f_H to remove the hypothesis bias. However, as the authors mentioned, the key part of the NLI task is appropriately finding the proper mapping function (g_NLI) between two sentences. How do you guarantee that the adversarial losses (i.e. L_Adv, L_RanAdv) actually remove the hypothesis bias while preserving the original meaning representation and so not hurting the performance in g_NLI?

InferSent (Conneau et al., 2017) is not the state-of-the-art (SOTA) model for the tasks. Are the accuracy difference in Table 1 similar to other SOTA baselines? I know this might be an unnecessary question because authors like to show a proof-of-concept system of the proposed architecture instead of proposing another SOTA model. But, I am just curious about how the adversarial loss could be independent of other types of complicated classifier function (i.e. g_NLI). 

The proposed architecture is not actually making the original g_NLI more robust, but it (probably) hurts the performance of it, even though it resolves the hypothesis bias in some sense. So, this is not a robust model but a trade-off design at this moment. Would you provide some experimental results that show improvements on the same training/testing data for each type of dataset? 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1gaH-DQpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>1/2: Thank you for detailed review. Highlighting some analysis and description of different biases</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkMlSnAqYX&amp;noteId=r1gaH-DQpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1512 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1512 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your detailed review. We are happy that you liked the general idea and that you appreciate our large survey on previous works. We would like to highlight some of our analysis and description of different biases in the different datasets that you might have missed.

(1) “ Would you please conduct  a simple experiment on synthetic data which are artificially generated by a certain degree of bias you set”

We like the idea of conducting a simple experiment using synthetic data created with specific known biases, and plan to add such an experiment. 

(2) “unclear evidence for the generalization of the model”, “Otherwise, given the ambiguous observation from the absolute accuracy improvements between the baseline and the proposed model, I can’t believe it so.“

We do not rely just on the absolute accuracy improvements between the baseline and the proposed model as evidence for the generalization of the model. We believe the analysis in Table3 further demonstrates the generalizability of our approach since we see that the models trained in this adversarial method do not give indicator words in the hypotheses (from [1]) as much weight as before, i.e. the models learn to not attribute as much weight to indicator words in the hypotheses.

(3) “Removing the bias in the training data seems to be relevant to active learning setting which finds informative points from the training data in general ML.”

The connection between our work and AL was not something we initially considered or thought of. You do bring up a very interesting point. Some AL approaches, specifically the expected error reduction [2] or query by committee approaches [3], result in a labeled training set that contains a diverse, or homogeneous, set of examples. This can, in turn, allow models to become more robust and generalizable. Is this the connection between our work and AL that you have in mind? One big difference between AL and our approach is that in AL, the model is often fully in charge of determining the order of unlabeled examples to label. Here, we already know from prior work that NLI datasets contain hypothesis-specific biases and our goal is not to remove them from our training sets but rather to encourage our model to learn representations that do not leverage such biases. It would be interesting to find any theoretical connections between our adversarial training models and active learning, and we welcome any pointers you might have. 

(4) “For example, at least authors need to show how much bias each of the target NLI datasets have”

Please refer to Figure 4 (in the appendix) and to Section 5 (paragraphs 2+3) for this question. Specifically, the green bars in the figure evaluate how much bias each dataset has by training a hypothesis-only model trained on each dataset.  The yellow bars evaluate how well a hypothesis-only model trained on SNLI performs on each target dataset. The difference between green and yellow bars approximates how different a dataset’s hypothesis biases are from the hypothesis biases in SNLI.   
When a target dataset has no hypothesis biases or has different biases than in SNLI, our method performs especially well. We discuss all this in Section 5 but we will emphasize this more and improve our discussion to make these points clearer. 

</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1xoqbvmpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>2/2: Thank you for detailed review. Highlighting some analysis and description of different biases</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkMlSnAqYX&amp;noteId=S1xoqbvmpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1512 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1512 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">(5) “in the last paragraph of Section 6, the authors didn’t provide a clear effect of hyperparameters yet”

Regarding the effect of the hyperparameters, we report cross-validation results with different hyperparameter settings in Figure 5 in the appendix. As noted in the appendix, “performance degrades quickly when we increase the fraction of random premises” and, “in contrast, the results with the double classifier (Figure 5b) are more stable.” We also discuss the effect of increasing the hyperparameter values in Section 6, “Stronger adversary”. 

(6) “do you actually make a better performing model than the baselines ...”
We would like to clarify our goal. Our goal is not to make a better performing model per se, but to make a model that has less biases and therefore transfers better to new datasets. This is shown by (a) improved results on target datasets (Tables1+2); (b) analysis of indicator words that are less predictive of certain labels in the adversarial model (Tables3+5); and (c) analysis of hidden biases (Figure 3), showing that adversarially trained models perform worse than a hypothesis-only model and better than a random encoder, meaning they contain fewer biases (but not zero bias, as even a random encoder retains some biases).   

(7) “On the impact of our results” and “how much bias each target dataset has”

In Figure 4 we evaluate how much bias is in each target dataset and we briefly mention it in the 3rd paragraph in Section 4. We will make this more clear in our revision. 

(8) “a statistical test” and “real examples where the adversarial model can only answer”

We will add a statistical test and real examples where that the adversarial model predicts correctly compared to the non-adversarial model. 


(9) “ How do you guarantee that the adversarial losses actually remove the hypothesis bias while preserving the original meaning representation and so not hurting the performance in g_NLI?”

We train g_Hypoth to do well on the classification task to encourage the adversarial loss to remove the hypothesis bias while preserving the meaning of hypothesis. This is achieved by training g_Hypoth to do well (minimize the loss), but during backpropagation reverse the gradients only when going into the shared encoder, as is standard in domain-adversarial neural networks. This is explained in Section 3.1 and we will try to make it clearer.  

(10) “So, this is not a robust model but a trade-off design at this moment.” 
We believe that our approach makes our models more “robust” in the sense that the model is robust to new target NLI datasets that contain different hypothesis biases.

References 
[1] @inproceedings{poliak2018hypothesis,
  title={Hypothesis Only Baselines in Natural Language Inference},
  author={Poliak, Adam and Naradowsky, Jason and Haldar, Aparajita and Rudinger, Rachel and Van Durme, Benjamin},
  booktitle={Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics},
  pages={180--191},
  year={2018}
}


[2] @inproceedings{DBLP:conf/icml/RoyM01,
  author    = {Nicholas Roy and
               Andrew McCallum},
  title     = {Toward Optimal Active Learning through Sampling Estimation of Error
               Reduction},
  booktitle = {Proceedings of the Eighteenth International Conference on Machine
               Learning {(ICML} 2001), Williams College, Williamstown, MA, USA, June
               28 - July 1, 2001},
  pages     = {441--448},
  year      = {2001},
  crossref  = {DBLP:conf/icml/2001},
  timestamp = {Wed, 27 Nov 2002 10:53:35 +0100},
  biburl    = {<a href="https://dblp.org/rec/bib/conf/icml/RoyM01}," target="_blank" rel="nofollow">https://dblp.org/rec/bib/conf/icml/RoyM01},</a>
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

[3] @inproceedings{seung1992query,
  title={Query by committee},
  author={Seung, H Sebastian and Opper, Manfred and Sompolinsky, Haim},
  booktitle={Proceedings of the fifth annual workshop on Computational learning theory},
  pages={287--294},
  year={1992},
  organization={ACM}
}
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_Byer9rW53m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Cool idea on how to mitigate biases in NLI data, but unconvincing results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkMlSnAqYX&amp;noteId=Byer9rW53m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1512 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1512 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this paper, the authors attempt to mitigate annotation artifacts in natural language inference data. The authors propose an adversarial setup to discourage the model from overfitting to examples that can be solved by only considering the hypothesis. They consider two variants of the adversarial setup: 1) an independent adversarial hypothesis-only classifier trained jointly with an NLI model (“double classifier”) and 2) a single classifier that is trained adversarially with randomized premises (“single classifier”). 

While the idea of this paper is appealing and well-motivated. The results are not super strong, but they are relatively consistent (at least for the double classifier method). I also liked the extensive analysis. My main concern is that I am not entirely convinced that the experimental results support the main claim made by the authors---that the proposed adversarial approach is a viable solution to mitigating annotation artifacts. 

If I understand correctly, the authors make two explicit assumptions: (a) that the biases in SNLI are somewhat responsible to its success. As a result, eliminating them also leads to performance degradation both in SNLI and on other datasets with similar biases (page 6: “This is expected, as we saw relatively small gains with the adversarial models, and can be explained by SNLI and MNLI having similar biases”). (b) that these biases are preventing the model from generalizing to other datasets that do not contain biases (or alternatively---contain other biases).
As a result, eliminating bias is desired, as it will improve generalizability, at the expense of lower in-domain results, that are somewhat inflated to begin with.

This is a nice and appealing story (though it could have been written more coherently). What I am concerned is whether the experimental gains observed actually result from mitigating biases. I can think of two alternative (though related) explanations for these gains: 
Were the pretrained SNLI baseline hyperparameters tuned on each corresponding dataset as well, or was the same model used in all cases? If the latter, this could partially explain the observed gains using their method: although the model has not seen any development example, selecting the best model based on the dev results gives it an unfair advantage over a model tuned on the original SNLI dataset.
The adversarial models suggested by the authors can be seen as a type of regularization. Did the authors try more traditional regularization methods such as L1/L2/dropout on the original SNLI model? Using high regularization values could have a similar effect (reduced performance on SNLI, increased performance on other datasets). Assuming you tune the regularization values on the target dataset (as you do with your methods), this is expected to lead to similar phenomena. I suspect that most gains observed in Table 1 are small enough in most cases to be obtained by such methods.


 


Other points:

One experiment that I would like to see is whether biases are actually eliminated from SNLI. The authors could test the original SNLI model and the adversarial ones on a dataset containing the NULL string as premise (with the original hypothesis). Although the models are not trained on such data, based on the hypothesis-only results, I would expect the original model to do fairly well on it. If the authors’ claim is correct, we should see significantly lower performance by the adversarial ones.
The results on testing the adversarial models on the SNLI test set (Table 4), as well as Figure 4 should be in the main text rather than the appendix.

Minor: 
— Page 6: “The fact that the two architectures agree to a large extent on which datasets benefit from adversarial training is a validation of our basic approach.”: this claim is unconvincing: the double classifier improved on 9/12 cases (75%). Selecting 5 of them at random, it is likely that 4 of them will show improvement with double.

-- Typos and such:
- Last paragraph on page 1: "...biases.In this way..." (missing white space)
- Scitail is inconsistently spelled throughout the paper
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJgVmzwXaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your detailed review. Highlighting analysis in the paper that address your concerns</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkMlSnAqYX&amp;noteId=BJgVmzwXaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1512 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1512 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your feedback and review. We are happy that you found the idea of our work to be appealing and well motivated and that you found our analysis to be extensive. Based on your feedback, we would like to highlight some of our analysis that you might have missed.

(1)  “this could partially explain the observed gains using their method: although the model has not seen any development example, selecting the best model based on the dev results gives it an unfair advantage over a model tuned on the original SNLI dataset.”

The pretrained SNLI baseline hyperparameters were not tuned on each corresponding dataset as well, as all of the models were tuned on just SNLI. For each target NLI dataset, we report the test numbers that correspond to the hyperparameters that resulted in the best performance on each target NLI datasets’ development sets. We think that our fine-tuning experiments, where we train on each target dataset’s training set (Figure 2), provide an answer to this. Specifically, on SICK, adversarial pre-training eventually works better than no pre-training (and better than non-adversarial pre-training). 

(2) “The adversarial models suggested by the authors can be seen as a type of regularization”.

We agree that the adversarial models can be seen as a type of regularization. However, our goal is to improve a given standard NLI model by reducing already known biases (i.e. hypothesis biases), not to make the best overall model. There can be many hyperparameters to tune, both regularization hyperparameters and others, and we adopt the ones found to work best in InferSent (which by default has no regularization and represents a typical NLI model that separately encodes premises and hypotheses). Moreover, we strive for a fair comparison of non-adversarial and adversarial models. It’s possible to tune other (regularization) parameters for both models, but this is beyond our goal of improving a standard model by removing hypothesis biases. 


(3) “One experiment that I would like to see is whether biases are actually eliminated from SNLI”

We try determining whether biases are actually removed from SNLI by looking at the weight our model gives to the indicator words in SNLI (Table 3).  We also analyze whether the hidden representations no longer contain biases in the paragraph in Section 6 titled “Hidden biases in the representation”. The results in Figure 3 show that some of the biases are removed. 

Regarding the NULL string idea: we’re worried that (a) the models were not trained on this input, and (b) any string would still convey some information. Instead, we plan to perform an experiment of synthetic data with a known degree of bias, as proposed by Reviewer 3. 

(4) Thank you as well for the minor points and typo/style related comments. We have fixed those.

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1ea-B6Y2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting approach for removing bias while learning sentence representation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkMlSnAqYX&amp;noteId=H1ea-B6Y2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1512 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1512 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a method for removing bias of a textual entailment model through an adversarial training objective. While existing textual entailment datasets such as SNLI have been crucial for driving research on natural language inference and universal sentence representations, recent work showed that models only processing the hypothesis can achieve 67% accuracy, indicating that such models heavily exploit biases in the dataset. To mitigate these biases, the authors propose to let the model predict the label both from the premise-hypothesis representation and the hypothesis-only representation. On the backward pass, the sign of the gradient going into the hypothesis-only representation is flipped, making that representation invariant to biases that would otherwise allow for predicting the entailment label from the hypothesis only. The paper is written very clearly and the analysis of the methods is thorough. The method itself is a fairly simple trick and one could argue that, overall, this is incremental work. However, in my view learning less biased sentence representations is very relevant to the community and this paper is executed well. Particularly the improvements on the variety of downstream task compared to InferSent are impressive. I would be interested in hearing whether the authors have suggestions for applying similar techniques to entailment models that do not build up a specific premise and hypothesis representation (e.g. attention-based methods proposed for SNLI).</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1xvuMPQaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank your for your feedback and review. Comment about applying our method to attention-based models</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkMlSnAqYX&amp;noteId=B1xvuMPQaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1512 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1512 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your feedback and review. We are happy that you found our approach to be interesting, clearly written, and enjoyed our analysis. 

(1)“In my view, learning less biased sentence representations is very relevant to the community”

We agree 100% with this sentiment as this is a big motivation for our work.

(2)  “I would be interested in hearing whether the authors have suggestions for applying similar techniques to entailment models that do not build up a specific premise and hypothesis representation (e.g. attention-based methods proposed for SNLI).” 

During this work, we considered how to apply similar techniques to NLI models that do not build up a specific premise and hypothesis representation. Although we did not test attention-based models (which typically encode the premise and hypotheses together), it is possible that the single classifier approach might work here, as it does not rely on having a separate hypothesis-encoding that feeds into a different sub-network. We can test whether attention-based methods can leverage hypothesis only biases by replacing premises that entailed their corresponding hypotheses with nonce sentences that do not entail the corresponding hypotheses. For these examples, adversarial learning might similarly be used to possibly improve the generalizability of the learned representations. This approach would then not specifically target the hypothesis representations, but instead, target the jointly learned representation.  </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>