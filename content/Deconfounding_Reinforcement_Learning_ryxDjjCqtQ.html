<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Deconfounding Reinforcement Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Deconfounding Reinforcement Learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ryxDjjCqtQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Deconfounding Reinforcement Learning" />
      <meta name="og:description" content="In this paper, we propose a general formulation to cope with a family of reinforcement learning tasks in which confounder (i.e., a factor affecting both actions and rewards) exists in dynamic..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ryxDjjCqtQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Deconfounding Reinforcement Learning</a> <a class="note_content_pdf" href="/pdf?id=ryxDjjCqtQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019deconfounding,    &#10;title={Deconfounding Reinforcement Learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=ryxDjjCqtQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">In this paper, we propose a general formulation to cope with a family of reinforcement learning tasks in which confounder (i.e., a factor affecting both actions and rewards) exists in dynamic environments. Based on the proposed approach, we extend two representatives of reinforcement learning algorithms: Q-learning and Actor-Critic Methods, to their deconfounding variants. Due to lack of datasets in this direction, a benchmark is developed for deconfounding reinforcement learning algorithms by revising OpenAI Gym and MNIST. We demonstrate that the proposed algorithms are superior to traditional reinforcement learning algorithms in confounding environments. To the best of our knowledge, this is the first time that confounders are taken into consideration for addressing full reinforcement learning problems.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">confounder, causal inference, reinforcement learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">This is the first attempt to build a bridge between confounding and the full reinforcement learning problem.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">31 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SygU8Vhl0X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a> NaN issue during training</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=SygU8Vhl0X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper628 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">While reproducing experiments, we are getting NaN loss during training. We have clipped gradients to -1 to 1, Xavier initialized layers, Z-notmalized actions/rewards and L2 regularized dense layers. Any additional things to be taken care of?</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Byg9XiEs3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting problem</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=Byg9XiEs3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper628 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper628 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">this paper studies an interesting question of how to learn causal effects from observational data generated from reinforcement learning. they work with a very challenging setting where an unobserved confounder exists at each time step that affects actions, rewards and the confounder at next time step.

the authors fit latent variables models to the observational data and perform experiments.

the major concern is on the causal inference side, where it is not easy to claim anything causal in such a complicated system with unobserved confounders. causal inference with unobserved confounders cannot be simply solved by fitting a latent variable model. there exists negative examples even in the simplest setting that two distinct causal structure can lead to the same observational distribution. for example here, <a href="https://www.alexdamour.com/blog/public/2018/05/18/non-identification-in-latent-confounder-models/" target="_blank" rel="nofollow">https://www.alexdamour.com/blog/public/2018/05/18/non-identification-in-latent-confounder-models/</a>

it could be helpful if the authors can lay out the identification assumptions for causal effects. before claiming anything causal and justifying experimental results.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJelvuEjnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Strong and important idea - presentation and execution can be improved </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=BJelvuEjnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper628 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper628 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper addresses an important and often overlooked issue in off-policy reinforcement learning - the possibility of confounding between the agent's actions and the rewards. This is a subject which has been exhaustively explored in the causal inference literature, and the authors are very correct in suggesting that it should be incorporated into the world of reinforcement learning.  Specifically they propose a generative model with a global latent confounder that is inferred using a variational autoencoder architecture.  

The paper is generally well-written, though some points could be made clearer in my opinion, as detailed below. The experiments are constructed by introducing confounding into existing datasets; performance seems to be good, but I am not entirely sure whether the given architecture is necessary, see comments below. 

High-level comments:
(1) Classic RL deals with confounders all the time. The state is a confounder between the action and the reward. The issue of confounding becomes less trivial when one is performing off-policy RL when the original policy is *unknown*. This is exactly the case that the authors mention when they cite the recent work by Gottesman et al. (2018) who deal with using RL to learn from the actions of physicians in a hospital.  While I am sure the authors are aware of these distinctions, I think the paper would be better if this is spelled out very explicitly. This includes explaining why this issue doesn't come up in classic RL.

(2) Assuming the case above - off-policy RL with unknown confounders - one would usually assume "no unmeasured confounding", i.e. that the observed actions are an unknown but learnable function of the observed states. That is basically the scenario of most off-policy RL.

(3) However, the authors strive to go one step beyond the case (2), to a situation where there is an *unmeasured* confounder affecting both observed actions and rewards. If nothing is known about this unmeasured confounder, then it is generally impossible to learn effective policies, as the causal effects of actions are not identifiable from the observed data. In this paper, the authors make an implicit assumption that while the confounder is unmeasured, it can still be inferred from the data. This is an intermediate step between "no unmeasured confounding" and "complete unmeasured confounding". This is related to work on using proxy variables e.g. Kuroki &amp; Pearl (2014) and even more closely related to the work cited by Louizos et al. (2017).
Again, I think the paper would be much improved if all this is addressed explicitly. 

(4) An important consequence of point (3) above is that in fact adding the single global latent-confounder U is not, in itself, very important from a causal perspective. The sequence of variables Z_1... Z_T are already latent confounders that are assumed to be inferrable from data. It is true that the addition of the global U might change the statistical and optimization properties of the model. This leads to a very important conclusion: the authors should test their model with and without U. I think this specific ablation experiment is crucial. In many cases I am sure that the assumption of a global latent confounder is a good one and is especially useful in the VAE case where it will make optimization more stable. However, in principle, all of U's roles could be taken within the sequence of Z's, and I am curious to see in practice how big of an effect it has.

(5) I wish to add that even if the U variable turns out to not add much empirically, this work is still valid since the sequence of Z's can themselves be considered inferred latent confounders.

Specific comments:
(1) 2.3: there are more than 2 ways of computing the do-operator. RCTs and backdoor are the best known approaches, but not the only ones, e.g. there is frontdoor adjustment. 

(2) I think the paper would be easier to follow if there was one concrete example used throughout. This will make it easier to understand and possibly verify/criticize the assumptions of the generative model.

(3) Related to "higher-level point (4)" above, in eqs. 17 &amp; 18 note that Z_t is unknown, same as U. Both are inferred. This also leads to the question which Z_t is actually used in practice? Is it the mean, or is it also sampled from the approximate posterior q?

(4) Below eq. 19, it would be very useful for the readers if you could explain exactly when would there be a difference between the two versions p(r_{t+1}|z_t,a_t) and p(r_{t+1}|z_t, do(a_t=a))

(5) In the description of all the experiments I was missing a crucial point: how does the introduced confounder affect the reward? Is it only through the different actions? The way it is currently explained, it seems like the added variable introduces lack of *overlap*, but not strictly confounding.

(6) The description of the experiment in 4.3 could be more detailed. What exactly was the training and test? What RL method was used? What did the baseline optimize for? I would like to see an ablation experiment where U is not included in the model. 

(7) In 4.5, what is the "vanilla" method? And as mentioned above, I would like to see an ablation experiment where U is not included in the model.  


</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hyx9C2Vqhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Setting doesn't make sense for RL and experiments don't evaluate causal questions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=Hyx9C2Vqhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper628 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper628 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a method for reinforcement learning (RL) in settings where the relationship between action and reward is confounded by a latent variable (unobserved confounder). While I firmly believe that RL would benefit from taking causality more seriously, this paper has many fatal flaws that make it not ready for publication. 

First, and most importantly, the paper is unclear about the problem it is trying to solve. It talks about confounded RL as being settings in which a confounder affects both the action and reward. In typical RL settings this wouldn’t make sense: in RL you get to choose the policy so it doesn’t make sense to assume that the choice of action is confounded while you’re doing RL. To get around this, the authors assume that they’re working with observational data and doing RL on a generative model leant from the observational data. But by doing this, they have assumed away the key advantage that RL has over causal inference: the ability to experiment in the world. The authors justify this assumption by considering high-stakes settings where experimentation is either too risky or too costly, but they don’t explain why you would want to do RL at all when you could just do causal inference directly. If you can’t experiment, RL offers no advantages over standard causal inference methods and bring serious disadvantages (sample-efficiency, computational cost, etc.). 

# Method
The authors learn a variational approximation to a particular graphical model that they assume for their RL setting. They then treat the variational approximation as the true distribution which allows them to perform causal inference via the backdoor correction. They claim this is identified but this is false - it is only identified with respect to the variational distribution, not the true distribution and we have no a priori reason to  believe that the variational distribution well-approximated the true distribution. In principle, the authors could have tested how well this works experimentally but their experimental setup has problems which prevent this being evaluated. 

Quibbles:
 - Page 3: the authors claim the model is “without loss of generality” but this is false - there are many settings that would not conform to this model: e.g. the multi agent settings that economics studies; health settings with placebo effects where reward depends on observations directly; etc.
  - Page 4 above the equations: either the equations describe the variational approximation to the generative model or the equations shouldn’t all be factorized normal distributions. Real data isn’t made up of factorized normals.

# Experiments

The authors evaluate their method on three simulated datasets: Confounding MNIST, Confounding Cartpole and Confounding Pendulum. All three have the same methodological problems so I’ll only focus on the MNIST dataset. They synthesize their MNIST dataset but corrupting a subset of MNIST digits with noise and treating actions as rotations. Rewards are given by the absolute difference in angle between the rotated digit and the original unrotated digit. “Confounding” is added by having a binary latent variable affect the amount that the digit is rotated - but importantly, the reward isn’t affected directly by the latent variable. Because of this, there isn’t actually a confounding problem - the “confounder” simply changes the rotation of the digit and can be treated as additional experimentation from the perspective of causal inference. The authors evaluate their method by examining reconstructions of the MNIST digit, but this simply checks how well the variational inference is working, not whether the causal inference is working (there would be no way to evaluate the latter on this dataset because there is no confounding). Effectively all they find is a better-designed variational distribution will do a better job of reconstructing the input (without modelling the latent u, the VAE is forced to average over its two states resulting in more blurry samples). 

The RL evaluations aren’t described in enough detail to conclusively explain the difference observed, but it seems to be driven by the fact that the standard RL methods are working with worse variational approximation distributions.

# Summary
This work studies a setting in which the correct baselines would be causal inference algorithms (but they aren’t considered) and their experimental evaluation has serious flaws that prevent it supporting the claims made in the paper. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">2: Strong rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJlf2Fdv3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarification on confounding MNIST</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=rJlf2Fdv3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper628 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018</span><span class="item">ICLR 2019 Conference Paper628 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1. What is the purpose of the "sequence of three consecutive squares (2 x 2 in pixel)"? Are they added before or after the rotation?
2. What is the "some policy" that is perform on the images? Random rotations? Or rotations toward vertical?
3. Does the confounder, u, only affect the magnitude of rotation? i.e. rotation given u = 1 is between 22.5 and 45, while u=0 it is between 0 and 22.5? As far as I can see, u doesn't affect the reward directly? Is that correct?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HyxkLV9DhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=HyxkLV9DhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper628 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018</span><span class="item">ICLR 2019 Conference Paper628 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comment.

Re 1. We exactly followed the same setting of Healing MNIST in [1]. As [1] said, “the squares within the sequences are intended to be analogous to seasonal flu or other ailments that a patient could exhibit that are independent of the actions and which last several timesteps”. We want to show that our model can learn long-range patterns, which plays an important role in medical applications. The squares are added after the rotation. 

Re 2. “Some policy” can be any policy, e.g., random rotations or rotations toward vertical. But in our case, considering the confounder, we used the policy where action is affected by the confounder u. 

Re 3. Simply speaking, the confounder not only affects the action through the magnitude, 22.5 ≤ |a| ≤ 45 or 0 ≤ |a| &lt; 22.5, but also affects the reward through the direction (i.e., clockwise or counterclockwise), a or -a, where a and -a will result in different rewards.

We will clarify these in the new draft. Thank you for your suggestion. 

[1] Rahul G Krishnan, Uri Shalit, and David Sontag. Deep kalman filters. arXiv preprint arXiv:1511.05121, 2015.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJe23dR_3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>MNIST Reconstruction without Confounder</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=SJe23dR_3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018</span><span class="item">ICLR 2019 Conference Paper628 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">To get reconstructed image in Fig 3 top row, are you training a separate architecture over changed loss without u? Or is there a clever hack to mitigate retraining?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1xS8HyYn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=r1xS8HyYn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper628 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018</span><span class="item">ICLR 2019 Conference Paper628 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Exactly, we trained a separate architecture without u.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_B1gSHMy7nX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarification in Architecture</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=B1gSHMy7nX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Oct 2018</span><span class="item">ICLR 2019 Conference Paper628 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Lets take an example network f5 ,f6
{FC 100 , FC 100 , FC 100 } → FC 100 → FC 100 → FC 1 → {sigmoid, softplus}

1. Is the output of FC 100, FC 100, FC 100 concatenated and sent to FC100 followed by FC100 and then FC1? .
2. Are the activation functions applied after each layer or at the output of the final FC (FC1)?
3. The activation functions are denoted by a {.}. By definition, does that mean they are parallel operators? In that context what is Mu and sigma for f5 and f6 respectively?  It will be better if you can explain how the activation functions are applied and if possible show a visual representation to get a more intuitive understanding of the network topology



</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJgBBuGQ2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=SJgBBuGQ2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper628 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Oct 2018</span><span class="item">ICLR 2019 Conference Paper628 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comment.

Re 1: Exactly.

Re 2: In default, softplus is applied after each intermediate layer, and the explicitly claimed activation functions are applied to the output of the final layer. Here, take f5 and f6 for example, f5 and f6 share the parameters of the first five FC100 layers, each followed by softplus, but have different parameters in their own final layer, i.e., FC1 has two outputs (the output of f5 and the output of f6) which are respectively followed by sigmoid and softplus.

Re 3: In our architecture, each function modelled with a neural network has only one output: mu or sigma^2. Since each pair of functions (e.g., f1 and f2, f5 and f6, etc.) share the parameters of all layers except the final layer, that is, they have their own final layers which are parallel outputs, each followed by a respective activation function. In the case of f5 and f6, as described in Re 2, FC1 has two outputs, each followed by sigmoid and softplus, representing mu (the output of f5) and sigma^2 (the output of f6), respectively. We will clarify this with a figure in the appendix of the new draft. Thank you for your suggestion.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyelGt3E3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>FC1 of f1 and Dimension of u</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=SyelGt3E3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018</span><span class="item">ICLR 2019 Conference Paper628 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Clarifications:
1. How is u value chosen? Pg 14
2. How is f1 resized after FC1(512)? Depth not mentioned. Pg 15
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJewqGpE37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Just as described in the paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=SJewqGpE37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper628 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018</span><span class="item">ICLR 2019 Conference Paper628 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear reader,

Re 1: Actually dimension of u can be any number. In the paper we let u=2 because it is easy to visualise it in the 2D plot as shown in Figure 5.

Re 2: 512 = 4 x 4 x 32, so it can be reshaped to a square 4 x 4 with 32 channels.

We hope this helps.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJg3yE_t27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Architecture for measuring q(u|x,a,r) is unclear</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=BJg3yE_t27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018</span><span class="item">ICLR 2019 Conference Paper628 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1&gt;Since u is time independent which particular mean and variance are used to calculate the q(u|x,a,r).

2&gt; Just to confirm : should q(a|x) and q(r|x,a) be added to the loss function.

</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJg7qLoY2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=SJg7qLoY2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper628 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018</span><span class="item">ICLR 2019 Conference Paper628 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Re 1. As described in Section 3.3.2, q(u|x, a, r) is modelled in the same way as q(z |x, a, r), each of which is parameterised by a bi-directional LSTM. However, unlike q(z |x, a, r) in which z is time-dependent (i.e., z_t corresponds to x_t, a_t, r_t at each time step), in q(u|x, a, r) u is independent of time steps, meaning that u combines all the information of the whole sequence.

Re 2. Yes. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJxhQ0ejhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Feature independence assumption in p(x|z)?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=BJxhQ0ejhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018</span><span class="item">ICLR 2019 Conference Paper628 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">While calculating p(x|z) are you assuming that all the features (784) are independent? As X is 784 dimensional, calculating p(x|z) will give numerical instability. How are you mitigating this?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByxwiAPhhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=ByxwiAPhhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper628 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018</span><span class="item">ICLR 2019 Conference Paper628 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">You can think so, because all Gaussians in the paper are assumed to be with diagonal covariance matrices.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_HyxA4Dozom" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Counterfactual claim is not really "Counterfactual" nor "Interventional"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=HyxA4Dozom"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Oct 2018 (modified: 19 Oct 2018)</span><span class="item">ICLR 2019 Conference Paper628 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thoughts:
(1)Paper claims to have done counterfactual reasoning which according to Pearl's literature(as cited in paper) is not counterfactual. Counterfactual has to incorporate hindsight -&gt; What action a(t) will be taken if you are in state x(t) "given the fact" that different state x'(t) and action a'(t) were taken. "Given the fact" makes all the difference. 

Claim: 
Paper generates samples based on conditionals

Edit: 
(2)Why not additional experiment on state intervention as q(action=a | state=x) != q(action=a | do(state=x)) where inequality arises due to back door path x &lt;- z -&gt; a opening up. This changes equations accordingly. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJeq_vV7sm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>It actually is.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=rJeq_vV7sm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper628 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Oct 2018</span><span class="item">ICLR 2019 Conference Paper628 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your interest in our paper. Happy to discuss with you all the stuff about causal concepts.

Re Thought (1): What you understand about counterfactual reasoning is exactly right, which is also what we did in the paper. Given the fact that we know the training set (e.g., we knew the fact that what happened when we took action a_1 at x_1 in the training set), we want to know “what would have happened had we taken a different action a_2 at an unseen x_2 in the testing set?” That is exactly what counterfactual reasoning does. Note that, like ones did in [1], we primarily emphasise the inference on unseen data, which plays a pivotal role in the following RL part.

Re Thought (2): in our paper, we do not have the interventional distribution p(a | x). 
a) If what you meant is q(a | x), it is an auxiliary distribution in the variational inference part, for which we did not take intervention into account, because that is not what we studied in the paper;
b) If you meant p(a | z), that is actually the policy function in RL. Usually we also do not factor the intervention into the policy, because the definition of the policy is a conditional distribution.
Actually we only considered the intervention in the reward function p(r | z, do(a)) where z is treated as a fixed constant at each time step.

We hope this helps.

[1] Rahul G Krishnan, Uri Shalit, and David Sontag. Deep kalman filters. arXiv preprint arXiv:1511.05121, 2015.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rygqNuS7jX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good one!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=rygqNuS7jX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Oct 2018</span><span class="item">ICLR 2019 Conference Paper628 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Given the training data incorporates for hindsight. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_BylZvATko7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Formula Check</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=BylZvATko7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Oct 2018</span><span class="item">ICLR 2019 Conference Paper628 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">In Eq (10) Page 5, equation written is,
q(z, u | x, a, r) = q(u | x, a, r).q(z | x, a, r)

But q(u | z, x, a, r) != q(u | x, a, r) as v-structure z1 --&gt; r2 &lt;-- u opens up. 

This will affect Eq (12), Eq (18) etc. Please have a look.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJxsAvR1jX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>It is actually the widely used factorization assumption in variational inference.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=rJxsAvR1jX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper628 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Oct 2018</span><span class="item">ICLR 2019 Conference Paper628 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your interest in our paper and for checking our formula. 

In the model part, you are exactly right, where p(z, u | x, a, r) != p(u | x, a , r) p(z | x, a, r). However, in the variational inference part, for simplicity, we use the well known trick, factorization assumption, to obtain q(z, u | x, a, r) = q(u | x, a , r) q(z | x, a, r) as we claimed on Page 13. 

Actually, we can think q(u | z, x, a, r) = q(u | x, a, r) because (x, a, r) already contains all the information about z.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkg9HFNWsX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Training pipeline</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=Bkg9HFNWsX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Oct 2018</span><span class="item">ICLR 2019 Conference Paper628 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Amazing! I really like the idea. 

It would have been nice if you had specified training pipeline which tells what is trained after what. 

Can you elaborate on that?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Skluaf-shX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Sampling u in Eq 18.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=Skluaf-shX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018</span><span class="item">ICLR 2019 Conference Paper628 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Shouldn't u be sampled from the model because the Deconfounding Q learning pipeline starts after the model is built which doesn't have access to observations (x,a,r)?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1x1gx_hhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=r1x1gx_hhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper628 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018</span><span class="item">ICLR 2019 Conference Paper628 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Yes, u is sampled from the model. In Equation (18), (x, a, r) in the posterior q(u|x, a, r) are estimated from the model rather than the observations. We will use different notations in the updated version.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1lpVE_3nQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Why not just sample from p instead of q?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=H1lpVE_3nQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018</span><span class="item">ICLR 2019 Conference Paper628 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">You have built your model "p". Use it to sample u values. Why use q anyway? Why need to estimate x,a,r from z. That's a convoluted approach. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HkguruuhnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=HkguruuhnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper628 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018</span><span class="item">ICLR 2019 Conference Paper628 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Re "Why use q anyway?"
Please keep in mind that u is a latent variable. q(u|x, a, r) is the posterior containing the information from the data whilst p(u) is nothing but a prior. 

Re "Why need to estimate x,a,r from z."
Because q(u|x, a, r) depends on (x, a, r) which are unknown during the testing phase. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1gO5nK3nm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>KL Divergence b/w q(u|x,a,r) and p(u) takes care of it</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=S1gO5nK3nm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018</span><span class="item">ICLR 2019 Conference Paper628 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">KL loss ensures two distributions are close by. Draw as many "u" values from p(u) instead of estimating x,a,r. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyeZ0H9h2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=SyeZ0H9h2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper628 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018</span><span class="item">ICLR 2019 Conference Paper628 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">In our case, as shown in Fig 5, the posterior of u has an obvious difference from its unit Gaussian prior, even though their KL loss converged. Therefore, sampling from q is better.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_Byx_yQIbim" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your useful feedback.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=Byx_yQIbim"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper628 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Oct 2018</span><span class="item">ICLR 2019 Conference Paper628 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your useful feedback and we are glad you like the idea.

As described in the last paragraph on Page 1, generally speaking, the training pipeline consists of two steps: 
Step 1: Given the time-independent confounding assumption (Section 3.1), we learn the deconfounding model, as presented in Fig 2, from the observational data;
Step 2: We optimise the policy or Q-function based on the deconfounding model we learned in Step 1.

More specifically, in Step 1, given the observational data (x, a, r), we optimise the variational lower bound (Eq.11) with two extra terms (Eq.15 and Eq.16). Once the deconfounding model is learned, we know the state transition function p(z_t | z_{t-1}, a_{t-1}) and can also calculate the deconfounding reward function p(r_t | z_t, do(a_t)) according to Eq.17. In Step 2, we treat the learned deconfounding model as a RL environment like CartPole in OpenAI Gym, and directly exploit it to generate trajectories/rollouts through the state transition function and the deconfounding reward function. On the basis of the generated trajectories/rollouts, we can train the Q-network using Eq.19 or the policy network using Eq.20.

We will clarify this in the new draft. Thank you.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1lyWAJXoX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>N value in Eq 18</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=H1lyWAJXoX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Oct 2018</span><span class="item">ICLR 2019 Conference Paper628 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">What value of N is chosen in Eq 18?

While plotting Fig 5 N=128, but is it the value chosen even for experimental results? 
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SygNdtNXsQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Trade-off</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=SygNdtNXsQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper628 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Oct 2018</span><span class="item">ICLR 2019 Conference Paper628 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">As we know, there always is a trade-off between time and accuracy in MC methods. Considering this balance, we set N= 400 for experimental results. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HyePDuFziQ" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxDjjCqtQ&amp;noteId=HyePDuFziQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper628 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>