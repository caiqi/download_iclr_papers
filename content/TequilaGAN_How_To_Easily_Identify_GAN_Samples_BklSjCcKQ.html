<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>TequilaGAN: How To Easily Identify GAN Samples | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="TequilaGAN: How To Easily Identify GAN Samples" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Bkl2SjCcKQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="TequilaGAN: How To Easily Identify GAN Samples" />
      <meta name="og:description" content="In this paper we show strategies to easily identify fake samples generated with the Generative Adversarial Network framework. One strategy is based on the statistical analysis and comparison of raw..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Bkl2SjCcKQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>TequilaGAN: How To Easily Identify GAN Samples</a> <a class="note_content_pdf" href="/pdf?id=Bkl2SjCcKQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 13 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019tequilagan:,    &#10;title={TequilaGAN: How To Easily Identify GAN Samples},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Bkl2SjCcKQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=Bkl2SjCcKQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">In this paper we show strategies to easily identify fake samples generated with the Generative Adversarial Network framework. One strategy is based on the statistical analysis and comparison of raw pixel values and features extracted from them. The other strategy learns formal specifications from the real data and shows that fake samples violate the specifications of the real data. We show that fake samples produced with GANs have a universal signature that can be used to identify fake samples. We provide results on MNIST, CIFAR10, music and speech data.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Generative Adversarial Networks, Deep Learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We show strategies to easily identify fake samples generated with the Generative Adversarial Network framework.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HygqtR1O6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>1st reply to all reviewers</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkl2SjCcKQ&amp;noteId=HygqtR1O6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper120 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper120 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have uploaded the revised paper that addresses issues with writing style, layout, etc.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJlYx01dpQ" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkl2SjCcKQ&amp;noteId=HJlYx01dpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper120 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ryxOVnyua7" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkl2SjCcKQ&amp;noteId=ryxOVnyua7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper120 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1xFSKJupX" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkl2SjCcKQ&amp;noteId=B1xFSKJupX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper120 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HkgFFTLA3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Continuous generators in GANs don’t capture discrete statistics?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkl2SjCcKQ&amp;noteId=HkgFFTLA3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper120 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper120 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes statistics to identify fake data generated using GANs based on simple marginal (summary) statistics or formal specifications automatically generated from real data. The proposed statistics mostly boil down to the fact that continuous-valued generator neural networks can’t adequately generate data distributions that are topologically different from the distribution in the latent z-space. The differences in the summary data/ feature statistics and statistics corresponding to formal specifications between fake and real data are of the above nature. 

This seems fairly obvious but I haven’t seen this property of GANs being exploited to distinguish between GAN generated and real-data

This property/ shortcoming of the generator is not surprising at all and has been acknowledged before. See, for example, the discussions in,

Khayatkhoei et al, Disconnected Manifold Learning for Generative Adversarial Networks, arXiv:1806.00880 (NIPS, 2018)

This has spurred various approaches to mitigate this shortcoming. See, for example,

Ben-Yosef and Weinshall, Gaussian Mixture Generative Adversarial Networks for Diverse Datasets, and the Unsupervised Clustering of Images, arXiv:1808.10356

Jang, Gu and Poole, Categorical Reparameterization with Gumbel-Softmax, arXiv:1611.01144 (ICLR 2017)

So, the fact that summary statistics predicated on discreteness of data or discreteness of their dependencies can distinguish GAN-generated data from real data is not surprising at all. In fact, in the paper itself, figure 10 and 11 show that for continuous data like speech, the proposed statistics are unable to distinguish between the fake and real data. 

Beyond this, even though it's interesting, there isn’t enough contribution in the paper.  It would be great if the authors can extend this observation and show if such statistics can always be found and tricks like Gumbel-Softmax/ GMM-GANs etc are doomed to fail or if certain extentions of GAN architectures can handle such statistics.

Furthermore, the paper needs to provide more clarity/ clarifications about the following:

1.	Apart from formal specifications, the rest of the statistics are ad-hoc (e.g. the spectral centroid or the spectral slope which are just borrowed from the audio domain) – why should these be good for images? 
2.	Training choices do not seem principled – GANs are trained till generated samples look like real samples. Why not use parameter settings and train to produced state of the art results with chosen architectures?
3.	Figure 1: Why does the CDF for the real data start in the middle of the figure? The figure purportedly shows bimodal 1D data for which the CDF should be a step function whereas the reference data has an inclined line. Why?
4.	Using the term ‘spectral’ (centroid and slope) for image features is misleading when spectral features are not computed.  Do these features capture spectral properties of images. How? Why are these good features?
5.	What does an “asymptotically converging activation function” mean?

6. Some typos need to be corrected. Figure # and caption (with dataset name) for Figure 6 needs to be provided, etc.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1x3bCPx0m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>1st reply to AnonReviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkl2SjCcKQ&amp;noteId=H1x3bCPx0m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper120 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper120 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for sharing your comments and suggesting approaches that potentially would mitigate the shortcomings we described in our paper, including the GM-GAN and Categorical Reparameterization with Gumbel-Softmax. 
We would like to use this venue for an  exchange of ideas and knowledge and kindly ask you to let us know if further clarifications are needed.  

We first address your global comments and then address your numbered comments.

"This has spurred various approaches..."
Regarding the GM-Gan, the CIFAR10 generated images in the GM-GAN paper are blurry (smooth) and do not satisfy specifications, e.g. entities have deformed shapes. This is evidence that this approach fails to mitigate, even in small images, the mentioned shortcomings.
 
Regarding the Categorical Reparameterization with Gumbel-Softmax approach, given that the space in which images and most data live is intrinsically continuous, training on such data in a discrete manner is very likely intractable.

"So, the fact that summary statistics predicated..."
Please look at Figure 10b and compare the KS Two-Sample Test statistics between train, test, iwgan and bernoulli. This statistic computed on Bernoulli samples is the farthest from train and the IWGAN statistic is much closer to the Bernoulli statistic than it is to train or test. Hence, the statistics can be used to distinguish between fake and real data.

"1. Apart from formal specifications, the rest of the statistics..."
Given that deep models for vision come from a computer vision feature engineering approach, it is expected that these deep models will succeed in learning the distribution of such features. 
Nonetheless, the spectral centroid can provide valuable information about the distribution of “noise” per column in an image. As a matter of fact, we  the distribution of mean mention that 'spectral centroids from fake data and Bernoulli data suggest that the fake images have noise that are also equally spatially distributed.'

"2. Training choices do not seem principled..."
As we describe in the paper, the models are trained following the standard practice: 'trained until the loss plateaus and the generated samples look similar to the real samples'. Note that in our related work section we comment on the disadvantages of using methods such as Gaussian Parzen Window fitting and Inception Scores to guide training.

"3. Figure 1: Why does the CDF for the real data..."
With the exception of the Bernoulli sampled data, named random in Figure 1, the data is mainly bimodal, not exclusively, with modes at 0 and 1. Hence the “inclined line”, different from the step function on the last image representing the samples drawn from a Bernoulli distribution.

"4. Using the term ‘spectral’ (centroid and slope) for image features..."
We will update the terms to avoid confusion. As we described, the spectral centroid is a good feature because it provides valuable information about the distribution of “noise” in the image. For example, in Figure 2 we should that the distribution of spectral centroid between Bernoulli and Fake data are similar. The spectral slope was used in the context of audio as a counter-example where it is harder to identify fake samples.

"5. What does an “asymptotically converging activation..."
We mean saturating activation functions. We will update the term in the paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_r1lHsWW92m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Very interesting topic!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkl2SjCcKQ&amp;noteId=r1lHsWW92m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper120 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper120 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The ability to detect generated samples is a very interesting topic and has recently triggered a lot of discussion. The paper is well written, easy to follow and the authors have done an extensive evaluation in a number of different GAN applications.


Comments:

1) Methodology - The GANs being evaluated were trained ignoring the statistics that the authors use to detect generated samples, and thus it is expected that there will be a difference. Have the authors attempted to include those in the loss function? It is a fair argument to say that some are not differentiable, but there are ways one could still incorporate them e.g. using REINFORCE. What do the authors thing about that?

2) Following (1), as far as the specifications are known one could train a GAN to fake them. What do the authors think about that? Do the authors think they could detect samples from that or such statistics could be used to devise better GAN losses?

3) The authors conclude that the smoothness and the differentiability of a loss function will always result to an inductive bias. However, that's an assumption given that there are no experiments trying to fake the detection, or experimenting with a large number of different architectures.

4) In CIFAR10 the authors state that the distributions of pixels was quite different specially in the values close to -1. Another way to see neural networks is as differentiable compressors. Many times, value distortions are correlated to the amount of compression. Have the authors seen differences in e.g. larger architectures?

5) On a last note, I would change the title as there is no proof that these tests / assumptions would hold for further research in the field. It would be great to show that the statistics used to detect GAN samples cannot be tricked.

Minor comments:
1) p.1 In the context of Verified Artificial IntelligenceSeshia [...] - needs a space.
2) p.3 Spectral centroid in 2 [...] - 2 -&gt; Figure 2.
3) p.7 Figure 8 doesn't have a caption.
4) P.7 There are some figures above SPEECH without a figure number.
5) P.7 Reference to table 9b seems to be missing.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1e2wCPeAm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>1st reply to AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkl2SjCcKQ&amp;noteId=r1e2wCPeAm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper120 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper120 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comments on our paper and suggestions including using REINFORCE or training GANs with extra loss terms related to specifications. 
We plan to use this platform to exchange ideas. Let us know if you have further suggestions or if any of your questions were not addressed.

"1) Methodology ..."
It is expected that without the summary statistics a priori specified, having a good approximation to the entire distribution is a sufficient task to have a good approximation to summary statistics (transformations) of the distribution. Hence, incorporating them should not be needed. 

The REINFORCE idea is neat and although SeqGAN authors have applied it for text generation, the space in which images and other data live is intrinsically continuous. Training on such data in a discrete manner is likely intractable.

Also, Ian Goodfellow pointed out on reddit, GANs are only defined for real-valued data .
<a href="https://www.reddit.com/r/MachineLearning/comments/40ldq6/generative_adversarial_networks_for_text/" target="_blank" rel="nofollow">https://www.reddit.com/r/MachineLearning/comments/40ldq6/generative_adversarial_networks_for_text/</a>

"2) Following (1), ..."
We have very briefly explored the idea of adding to the GAN loss a term representing violation of specifications. However, we believe that simple reversible flow models with a single loss function, like OpenAI’s Glow, represent a better solution than hand-engineering loss terms. As we mentioned, having a good approximation of the entire distribution implies having a good approximation of specifications and summary statistics. Hopefully our research community will devote more time to such models.

"3) The authors conclude..." and "4) In CIFAR10 the authors..."
The inductive bias is a condition of parameter estimation in general and the inductive bias of models like neural networks is smooth interpolation. Consider, for example, approximating a complex non-linear function with a polynomials of increasing degree. The fact that we are still improving GAN models for image generation is evidence that the models are still smooth approximations of the real distribution.

"5) On a last note, ..."
Our research on this paper shows evidence that fake samples generated with GANs can be easily identified using summary statistics. We think it's fair to say that further research that shows evidence that the summary statistics “cannot be tricked” should be carried in further research.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Hkl96tvtn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Research that has some interesting observations, but needs to take the next step to have impact on the field</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkl2SjCcKQ&amp;noteId=Hkl96tvtn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper120 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper120 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The primary purpose of this paper, from what I understand, is to show that fake samples created with common generative adversarial network (GAN) implementations are easily identified using various statistical techniques. This can potentially be useful in helping to identify artificial samples in the real world.

I think that the reviewers did an excellent job of probing into different statistical perspectives, such as looking at the continuity of the distribution of pixel intensities and the various higher moments of spectral data. I also must applaud the fact that they did not relegate themselves to image data, but branched out to speech and music data as well.

One of the first findings is that, with MNIST and CIFAR, the pixel intensities of fake samples are noticeably different when viewed from the perspective of a Kolgomorov-Smirnov Test or Jensen-Shannon Divergence comparison. This is an interesting observation, but less so than it would be if compared to something such as a variational autoencoder (VAE), which fits a KL distribution explicitly. IWGAN and LSGAN are using different metrics in their loss functions (such as Wassertein and least squares), and thus the result is not surprising or novel. I think if the authors had somehow shown how they worked their metrics into IWGAN or LSGAN to achieve better results, this could have been interesting.

Another observation the authors make is about the smoothness of the GAN distributions. This may not be so easily wrapped into the loss function, but it seems easily remedied as a post-processing step, or perhaps even a smoothing layer in the network itself. Nevertheless, this is an observation that I have not seen discussed in the literature so there is merit to at least noting the difference. It is confusing that on page 4, the authors state that they hypothesized that the smoothness was due to the pixel values themselves, and chose to alter the distribution of the original pixels in [0,1]. However, they state that in Figure 5, the smoothness remained "as expected." Did the authors misspeak here?

I found the music and speech experiments very interesting. The authors note that the synthetic Bach chorales, for instance, introduce many transitions that are not seen in the training and testing set of real Bach chorales. This, again, is interesting to note, but not surprising as the authors are judging the synthetic chorales on criteria for which they were not explicitly optimized. I do not believe these observations to be paper-worthy by themselves. However, the authors I believe have a good start on creating papers in which they specifically address these issues, showing how they can create better synthetic samples by incorporating their observations.

As to the writing style, there are many places where the writing is not quite clear. I would suggest getting an additional party to help proofread to avoid grammatical mistakes. I do not believe that the mistakes are so egregious as to impede understanding. However, it could distract from the importance on the authors future innovations if not corrected.

One last note. The title of the paper is "TequilaGAN: How to Easily Identify GAN Samples." This makes it seem as if the authors were introducing another type of GAN, like LSGAN or DCGAN. However, they are not. As a matter of fact, nowhere else in the paper is the word "TequilaGAN" mentioned. This title seems a bit sensational and misleading.

In the end, although I did find this paper to be an interesting read, I cannot recommend it for publication in ICLR.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkeWSRDxRQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>1st reply to AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bkl2SjCcKQ&amp;noteId=rkeWSRDxRQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper120 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper120 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your valuable comments. 
We are hoping to exchange ideas and knowledge in this platform. Please let us know if any of your questions were not addressed here.

"...if compared to something such as a variational autoencoder (VAE)..."
We briefly speculated that fitting a KL distribution would circumvent such problems and immediately discarded this speculation because in the literature VAEs (and AEs) produce blurrier (smoother) samples than GANs do. 

Nonetheless, our paper explicitly focuses on GANs, hence we did not include comparisons with non-GANs models. Although comparing with other models is not within the scope of our paper, we believe and mentioned in the paper that the smoothness is due to the requirements of differentiation, etc...

"in Figure 5, the smoothness remained..."
To verify that the smoothness of fake samples was independent of the data itself, we binarized the data and showed that the smoothness remained, as expected.

"...the authors are judging the synthetic chorales on criteria for which they were not explicitly optimized..."
Having a good approximation to the entire distribution is a sufficient task to have a good approximation to features and their summary statistics, e.g transitions, computed on the entire distribution. 

In our paper, we show that the fake samples have considerably more specification violations than the test data. This is evidence that the generator has not properly learned the distribution of the data. 

"...The title of the paper is "TequilaGAN:..."
The title is a wordplay between “To Kill A” and “Tequila”, hence TequilaGAN.
We will add a footnote to clarify the wordplay.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>