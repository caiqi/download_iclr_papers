<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>LeMoNADe: Learned Motif and Neuronal Assembly Detection in calcium imaging videos | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="LeMoNADe: Learned Motif and Neuronal Assembly Detection in calcium imaging videos" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SkloDjAqYm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="LeMoNADe: Learned Motif and Neuronal Assembly Detection in calcium..." />
      <meta name="og:description" content="Neuronal assemblies, loosely defined as subsets of neurons with reoccurring spatio-temporally coordinated activation patterns, or " motifs",="" are="" thought="" to="" be="" building="" blocks="" of="" neural..."="" />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SkloDjAqYm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>LeMoNADe: Learned Motif and Neuronal Assembly Detection in calcium imaging videos</a> <a class="note_content_pdf" href="/pdf?id=SkloDjAqYm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019lemonade:,    &#10;title={LeMoNADe: Learned Motif and Neuronal Assembly Detection in calcium imaging videos},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SkloDjAqYm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=SkloDjAqYm" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Neuronal assemblies, loosely defined as subsets of neurons with reoccurring spatio-temporally coordinated activation patterns, or "motifs", are thought to be building blocks of neural representations and information processing. We here propose LeMoNADe, a new exploratory data analysis method that facilitates hunting for motifs in calcium imaging videos, the dominant microscopic functional imaging modality in neurophysiology. Our nonparametric method extracts motifs directly from videos, bypassing the difficult intermediate step of spike extraction. Our technique augments variational autoencoders with a discrete stochastic node, and we show in detail how a differentiable reparametrization and relaxation can be used. An evaluation on simulated data, with available ground truth, reveals excellent quantitative performance. In real video data acquired from brain slices, with no ground truth available, LeMoNADe uncovers nontrivial candidate motifs that can help generate hypotheses for more focused biological investigations.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">VAE, unsupervised learning, neuronal assemblies, calcium imaging analysis</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We present LeMoNADe, an end-to-end learned motif detection method directly operating on calcium imaging videos.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SJlbVaTcTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting ideas applied in the neural domain</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkloDjAqYm&amp;noteId=SJlbVaTcTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper295 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper295 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Thank you for a pleasurable and informative read, I consider the writing and structure of the paper to be coherent and well written. 

Given an end-to-end learning of neural motifs, a great deal of time can be avoided, reducing the several intermediary steps required to detect motifs from calcium imaging. This paper may very well improve researchers efficiency, in particular when working with calcium imaging. The question remain to what extent these ideas may be useful in other imaging modalities, i.e. fMRI.

My main critique would be to be more explicit about why the VAE you propose, is superior to other models in the generative modelling domain.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkluQYSgRX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer4</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkloDjAqYm&amp;noteId=SkluQYSgRX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper295 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper295 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We are gratefully for AnonReviewer4's extra effort in order to provide us with a third review. We also thank him/her for the positive comments and for considering our paper to be well written and structured, and improving researchers efficiency. 

We agree that the application to fMRI and other imaging modalities makes for interesting future work. 

We took the reviewers comment into account and added a remark in the revised version at the beginning of section 3. The great benefit of this generative model in combination with the proposed VAE is the possibility to directly extract the temporal motifs and their activations and at the same time take into account the sparse nature of neuronal assemblies. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Sye_xEmt2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting problem but the advantages of the model over other deep generative models are unclear</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkloDjAqYm&amp;noteId=Sye_xEmt2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper295 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper295 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a VAE-style model for identifying motifs from calcium imaging videos. As opposed to standard VAE with Gaussian latent variables it relies on Bernouli variables and hence, requires Gumbel-softmax trick for inference. Compared to methods based on matrix factorization, the proposed method has the advantage of not requiring any preprocessing on the imaging videos. My main comments are as follows:

- How sensitive is the method to the choice of beta and other hyperparameters?  Compared to SCC which has fewer hyperparameters, how robust is the method?
- How does it perform on real data compared to methods based on spike time matrices? Do they generate similar motifs? 
- The application of the method seems quite limited to calcium imaging videos and it does not provide comparison with other deep generative models for videos. Methods such as Johnson et al. NIPS 2016 (Composing graphical models with neural networks for structured representations and fast inference) can also be applied to calcium imaging datasets and can potentially infer the motifs.

I believe the problem of inferring the neural motifs is an interesting problem; however, I think this paper requires more work to it shows its advantages over other deep generative models for video data and also itâ€™s performance on real data compared to SCC (or some other matrix factorization based approach). </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1lDvaPu6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer1 </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkloDjAqYm&amp;noteId=H1lDvaPu6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper295 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper295 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We appreciate the reviewers comments and will address them in the following and in the revised version of the manuscript which is already uploaded. 

Sensitivity to parameters: 
The main parameters that need to be chosen for each dataset individually are the maximum number of motifs and the maximum motif length. In appendix E.1 and E.2 we show the effects of over- and under-estimating these numbers for LeMoNADe and that they can be set to quite liberal values. Additionally, one of the sparsity parameters beta or Ã¢ has to be adapted to the dataset. In appendix E.3 of the revised version we provide examples of different settings of Ã¢ and beta, showing that they are complementary. This leaves us with three parameters that have to be adapted to a new dataset. For SCC also three parameters have to be chosen: number of motifs, motif length, penalty on l_1 norm of the assemblies = sparsity parameter. 
The examples in appendix E.3 also indicate that LeMoNADe's results are robust to small variations of Ã¢ and beta and the results only change significantly when the parameters are varied by more than one order of magnitude. Peter et al. describe a similar sensitivity of SCC to the variation of their sparsity parameter. 
Other hyper parameters of LeMoNADe (e.g. temperatures of the BinConcrete relaxation, learning rate) do not need to be adapted to different datasets. We found that our default settings worked well for different kinds of data. 

Results on real data compared to SCC results:
In appendix D.3 of the revised version we now show the results obtained with SCC on calcium traces of manually extracted ROIs from one of the datasets discussed in the paper. We also show, using traces extracted from the motif identified with LeMoNADe on the original dataset, that SCC and LeMoNADe find highly similar motifs on real data.  

Other generative models: 
As we mention in the related work section, a few deep generative models exist dealing with video data. However, to the best of our knowledge, none of these models is directly applicable to the task of detecting motifs with temporal structure in calcium imaging data.  
Indeed, Johnson et al. present an interesting generative model for the analysis of video data. However, we consider this model as not being able to identify motifs with temporal structure from calcium imaging data due to two limitations (for the detection of motifs in calcium videos) of the model by Johnson et al.:
1. Neuronal assemblies are expected to extend over multiple frames (depending on
the frame rate of the recording this could be easily more than 20 frames). Since in Johnson
et al.'s model the underlying latent process is a relatively simple first-order Markovian (switching) linear process, representing longer-term temporal dependencies will be very hard to achieve due to the usually exponential forgetting in such systems. In fact, Johnson et al.'s framework would need to be significantly extended, e.g. using LSTM units, to adapt their model for this task, which is a non-trivial task and could be considered to be a paper in its own right.
2. In the model of Johnson et al. each frame is generated from exactly one of K latent states. For calcium imaging, however, most frames are not generated by one of the motifs but from noise. While LeMoNADe has the chance to set the latent variables for noise frames simply to zero, Johnson et al.'s model would have to choose one motif as responsible for the frame even if it contains only noise. Moreover, LeMoNADe has the flexibility to also allow the different motifs to temporally overlap. This is also not possible in the model by Johnson et al., since they allow always only exactly one latent state for each frame.  
For this reason, we cannot compare to Johnson et al. on the task of detecting motifs in calcium imaging data. In the revised version of the manuscript we extended our citation of Johnson et al. with a short explanation why the model is not directly applicable to our setup of motif detection from calcium imaging data. 

The application is limited to calcium imaging data:
The model and network architectures are indeed optimised for the task of detecting motifs in calcium imaging data. This is, however, no downside of the method. Calcium imaging is a method of first importance in neurophysiology. It allows the concurrent monitoring of the individual actions of thousands of neurons at the same time. As explained above, no other method is directly applicable to finding temporal motifs in calcium imaging data and in order to do so we had to adapt our method to the special properties of calcium imaging and neuronal assemblies. Nevertheless, our approach could also be adapted for detecting spatio-temporal motifs in data from other imaging techniques, such as voltage-sensitive dyes or functional magnetic resonance imaging (fMRI).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1l-SROznQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>i liked this paper last time i reviewed it, and i like it still :)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkloDjAqYm&amp;noteId=B1l-SROznQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper295 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper295 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">last time i had two comments:
1. the real data motifs did not look like what i'd expect motifs to look like. now that the authors have thresholded the real data motifs, they do look as i'd expect.
2. i'm not a fan of VAE, and believe that simpler optimization algorithms might be profitable.  i acknowledge that SCC requires additional steps; i am not comparing to SCC. rather, i'm saying given your generative model, there are many strategies one could employ to estimate the motifs.  i realize that VAE is all the rage, and is probably fine.  in my own experiments, simpler methods often work as well or better for these types of problems.  i therefore believe this would be an interesting avenue to explore in future work.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hyxv_pZwa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>we liked your review last time, and we like it still ;)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkloDjAqYm&amp;noteId=Hyxv_pZwa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper295 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper295 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We kindly thank the reviewer for his/her positive feedback. We would appreciate it if you could state again - as you did in your last review - why the problem we address is an important one and why quote "having an end-to-end procedure to learn motifs would be awesome". :) 
Of course there are different approaches in general to infer generative models. However, given our particular generative model our approach to do inference via VI in the form of a VAE is rather intuitive and stable compared to e.g. sampling based approaches which are computationally more expensive or EM based approaches which are usually less flexible. So we are not using VAEs just because they are all the rage right now (then we would use a GAN anyway ;) ). Nevertheless, we will of course continue doing research on this topic and will also investigate other approaches in future work. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1xhWlnDTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>i like your response :)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkloDjAqYm&amp;noteId=r1xhWlnDTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper295 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper295 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">to address why "end-to-end" is so important, the other strategy is both philosophically and statistically unpleasant.  philosophically, pipelining together many disparate processing stages creates all sorts of problems, as one cannot ever quite evaluate the impact of each decision on the final solution, because it is combinatorial. statistically, when one pipes together many algorithms, typically, the uncertainty associated with the output of each algorithm is lost when piping its MLE or MAP into the next algorithm.  so, if one were to report confidence intervals, they would be way too confident.  moreover, the goal in generating calcium imaging data is to discover motifs (or other interesting patterns), rather than, say, find all the spikes, which is really just a nuisance parameter.  so, piping things together results in optimizing each algorithm for the wrong metric. 

previous review is below:

i am extremely knowledgeable in calcium imaging analysis, much less so in DL, VAE, etc. the problem that the authors address is very important and timely. the pre-processing of datasets for calcium imaging (and other modalities) is a mess, lots of steps, each with lots of parameters, inferences in downstream steps do not typically consider uncertainty in upstream tasks, etc. so, having and "end to end" procedure to learn motifs would be awesome.

there are two aspects of this manuscript that i didn't love

1. is VAE really necessary here? it seems like an extension of the sparse dictionary learning stuff might be sufficient, and much simpler? i'm concerned because more complex methods have more algorithmic parameters to tune, and therefore, having one "end to end system" that is nearly as complicated as 3 disparate methods does not really one of the main motivating problems. the authors are clearly capable of doing a simpler thing. i realize this paper is not about that. but, for this paper, some discussion on why this approach was taken instead of the dictionary learning one, and some insight into the complications associated with actually running this method successfully on a new dataset would be highly desirable. fig 1 implies that lemonade is not just better, but also simpler, than other stuff. perhaps you could demonstrate or justify that a bit more?

2. the learned motifs in the simulated data looks like what i'd expect the motifs to look like. however, in the real data, they kind of look mostly like noise. i realize you did a bootstrap thing, etc. nonetheless, i am not convinced that you found legit motifs. specifically you even mention that the 3rd motif is not really even a motif at all, rather, just a single event. i wish there was a more convincing visualization or movie you could provide that made it really clear that you identified real biological motifs. i don't really have any good ideas for how to do that though.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>