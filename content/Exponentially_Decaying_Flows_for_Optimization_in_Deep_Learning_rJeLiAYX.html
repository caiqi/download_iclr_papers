<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Exponentially Decaying Flows for Optimization in Deep Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Exponentially Decaying Flows for Optimization in Deep Learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rJe-LiA5YX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Exponentially Decaying Flows for Optimization in Deep Learning" />
      <meta name="og:description" content="The field of deep learning has been craving for an optimization method that shows outstanding property for both optimization and generalization.  We propose a method for mathematical optimization..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rJe-LiA5YX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Exponentially Decaying Flows for Optimization in Deep Learning</a> <a class="note_content_pdf" href="/pdf?id=rJe-LiA5YX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019exponentially,    &#10;title={Exponentially Decaying Flows for Optimization in Deep Learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rJe-LiA5YX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">The field of deep learning has been craving for an optimization method that shows outstanding property for both optimization and generalization.  We propose a method for mathematical optimization based on flows along geodesics, that is, the shortest paths between two points, with respect to the Riemannian metric induced by a non-linear function. In our method, the flows refer to Exponentially Decaying Flows (EDF), as they can be designed to converge on the local solutions exponentially. In this paper, we conduct experiments to show its high performance on optimization benchmarks (i.e., convergence properties), as well as its potential for producing good machine learning benchmarks (i.e., generalization properties).</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">optimization, deep learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Introduction of a new optimization method and its application to deep learning.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SJl2fxm937" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Missing critical references; lacking novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJe-LiA5YX&amp;noteId=SJl2fxm937"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper151 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper151 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The first abstract reads, "The field of deep learning has been craving for an optimization method that shows outstanding property for both optimization and generalization", what does it mean to say that the field is "craving"? Yes, a rigorously (mathematically proven or empirically tested extensively) better algorithm (compared to the existing algorithm) to optimize the ERM problems that appear in training deep neural networks is always valuable. But I find that this paper is not delivering on both these ends. To summarize, the main idea of the paper is to view ERM optimization in continuous time to develop algorithm to train neural networks. Section 2 introduces geodesic flows with NO references which can easily mislead readers that the math is new. For a field that is known in mathematics for essentially essentiall hundreds of years, I'd expect that the section is filled with references. Here's one in the recent years that tries to give a good overview -- <a href="https://arxiv.org/abs/1609.03890." target="_blank" rel="nofollow">https://arxiv.org/abs/1609.03890.</a>

Essentially Theorem 2.1 is a restatement of Gronwall's Lemma with the implications of Theorem 2.1 not clear at all. Yes, we can choose F(t) to be anything and get exponential convergence rate in the continuous time, but it is *not* true that it is possible to get such exponential convergence in the discrete time (which is where training happens eventually). The technical difficulty is in figuring out the right discretization that can achieve the maximum possible convergence rate. See https://arxiv.org/abs/1805.00521 for example. The authors Euler's explicit discretization which is not the best even in simple settings.  

All the modification the authors propose in Section 3, 4 and 5 are generic and straight out of standard numerical linear algebra textbooks.

As far as I can see, the experiments are setup in a somewhat standard way with CIFAR 10/100. I think MSGD should be tested with mini-batch 64 not 250 since smaller batch size leads to better generalization performace as recent works indicate (See for example, https://openreview.net/forum?id=HyWrIgW0W). </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BkxhF7W9hm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A lot of sophisticated mathematical concepts, not much explained, hard to understand.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJe-LiA5YX&amp;noteId=BkxhF7W9hm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper151 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper151 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
I found the paper poorly written, with many typos and incorrect formulations. It contains several sophisticated mathematical concepts (probably from differentiable geometry, dynamical systems and differential equations) that I believe a majority of ICLR audience would not be so familiar with; and these notions are not defined nor explained (e.g Riemannian metrics/manifold, Krylov subspaces). Section 2 presents some theoretical results and formal definition of exponentially decaying flows but I could not see the intuition or the goal of these results. In Section 3, for the main
optimization problem (13), it is assumed that "there exists no stationary point except for a minima." and so, I understand that there is only "a minima" or at least that all the local minima are assumed to have the same value, which not realistic for deep learning.


Comments and questions

Section 1:

After speaking of gradient descent methods, the authors say "Another class of methods... is adaptive methods such as AdaGrad and Adam": These are all based on stochastic gradient descent algorithm, so this distinction is surprising.

Section 2:

"Jacobian with variable w": does not make sense (although I can guess the author mean w.r.t.)

"J is regular and the equation has a unique solution": what is regular here? No equation has been mentioned before

"...becomes a Riemannian manifold under some appropriate conditions": which conditions?

Section 3:

"Applying Theorem 2.2, we obtain the differential equation": I can't see how applying Theorem 2.2 leads to a differential equation.

Section 4:

"we consider a projection which maps r to a vector P_k(A, v) in the k-th order Krylov subspace such that r = P_\infty(A, v)": I don't understand.

"Particularly, in the case that χ = −1, we set λ = akJ φ T F k b with a, b &gt; 0, so that the convergence rate of (24) stays exponential": why is the case χ = −1 relevant to point out?

"Finally, to accelerate the EDF-based methods, it is sometimes effective to change equation (18) into a second-order differential equation": is it an empirical observation? or is there an explanation?

Section 5:

Again, so many notations (not all conventional: e.g. \theta denotes the softmax function), just to present a standard loss for a deep learning model.

Section 6:

"As has been found, second-order-methods on full-batch training converge to the solution within a few iterations.":  Reference?

The method is compared to Nesterov accelerated gradient (NAG) for the data-fitting problem (fig 1) but not on the other tasks (figs 2 to 5).

In the end, it was still unclear to me what the training consists of with the introduced exponentially decaying flows. I understand that a differential equation was formulated but then how is it solved iteratively? What is the cost of solving it? In Section 6, different algorithms are compared in terms of number of steps. What about the computational cost?

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1gU-JsW37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Paper needs a major revision, too many unjustified claims are made</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJe-LiA5YX&amp;noteId=H1gU-JsW37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper151 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">27 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper151 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper is overall unclear and not well-written. There are lots of typos, grammar mistakes, misuse of terminology,... that obscure the clarity of the paper. More importantly, the contribution is unclear to me, and I think the paper needs a major rewrite in order to comply with the type of standard we expect from an international publication in a machine learning venue. Below I give some comments that I hope will help the authors to revise their submission.

1) “In addition, it is unclear whether the method has advantages in generalization performance.“
There is some theoretical  results that are worth citing such as
Bottou, L., &amp; Bousquet, O. (2008). The tradeoffs of large scale learning. In Advances in neural information processing systems (pp. 161-168).
Under certain assumptions, second-order methods are known to generalize more poorly.

2) Page 2, “positive matrix G”
You mean positive-definite matrix since G is a covariance matrix. Positive and Positive-definite matrices are different concepts.

3) Assumption section 3
The authors make the assumption “there exists no stationary point except for a minima”. This is a rather strong assumption which of course does not apply do deep neural networks. The authors should discuss how to relax this assumption.

4) Derivation section 3
The authors drop the first term in the decomposition of the Hessian-momentum term without providing any justification. Why are you making this simplification? It is probably worth pointing out this resembles the Gauss-Newton approximation and you are left with a positive-definite approximation of the Hessian, therefore ignoring any negative curvature.

5) Computation of the matrix
“ One of the basic methods to construct such a projection... requires only the matrix multiplication”
I would also suggest mentioning that these methods usually construct sparse matrices such as tri-diagonal matrices, see e.g. Nocedal, J., &amp; Wright, S. J. (2006). Numerical optimization 2nd.

6) Suggested approach
a) The whole derivation is rather unclear, the author seem to arrive to a second-order equation similar to the one derived for Nesterov accelerated gradient in Su et al. 2014. You should contrast how your equation differs from theirs.
b) What are the convergence guarantees for your approach? Consider deriving a proof of convergence as in Su et al. 2014 (at the very least I would expect an asymptotic result)
c) Second-order ODES are difficult to discretize (see discussion in Su et al.) and even if the continuous method is guaranteed to converge, the discretization procedure might not have such guarantees. You need to add a discussion about discretization and explain what approach you used in practice.

7) Deterministic vs Stochastic setting
The derivation presented in the main text assumes full gradients are computed but of course, in practice, one would only use mini-batches. The authors make some rather bold and unjustified claims regarding the ability of their method to generalize to a stochastic setting. In particular they claim “setting the hyperparameter k small makes EDF be compatible with stochastic
approaches and take their advantages”. This statement requires a solid justification. I do not believe this is true. If you add noise to your differential equation, you need to ensure the noise has certain properties (vanishing noise in the limit or bounded noise) if you want to guarantee convergence. I recommend the authors read the relevant literature, for instance:
Li, Q., Tai, C., et al. (2015). Stochastic modified equations and adaptive stochastic gradient algorithms. arXiv preprint arXiv:1511.06251.
Krichene, W., Bayen, A., and Bartlett, P. L. (2015). Accelerated mirror descent in continuous
and discrete time. In Advances in neural information processing systems, pages 2845–2853.

8) Experiments
Choice of hyper-parameters: the authors need to explain how they pick the hyper-parameters. Simply listing what values are used is not sufficient. You need to show you’ve tried different settings for the competing methods. You said
“The step sizes for Momentum, NAG, and Adam were fixed to 0.01, 0.001, and 0.001, respectively”. How did you pick these values?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">2: Strong rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>