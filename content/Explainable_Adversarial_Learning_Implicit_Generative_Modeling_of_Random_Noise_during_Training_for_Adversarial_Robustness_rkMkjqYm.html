<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Explainable Adversarial Learning: Implicit Generative Modeling of Random Noise during Training for Adversarial Robustness | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Explainable Adversarial Learning: Implicit Generative Modeling of Random Noise during Training for Adversarial Robustness" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rkMk9j0qYm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Explainable Adversarial Learning: Implicit Generative Modeling of..." />
      <meta name="og:description" content="We introduce Explainable Adversarial Learning, ExL, an approach for training neural networks that are intrinsically robust to adversarial attacks. We find that the implicit generative modeling of..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rkMk9j0qYm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Explainable Adversarial Learning: Implicit Generative Modeling of Random Noise during Training for Adversarial Robustness</a> <a class="note_content_pdf" href="/pdf?id=rkMk9j0qYm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019explainable,    &#10;title={Explainable Adversarial Learning: Implicit Generative Modeling of Random Noise during Training for Adversarial Robustness},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rkMk9j0qYm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We introduce Explainable Adversarial Learning, ExL, an approach for training neural networks that are intrinsically robust to adversarial attacks. We find that the implicit generative modeling of random noise with the same loss function used during posterior maximization, improves a model's understanding of the data manifold furthering adversarial robustness. We prove our approach's efficacy and provide a simplistic visualization tool for understanding adversarial data, using Principal Component Analysis. Our analysis reveals that adversarial robustness, in general, manifests in models with higher variance along the high-ranked principal components. We show that models learnt with our approach perform remarkably well against a wide-range of attacks. Furthermore, combining ExL with state-of-the-art adversarial training extends the robustness of a model, even beyond what it is adversarially trained for, in both white-box and black-box attack scenarios.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Adversarial Robustness, PCA variance, PCA subspace, Generative Noise Modeling, Adversarial attack, Adversarial Robustness Metric</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Noise modeling at the input during discriminative training improves adversarial robustness. Propose PCA based evaluation metric for adversarial robustness</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rylae2Pk6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Potentially interesting ideas but exposition and motivation are too imprecise</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkMk9j0qYm&amp;noteId=rylae2Pk6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper499 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper499 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors suggest a novel approach, ExL to adversial training using multiplicate noise that is learned jointly with model parameters using SGD. They propose a likelihood framework to interpret why the approach is successful. PCA of intermediate layers is used to suggest that ExL trained NNs are more robust because they better cover space around the observed data manifold. Results on three canonical datasets using blackbox and whitebox adversarial attacks suggest that ExL can be helpful in defending against BB attacks, and is easily combined with other adversial training approaches such as PGD to further improve robustness. 

I think there are some interesting ideas in the paper but the motivation is too hand-wavey and the method exposition is insufficient. Firstly, refering to N as "noise" seems problematic. N is learned so is more akin to a latent variable than noise. In particular it is not random at any point apart from initialization (which is equally true for model parameters). Additionally it is very strange to me that these "learned masks" are fixed in position in the mini-batch, since an index in the minibatch has no external meaning. The authors don't say whether the data points are permuted at each epoch, which is important since it effects whether the same data point at subsequent epochs uses the same mask (I don't even know from the exposition whether this is intended or not). 

Secondly, the "likelihood framework" is very hand-wavey. When modeling P(Y|X) we think of Y and X as random variables, with the training set (X_train, Y_train) as samples. Nothing in this says that P(Y|X) should not hold for X other than X_train, e.g. X=A. Thus the introduction of P(Y|X,A) is superfluous. While Eq 1 is mathematically correct it is therefore pretty meaningless in terms of understanding what ExL is doing. The text does little to help. 

Thirdly, "Explainable" in the title is dangerously close to "interpretable", which ExL is certainly not. The N give only the vaguest sense of the features being used in the data (see Fig 1b). 

There might be Bayesian/likelihood-based interpretation of ExL, other than that proposed by the authors. I was surprised the similarity to dropout wasn't mentioned. Yarin Gal's work pointed out the similarity between dropout and variational Bayes with a specific variational posterior, and the ExL looks a look like opitmizing the resulting objective including per pixel dropout weights (although not stochastically which is strange). Alternatively one could think of X*N as latent variables and we are modeling P(Y|X*N). This could be interpreted as modeling noise in X for example. 

The connection to dropout makes me suspicious that while ExL is proposed as an adversarial training method it is really just performing more effective regularization, which should in itself smooth the prediciton surface and improve robustness to adversarial examples. It would be feasible to test this, for example by comparing different formes of regularization with and without ExL. 

Why threshold grad L &lt; 0 when learning N? What stops the model from just setting N=0? 

Minor comments
- Abstact: "prove" -&gt; "show" (proofs require proofs!)
- Don't re-use N as the number of training images. 
- page 5. X=X+alpha... ugh. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hygb87Ns2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Trained multiplicative noise improves robustness to adversarial attacks</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkMk9j0qYm&amp;noteId=Hygb87Ns2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper499 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper499 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper includes multiplicative noise N in training data, such that it trains a model on X\timesN.  The model then trains on both model parameters theta and on the noise itself in an effort to achieve adversarial robustness

Quality
- The empirical results appear sound, which suggest that multiplicative trained noise is a sensible approach
- The writing that leads up to these results is in my opinion problematically vague, as it makes a variety of loose claims and theoretical connections and uses imprecise language.  This results in scientific imprecision, which I detail below in "clarity".
- Some key choices are unsupported.  For example, the choice of fixing noise across batch (as in, there are k noise maps N that are reused across each minibatch) is unclear.  Certainly if you want to train these noise maps, some reuse is required, but the  given sentence "since we want to learn the noise, we use the same k noise templates across all mini-batches..." does not explain this critical choice.
- What does it mean to train a noise map?  I know what it means mechanically, but there is a vague set of explanations that do not leave an empirical or theoretical understanding of why this is a sensible choice.  Connecting this to similarly ill-defined comments on "excessive linearity" or "off manifold" deepen this issue.

Clarity
- The mechanics of the algorithm are sufficiently clear, but the interpretation is deemed highly vague, such that the result is a rather simple algorithm that is wrapped up with loose and unclear explanations.  Some examples are below:
- critical terms are introduced without technical explanation.  For example, "model viability" is introduced in \emph{} as the key questoin in the third paragraph of the introduction.  However, after reading this sentence several times, I don't know what this question is trying to ask.  Is it just "we seek to improve robustness to adversarial attacks"?  Is "such out-of-sample data" meant to point to "off manifold" points?  Other terms like "inculcates", "off manifold", "inherits", "right prediction vs right explanation", etc. add to this issue.
- On that point, the term "off manifold" is used often but is highly vague.  We all know what this means colloquially, but if you want to make a scientific point about it, there needs to be rigor applied to this definition.
- The likelihood perspective (section 2.2) is not rigorous.  These distributions are undefined to a problematic extent.  For example, p(Y|X,\theta) is clear enough, but then what is p(Y|X,A,\theta)?  I understand A is the adversarial inputs, but what *specifically* is this model?  How *specifically* does A represent the adversarial inputs?  Absent this level of definition and detail on this and other points, the likelihood section is deemed highly vague. 

Originality and Significance
- At a basic level, this manuscript offers multiplicative and trained noise maps.  That is a meaningful (albeit small) contribution.  The results are rigorous enough to make it interesting in its own right, but the remainder of the choices are justified via vague language rather than rigorous empiricism.  This limits the perceived significance.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJl7tASBnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Training networks with multiplicative input noise: interesting but with questions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkMk9j0qYm&amp;noteId=rJl7tASBnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper499 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper499 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this paper, a so-called explainable adversarial learning approach is proposed. It is shown that the use of multiplicative input noise can enhance the robustness of neural networks. My detailed comments are as below. 

1) The definition of 'Explainable' is not convinced. The authors claimed that "the model not only finds the right prediction but also the right explanation. Noise inculcates this explainable behavior by discovering some knowledge about the input/output distribution during training." The distribution of the learnt noise is not surprising since by minimizing the training loss, the noise should reflect the example pattern. I am not convinced about the name 'explainable adversarial learning'.

2) In ExL (Algorithm 1), why not consider the affine transformation X N_1 + N_2, where both N_1 and N_2 are learnt from the training process? Is this better than X N_1? What is the rationale behind using universal noise pattern over different minibatches. 

3) The proposed ExL framework is quite similar to training over perturbed examples. Here the perturbation is given by X \times N.  It is expected that ExL will not outperform adversarial training but will outperform the plain training in robustness. From this perspective, I feel that the results are not very impressive. 

4) The study on Sec. 2.3 via PCA is nice and Figure 3 is informative. 

5) Since the proposed ExL training framework is scaled to large datasets, it is better to cover more experiments, e.g., ImageNet. 

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>