<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Modulating transfer between tasks in gradient-based meta-learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Modulating transfer between tasks in gradient-based meta-learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HyxpNnRcFX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Modulating transfer between tasks in gradient-based meta-learning" />
      <meta name="og:description" content="Learning-to-learn or meta-learning leverages data-driven inductive bias to increase the efficiency of learning on a novel task. This approach encounters difficulty when transfer is not mutually..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HyxpNnRcFX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Modulating transfer between tasks in gradient-based meta-learning</a> <a class="note_content_pdf" href="/pdf?id=HyxpNnRcFX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019modulating,    &#10;title={Modulating transfer between tasks in gradient-based meta-learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HyxpNnRcFX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Learning-to-learn or meta-learning leverages data-driven inductive bias to increase the efficiency of learning on a novel task. This approach encounters difficulty when transfer is not mutually beneficial, for instance, when tasks are sufficiently dissimilar or change over time. Here, we use the connection between gradient-based meta-learning and hierarchical Bayes to propose a mixture of hierarchical Bayesian models over the parameters of an arbitrary function approximator such as a neural network. Generalizing the model-agnostic meta-learning (MAML) algorithm, we present a stochastic expectation maximization procedure to jointly estimate parameter initializations for gradient descent as well as a latent assignment of tasks to initializations. This approach better captures the diversity of training tasks as opposed to consolidating inductive biases into a single set of hyperparameters. Our experiments demonstrate better generalization on the standard miniImageNet benchmark for 1-shot classification. We further derive a novel and scalable non-parametric variant of our method that captures the evolution of a task distribution over time as demonstrated on a set of synthetic regression tasks as well as an evolving dataset adapted from miniImageNet.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">meta-learning, clustering, learning-to-learn, mixture, hierarchical Bayes, hierarchical model, gradient-based meta-learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We use the connection between gradient-based meta-learning and hierarchical Bayes to learn a mixture of meta-learners that is appropriate for a heterogeneous and evolving task distribution.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_S1l6hQFp3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A more systematic evaluation is necessary</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxpNnRcFX&amp;noteId=S1l6hQFp3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1493 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1493 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a mixture of hierarchical Bayesian models for meta-learning to modulate transfer between various tasks to be learned. A non-parametric variant is also developed to capture the evolution of a task distribution over time. These are very fundamental and important problems for meta-learning. However, while the proposed model appears to be interesting, the evaluation is less convincing. 

1. The performance of few-shot classification on MiniImageNet is not comparable to the state of the art (Table 2, Table 1). Especially, by Table, the proposed model performs much worse than existing methods (50% vs 60%). More discussions and explanations on this experiment are clearly required.

2. A more systematic and realistic evaluation is necessary to justify the proposed method. As a method that aims to cope with heterogeneous or even evolving task distributions, it is expected to work well in practice and outperform those baselines that are designed for a single task distribution. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Skl0gf6On7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Promising, but more work needed </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxpNnRcFX&amp;noteId=Skl0gf6On7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1493 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1493 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a mixture of MAMLs (Finn et al., 2017) by exploiting the interpretation of MAML as a hierarchical Bayesian model (Grant et al. 2018). They propose an EM algorithm for joint training of parameter initializations and assignment of tasks to initializations. They further propose a non-parametric approach to dynamically increase the capacity of the meta learner in continual learning problems. The proposed method is tested in a few-shot learning setup on miniImagenet, on a synthetic continual learning problem, and an evolutionary version of miniImagenet.

[Strengths]

+ Modeling the initialization space is an open research question and the authors make a sound proposal to tackle this.
+ The extension to continual learning is particularly interesting, as current methods for avoiding catastrophic forgetting. inevitably saturate model parameters. By dynamically increasing the meta-learner's capacity, this approach can in principle bypass catastrophic forgetting.

[Weaknesses]

- There is nothing in the algorithm that prevents mode collapse, and the only thing breaking symmetry is random initialization. In fact, figure 5 and 6 suggest mode collapse occurs even in the non-parametric case. A closely related paper that may be of interest ( Kim et al., 2018, <a href="https://arxiv.org/abs/1806.03836" target="_blank" rel="nofollow">https://arxiv.org/abs/1806.03836</a> ) address this issue by using Stein Variational SGD.
- Results on miniImagenet are not encouraging; the gains on MAML are small and similar methods that generalize MAML (Kim et al., 2018, Rusu et al., 2018) achieve significantly better performance.
- Experiments on evolving tasks suggest the method is not able to capture task diversity. In the synthetic experiment (figure 5), the model suffers mode collapse when a sufficiently difficult task is introduced. Ultimately, it performs on par with MAML, despite having three times the capacity. Similarly, on the evolving miniImagenet dataset, figure 6 indicates there is no cluster differentiation across tasks.
- The paper needs major polishing.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJeOO--12Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Gradient-base few-shot learning. Extends MAML to a mixture distribution, to allow for internal task clustering. Falls short of recent state-of-art results, while being even a lot slower than MAML</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxpNnRcFX&amp;noteId=BJeOO--12Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1493 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1493 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:

This work tackles few-shot (or meta) learning, providing an extension of the gradient-based MAML method to using a mixture over global hyperparameters. Each task stochastically picks a mixture component, giving rise to task clustering. Stochastic EM is used for end-to-end learning, an algorithm that is L times more expensive than MAML, where L is the number of mixture components. There is also a nonparametric version, based on Dirichlet process mixtures, but a large number of approximations render this somewhat heuristic.

Comparative results are presented on miniImageNet (5-way, 1-shot). These results are not near the state-of-the art anymore, and some of the state-of-art methods are simpler and faster than even MAML. If expensive gradient-based meta-learning methods are to be consider in the future, the authors have to provide compelling arguments why the additional computations pay off.

- Quality: Paper is technically complex, but based on simple ideas. In the case of
   infinite mixtures, it is not clear what is done in the end in the experiments.
   Experimental results are rather poor, given state-of-the-art.
- Clarity: The paper is not hard to understand. What is done, is done cleanly.
- Originality: The idea of putting a mixture model on the global parameters is not
   surprising. Important questions, such as how to make this faster, are not
   addressed.
- Significance: The only comparative results on miniImageNet are worse than the
   state-of-the-art by quite a margin (admittedly, the field moves fast here, but it
   is also likely these benchmarks are not all that hard). This is even though better
   performing methods, like Versa, are much cheaper to run

While the idea of task clustering is potentially useful, and may be important in practical use cases, I feel the proposed method is simply just too expensive to run in order to justify mild gains. The experiments do not show benefits of the idea.

State of the art results on miniImageNet 5-way, 1-shot, the only experiments here which compare to others, show accuracies better than 53:
- Versa: <a href="https://arxiv.org/abs/1805.09921." target="_blank" rel="nofollow">https://arxiv.org/abs/1805.09921.</a>
   Importantly, this method uses a simpler model (logistic regression head models)
   and is quite a bit faster than MAML, so much faster than what is proposed here
- BMAML: https://arxiv.org/abs/1806.03836.
   This is also quite complex and expensive, compared to Versa, but provides good
   results.

Other points:
- You use a set of size N+M per task update. In your 5-way, 1-shot experiments,
   what is N and M? I'd guess N=5 (1 shot per class), but what is M? If N+M &gt; 5,
   then I wonder why results are branded as 5-way, 1-shot, which to mean means
   that each update can use exactly 5 labeled points.
   Please just be exact in the main paper about what you do, and what main
   competitors do, in particular about the number of points to use in each task
   update.
- Nonparametric extension via Dirichlet process mixture. This is quite elaborate, and
   uses further approximations (ICM, instead of Gibbs sampling).
   Can be seen as a heuristic to evolve the number of components.
   What is given in Algorithm 2, is not compatible with Section 4. How do you merge
   your Section 4 algorithm with stochastic EM? In Algorithm 2, how do you avoid
   that there is always one more (L -&gt; L+1) components? Some threshold must be
   applied somewhere.
   An alternative would be to use split&amp;merge heuristics for EM.
- Results reported in Section 5 are potentially interesting, but entirely lack a
   reference point. The first is artificial, and surely does not need an algorithm of this
   complexity. The setup in Section 5.2 is potentially interesting, but needs more
   work, in particular a proper comparison to related work.
   This type of effort is needed to motivate an extension of MAML which makes
   everything quite a bit more expensive, and lacks behind the state-of-art, which
   uses amortized inference networks (Versa, neural processes) rather than
   gradient-based.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>