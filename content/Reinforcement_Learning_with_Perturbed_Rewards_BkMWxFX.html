<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Reinforcement Learning with Perturbed Rewards | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Reinforcement Learning with Perturbed Rewards" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BkMWx309FX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Reinforcement Learning with Perturbed Rewards" />
      <meta name="og:description" content="Recent studies have shown the vulnerability of reinforcement learning (RL) models in noisy settings. The sources of noises differ across scenarios. For instance, in practice, the observed reward..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BkMWx309FX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reinforcement Learning with Perturbed Rewards</a> <a class="note_content_pdf" href="/pdf?id=BkMWx309FX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019reinforcement,    &#10;title={Reinforcement Learning with Perturbed Rewards},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BkMWx309FX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Recent studies have shown the vulnerability of reinforcement learning (RL) models in noisy settings. The sources of noises differ across scenarios. For instance, in practice, the observed reward channel is often subject to noise (e.g., when observed rewards are collected through sensors), and thus observed rewards may not be credible as a result. Also, in applications such as robotics, a deep reinforcement learning (DRL) algorithm can be manipulated to produce arbitrary errors. In this paper, we consider noisy RL problems where observed rewards by RL agents are generated with a reward confusion matrix. We call such observed rewards as perturbed rewards. We develop an unbiased reward estimator aided robust RL framework that enables RL agents to learn in noisy environments while observing only perturbed rewards. Our framework draws upon approaches for supervised learning with noisy data. The core ideas of our solution include estimating a reward confusion matrix and defining a set of unbiased surrogate rewards. We prove the convergence and sample complexity of our approach. Extensive experiments on different DRL platforms show that policies based on our estimated surrogate reward can achieve higher expected rewards, and converge faster than existing baselines. For instance, the state-of-the-art PPO algorithm is able to obtain 67.5% and 46.7% improvements in average on five Atari games, when the error rates are 10% and 30% respectively. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">robust reinforcement learning, noisy reward, sample complexity</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A new approach for learning with noisy rewards in reinforcement learning</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HJxmW5NZ67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Related work on robust RL with perturbed state transitions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkMWx309FX&amp;noteId=HJxmW5NZ67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1050 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your work! A related work on robust RL with the perturbations on the state transitions (rather than the rewards as in your setting) is [1].

[1] <a href="https://papers.nips.cc/paper/6897-reinforcement-learning-under-model-mismatch." target="_blank" rel="nofollow">https://papers.nips.cc/paper/6897-reinforcement-learning-under-model-mismatch.</a> </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1xorCQWp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting and relatively unexplored variant of RL.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkMWx309FX&amp;noteId=H1xorCQWp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1050 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1050 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
This paper investigates reinforcement learning with a perturbed reward signal. In particular, the paper proposes a particular model for adding noise to the reward function via a confusion matrix, which offers a nuanced notion of reward-noise that is not too complicated so-as to make learning impossible. I take this learning setting to be both novel and interesting for opening up areas for future work. The central contributions of the work are to 1) leverage a simple estimator to prove the convergence of Q-Learning under the reward-perturbed setting along with the sample-complexity of a variant of (Phased) Q-Learning which they call "Phrased" Q-Learning, and 2) An algorithmic scheme for learning in the reward-perturbed setting (Algorithm 1), and 3) An expansive set of experiments that explore the impact of various reward models on learning across different environment-algorithm combinations. The sample complexity term extends Phased Q-Learning to incorporate aspects of the reward confusion matrix, and to my knowledge is novel. Further, even though Theorem 1 is unsurprising (as the paper suggests), I take the collection of Theorem 1, 2, and 3 to be collectively novel.

Indeed, the paper focuses on an interesting and relatively unexplored direction for RL. Apart from the work cited by the paper (and perhaps work like Krueger et al. (2016), in which agents must pay some cost to observe true rewards), there is little work on learning settings of this kind. This paper represents a first step in gaining clarity on how to formalize and study this problem. I did, however, find the analysis and the experiments to be relatively disjointed -- the main sample complexity result presented by the paper (Theorem 2) was given for Phrased Q-Learning, yet no experiments actually evaluate the performance of Phrased Q-Learning. I think the paper could benefit from experiments focused on simple domains that showcase how traditional algorithms do in cases where it is easier to understand (and visualize) the impact of the reward perturbations (simple chain MDPs, grid worlds, etc.) -- and specifically experiments including Phrased Q-Learning. 

Pros:
	- General, interesting new learning setting to study.
	- Initial convergence and sample complexity results for this new setting.
	- Depth and breadth of experimentation (in terms of diversity of algorithms and environments), includes lots of detail about the experimental setup.

Cons:
	- Clarity of writing: lots of typos and bits of math that could be more clear (see detailed comments below)
	- The plots in Section 4 are all extremely jagged. More trials seem to be required. Moreover, I do think simpler domains might help offer insights into the reward perturbed setting.
	- The reward perturbation model is relatively simple.

Some high level questions/comments:
	- Why was Phrased Q-Learning not experimented with?
	- Why use majority voting as the rule? When this was introduced it sounded like any rule might be used. Have you tried/thought about others?
	- Your citation to Kakade's thesis needs fixing; it should read:
		"Kakade, Sham Machandranath. On the sample complexity of reinforcement learning. Ph.D Thesis. University of London, 2003."

		(right now it is cited as "(Gatsby 2003)" throughout the paper)
	- You might consider picking a new name for Phrased Q-Learning -- right now the name is too similar to Phased Q-Learning from [Kearns and Singh NIPS 1999].
        - As mentioned in the "cons" section, the confusion matrix is still a somewhat simple model of reward noise. I was left wondering: what might be the next most complicated form of adding reward noise? How might the proposed algorithm(s) respond to this slightly more complex model? That is, it's unclear how general the results are, or if they are honed too tightly to the specific proposed reward noise model. I was hoping the authors could respond to this point.

	
Section 0) Abstract:
	- Not immediately clear what is meant by "vulnerability" or "noisy settings". Might be better to pick a more clear initial sentence (same can be said of the "sources of noise..."")

Section 1) Introduction:
	- "adversaries in real-world" --&gt; "adversaries in the real-world"
	- You might consider citing Loftin et al. (2014) regarding the bulleted point about "Application-Specific Noise".
	- "unbiased reward estimator aided reward robust reinforcement learning framework" --&gt; this was a bit hard to parse. Consider making more concise, like: "unbiased reward estimator for use in reinforcement learning with perturbed rewards".
	- "Our solution framework builds on existing reinforcement learning algorithms, including the recently developed DRL ones" --&gt; cite these up front So, cite: Q-Learning, CEM, SARSA, DQN, Dueling DQN, DDPG, NAF, and PPO, and spell out the acronym for each the first time you introduce them.
	- "layer of explorations" --&gt; "layer of exploration"

Section 2) Problem Formulation
	- "as each shot of our" --&gt; what is 'shot' in this context?
	- "In what follow," --&gt; "In what follows,"
	- "where 0 &lt; \gamma \leq 1" --&gt; Usually, $\gamma \in [0,1)$, or $[0,1]$. Why can't $\gamma = 0$?
	- The transition notation changes between $\mathbb{P}_a(s_{t+1} | s_t)$ and $\mathbb{P}(s_{t+1} | s_t, a_t)$. I'd suggest picking one and sticking with it to improve clarity.
	- "to learn a state-action value function, for example the Q-function" --&gt; Why is the Q-function just an example? Isn't is *the* state-action value function? That is, I'd suggest replacing "to learn a state-action value function, for example the Q-function" with "to learn a state-action value function, also called the Q-function"
	- "Q-function calculates" --&gt; "The Q-function denotes"
	- "the reward feedbacks perfectly" --&gt; "the reward feedback perfectly"
	- I prefer that the exposition of the perturbed reward MDP be done with C in the tuple. So: $\tilde{M} = \langle \mathcal{S}, \mathcal{A}, \mathcal{R}, C, \mathcal{P}, \gamma \rangle$. This seems the most appropriate definition, since the observed rewards will be generated by $C$.
	- The setup of the confusion matrix for reward noise over is very clean. It might be worth pointing out that $C$ need not be Markovian. There are cases where C is not just a function of $\mathcal{S}$ and $\mathcal{R}$, like the adversarial case you describe early on.


Section 3) Learning w/ Perturbed Rewards
	- Theorem 1 builds straightforwardly on Q-Learning convergence guarantee (it might be worth phrasing the result in those terms? That is: the addition of the perturbed reward does not destroy the convergence guarantees of Q-Learning.)
	- "we firstly" --&gt; "we first"
	- "value iteration (using Q function)" --&gt; "value iteration"
	- "Definition 2. Phased Q-Learning" --&gt; "Definition 2. Phrased Q-Learning". I think? Unless you're talking about Phased Q from the Kearns and Singh '99 work.
	- "It uses collected m samples" --&gt; "It uses the collected m samples"
	- Theorem 2: it would be helpful to define $T$ since it appears in the sample complexity term. Also, I would suggest specifying the domain of $\epsilon$, as you do with $\delta$.
	- "convergence to optimal policy" --&gt; "convergence to the optimal policy"
	- "The idea of constructing MDP is similar to" --&gt; this seems out of place. The idea of constructing which MDP? Similar to Kakade (2003) in what sense?
	- "the unbiasedness" --&gt; "the use of unbiased estimators"
	- "number of state-action pair, which satisfies" --&gt; "number of state-action pairs that satisfy"
	- "The above procedure continues with more observations arriving." --&gt; "The above procedure continues indefinitely as more observation arrives." Also, which procedure? Updating $\tilde{c}_{i,j}$? If so, I would specify.
	- "is nothing different from Eqn. (2) but with replacing a known reward confusion" --&gt; "replaces a known reward confusion"


4) Experiments:
	- Diverse experiments! That's great. Lots of algorithms, lots of environment types.
	- I expected to see Phrased Q-Learning in the experiments. Why was it not included?
	- The plots are pretty jagged, so I'm left feeling a bit skeptical about some of the results. The results would be strengthened if the experiments were repeated for more trials.

5) Conclusion:
	- "despite of the fact" --&gt; "despite the fact"
	- "finite sample complexity of Q-Learning with estimated surrogate rewards are given" --&gt; It's not really Q-Learning, though. It's a variant of Q-Learning. I'd suggest being explicit about that.

Appendix:

	- "It is easy to validate the unbiasedness of proposed estimator directly." --&gt; "It is easy to verify that the proposed estimator is unbiased directly."
	- "For the simplicity of notations" --&gt; "For simplicity"
	- "the Phrased Q-Learning could converge to near optimal policy" --&gt; ""the algorithm Phrased Q-Learning can converge to the near optimal policy""
	- "Using union bound" --&gt; "Using a union bound"
	- Same comment regarding $\gamma$: it's typically $0 \leq \gamma &lt; 1$.
	- Bottom of page 16, the second equation from the bottom, far right term: $c.j$ --&gt; $c,j$.
	- "Using CauchySchwarz Inequality" --&gt; "Using the Cauchy-Schwarz Inequality"


References:
	Loftin, Robert, et al. "Learning something from nothing: Leveraging implicit human feedback strategies." Robot and Human Interactive Communication, 2014 RO-MAN: The 23rd IEEE International Symposium on. IEEE, 2014.

	Krueger, D., Leike, J., Evans, O., &amp; Salvatier, J. (2016). Active reinforcement learning: Observing rewards at a cost. In Future of Interactive Learning Machines, NIPS Workshop.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJliWdLsh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting general surrogate reward which has wide applicability, and can be flexibly included alongside a variety of algorithms.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkMWx309FX&amp;noteId=BJliWdLsh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1050 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1050 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">## Summary

The authors present work that shows how to deal with noise in reward signals by creating a surrogate reward signal. The work develops a number of results including: showing how the surrogate reward is equal in expectation to the true reward signal, how this doesn't affect the fixed point of the Bellman equation, how to deal with finite and continuous rewards and how the convergence time is affected for different levels of noise. They demonstrate the value of this approach with a variety of early and state-of-the-art algorithms on a variety of domains,, and the results are consistent with the claims.

It would be useful to outline how prior work approached this same problem and also to evaluate the proposed method with existin approaches to the same problem. I realise that this is the first method that estimates the confusion matrix rather than assuming it is known a priori but there are obvious ways around this, e.g. the authors first experiment assumes the confusion matrix is known, so this would be a good place to compare with other competing techniques. Also, the authors have a way of estimating this, so they could plug it into the other algorithms too.

I also have some concerns about the clarity and precision of the proofs, although I do not have any reason to doubt the Lemma/Theorem correctness (see below).

The weakest part of the approach is in how the true reward is estimated in order to estiamate the confusion matrix. It uses majority vote (which is only really possible in the case of finite rewards with noise sufficiently low that this will be a robust estimate). Perhaps some other approaches could be explore here too.

Finally, there is discussion about adversarial noise in rewards at the beginning but I am not sure the theory really addresses it nor the evaluations.

Nonetheless, given that I do not know whether the claim of originality is true (in terms of the estimation of the confusion matrix). If it is, then the work is a significant and interesting advance, and is clearly widely applicable in domains with noisy rewards. It would be interesting to see a more tractable approach for continous noise too, but this would probably involve assumptions (smoothness? Gaussianity?), and doesn't impact the value of this work.

## Detailed notes

There is a slight sloppiness in  notation in equation (1). This uses \tilde{r} as a subscript of e, but r is +1 or -1 and the error variables are e_+ and e_- (not e_{+1} and e_{-1}).


The noise levels in Atari (Figure 3) show something quite interesting which could be commented upon. For noise below 0.5 the surrogate reward works roughly  similarly to the noisy reward, but when the noise level goes above this, the surrogate reward clearly exploits the increased information content (similar to a noisy binary channel with over 0.5 noise). This may have  implications for adversarial noise.

There are also some issues with the proofs which I spotted outlined below:

### Lemma 1 proof
The proof of Lemma 1, I think, fails to achieve its objective. The first pair of equations is not a rewrite of equation (1). I believe that the authors intend for this to be a consequence of Equation (1) but do not really demonstrate this clearly. Also, the authors seem to switch between binary rewards -1 and +1 and two levels of reward r- and r+ leading to some confusion. I would suggest the latter throughout as it is more general but involves no more terms.

I suggest the following as an outline for the proof. It would help for them to define what they mean by the different rhats (as they currently do) and explain that these values are therefore:

  rhat- = [(1 - e+) r- - e- r+ ]/(1 - e+ - e-)
  rhat+ = [(1 - e-) r+ - e+ r-]/(1- e+ - e-)

from equation (1). What is left is for them to actually prove the Lemma, namely that the expected value of rhat is:

  E(rhat ) = p1(rhat=rhat-) rhat- + p(rhat=rhat+) rhat+ = E(r)

where the probabilities relate to the surrogate reward taking their respective values. And just stylistically, I would avoid writing "we could obtain" and simply write "we obtain".

Lemma 2 achieves this more clearly with greater generality.


### Theorem 1 proof
At the end of p13, the proof of the expected value loses track of the chosen action a. I would suggest the authors replace: $$\mathbb{P}'(s,s',\hat{r})$$ with $$\mathbb{P}'(s,a, s',\hat{r})$$ then define it. Likewise $$\mathbb{P}(s,s')$$ should be $$\mathbb{P}(s,a,s')$$ (and also defined).

I am also a little uncomfortable with the switch from: $$max_{b \in \mathcal{A}} | Q(s',b) - Q*(s',b)|$$ in the second to last line of p13, which refers to the maximum Q value associated with some state s', to  $$||Q-Q*||_{\infty}$$ in the next line which is the maximum over all states and actions. The equality should probably be an inequality there too.

Throughout this the notation could be much better defined, including how to interpret the curly F and how it acts in the conditional part of an expectation and variance.

Finally, there is a bit too free a use of the word "easily" here. If it were easy, then the authors could do it more clearly I think. Otherwise, please refer to the appropriate result in the literature.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Bkl75YQ92Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting but seems to tackle a too narrow problem</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkMWx309FX&amp;noteId=Bkl75YQ92Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1050 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1050 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper aims at studying the setting of perturbed rewards in a deep RL setting. Studying the effect of noise in the reward function is interesting. The paper is quite well-written. However the paper studies a rather simple setting, the limitations could be discussed more clearly and there are one or two elements unclear (see below).

The paper assumes first the interesting case where the generation of the perturbed reward is a function of S*R into the perturbed reward space. But then the confusion matrix does *not* take into account the state, which is justified by "to let our presentation stay focused (...)". I believe these elements should at least be clearly discussed. Indeed, in that setting, the theorems given seem to be variations of existing results and it is difficult to understand what is the message behind the theorems.

In addition, it is assumed that the confusion matrix C is known or estimated from data but it's not clear to me how this can be done in practice.  In equation 4, how do you have access to the predicted true rewards?

Additional comments:
- The discount factor can be 0 but can not, in general, be equal to 1. So the equation in paragraph 2.1 "0 &lt; γ ≤ 1" is wrong.
- The paper mention that "an underwhelming amount of reinforcement learning studies have focused on the settings with perturbed and noisy rewards" but there are some works on the subject (e.g., <a href="https://arxiv.org/abs/1805.03359)" target="_blank" rel="nofollow">https://arxiv.org/abs/1805.03359)</a> and a discussion about the differences with the related work would be interesting.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>