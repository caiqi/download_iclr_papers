<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Fatty and Skinny: A Joint Training Method of Watermark Encoder and Decoder | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Fatty and Skinny: A Joint Training Method of Watermark Encoder and Decoder" />
        <meta name="citation_author" content="Sanghyun Hong" />
        <meta name="citation_author" content="Mahmoud Mohammadi" />
        <meta name="citation_author" content="Noseong Park" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ryfDoiR5Ym" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Fatty and Skinny: A Joint Training Method of Watermark Encoder and..." />
      <meta name="og:description" content="Watermarks have been used for various purposes. Recently, researchers started to look into using them for deep neural networks. Some works try to hide attack triggers on their adversarial samples..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ryfDoiR5Ym" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Fatty and Skinny: A Joint Training Method of Watermark Encoder and Decoder</a> <a class="note_content_pdf" href="/pdf?id=ryfDoiR5Ym" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=shhong%40cs.umd.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="shhong@cs.umd.edu">Sanghyun Hong</a>, <a href="/profile?email=mmoham12%40uncc.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="mmoham12@uncc.edu">Mahmoud Mohammadi</a>, <a href="/profile?email=npark9%40gmu.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="npark9@gmu.edu">Noseong Park</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Watermarks have been used for various purposes. Recently, researchers started to look into using them for deep neural networks. Some works try to hide attack triggers on their adversarial samples when attacking neural networks and others want to watermark neural networks to prove their ownership against plagiarism. Implanting a backdoor watermark module into a neural network is getting more attention from the community. In this paper, we present a general purpose encoder-decoder joint training method, inspired by generative adversarial networks (GANs). Unlike GANs, however, our encoder and decoder neural networks cooperate to find the best watermarking scheme given data samples. In other words, we do not design any new watermarking strategy but our proposed two neural networks will find the best suited method on their own. After being trained, the decoder can be implanted into other neural networks to attack or protect them (see Appendix for their use cases and real implementations). To this end, the decoder should be very tiny in order not to incur any overhead when attached to other neural networks but at the same time provide very high decoding success rates, which is very challenging. Our joint training method successfully solves the problem and in our experiments maintain almost 100\% encoding-decoding success rates for multiple datasets with very little modifications on data samples to hide watermarks. We also present several real-world use cases in Appendix.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Adversarial Machine Learning, Watermarking, Generative Adversarial Networks</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a novel watermark encoder-decoder neural networks. They perform a cooperative game to define their own watermarking scheme. People do not need to design watermarking methods any more.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_B1l-RkgYTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryfDoiR5Ym&amp;noteId=B1l-RkgYTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper631 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper631 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJxRsDxwTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A GAN-inspired way to watermark (embed special identification code) into deep neural networks.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryfDoiR5Ym&amp;noteId=SJxRsDxwTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper631 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper631 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper considers watermarking neural networks - making them react to a special, hidden to observers patterns in input images.

In the proposed approach an Encoder E takes an input image, and modifies it to embed the watermark. Then a decoder, D is tasked to find images with the added watermark. The magnitude of the modification is constrained by limiting the distance between the activations of a VGG19 network. The method proposed in the paper seems to be simple and effective on some benchmark datasets.

The paper is poorly written: the concept of watermaking is only explained on page 2, and the description given refers to a more general use than the one facilitated by the methods in this paper (watermarking is defined to embed a user-defined signal s into a sample whereas the proposed technique can only transmit one bit - the sample has been watermarked or not).  Then. the technique is presented from a GAN perspective, even though the Encoder and Decoder networks cooperate and need a third network to ensure that the watermarking is hard to detect (this third, VGG19 net is revealed only on page 5). This third network seems to be more important than the exact fatty-skinny architecture of the encoder decoder, but its properties are not evaluated in the paper. In fact, without this third network, the proposed approach could just learn to change a single pixel of the image for watermarking purposes, a trivial operation.

Furthermore, the paper lacks proper design goals for the watermarking procedure. This makes it impossible to judge the value of the proposed architectural choices. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkxj4eJ167" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryfDoiR5Ym&amp;noteId=rkxj4eJ167"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper631 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper631 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a GAN-based technique to generate watermarked images. In the presented setup, a high-capacity generator generates watermarked samples that should be decodable by a low-capacity detector. The idea is that the decoder can then be used within other neural networks for protection or to attack them. Different to a vanilla GAN setup, the decoder and encoder work together. The watermark, in the proposed setup, is added by combining residual blocks and an attention mechanism. Experiments are presented for a couple of publicly available datasets.

From a technical point of view, I think this is a valid strategy to follow, however, the motivation is unclear to me. If the goal is to create watermarked images that can be detected later on, many methods in the information security literature exist. The authors argue two particular points: (1) the detector needs to be lightweight and (2)  they do not want to implant a predefined signal into the images, but rather let the encoder decide what needs to be implanted. Regarding point (1), as
mentioned, many methods exist that are (non neural-network based) fast in detection (AND fast in embedding), as they essentially only require correlation with a watermark signal in a transform domain (DWT, DCT, etc.), see, e.g.,

J. R. Hernandez, M. Amado, and F. Perez-Gonzalez. 
DCT-domain watermarking techniques for still images: Detector performance analysis and a new structure.
IEEE Transactions on Image Processing, 9(1):55â€“68, Jan. 2000.

A. Nikolaidis and I. Pitas. 
Asymptotically optimal detection for additive watermarking in the DCT and DWT domains. 
IEEE Transactions on Image Processing, 12(5):563â€“571, May 2003.

A. Briassouli, P. Tsakalides, and A. Stouraitis. 
Hidden Messages in Heavy-Tails: DCT-Domain Watermark Detection Using Alpha-Stable Models. 
IEEE Transactions on Multimedia, 7(4):700â€“714, Aug. 2005.

R. Kwitt, P. Meerwald and A. Uhl
A Lightweight Rao-cauchy Detector for Additive Watermarking in the Dwt-domain.
ACM Workshop on Multimedia and Security, 2008.

...

These methods further provide optimality guarantees for detection (under certain assumptions) and allow
to directly control the embedding strengths which affects the PSNR for instance.  

Regarding issue (2), it's unclear to me why implanting a signal is not desirable. Typically this is just a sequence of random numbers generated by a specific seed.

Additionally, the authors claim that their method produces "robust" watermarks, however, this is not evaluated. Typically, one would try to attack the watermarking scheme to address this question. Also, the produced watermarks (e.g., Fig. 2) are not imperceptible. This is also not directly controllable, as it depends on the optimization procedure. My suggestion for the authors is to perform a more fine-grained comparison to existing techniques and assess robustness as well as the impact of existing schemes on classification performance (as done for their method). Maybe, the authors can also comment on the achieved PSNR values  which are rather low, compared to SWM for instance. Modifying images in such a way might be undesirable in practical applications. Typically, it is also important to assess how the embedding strength affects the detection success in a more fine-grained manner (similar to Table 1). 







</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1gYBeX927" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting paper, confusing at times, needs more work.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryfDoiR5Ym&amp;noteId=S1gYBeX927"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper631 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper631 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper starts by discussing the usage of watermarking for Deep-NN's. Then changes topic and focuses on using DNN's for image watermarking. This is perhaps the most confusing point of this paper, since it's foundational to the paper structure.

If my understanding is correct, and let me be clear I am not 100% sure it is, but if this this paper is about "using a DNN's to embed watermarking in images", the authors should state it clearly at the beginning, including in the abstract and in the introduction, and resubmit. The discussion about using adversarial examples to watermark DNN's (so that the networks themselves are not stolen) is a departure from the core message, and a very confusing one indeed.

Based on this current understanding, my review follows.

This paper proposes a encoder-decoder method, based on CNN's to embed a watermark into images (and audio).

To be clear, this is watermarking by detection, which means the decoder will output 1 if an image is watermarked, 0 otherwise. Effectively this watermarking is adding 1-bit of information to the content. I say this because usually (classic) watermarking methods can add codewords of more than one bit to content, thus allowing to embed the copyright owner private key(s) for future retrieval and proof of ownership; more on the implications of this later, at the end of the paper.

The network architecture(s) proposed are asymmetrical, in the sense that the encoder is high capacity and decoder is small. This makes sense, given that one want to perform decoding quickly, but adding the watermark can tolerate more computation/delay. The encoder is based on attention/convolutions and residual connections.

The decoder is based on a small DCGAN discriminator. The authors then state "is very hard to identity the decoder after being implanted into a neural network model because it is tiny and uses only very standard neural operators.". This again is confusing - under my understanding that this method is about image watermarking, why would we add this decoder into another network, covertly? Can the authors please clarify this?

The authors then proceed by discussing about GAN and GAN related loss functions in section 3.3, but then in section 4.3 they show a loss function that in their own words is not GAN related, because there is no "Adversarial" game. 
Confusing again: why to talk about GAN when there is no "A" - this is clearly NOT an adversarial game, there is an encoder and a recognizer which looks at a image and tells if it's watermarked or not. A better way to write this loss function would be a simple softmax cross-entropy of the output of the recognizer (D) to the image class of 1=watermarked ~ 0=not-watermarked. When I look at eq.3, I am not 100% sure it achieves that.

The authors correctly point out that for the watermark to not be detectable, it has to be small in some sense. To achieve that the authors  penalize the detection loss (eq.3) with a VGG hinge-loss on the watermarked vs non-watermarked image. The authors simply state that this works better than pixel-wise features.  
This makes sense, but only to some extent; it would have been better to show comparisons of watermarks between L1/L2 and VGG losses, because VGG loss will tend to prefer visually appealing distortions which might still be rather large distortions in term of PSNR. In some applications this is OK, for in some other applications this is not OK. You can imagine some content owners now wanting their images distorted more than a certain margin in a "euclidean" sense, in which case VGG loss might not work.

The paper then provides results for image and also speech recognition. Results show distortions levels as well changes in recognition accuracy due to the added watermarking noise, as well as watermarking detection rate. The results are good, with low distortion, negligible loss in accuracy, and almost perfect detection rate. 

The experimental results are clearly the strong point of this paper. However, and this is a big however, the authors do not provide public source code to reproduce the results. I strongly encourage authors to submit publicly available code and data.

Finally, to conclude, I would like to elaborate on the validity of the embed 1-bit / detect approach to watermarking.
This is very important, since this method does not add one code-word but only 1-bit, then the question is how can we really evaluate this system?

Picture the following scenario: Let's assume I am owner-A ~ I train my network, I embed my one 1-bit into my images. Let's say that the esteemed member of our program committee is owner-B. S/He does the same, trains her network embeds the watermark into her images. We both test our detection result, based on the protocol proposed on this paper -  we both separately feed our images and our watermarked images into our detectors, and get 100% accuracy! We are very happy, watermarks look unnoticeable to the human eye and are detected by our networks!

Now the BIG QUESTION IS ~ we both own 1-bit of information only right? Then what would happen if I feed the watermarked images of the esteemed-member of our program committee - owner-B into my network? Will my detection network say it's watermaked or not? Because if it says it's watermarked, now I could claim I own the content of owner-B ... same issue the other way around, owner-A watermarked data into owner-B network. What would happen? I am not saying this issue will arise, but given we only embed 1-bit of information, who owns that 1-bit? This should at least be tested. 

Based on all these comments I feel this is an interesting paper, which requires more work, more experiments, a more clear focus and more clarifications.

Thank you!</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>