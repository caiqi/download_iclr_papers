<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Variational Autoencoders for Text Modeling without Weakening the Decoder | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Variational Autoencoders for Text Modeling without Weakening the Decoder" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1eZ6sRcFm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Variational Autoencoders for Text Modeling without Weakening the..." />
      <meta name="og:description" content="Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1eZ6sRcFm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Variational Autoencoders for Text Modeling without Weakening the Decoder</a> <a class="note_content_pdf" href="/pdf?id=H1eZ6sRcFm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019variational,    &#10;title={Variational Autoencoders for Text Modeling without Weakening the Decoder},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=H1eZ6sRcFm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Previous work (Bowman et al., 2015; Yang et al., 2017) has found difficulty developing generative models based on variational autoencoders (VAEs) for text. To address the problem of the decoder ignoring information from the encoder (posterior collapse), these previous models weaken the capacity of the decoder to force the model to use information from latent variables. However, this strategy is not ideal as it degrades the quality of generated text and increases hyper-parameters. In this paper, we propose a new VAE for text utilizing a multimodal prior distribution, a modified encoder, and multi-task learning. We show our model can generate well-conditioned sentences without weakening the capacity of the decoder. Also, the multimodal prior distribution improves the interpretability of acquired representations.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">variational autoencoders, generative model, deep neural network, text modeling, unsupervised learning, multimodal</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a model of variational autoencoders for text modeling without weakening the decoder, which improves the quality of text generation and interpretability of acquired representations.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BJgVk6Lp3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Somewhat weak contribution</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1eZ6sRcFm&amp;noteId=BJgVk6Lp3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper775 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper775 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper describes a generative model for natural language, proposing various modifications to the previous work, including: a modified encoder which combines a self-attention model and a bag-of-words model, a modified multi-modal prior, and a modified training procedure. The claim is that the proposed improvements alleviate the "posterior collapse" issue, where the decoder ignores the information from the encoder.

Technical significance:
As the paper acknowledges, the proposed modifications have already been introduced in the literature and the current paper applies them to the natural language modeling task, showing that: combining self-attention and bag-of-word encoders, using an additional training task, and using a multi-modal prior improve performance (e.g. as measured by BLUE scores). From this perspective, the contribution is somewhat limited.

Empirical significance:
The main focus is avoiding "posterior collapse" and, unfortunately, this point is not illustrated strongly. Do the baselines show mode collapse and the work improves is that respect? Something is missing here: maybe a (toy) data-set where mode-collapse arises when training baselines, but not when the present modifications are introduced (as another comment points out), maybe training curves showing that the modifications provide a mode stable training process, etc.

Clarity:
The paper is generally well written, with the proposed modifications being clearly explained. The non-VAE sequence prediction task could be described in a bit more details where it's being used, i.e. in section 4.2. or in the caption of Table 1.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HkgIYeQq2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Issues in Theory and Experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1eZ6sRcFm&amp;noteId=HkgIYeQq2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper775 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper775 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper suggests that one way to prevent VAE from ignoring the encoder when trained on text is to make the prior distribution multi-modal. The author further suggests that injecting the encoder's output with BoW features can make the prior distribution multi-modal, and in turn that can help VAE to learn better distribution on text without ignoring the encoder, tested on a self-attention encoder and an LSTM encoder.

There are several issues with the theory and the experiments that make the paper incorrect on several fronts.

Issue #1 (theoretical): multi-modality of prior distribution can be achieved by injecting the BoW feature.
This is incorrect in the sense that the prior distribution Pr(z) is a given distribution for the model. At best, it is an additive shift of the feature vector from the encoder, which parameterizes the mean and standard deviation of Pr(z|x) by assuming it is a parameterized Gaussian, which is still uni-modal. Besides, the experimental setting in section 3.1.1 suggests the same re-parameterization trick from Kingma &amp; Welling (2013) is used, which assumes the prior distribution Pr(z) is N(0,1) -- and it is uni-modal!

Issue #2 (theoretical): prior multi-modality implies that posterior distribution cannot ignore the encoder.
This issue is a two-fold problem. 1) In Appendix A, the paper suggests that (assuming p(z) is multi-modal) "if q(z|x) is a uni-modal distribution, there is no way to satisfy p(z) = q(z|x)". This is true, however this does not imply that posterior distribution will not ignore the encoder. In the case that Pr(z) is multi-modal, it is entirely possible that the encoder simply performs a clustering on the input x for each mode of z, ignoring the detailed textual information. The posterior collapse is therefore could not be entirely prevented by simply making Pr(z) multi-modal. 2) In Appendix A, the paper suggests "a hypothesis that the modifiction of the decoer is not necessary if multi-modal prior distribution is used". This is not only a logical fallacy of circular reasoning between theory and experiments, but also a faulty result because the actual experiments use a uni-modal prior (see issue #1).

Issue #3 (experimental): benchmark methods only reflect the quality of auto-encoding reconstruction
The paper's benchmark methods do not reflect performance in text generation. According to section 4.3, the 3 benchmark methods used (reconstruction loss, KL-divergence, and BLEU) are all performed against a given text, which can only benchmark the auto-encoding reconstruction Pr[x|z]Pr[z|x] but not the quality of generated text Pr[x|z]. Here I am assuming the KL-divergence is performed on the output word probabilities given a sample, not on the entire generated sentence (which is not feasible to compute). This defies the purpose of VAE, which is to learn a generative model.

Issue #4 (experimental): the title suggests no weakening of the decoder is needed, but experiments are not performed with decoders of varying sizes
Despite all the issues above with incorrect theory and experiments, it could still be possible that feature injection with BoW is helpful to prevent posterior collapsing of text VAEs. The paper failed to show this in a convincing manner either, because there is no experiment on different capacities of the decoder. One would expect that increasing the size (capacity) of the decoder will gradually make the posterior collapsing problem worse, and BoW feature injection could help because it can tolerate a larger decoder than a baseline model.

Due to these issues, I recommend rejection for the paper. If the authors did observe that BoW feature injection is helpful for the posterior collapsing phenomenon, I encourage the authors to re-consider the theory, the benchmark methods, and perform ablation study as suggested in issue #4.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">1: Trivial or wrong</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkgxTjwUh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1eZ6sRcFm&amp;noteId=rkgxTjwUh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper775 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper775 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper focuses on applying variational autoencoders to text data. A well-known problem in this domain is posterior collapse, where the posterior "collapses" to an uninformative prior distribution and the decoder becomes a standard language model which does not utilize the latent code. This paper assembles a few tricks to improve VAEs for text, including using a self-attention-based encoder, a mixture-of-Gaussians prior on the latent code, and a "multi-task" loss which attempts to reconstruct a BoW representation in addition to the original sentence. Some experiments are carried out on the Yelp dataset, including some qualitative study.

The issue of posterior collapse is important and modeling the global structure of text with a latent variable is a valuable endeavor. This paper provides some promising results in that direction. My main issue with the paper is that it is primarily the combination of a few pre-existing tricks to the text-VAE problem. Bringing together multiple tricks and showing that they can enable significant progress is potentially valuable, but only if the results are obviously better or they represent a big step forward in an important application (for example, showing convincing sentence interpolations, attribute vector arithmetic, autoencoding of very long text sequences, etc). This paper does not demonstrate any sufficiently strong result; instead, it just provides some limited evaluation that things get a little better along with some basic qualitative analysis of potential benefits of using a mixture prior. Apart from this, the paper claims that combining these tricks avoids "weakening the decoder" but uses a decoder with word dropout in all experiments, and also claims they decrease the number of hyperparameters, which is not shown in any rigorous way. The proposed additions also are described in insufficient detail - how are the bag of word representations constructed? How are they predicted? What is the additional loss term? How is it combined to the standard reconstruction loss for the autoregressive decoder? Etc. Finally, it is missing discussion (and comparison, as baselines) to a great deal of prior work on avoiding posterior collapse. As-is, the paper is not strong enough for publication. Below, I go into a bit more detail about specific issues and suggestions.

Specific comments:

- The paper blames posterior collapse on teacher forcing ("Because decoders for textual VAEs are trained with “teacher forcing” (Williams &amp; Zipser, 1989), they can be trained to some extent without relying on latent variables.") This is a misleading claim. Even with teacher forcing, it could potentially be advantageous to the model to utilize the latent (recall that the latent can capture information about the entire input sequence, so p(x_t | x_1, ... x_{t - 1}) is less informative than p(x_t | x_1, ... x_{t - 1})), but because of the architecture/learning dynamics/hyperparameters/etc the model tends to choose to "ignore" z. This is an independent problem of teacher forcing. The main issue caused by teacher forcing is exposure bias, which is a related but different issue. Teacher forcing is not necessary for training latent variable models with autoregressive decoders, it just happens to work well because it's a remarkably good search strategy.
- "existing models use a LSTM as the encoder, it is known that this simple model is not sufficient for text generation tasks (Bahdanau et al., 2014; Luong et al., 2015; Vaswani et al., 2017)." I assume you are citing these papers because they deal with attention and show that attention improves sequence-to-sequence models. The fact that attention helps is because the decoder is provided with additional conditioning information, not because the encoder is different (in Bahdanau and Luong the decoder remains a simple LSTM).
- There are many missing related works (and baselines) for avoiding posterior collapse, such as "Avoiding Latent Variable Collapse with Generative Skip Models", "Adversarially Regularized Autoencoders", "Variational Lossy Autoencoder", etc.
- Your equation for p(x|z) at the beginning of 3.1.1 is a bit odd because you are talking about a deterministic z, and using it to point out that z might not learn good global features. However, when z is a stochastic latent variable the equation would be exactly the same. Writing it in this way does not make this distinction clear. A deterministic z can indeed capture interpretable global structure (see Figure 2 of Sutskever et al. 2014). If you want to claim that using a stochastic z may _better_ capture global structure, I would recommend making this argument from an information theoretic perspective (as was done in "Variaional Lossy Autoencoder", for example).
- That weaking the decoder "requires additional hyper-parameters specifying decoder capacity" is not really a compelling arugment in your paper since your approach also requires additional hyperparameters (number of mixture components, coefficients for additional loss terms, etc.)
- You call posterior collapse a "trivial local minimum". Are you sure it's a local minimum? This implies that the gradient of the loss w.r.t. every one of the parameters is zero at this point. I think you are using "local minimum" in a loose sense; please don't do this.
- What are pseudo-inputs? Do you basically mean they are model parameters which are updated w.r.t. the loss function?
- "Recent research into text generation has found that simple LSTMs do not have enough capacity to encode information from the whole text." Citation needed!
- I think what you are calling "self-attention" is actually the "feed-forward attention mechanism" from "Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems".
- "We simply summarize word representation of all words in the input text and project this vector with a linear layer." I don't think this adequately describes what you are doing. How is this summarization done? Via a histogram of the words which are present? An average of the words' embedding vectors? Same with the multi-task part. What exactly are you predicting? How?
- I presume your BLEU scores in Table 1 are reflecting the BLEU score for the input vs. the reconstructed output. Please clarify.
- In the appendix it is written that "We applied 0.4 word dropout for input text to the decoder for our model and the model from Bowman et al. (2015)." This is definitely "weakening the decoder" - that is the primary motivation for word dropout. I don't think you can claim your approach can avoid weakening the decoder if all of your results are with word dropout.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJgm0NzVsX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper should compare with STRONG enough decoder baselines especially given it talks about posterior collapse</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1eZ6sRcFm&amp;noteId=BJgm0NzVsX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Oct 2018</span><span class="item">ICLR 2019 Conference Paper775 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for bringing up the posterior collapse issue in text modeling, which is an important problem to study. However, I have several comments and questions regarding the experiment settings.

1. The posterior collapse is commonly observed by previous work where strong decoders are used and the KL divergence KL(q(z|x), p(z)) becomes 0 during training. Since this paper focuses on posterior collapse, I think it is important to report a "collapsed" baseline, as [1] and [2] did, but In Table 2 the reported LSTM baseline has a pretty large KL term, which implies that the posterior collapse might not exist at all in the experimental settings. I doubt that the LSTM decoder used in this paper is too weak to trigger posterior collapse. This actually contradicts with the paper's title "without weakening the decoder" since all the decoders used in the experiments may be already very weak. I think it is necessary to employ a large decoder (where you can observe posterior collapse) and apply the proposed methods on that decoder to observe if you can achieve improvements.

2. Also, I want to point out reconstruction loss is NOT negative log likelihood (Section 4.3 says reconstruction loss (negative log likelihood) ). Comparing in terms of reconstruction loss does not say anything about language modeling performance, thus the analysis in this part is meaningless to me. The paper should try to approximate the negative log likelihood with importance weighted samples or AIS (or at least compare ELBO) and compare with the approximate negative log likelihood. I would not suggest using ELBO as NLL since ELBO is usually very loose to marginal data likelihood in the case of large KL (q(z|x), p(z)).


[1] Yang, Zichao, et al. "Improved Variational Autoencoders for Text Modeling using Dilated Convolutions." International Conference on Machine Learning. 2017.
[2] Kim, Yoon, et al. "Semi-Amortized Variational Autoencoders." arXiv preprint arXiv:1802.02550 (2018).</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1gtZ124i7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your comment.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1eZ6sRcFm&amp;noteId=H1gtZ124i7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper775 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Oct 2018</span><span class="item">ICLR 2019 Conference Paper775 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comment.

1. The LSTM decoder baseline is trained with word dropout [1].
The KL divergence of this model becomes 0 when word dropout is not applied.
However, a collapsed result should be reported to make clear that posterior collapse is alleviated by the modifications.
Thank you for your suggestion. We will report the result in the updated version.

2. We realized our misuse of the terms and inadequate evaluation.
We will update the results soon. Thank you for pointing it out.

[1] Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Jozefowicz, R., &amp; Bengio, S. (2015). Generating Sentences from a Continuous Space. In SIGNLL Conference on Computational Natural Language Learning (CoNLL).</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rye3uUxSsQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Word dropout is also one way to weaken the decoder, and sometimes hurts the generative modeling performance</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1eZ6sRcFm&amp;noteId=rye3uUxSsQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Oct 2018</span><span class="item">ICLR 2019 Conference Paper775 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your reply ! With the word dropout I think the results make more sense to me. 

However word dropout is also one way to weaken the decoder (i.e. the conditional distribution). In my opinion, we don't want to weaken the decoder because the weakened decoder is less powerful and might sacrifice generative modeling performance, but word dropout -- one way to implicitly weaken the decoder -- may also hurt the performance as shown in [1]. Without further ablation study, it is hard to tell whether the non-collapsed results come from the weakened decoder or the proposed methods. The former case contradicts with the claim in the paper's title. Clearly more thorough experimental analysis is needed to support the claim and contributions in this paper, look forward to future revisions !

[1] Kim, Yoon, et al. "Semi-Amortized Variational Autoencoders." arXiv preprint arXiv:1802.02550 (2018).</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1e6j6eSsQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your comment again</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1eZ6sRcFm&amp;noteId=r1e6j6eSsQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper775 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Oct 2018</span><span class="item">ICLR 2019 Conference Paper775 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comment again.
To show the effectiveness of our methods, it is true that we should remove the effect of word dropout.
We will report additional experimental results. Thank you for your suggestion.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_SkxOKezT5m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Missing references</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1eZ6sRcFm&amp;noteId=SkxOKezT5m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Adji_Bousso_Dieng1" class="profile-link">Adji Bousso Dieng</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Oct 2018</span><span class="item">ICLR 2019 Conference Paper775 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi,

You are missing too many references. There is an extensive literature on posterior collapse and you are missing a lot of work from that literature. See below for some references.

<a href="http://approximateinference.org/accepted/HoffmanJohnson2016.pdf" target="_blank" rel="nofollow">http://approximateinference.org/accepted/HoffmanJohnson2016.pdf</a>
https://arxiv.org/abs/1611.02731
https://arxiv.org/abs/1711.00464
https://arxiv.org/abs/1802.02550
https://arxiv.org/abs/1807.04863

Those are the ones I can think of. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByeAbff0c7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your comment. We will add references.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1eZ6sRcFm&amp;noteId=ByeAbff0c7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper775 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Oct 2018 (modified: 13 Oct 2018)</span><span class="item">ICLR 2019 Conference Paper775 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comment.
Because this paper stands on a wide range of studies, we had to pick out references directly related to our research.
However, posterior collapse is the main theme of this paper, and the references you suggested are important work.
Especially, we should not miss [1] and [2] since they are work on VAE for text. Thank you for pointing it out.

We will add references in our updated version.
Also, we will report additional experimental results on prior work.
Thank you.

[1] Kim, Y., Wiseman, S., Miller, A. C., Sontag, D., &amp; Rush, A. M. (2018). Semi-Amortized Variational Autoencoders. In International Conference on Machine Learning (ICML).
[2] Dieng, A. B., Kim, Y., Rush, A. M., &amp; Blei, D. M. (2018). Avoiding Latent Variable Collapse with Generative Skip Models. In ICML workshop on Theoretical Foundations and Applications of Deep Generative Models.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>