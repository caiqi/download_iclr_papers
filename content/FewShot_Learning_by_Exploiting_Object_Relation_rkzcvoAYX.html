<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Few-Shot Learning by Exploiting Object Relation | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Few-Shot Learning by Exploiting Object Relation" />
        <meta name="citation_author" content="Liangqu Long" />
        <meta name="citation_author" content="Wei Wang" />
        <meta name="citation_author" content="Jun Wen" />
        <meta name="citation_author" content="Meihui  Zhang" />
        <meta name="citation_author" content="Qian  Lin" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rkzcvoA9YX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Few-Shot Learning by Exploiting Object Relation" />
      <meta name="og:description" content="&#10;  Few-shot learning trains image classifiers over datasets with few examples per category. &#10;  It poses challenges for the optimization algorithms, which typically require many examples to fine-tune..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rkzcvoA9YX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Few-Shot Learning by Exploiting Object Relation</a> <a class="note_content_pdf" href="/pdf?id=rkzcvoA9YX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=liangqu.long%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="liangqu.long@gmail.com">Liangqu Long</a>, <a href="/profile?email=wangwei%40comp.nus.edu.sg" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="wangwei@comp.nus.edu.sg">Wei Wang</a>, <a href="/profile?email=jungel2star%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="jungel2star@gmail.com">Jun Wen</a>, <a href="/profile?email=meihui_zhang%40bit.edu.cn" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="meihui_zhang@bit.edu.cn">Meihui  Zhang</a>, <a href="/profile?email=linqian%40comp.nus.edu.sg" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="linqian@comp.nus.edu.sg">Qian  Lin</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">
Few-shot learning trains image classifiers over datasets with few examples per category. 
It poses challenges for the optimization algorithms, which typically require many examples to fine-tune the model parameters for new categories. 
Distance-learning-based approaches avoid the optimization issue by embedding the images into a metric space and applying the nearest neighbor classifier for new categories. In this paper, we propose to exploit the object-level relation to learn the image relation feature, which is converted into a distance directly.
For a new category, even though its images are not seen by the model, some objects may appear in the training images. Hence, object-level relation is useful for inferring the relation of images from unseen categories. Consequently, our model generalizes well for new categories without fine-tuning.
Experimental results on benchmark datasets show that our approach outperforms state-of-the-art methods.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">few-shot learning, relation learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Few-shot learning by exploiting the object-level relation to learn the image-level relation (similarity)</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rJxQIluIam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkzcvoA9YX&amp;noteId=rJxQIluIam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper288 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper288 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJx1nrGn27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Strong results but paper is based somewhat on minor changes from previous work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkzcvoA9YX&amp;noteId=BJx1nrGn27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper288 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper288 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper deals with the problem of few-shot learning by proposing an embedding-based approach that learns to compare object-level features between support and query set examples. It proposes an extension to Learning to Compare (Sung 2017), where Learning to Compare learns a relation module that is parametrized to product a similarity score between whole feature-maps of embedded support and query set examples. Rather than measure similarity between examples at the feature-map level, this work proposes measuring similarity at the object-level. Specifically, an object is represented by each (i,j) position on the final feature-map derived through the embedding network. Then to compare a support image and query image, we compare pairwise across all (i,j) positions on the final feature-map between support and query images by computing object similarity features. The pairwise object similarity features are then summed and mapped to a similarity probability between the support and query example. Note that the network structure used for object comparison here is derived from the Relation network from Santoro 2017. The model is then trained in the same way as Learning to Compare, where we want support and query examples with the same class label to have similarity probability of 1. Experiments are conducted on Omniglot and Mini-Imagenet, comparing the proposed method to previous work.

Pros:
- Decent improvement in Mini-Imagenet results relative to Learning to Compare (close to +9% for 1-shot and +5% for 5-shot relative to Learning To Compare) .
- One concern I had was that improvement in performance was due to working with 224 x 224 images in this paper (rather than resized 84 x 84 images as most previous work does with Mini-Imagenet); however, additional comparison was performed with Learning To Compare where that model also uses 224 x 224 images but there was no improvement there just by using larger image input, thus establishing that larger image input is not a reason for the improved performance.

Cons:
- The proposed idea is not particularly novel, as it is basically the Learning to Compare model but with pairwise comparison at each location in the final feature map rather than comparing the feature maps as a whole.

Remarks:
- Figure 4 does not seem to be that useful. It is used to comment that overfitting is not an issue by showing training and testing performance on Omniglot dataset but maybe would be more useful if Learning to Compare was also shown in those plots to compare the level of overfitting between the two models.

Santoro et al. A simple neural network module for relational reasoning. 2017.
Sung et al. Learning to Compare: Relation Network for Few-Shot Learning. 2017.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1xgGJa9hX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Minor extension of the existing method</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkzcvoA9YX&amp;noteId=H1xgGJa9hX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper288 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper288 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">- In evaluation of omniglot, ("a small improvement over the state-of-the-art should be considered as significant") since all the results are already quite high, the small difference are not much beneficial in practice. Further, to justify the significance of the proposed method, a statistical test should be conducted and reported at least.

- In comparison to the LearningToCompare, Sung et al’s method compares the query image to all the support images, while the proposed method compares in a pairwise manner.  Thus, from a computational complexity perspective, the proposed method can be much expensive.

- The statement of "averaging the object relation features has similar effect as ensemble modeling" needs theoretical or at least empirical supporting results along with explanations.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SylQLY9q2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Theoretical and experimental justifications are weak. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkzcvoA9YX&amp;noteId=SylQLY9q2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper288 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper288 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a few shot learning method by exploiting the object-level relation between different images. The proposed method is based on neared neighbor search and concatenates feature maps of two input images into one feature map by considering all location combinations. Then for each slice of the location combination, the method applies fully connected layers g(). The outputs of g() are summed along location combinations and used as a feature to calculate the similarity of two inputs.  

Overall, the idea of object relation is interesting. However, the proposed method is incremental from Relation Network (Learning2Compare Sun te al. 2017) and lacks theoretical and experimental justification why the proposed model works well. 

Pros
+ Interesting idea to improve the performance of few-shot learning. 
+ The performance improvements from Relation Network seems to be high. 

Cons 
-Relation Network also concatenates feature maps of different objects. The main novelty of this paper seems to be all combination is considered in the concatenation and applied the separate network g() for each combination. However, why this feature extraction process works well is not theoretically justified.  

-Relation Networks use different classification and learning scheme to this paper. Thus, it is not clear which parts of the proposed method, eg, the proposed concatenation procedure or the nearest neighbor model produced better results than Relational Networks. At least, the author should compare the performance when the proposed method concatenates two feature maps as in the Relation Network. 

-In the Sec.5, authors wrote that when the feature map size is 1 \times 1 the proposed method corresponds to Relational Network. However, as far as I read the paper of Relational Network, the size of the feature map is not 1 \times 1. 

-Each location of the feature map seems to be not an object. It needs more explanations why the feature of each location is an object. 

-It is not clear if the parameter of g() is common for all location combinations or learn different g() for each location combination. 

- Fig.2 lacks the explanation of the concatenation process of feature maps. The process is not only concatenation of feature maps, but concatenating features of all combination of location. It is hard to understand why the column size is w \times h \times w \times h only writing it concatenation.  
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>