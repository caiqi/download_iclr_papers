<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Subgradient Descent Learns Orthogonal Dictionaries | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Subgradient Descent Learns Orthogonal Dictionaries" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HklSf3CqKm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Subgradient Descent Learns Orthogonal Dictionaries" />
      <meta name="og:description" content="This paper concerns dictionary learning, viz., sparse coding, a fundamental representation learning problem. We show that a subgradient descent algorithm, with random initialization, can recover..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HklSf3CqKm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Subgradient Descent Learns Orthogonal Dictionaries</a> <a class="note_content_pdf" href="/pdf?id=HklSf3CqKm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 15 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019subgradient,    &#10;title={Subgradient Descent Learns Orthogonal Dictionaries},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HklSf3CqKm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=HklSf3CqKm" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">This paper concerns dictionary learning, viz., sparse coding, a fundamental representation learning problem. We show that a subgradient descent algorithm, with random initialization, can recover orthogonal dictionaries on a natural nonsmooth, nonconvex L1 minimization formulation of the problem, under mild statistical assumption on the data. This is in contrast to previous provable methods that require either expensive computation or delicate initialization schemes. Our analysis develops several tools for characterizing landscapes of nonsmooth functions, which might be of independent interest for provable training of deep networks with nonsmooth activations (e.g., ReLU), among other applications. Preliminary experiments corroborate our analysis and show that our algorithm works well empirically in recovering orthogonal dictionaries.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Dictionary learning, Sparse coding, Non-convex optimization, Theory</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Efficient dictionary learning by L1 minimization via a novel analysis of the non-convex non-smooth geometry.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">13 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_H1gObZmc6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Revision: expanded synthetic experiments + real data experiments + conclusion</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HklSf3CqKm&amp;noteId=H1gObZmc6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1259 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1259 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have made a revision of our paper. The major changes are summarized as follows:

(1) The synthetic experiment (Section 5) is slightly expanded with results on different sparsity (\theta = 0.1, 0.3, 0.5). Recovery is easier when the sparsity is higher (i.e. \theta is lower), but in all cases we get successful recovery when m &gt;= O(n^2).

(2) We added an experiment on real images (Appendix H), which shows that complete dictionaries offer a reasonable sparsifying basis for real image patches.

(3) We have added a conclusion section (Section 6) with discussions of our contributions and future directions. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ByeePKFwaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice work on nonconvex nonsmooth theory, needs more work on experiments and relation to loss landscape of neural networks mentioned in abstract</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HklSf3CqKm&amp;noteId=ByeePKFwaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1259 AnonReviewer5</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1259 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper provides a very nice analysis for the nonsmooth (l1) dictionary learning minimization in the case of orthogonal complete dictionaries and linearly sparse signals. They utilize a subgradient method and prove a non-trivial convergence result.

The theory provided is solid and expands on the earlier works of sun et al. for the nonsmooth case. Also interesting is the use a covering number argument with the d_E metric.

A big plus of the method presented is that unlike previous methods the subgradient descent based scheme presented is independent of the initialization.

Despite a solid theory developed, lack of numerical experiments reduces the quality of the paper. Additional experiments with random data to illustrate the theory would be beneficial and it would also be nice to find applications with real data.

In addition as mentioned in the abstract the authors suggest that the methods used in the paper may also aid in the analysis of shallow non-smooth neural networks but they need to continue and elaborate with more explicit connections.

Minor typos near the end of the paper and perhaps missing few definitions and notation are also a small concern

The paper is a very nice work and still seems significant! Nonetheless, fixing the above will elevate the quality of the paper.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkgvlzm96Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HklSf3CqKm&amp;noteId=rkgvlzm96Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1259 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1259 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your valuable feedback!

We have expanded our synthetic experiment section, added an experiment with real data, and added a conclusion section which discusses some connections to shallow neural nets. Please feel free to take a look at our revision.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SkxX-tyrT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A good paper </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HklSf3CqKm&amp;noteId=SkxX-tyrT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1259 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1259 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies dictionary learning problem by a non-convex constrained l1 minimization. By using subgradient descent algorithm with random initialization, they provide a non-trivial global convergence analysis for problem. The result is interesting, which does not depend on the complicated initializations used in other methods. 

The paper could be better, if the authors could provide more details and results on numerical experiments.   This could be used to confirm the proved theoretical properties in practical algorithms. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1gomAzrpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks &amp; will have more detailed experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HklSf3CqKm&amp;noteId=r1gomAzrpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1259 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1259 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the positive feedback!

We are performing some more experiments as well as expanding the experiments section in more details. Please stay tuned and we will let you know when it’s done.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkg-4f75pQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Update: paper revised</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HklSf3CqKm&amp;noteId=rkg-4f75pQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1259 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1259 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have expanded the synthetic experiments in Section 5 and added a real data experiments in Appendix H. Please feel free to take a look at our revision.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_Byl9W4UmTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>solid analysis and new insights</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HklSf3CqKm&amp;noteId=Byl9W4UmTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1259 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1259 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies nonsmooth and nonconvex optimization and provides a global analysis for orthogonal dictionary learning. The analysis is highly nontrivial compared with existing work. Also for dictionary learning nonconvex $\ell_1$ minimization is very important due to its robustness properties. 

I am wondering how extendable is this approach to overcomplete dictionary learning. It seems that overcomplete dictionary would break the key observation of "sparsest vector in the subspace". 

Is it possible to circumvent the difficulty of nonsmoothness using (randomized) smoothing, and then apply the existing theory to the transformed objective? My knowledge is limited but this seems to be a more natural thing to try first. Could the authors compare this naive approach with the one proposed in the paper?

Another minor question is about the connection with training deep neural networks. It seems that in practical training algorithms we often ignore the fact that ReLU is nonsmooth since it only has one nonsmooth point — only with diminishing probability, it affects the dynamics of SGD, which makes subgradient descent seemingly unnecessary. Could the authors elaborate more on this connection?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BklN3TzB6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Extension to overcomplete case; role of nonsmoothness</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HklSf3CqKm&amp;noteId=BklN3TzB6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1259 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1259 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the positive feedback! We respond to the questions in the following.

“Extending to overcomplete DL” --- We believe that our theory has the potential to generalize into the overcomplete case. There, a natural generalization of the orthogonality assumption is that the dictionary A is a well-conditioned tight frame (n x L “fat” matrix with orthonormal rows and suitably widespread columns in the n-dim space). Although the "sparse vectors in a linear subspace" intuition fails there, we would still expect the columns a_i of A minimize the population objective ||a^T Y||_1 = ||a^T A X||_1: due to the widespread nature of columns of A, a_i^T A would be an “approximately 1-sparse” vector (i.e., with one dominant entry and others having small magnitudes) and so vectors a_i^T AX are expected to be noisy estimates of rows of X, which are the sparest (in a soft sense) vectors among all vectors of the form a^T AX. Figuring out the precise optimization landscape in that case would be of great interest.  

“Nonsmooth approach vs. (randomized) smoothing” --- We wonder whether you’re referring to the smoothed *objective*, or applying smoothing *algorithms* on our non-smooth objective. We will discuss both as follows.

A smoothed objective was analyzed in Sun et al.‘15. Smoothing therein helped to make conventional calculus tools and expectation-concentration style argument readily applicable conceptually, but the smoothed objective and its low-order derivatives led to involved technical analysis---the smoothed objective loses the simplicity of the L1 function. This tends to be the case for several natural smoothing schemes. Also, L1 function is the regularizer people use in practical dictionary learning. This paper directly works with the non-smooth L1 objective and is able to obtain stronger results with a substantially cleaner argument, using unconventional yet highly accessible tools from nonsmooth analysis, set-valued analysis, and random set theory. 

Smoothing algorithms on non-smooth objective is an active area of ongoing research. For example, Jin et al. ‘18 showed that randomized smoothing algorithms succeed on minimizing non-smooth objectives as long as it is point-wise close to a smooth objective, which is often chosen to be its expected version. However, in our case, even the expected objective is non-smooth (see e.g. Section 3.1), so it is not readily applicable. Moreover, the result there is based on a zero-th order method, which is a conservative algorithmic choice when the (sub)gradient information is readily available---this is the case for us. In this paper, we are able to show the convergence of subgradient descent (i.e., a first-order method) directly on the non-smooth objective. It would be of interest to see whether first-order smoothing algorithms work as well.

“Nonsmoothness in neural networks” --- It depends on what perspective we take. 

If we are interested in the landscape (i.e. the global geometry of the loss function), then the nonsmoothness matters a lot as the nonsmooth points are scattered everywhere in the space, and if one initializes the model adversarially near the highly nonsmooth parts, intuitively the performance can be hurt by the nonsmoothness.

However, if we are more interested in the trajectory of some particular algorithms (say, SGD), then maybe the non-smoothness won’t hurt a lot --- as long as nice properties on the trajectory can be established. Such a trajectory-specific analysis has been done recently in, e.g., Du et al. ‘18. Even in this kind of results, there is no formal theory or statement saying that the nonsmooth points won’t be encountered. 

Besides our work, there are other recent papers showing why nonsmoothness should and can be handled on a rigorous basis, e.g., Laurent &amp; von Brecht ’17, Kakade &amp; Lee ’18. 

Reference:
Sun, J., Qu, Q., &amp; Wright, J. (2015). Complete Dictionary Recovery over the Sphere I: Overview and the Geometric Picture. arXiv preprint arXiv:1511.03607.

Jin, C., Liu, L. T., Ge, R., &amp; Jordan, M. I. (2018). Minimizing Nonconvex Population Risk from Rough Empirical Risk. arXiv preprint arXiv:1803.09357.

Du, S. S., Zhai, X., Poczos, B., &amp; Singh, A. (2018). Gradient Descent Provably Optimizes Over-parameterized Neural Networks. arXiv preprint arXiv:1810.02054.

Laurent, T., &amp; von Brecht, J. (2017). The Multilinear Structure of ReLU Networks. arXiv preprint arXiv:1712.10132.

Kakade, S., &amp; Lee, J. D. (2018). Provably Correct Automatic Subdifferentiation for Qualified Programs. arXiv preprint arXiv:1809.08530.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1lW-zgChm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Non-smooth non-convex optimization approach to complete dictionary learning</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HklSf3CqKm&amp;noteId=H1lW-zgChm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1259 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1259 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper is a direct follow-up on the Sun-Qu-Wright non-convex optimization view on the Spielman-Wang-Wright complete dictionary learning approach. In the latter paper the idea is to simply realize that with Y=AX, X being nxm sparse and A a nxn rotation, one has the property that for m large enough, the rows of X will be the sparsest element of the subspace in R^m generated by the rows of Y. This leads to a natural non-convex optimization problem, whose local optimum are hopefully the rows of X. This was proved in SWW for *very* sparse X, and then later improved in SQW to the linear sparsity scenario. The present paper refines this approach, and obtain slightly better sample complexity by studying the most natural non-convex problem (ell_1 regularization on the sphere).


I am not an expert on SQW so it is hard to evaluate how difficult it was to extend their approach to the non-smooth case (which seems to be the main issue with ell_1 regularization compared to the surrogate loss of SQW).


Overall I think this is a solid theoretical contribution, at least from the point of view of non-smooth non-convex optimization. I have some concerns about the model itself. Indeed *complete* dictionary learning seemed like an important first step in 2012 towards more general and realistic scenario. It is unclear to this reviewer whether the insights gained for this complete scenario are actually useful more generally.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJeARTGrpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response on SQW and potential generalizations</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HklSf3CqKm&amp;noteId=HJeARTGrpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1259 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1259 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the positive feedback! We respond to the specific questions in turn.

“Challenge of extending SQW to non-smooth case” --- The high-level ideas of obtaining the two results are the same: characterizing the nice global landscape of the respective objectives on the sphere, and then designing specific optimization algorithms taking advantage of the particular landscapes. Characterization of the landscape is through the use of first-order (and second-order) derivatives. For our nonsmooth setting, we have to use the subdifferential to describe the first-order geometry, which involves dealing with set-valued functions and random sets (due to the randomness in the data assumption)---very different than dealing with the gradient and Hessian in the smooth calculus, as in SQW. Moreover, traditional argument of uniform convergence of random quantities to their expectation often relies on Lipschitz property of the quantities of interest. For random sets, the notion of concentration is unconventional, and the desired Lipschitz property also fails to hold. We introduce tools from random set theory and construct a novel concentration argument getting around the Lipschitz requirement. This in turn implies that the first-order geometry of the sample objective is close to the benign population objective, from which the algorithmic guarantee follows.

“Potential generalizations” ---  We believe that our theory has the potential to generalize into the overcomplete case. There, a natural generalization of the orthogonality assumption is that the dictionary A is a well-conditioned tight frame (n x L “fat” matrix with orthonormal rows and suitably widespread columns in the n-dim space). Although the "sparse vectors in a linear subspace" intuition fails there, we would still expect the columns a_i of A minimize the population objective ||a^T Y||_1 = ||a^T A X||_1: due to the widespread nature of columns of A, a_i^T A would be an “approximately 1-sparse” vector (i.e., with one dominant entry and others having small magnitudes) and so vectors a_i^T AX are expected to be noisy versions of rows of X, which are the sparest (in a soft sense) vectors among all vectors of the form a^T AX. Figuring out the precise optimization landscape in that case would be of great interest.  </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_r1g5pn1527" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Relevant problem,  incomplete paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HklSf3CqKm&amp;noteId=r1g5pn1527"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1259 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1259 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=r1g5pn1527" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a subgradient descent method to learn orthogonal, squared /complete n x n  dictionaries under l1 norm regularization. The problem is interesting and relevant, and the paper, or at least the first part, is clear.

The most interesting property is that the solution does not depend on the dictionary initialization, unlike many other competing methods. 

The experiments sections in disappointingly short. Could the authors play with real data? How does sparsity affect the results? How does it change with different sample complexities? Also, it would be nice to have a final conclusion section. I think the paper contains interesting material but, overall, it gives the impression that the authors rushed to submit the paper before the deadline!</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">1: The reviewer's evaluation is an educated guess</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkxa-CzHpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Will expand experiments and add conclusion/discussion</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HklSf3CqKm&amp;noteId=Bkxa-CzHpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1259 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1259 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the thoughtful feedback! 

Our preliminary experiments do show the effect of sample complexity -- in particular, empirically the subgradient descent algorithm almost always succeed as long as m = O(n^2), which is even better than the O(n^4) suggested by our theory.

We are working on additional experiments comparing different sparsity, and real data experiments. (The experiments are indeed a bit time-consuming and would require days.) 

We are also working on adding a conclusion section and revising the paper a bit. Please stay tuned and we will let you know when it’s done. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJgbLGX96Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Update: paper revised</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HklSf3CqKm&amp;noteId=SJgbLGX96Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1259 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1259 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have expanded our synthetic experiment section, added an experiment with real data, and added a conclusion section which discusses some connections to shallow neural nets. Please feel free to take a look at our revision.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>