<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>MEAN-FIELD ANALYSIS OF BATCH NORMALIZATION | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="MEAN-FIELD ANALYSIS OF BATCH NORMALIZATION" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1eSg3C9Ym" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="MEAN-FIELD ANALYSIS OF BATCH NORMALIZATION" />
      <meta name="og:description" content="Batch Normalization (BatchNorm) is an extremely useful component of modern neural network architectures, enabling optimization using higher learning rates and achieving faster convergence. In this..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1eSg3C9Ym" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>MEAN-FIELD ANALYSIS OF BATCH NORMALIZATION</a> <a class="note_content_pdf" href="/pdf?id=B1eSg3C9Ym" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 12 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019mean-field,    &#10;title={MEAN-FIELD ANALYSIS OF BATCH NORMALIZATION},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=B1eSg3C9Ym},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=B1eSg3C9Ym" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Batch Normalization (BatchNorm) is an extremely useful component of modern neural network architectures, enabling optimization using higher learning rates and achieving faster convergence. In this paper, we use mean-field theory to analytically quantify the impact of BatchNorm on the geometry of the loss landscape for multi-layer networks consisting of fully-connected and convolutional layers. We show that it has a flattening effect on the loss landscape, as quantified by the maximum eigenvalue of the Fisher Information Matrix. These findings are then used to justify the use of larger learning rates for networks that use BatchNorm, and we provide quantitative characterization of the maximal allowable learning rate to ensure convergence. Experiments support our theoretically predicted maximum learning rate, and furthermore suggest that networks with smaller values of the BatchNorm parameter achieve lower loss after the same number of epochs of training.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">neural networks, optimization, batch normalization, mean field theory, Fisher information</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SyxNantt27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1eSg3C9Ym&amp;noteId=SyxNantt27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1071 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1071 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies the effect of batch normalization via a physics style mean-field theory. The theory yields a prediction of maximal learning rate for fully-connected and convolutional networks, and experimentally the max learning rate agrees very well with the theoretical prediction.

This is a well-written paper with a clean, novel result: when we fix the BatchNorm parameter \gamma, a smaller \gamma stabilizes the training better (allowing a greater range of learning rates). Though in practice the BatchNorm parameters are also trained, this result may suggest using a smaller initialization. 

A couple of things I was wondering:

-- As a baseline, how would the max learning rate behave without BatchNorm? Would the theories again match the experimental result there?

-- Is the presence of momentum important? If I set the momentum to be zero, it does not change the theory about the Fisher information and only affects the dependence of \eta on the Fisher information. In this case would the theory still match the experiments?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1e2x7XI6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for your review! Additional experiments and results have been added.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1eSg3C9Ym&amp;noteId=B1e2x7XI6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1071 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1071 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for your review and valuable comments. We address your questions and comments below:

1. As a baseline, how would the max learning rate behave without BatchNorm? Would the theories again match the experimental result there?

We also wondered how the max learning rate would behave without BatchNorm, and thus we did an experiment for a network without BatchNorm where we varied \sigma_w, the weight initialization variance, and found that the theory again matches the experimental result. However, we didn’t include this result in the previous draft. We have now added this result to the SM in the new revised version as a baseline.

2. Is the presence of momentum important? If I set the momentum to be zero, it does not change the theory about the Fisher information and only affects the dependence of $\eta$ on the Fisher information. In this case would the theory still match the experiments?

The presence of momentum doesn't change the picture dramatically. We set momentum to 0.9 to match the value frequently used in practice. Indeed, changing the momentum only affects the dependency of \eta on the FIM. We have performed an additional experiment on training without momentum and find that in this case the theory still matches the experiment. 

3. This is a well-written paper with a clean, novel result: when we fix the BatchNorm parameter \gamma, a smaller \gamma stabilizes the training better (allowing a greater range of learning rates). Though in practice the BatchNorm parameters are also trained, this result may suggest using a smaller initialization. 

Thanks for the positive feedback! We performed additional experiments in the updated version of our paper with VGG11 and Preact-Resnet18, with various \gamma-initializations, trained on CIFAR-10.  We find that the smaller \gamma-initialization indeed increase the speed of convergence.  This result can be found in the SM of the latest version of our paper.

Thank you again for your review and comments. We believe that the inclusion of a baseline without BatchNorm as well as clarification on the role of momentum has improved the results and clarity of the paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_r1xtU48Y27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting application of MFT on FIM to understand Batch Normalization</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1eSg3C9Ym&amp;noteId=r1xtU48Y27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1071 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1071 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Interesting application of MFT on FIM to understand Batch Normalization

This paper applies mean field analysis to networks with batch normalization layers. Analyzing maximum eigenvalue of the Fisher Information Matrix, the authors provide theoretical evidence of allowing higher learning rates and faster convergence of networks with batch normalization. 

The analysis reduces to providing lower bound for maximum eigenvalue of FIM using mean-field approximation. Authors provide lower bound of the maximum eigenvalue in the case of fully-connected and convolutional networks with batch normalization layers. Lastly authors observe empirical correlation between smaller \gamma and lower test loss. 

Pro: 
 - Clear result providing theoretical ground for commonly observed effects. 
 - Experiments are simple but illustrative. It is quite surprising how well the maximum learning rate prediction matches with actual training performance curve. 
	

Con:
 - While mean field analysis a-priori works in the limit where networks width goes to infinity for fixed dataset size, the analysis of Fisher and Batch normalization need asymptotic limit of dataset size. 
 - Although some interesting results are provided. The content could be expanded further for conference submission. The prediction on maximum learning rate is interesting and the concrete result from mean field analysis
 - While correlation between batch norm \gamma parameter and test loss is also interesting, the provided theory does not seem to provide good intuition about the phenomenon. 

Comments:
- The theory provides the means to compute lower bound of maximum eigenvalue of FIM using mean-field theory. In Figure 1, is \bar \lambda_{max} computed using the theory or empirically computed on the actual network? It would be nice to make this clear. 
- In Figure 2, the observed \eta_*/2 of dark bands in heatmap is interesting. While most of networks without Batch Norm, performance is maximized using learning rates very close to maximal value, often networks using batch norm the learning rate with maximal performance is not the maximal one and it would be interesting to provide theoretical 
- I feel like section 3.2 should cite Xiao et al (2018). Although this paper is cited in the intro, the mean field analysis of convolutional layers was first worked out in this paper and should be credited. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BygxB4QL6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for your review! Additional experiments and results have been added.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1eSg3C9Ym&amp;noteId=BygxB4QL6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1071 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1071 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for your review and helpful comments. We address your questions and concerns individually below:

1. While mean field analysis a priori works in the limit where networks width goes to infinity for fixed dataset size, the analysis of Fisher and Batch normalization need asymptotic limit of dataset size.

Thank you for pointing this out. Our derivation of Claim 3.1 from (153) to (154) in SM is based on older definitions of order parameters, where E_{x, y} was replaced by E_{x \neq y}, and therefore the asymptotic limit of large dataset size was required. 

However, based on our new definitions of order parameters, (153) to (155) are exact, and we should have removed (154) and revised Claim 3.1. Therefore in our new version, the asymptotic limit of large dataset size is not required in Claim 3.1. We apologize for this mistake and concomitant confusion.

Derivation of recursion relation also requires large dataset size, m, where the error for finite m is O(1/m). Therefore even for a dataset of size 100, the error is around 1%, and the error introduced by finite m is negligible for most of the frequently-used datasets. We have added an explanation of this issue in the latest version of the submission. 

The other place where there is a potential issue of large dataset size is in using the empirical FIM to approximate the true FIM in Section 2.1. However, since we are concerned here with the convergence of the learning dynamics on the training set, the empirical FIM is actually sufficient for our analysis. For future work on extending this theory to study generalization, limited dataset size must be taken into account.

2. Although some interesting results are provided. The content could be expanded further for conference submission. The prediction on maximum learning rate is interesting and the concrete result from mean field analysis...[did this get cut off?]
While correlation between batch norm \gamma parameter and test loss is also interesting, the provided theory does not seem to provide good intuition about the phenomenon.

Indeed, this is correct. Our approach targets exploring the change of the FIM spectrum, and hence the maximal learning rate, with/without BatchNorm, and hence isn't able to directly make statements about generalization. However, our theory predicts that faster convergence is linked to smaller \gamma-initialization, which is a new practical finding to our knowledge. Following this intuition, we performed additional experiments in the updated version of our paper with VGG16 and Preact-Resnet18, with various \gamma initializations, trained on CIFAR-10. We find that the smaller \gamma initialization indeed increase the speed of convergence. This result can be found in the SM of the latest version of our paper.

3. The theory provides the means to compute lower bound of maximum eigenvalue of FIM using mean-field theory. In Figure 1, is \lambda_{max} computed using the theory or empirically computed on the actual network? It would be nice to make this clear.

We are sorry for this confusion. It is computed using the theory and we have clarified this in our latest version. This is also useful in practice because direct numerical calculation of \lambda_max is difficult for realistic deep neural networks due to high computational cost.

4. In Figure 2, the observed \eta_*/2 of dark bands in heatmap is interesting. While most of networks without Batch Norm, performance is maximized using learning rates very close to maximal value, often networks using batch norm the learning rate with maximal performance is not the maximal one and it would be interesting to provide theoretical.

This is indeed an interesting observation, but since our theory can't directly speak to performance (it analyzes the maximal allowed rate instead of the optimal rate), a different approach would be required to explain this phenomenon.

5. I feel like section 3.2 should cite Xiao et al (2018). Although this paper is cited in the intro, the mean field analysis of convolutional layers was first worked out in this paper and should be credited.

Yes certainly, and we apologize for the oversight. We have updated the citation in our latest version.

Thank you again for your review and comments. Hopefully our reply has addressed your question and concerns.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Hkx4CIzQ2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Theoretical but not rigorous</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1eSg3C9Ym&amp;noteId=Hkx4CIzQ2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1071 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1071 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this paper, the effect of batch normalization to the maximum eigenvalue of the Fisher information is analyzed. The techinique is mostly developed by Karakida et al. (2018). The main result is an informal bound of the maximum eigenvalue, which is given without proof. Though, the numerical result corresponds to the derived bound.

The paper is basically well written, but the technical part has several notational problems. For example, there is no definition of "\otimes", "\odot", and "Hess" operators.

The use of the mean-field theory is an interesting direction to analyze batch normalization. However, in this paper, it seems failed to say some rigorous conclusion. Indeed, all of the theoretical outcomes are written as "Claims" and no formal proof is given. Also, there is no clear explanation of why the authors give the results in a non-rigorous way, where is the difficult part to analyze in a rigorous way, etc. 

Aside from the rigor issue, the paper heavily depends on the study of Karakida et al. (2018). The derivation of the bound (44) is directly built on Karakida's results such as Eqs. (7,8,20--22), which reduces the paper's originality.

The paper also lacks practical value. Can we improve an algorithm or something by using the bound (44) or other results?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1lvsPXITQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for your review! Additional experiments and results have been added. Part 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1eSg3C9Ym&amp;noteId=H1lvsPXITQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1071 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1071 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for your review and helpful comments. We address your specific questions and comments below:

1. The main result is an informal bound of the maximum eigenvalue, which is given without proof. Though, the numerical result corresponds to the derived bound.

We omitted some important steps in the proof of the bound for the maximum eigenvalue in the original version. We have updated the detailed proof in the SM of our latest version, and apologize for any confusion this caused.

2. The paper is basically well written, but the technical part has several notational problems. For example, there is no definition of "$\otimes$", "$\odot$", and "Hess" operators.

Thanks for the comments. We have updated the paper and added definitions and explanations for all notations.

3. The use of the mean-field theory is an interesting direction to analyze batch normalization. However, in this paper, it seems failed to say some rigorous conclusion. Indeed, all of the theoretical outcomes are written as "Claims" and no formal proof is given. Also, there is no clear explanation of why the authors give the results in a non-rigorous way, where is the difficult part to analyze in a rigorous way, etc.

 Thanks for raising this issue, and allow us an attempt to clarify. Our approach to estimating the maximal eigenvalue of the FIM for a random neural network involves two assumptions. First, we assume a large layer width in the network so that the behavior of a hidden node can be approximated by Gaussian distribution due to central limit theorem. Second, we assume that the averages for the forward and backward pass in the network are uncorrelated. Both assumptions are common and empirically successful in existing literature on mean-field theory of neural networks[1][2], however the second one in particular lacks a rigorous justification. Therefore we present our results as claims instead of theorems to emphasize that additional work is needed to rigorously justify the existing assumptions in the mean field literature generally.
    
    To make as explicit as possible our assumptions mentioned above, we have added a clear derivation in our latest version that hopefully will give the reader greater confidence in the rigor of our results.
    
    In addition, we acknowledge that the assumptions stated above have not been rigorously justified, albeit being well-accepted in other papers. Thus we performed extensive experiments to test the validity of our theoretical results, finding that indeed the experiments correspond strikingly well to the theory.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SkeeYvXLa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for your review! Additional experiments and results have been added. Part 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1eSg3C9Ym&amp;noteId=SkeeYvXLa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1071 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1071 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">4. Aside from the rigor issue, the paper heavily depends on the study of Karakida et al. (2018). The derivation of the bound (44) is directly built on Karakida's results such as Eqs. (7,8,20--22), which reduces the paper's originality.
    The paper also lacks practical value.  Can we improve an algorithm or some-thing by using the bound (44) or other results?

   Although our paper is motivated by their approach, Karakida et al. (2018) have different goals than us, and we significantly extend the framework to address our questions. While Karakida et al. (2018) focuses on studying the statistics of the FIM for vanilla (no BatchNorm) fully-connected neural networks, our aim is to study the role of BatchNorm. Therefore we extend the theory significantly, to both fully-connected and convolutional neural networks, with and without BatchNorm, and derive a new lower bound for ConvNets. We find that adding BatchNorm can greatly reduce the maximal eigenvalue of the FIM, and perform experiments to verify this.
    
    A practical upshot of the paper is that faster convergence is linked to smaller \gamma-initialization, which is a new practical finding to our knowledge. To justify this, we have performed additional experiments in the updated version of our paper with VGG16 and Preact-Resnet18 with various \gamma initializations trained on CIFAR-10. We find that a smaller \gamma initialization indeed increases the speed of convergence. This result is included in the SM of the latest version of our paper. Thus, we believe that our work has both theoretical and practical value that should be of use to other researchers.
    
    More generally, by excluding unfeasible regions of parameters space, our analysis can be used for  hyperparameter search in more realistic architectures than the fully-connected ones considered in Karakida.

Thank you again for your review and comments. Hopefully our reply has addressed your question and concerns.

[1]Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep informationpropagation. In International Conference on Learning Representations (ICLR), 2017.
[2]Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S. Schoenholz, and Jeffrey Penning-ton. Dynamical isometry and a mean field theory of cnns:  How to train 10,000-layer vanilla convolutional neural networks. In International Conference on Machine Learning (ICML), 2018.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SyluPP7Uam" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1eSg3C9Ym&amp;noteId=SyluPP7Uam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper1071 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>