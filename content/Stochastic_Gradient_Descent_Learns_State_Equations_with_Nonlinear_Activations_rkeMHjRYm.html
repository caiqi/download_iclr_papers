<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Stochastic Gradient Descent Learns State Equations with Nonlinear Activations | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Stochastic Gradient Descent Learns State Equations with Nonlinear Activations" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rkeMHjR9Ym" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Stochastic Gradient Descent Learns State Equations with Nonlinear..." />
      <meta name="og:description" content="We study discrete time dynamical systems governed by the state equation $h_{t+1}=ϕ(Ah_t+Bu_t)$. Here A,B are weight matrices, ϕ is an activation function, and $u_t$ is the input data. This relation..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rkeMHjR9Ym" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Stochastic Gradient Descent Learns State Equations with Nonlinear Activations</a> <a class="note_content_pdf" href="/pdf?id=rkeMHjR9Ym" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019stochastic,    &#10;title={Stochastic Gradient Descent Learns State Equations with Nonlinear Activations},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rkeMHjR9Ym},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We study discrete time dynamical systems governed by the state equation $h_{t+1}=ϕ(Ah_t+Bu_t)$. Here A,B are weight matrices, ϕ is an activation function, and $u_t$ is the input data. This relation is the backbone of recurrent neural networks (e.g. LSTMs) which have broad applications in sequential learning tasks. We utilize stochastic gradient descent to learn the weight matrices from a finite input/state trajectory $(u_t,h_t)_{t=0}^N$. We prove that SGD estimate linearly converges to the ground truth weights while using near-optimal sample size. Our results apply to increasing activations whose derivatives are bounded away from zero. The analysis is based on i) an SGD convergence result with nonlinear activations and ii) careful statistical characterization of the state vector. Numerical experiments verify the fast convergence of SGD on ReLU and leaky ReLU in consistence with our theory.</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We study the state equation of a recurrent neural network. We show that SGD can efficiently learn the unknown dynamics from few input/output observations under proper assumptions.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">recurrent neural network, state equation, gradient descent, sample complexity</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SJx2X4Rpn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting result on learning a non-linear dynamical system</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkeMHjR9Ym&amp;noteId=SJx2X4Rpn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper69 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper69 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This work considers the problem of learning a non-linear dynamical system in which the output equals the state.  Under several assumptions (input is Gaussian, non-linear activation is strictly increasing, stable system) it is shown that SGD converges linearly to the ground truth system with near-optimal sample complexity. The proof idea is to reduce this problem to the problem of learning a single non-linear neuron in the case that the covariance matrix of the data is well-conditioned. The main challenge is to show the covariance is well-conditioned under the reduction. In a nutshell, this is done by splitting the trajectory to sub-trajectories with independent states and using results from random matrix theory on matrices with independent rows.
 
This work tackles a very challenging problem and the results are interesting. The guarantees are strong – linear convergence to the ground truth parameters and near-optimal sample size. Given that not much is known on deep non-linear networks, I think that the result is significant. The main weakness of the paper is the assumption that the state equals the output. Another minor weakness is the clarity and presentation of results:
1.       The proof outline of the main result is hard to follow. There is no proof outline of Theorem 4.2 in the main text. The proof is highly technical and there are many technical ideas that were moved to the appendix. For instance, the proofs in sections C and D are not mentioned in the main text. I suggest to write a summary of the steps required to prove the main result and how all of the technical ideas are combined together.
2.       There is no reference and comparison to the paper of Mei et al. [1] that study single neuron models.
3.       It is claimed that by increasing beta the convergence is faster. However, I am not sure why this is meaningful. By changing beta the ground truth changes as well. For beta = 0 the ground truth dynamical system is linear and for beta = 1 the ground truth is a non-linear dynamical system with ReLU. Since a ReLU network is more expressive, generally in the case of beta = 1 the ground truth is more difficult to learn than beta = 0. Therefore, we should expect convergence to be slower than beta=0 or not occur at all. Am I missing something?
4.       The Gaussian assumption is not stated clearly in the text. It can be deduced only from the statements of the theorems and the conclusion section.
5. In Theorem F.1, it is claimed that all rows of E are equal. However, in the statement of the theorem it is not mentioned that the rows of A are identically distributed. Should this assumption be included in the statement?

[1] Mei, Song, Yu Bai, and Andrea Montanari. "The landscape of empirical risk for non-convex losses." arXiv preprint arXiv:1607.06534 (2016).‏ 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJxxv9Bp37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting and challenging problem, but assumptions weaken the results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkeMHjR9Ym&amp;noteId=HJxxv9Bp37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper69 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper69 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies the ability of SGD to learn dynamics of a linear system + non-linear activation. That is, in the standard LTI setting, the dynamics of a system evolve according to

h_{t+1} = Ah_t + Bu_t,

on input u_t.

In addition, this paper considers the setting where the evolution is:

h_{t+1} = \phi(Ah_t + Bu_t)

for \phi a non-linear activation function. 

This is a difficult problem. Though system identification was for many decades a large and active area in the control community, the understanding of system identification from a modern statistical perspective (understanding sample complexity and computational complexity simultaneously) is surprisingly lacking. This is evidenced by the fact that the first results along these lines for the simplest possible (SISO, LTI) system, came only recently (Hardt, Ma, Recht ’16). 

This paper attacks a more general setting, due to the presence of the nonlinearity.

However, the present setting is significantly limited in another sense: the authors assume that the state is observed directly. This is in contrast to the typical situation where we observe only a projection of the state, or possibly even a noisy such projection. Indeed, this is one of the critical complications in the work of Hardt, Ma and Recht. Without it, i.e., under the assumption that the entire state trajectory can be directly observed, much more is possible, and indeed much more has been done. For example, work by Bento, Ibrahimi and Montanari ’10, solves a more difficult problem in that they estimate sparse dynamics (in appropriate sample complexity). Jalali and Sanghavi ’11 generalized the work of Bento et al., to the setting where some of the components of the state are not all observed, but rather some are latent.

The motivating application for this work is estimating RNNs. In this case, the state variable represents the critical information that is carried from one time to the next in the RNN. Presumably the setting here is to show that if indeed data are generated by an RNN, then we can compute this using SGD and backprop. Towards this, the assumption of having access to the internal state is a difficult one. On the one hand, this is a hard and important problem. On the other, we really won’t have access to such an internal state. There are of course other problematic aspects, such as robustness, the inability to use ReLU (Defn 3.1). But the observation model seems important. Again, I believe this is especially so, because the considerable complications present in Hardt, Ma, Rect ’16 specifically seemed to be a consequence of the observation model being partial.

The inability to use ReLU at first look does not seem like a great limitation. But then one problematic aspect here seems that the proof concept and direction critically rely on this, as they basically reduce to the setting of linear activations — something which, presumably, is impossible for something like ReLU. So it is not only the results, but also the developed machinery, that seem to be inherently limited.

This is, overall, an interesting paper, attacking an important and also very challenging area. As with all papers in this vein, we are left with having to make a judgement call on whether this simplified scenario is indeed a good first step towards solving the problems we are hoping to solve. Is it developing the right insight, right tools, etc. While I find there is a lot of interesting and good work in this paper, I am not completely convinced about this last point.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Sygf9aE927" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good convergence result for non-convex dynamic problem under stable system condition</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkeMHjR9Ym&amp;noteId=Sygf9aE927"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper69 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper69 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper studies discrete time dynamical systems with a non-linear state equation.  They assume the non-linear function is assumed to be \beta-increasing like leaky ReLU. Under this setting, the authors prove that for the given state equation for stable systems with random gaussian input at each time step, running SGD on a fixed length trajectory gives logarithmic convergence.

The paper is well-written and proves strong convergence properties. The deterministic result does not seem very novel and uses the idea of one-point strong convexity which has been studied in various prior works. However the bounding of the condition number of the data matrix is interesting and guarantees are near-optimal. The faster convergence for odd activations is a good observation. Overall, I think the paper is good. I do list some concerns:
Questions/concerns:
- The deterministic theorem (Theorem 4.1) seems similar to Theorem 3 in [1] with SGD instead of GD. Also under the distribution being symmetric, it can be derived from [2] with $k=1$. 
- Can the ideas be extended to other commonly used activations such as ReLUs/Sigmoids? Sigmoids have exponentially small slope near origin.
- The proof seems to rely on the fact that due to the gaussian input added each time step and stable system assumption after a sufficient number of time steps, the input-output pairs will not be highly correlated. So the data is sufficiently uncorrelated taking enough data. What happens if this data at each step is not gaussian?
- In the unstable setting, the solution proposed just samples from different trajectories which by default are independent hence correlation is not an issue, this seems a bit like cheating. 
- In RNNs, the motivation of the work, the hidden vectors are not observed, thus this setting seems a bit restrictive.
- If SGD was performed on only one truncated series, do the results still hold?

Other comments:
- There has been previous work on generalized linear models which work in more general settings like GLMtron [3]. The authors should update prior work on generalized linear models as well as neural networks.
- Typo on Page 2 y_t = h_{t+1} not y_t = h_t.

[1] Dylan J. Foster, Ayush Sekhari, and Karthik Sridharan. Uniform Convergence of Gradients for Non-Convex Learning and Optimization. NIPS 2018.
[2] Surbhi Goel, Adam Klivans, and Raghu Meka. Learning One Convolutional Layer with Overlapping Patches. ICML 2018.
[3] Sham M. Kakade et al. Efficient learning of generalized linear and single index models with isotonic regression. NIPS 2011.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>