<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Logit Regularization Methods for Adversarial Robustness | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Logit Regularization Methods for Adversarial Robustness" />
        <meta name="citation_author" content="Cecilia Summers" />
        <meta name="citation_author" content="Michael J. Dinneen" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Bylj6oC5K7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Logit Regularization Methods for Adversarial Robustness" />
      <meta name="og:description" content="While great progress has been made at making neural networks effective across a wide range of tasks, many are surprisingly vulnerable to small, carefully chosen perturbations of their input, known..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Bylj6oC5K7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Logit Regularization Methods for Adversarial Robustness</a> <a class="note_content_pdf" href="/pdf?id=Bylj6oC5K7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=ceciliasummers07%40gmail.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="ceciliasummers07@gmail.com">Cecilia Summers</a>, <a href="/profile?email=mjd%40cs.auckland.ac.nz" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="mjd@cs.auckland.ac.nz">Michael J. Dinneen</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">While great progress has been made at making neural networks effective across a wide range of tasks, many are surprisingly vulnerable to small, carefully chosen perturbations of their input, known as adversarial examples. In this paper, we advocate for and experimentally investigate the use of logit regularization techniques as an adversarial defense, which can be used in conjunction with other methods for creating adversarial robustness at little to no cost. We demonstrate that much of the effectiveness of one recent adversarial defense mechanism can be attributed to logit regularization and show how to improve its defense against both white-box and black-box attacks, in the process creating a stronger black-box attacks against PGD-based models.
</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">adversarial</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Logit regularization methods help explain and improve state of the art adversarial defenses</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">19 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rkgsKwH667" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bylj6oC5K7&amp;noteId=rkgsKwH667"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper833 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper833 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1leI9Iah7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting approach, not sure how significant.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bylj6oC5K7&amp;noteId=H1leI9Iah7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper833 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper833 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper studies how logit regularization techniques can affect adversarial robustness. The authors study the recently proposed technique of adversarial logit pairing (ALP) and attempt to understand the different effects it has on the resulting model and how each of them can affect robustness. They argue that a significant part of ALP's success stems from regularizing the logits and that alternative logit regularization methods can lead to similar robustness. Finally, they propose an alternative to ALP that explicitly disentangles logit similarity and logit regularization to train models that are claimed to be more robust than state-of-the-art models.

Before continuing with my review, I would like to note that the effectiveness of ALP in increasing robustness is still being debated. The robustness of ALP models has not been verified by a third party. The models released by the authors were shortly found to vulnerable to simply running a standard attack for more steps (<a href="https://arxiv.org/abs/1807.10272)." target="_blank" rel="nofollow">https://arxiv.org/abs/1807.10272).</a> The ALP authors claim that this is due to releasing a smaller model for a competition but they still have not released another model that is claimed to be robust. Nevertheless, I believe that this situation should not affect how the current paper is reviewed. The idea of regularizing the logits is natural and precisely understanding its effect on the model is a valuable research direction.

Overall I like the idea of the paper. I agree that the effect of logit regularization methods on the model is unclear and attempting to dissect these techniques is a very important research direction. On the other hand, this is a very tricky area to perform such experiments. Accurately measuring the robustness of a model is a challenging task. It is therefore hard to understand if an increase in adversarial accuracy is due to a true increase in the robustness of a model or due to the attack being used performing sub-optimally when the setting is modified.

Most of the results of the paper revolve around rather fine-grained differences in the adversarial accuracy of &lt;5% (e.g. Figure 1, Table 1). Such small differences might in fact be artifacts of the PGD attack used performing sub-optimally. Moreover, for some of the results, it is clear that they are indeed an artifact of the attacks performing sub-optimally. For instance the left of Figure 2, label smoothing is reported to achieve high accuracy again PGD yet in Section 5.3 it is reported that more steps can reduce the accuracy to &lt;15%. It is thus clear that the claim about label smoothing increasing robustness is misleading.

I believe it is essential that the authors validate theirs results, at the very least Table 1, by evaluating the models on different attacks.
Given that the models regularizes the logits, evaluating an attack optimizing the logits directly (e.g. the CW attack, https://arxiv.org/abs/1608.04644) is essential. Moreover, given that the gradients of the loss appear to not be informative, an attack based on finite differences is also crucial here (see SPSA attack of https://arxiv.org/abs/1802.05666).

Moreover, I find the reasoning behind why should logit regularization techniques increase robustness very limited. The change in logit distributions (Figure 1,2) are rather marginal and do not suggest fundamental differences of the underlying models.

I believe that additional experiments and justifications of the results are needed before the paper is considered for acceptance. I thus recommend rejection at this time.

[UPDATE after reading the public discussion]: It appears the someone already suggested running SPSA as an attack. I really appreciate the authors evaluating the attack and reporting their findings. Indeed it seems that the reported robustness from logit regularization does not stand up to an SPSA attack. As a result, I think it is fairly clear that logit regularization has a limited effect on the robustness of the model and is only making PGD attacks less effective. I update my score to a clear reject.

Minor comments to authors:
-- I would suggest remove the sentence close to the end of the intro about ALP being perhaps the most robust defense. As I outlined above, the robustness of ALP has not been convincingly verified yet. Similarly for the text above equation 2. Perhaps most importantly, the last paragraph before Section 4 where you state that "...ALP _works_..." in a very definitive tone.
-- Paragraph after (5): the logits of the other classes are not guaranteed to increase. Some of them might decrease in the process of making the loss as large as possible.
-- I like equation (10) where you attempt to decompose that effect of similarity and regularization. It would be interesting to understand the interplay between lambda and beta.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ByxXydBFhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice observation and result on CIAFR10. Would like to see how it performs on CIAFR100.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bylj6oC5K7&amp;noteId=ByxXydBFhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper833 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper833 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies adversarial training of robust classification models. It starts off by analyzing ALP method, and hypothesizes that smaller logits may bring more robustness. Based on this observation, the authors experiment with several logit shrinking method including label smoothing and mixup. The authors further propose a variant loss (Eq. 10) of traditional ALP. Experiments on CIFAR10 demonstrate the observations and the proposed method LRM.

0. My overall evaluation: This paper is well-motivated and the result is good. But it lacks novelty. And more experiments will definitely make it stronger. 

1. The Eq. (2) slightly differs from Kannan's ALP paper. They use half clean and half adversarial images in a batch, and here, you use both clean and adversarial images in a batch. Personally I think using both may lead to more robust models. Also, in Table 1, how do you implement ALP? Setting \alpha = 0? Then this is basically adding one regularization term on top of Madry's loss function.

2. When you expand the square and get Eq. 10, this can also illustrates that: 1) minimize logits 2). minimize a distance measure between clean and adversarial images. Also, when it comes to Eq. 10, some parenthesis typos?

3. For Table 1, I would like to see PGD20 (iterations) + 2 (step size in pixels), PGD100 + 2 and PGD200 + 2. Also, I am interested in seeing CW loss which is based on logit margin. My best guess is that PGD, ALP, LRM may not go down for large iterations, because they all depend on adversarial examples (e.g., PGD7) during training. But I am not sure if the gain will be still there. Also, I think label smoothing may go down significantly.

4. I would like to see results using the "wide" model in [madry17] paper for ALP and LRM. I think results from large-capacity models are more convincing.

5. I would like to see results on CIFAR100, which is a harder dataset, 100 classes and 500 images per class. I think CIFAR10 alone is not sufficient for justification nowadays (maybe enough one year ago). It is possible that ALP or logit regularization-based method work only on simple dataset and simple decision boundary (10 classes). Since ImageNet is,  to some extent, computationally impossible for schools, I want to see the justification results on CIFAR100.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJxeOLiooQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Unconvincing</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bylj6oC5K7&amp;noteId=SJxeOLiooQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper833 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">23 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper833 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors examine the effect of logit regularization techniques on the robustness of neural network models to adversarial examples, under the threat model of an L-infinity attacker. Logit pairing is a recently proposed defense technique, which adds an L-2 loss penalizing the difference of the logits on the correct class between the clean and adversarial example. The authors of this paper argue that logit pairing effect tends to make the distribution of clean logits smaller and adversarial logits larger. Thus this can be thought of as regularizing the logits (similar to weight decay) and the authors argue that some other recently proposed measures of defense (like label smoothing, paired example data augmentation etc) can also be understood through the lens of logit regularization. From this insight, the authors propose a new defense: by separating the difference of logit terms from the original ALP loss using (a-b)^2 = a^2 + b^2 - 2ab, they can now vary the effect of logit regularization by having different multipliers on the logit norms, and claim to have more robust models on CIFAR-10.

My main concern with this paper is that it is basing it's arguments on defenses that are well known to cause gradient masking/obfuscation. See this paper <a href="https://arxiv.org/abs/1807.10272" target="_blank" rel="nofollow">https://arxiv.org/abs/1807.10272</a> for an illustration of how ALP just makes the optimization landscape harder, but it is not inherently more robust. Increasing the number of PGD steps with random restarts reduces model accuracy from the claimed 27.9% to 0.6% which is worse than a PGD trained model (which gets 1.5%) on Imagenet. Therefore, one can conclude that the ALP defense is broken and I suspect that the gain from the logit regularization method that the current authors report will not stand up to scrutiny - in particular the authors only report numbers for 20 step PGD and show better numbers compared to PGD, but it is likely to be broken on increasing the number of iterations. Given the large number of questionable defense papers being accepted to deep learning conferences which are then immediately broken following publication, I think it would serve the community to reject such papers, thereby reducing the noise in this research area.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">2: Strong rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkgkx-auoQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Evaluation questions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bylj6oC5K7&amp;noteId=rkgkx-auoQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Oct 2018</span><span class="item">ICLR 2019 Conference Paper833 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Is the code or a pretrained model available for third party evaluation?

Have you tried running PGD for 1000 steps? The problem with the ImageNet evaluation in the ALP paper was that the PGD evaluation was not run for enough steps. On a quick skim of this paper, it looks like the evaluation was run for only 20 steps, so it seems likely that running PGD for 1000 steps would expose the same problem as the ImageNet ALP evaluation.

Have you tried running any tests to check if PGD is getting broken by gradient masking? Using the SPSA attack is a good way to do this.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hkgxu1t6j7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: questions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bylj6oC5K7&amp;noteId=Hkgxu1t6j7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper833 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">24 Oct 2018</span><span class="item">ICLR 2019 Conference Paper833 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your interest -- we hadn't initially planned to make the code and pretrained models available before reviews are complete, but could potentially work to do this if you or the reviewers are interested in evaluating during the review process.

PGD for 1000 steps: We briefly noted in Sec. 5.3 that PGD, ALP, and LRM weren't affected by increasing the number of iterations to 200 (only Label Smoothing was). Here are the results for 1000 steps, which show the same trend:
White-box:
-Regular Training: 0.00%
-Label Smoothing: 7.17%
-PGD: 45.15%
-ALP: 48.34%
-LRM: 51.06%

Compared to 20-step PGD, only Label Smoothing was significantly affected, continuing the trend noted in Sec. 5.3.

Black-box results also tend to decrease by a small amount, depending on the method:

								Source
				Reg		LS		PGD	ALP		LRM
		Reg		20.0		26.4		75.1		73.7		67.2
		LS		60.1		60.8		75.6		74.2		67.4
Target	PGD	68.7		68.0		58.0		57.6		55.7 
		ALP		68.4		67.3		60.4		59.1		56.5
		LRM	70.7		70.4		60.4		59.3		54.1


SPSA: Here are the results for white-box SPSA attacks on CIFAR-10, using the parameters and code provided by the SPSA authors (delta=0.01, learning rate=0.01, iterations=100, batch size=8192). As in the SPSA paper, we used a 1,000-image subsample for efficiency, since an evaluation over the full 10,000-image test set would take roughly 90 hours per evaluation.

-Label Smoothing: 9.2% (8.2% for 1000-step PGD on the same data subset)
-PGD: 45.0% (45.0% for PGD)
-ALP: 43.8% (46.8% for PGD)
-LRM: 45.9% (49.6% for PGD)
(We did not evaluate regular training, but it is presumably still at 0%.)

Though this is fairly preliminary since it's only a subset of the data and one set of hyperparameters for SPSA, the results are very interesting, as this might be the first example of PGD not being a sufficient attack to distinguish adversarial robustness between models, especially when such a strong 1000-iteration PGD attack is used. For example, if the difference in performance between PGD and SPSA attacks is caused by some type of gradient masking, then the models would have to be gradient masking nearly everywhere (due to the random initialization of the PGD-based attack), in a way that can't be detected with even 1,000 steps of PGD, which would be unusual. On the other hand, it could be that ALP or LRM are more effective than PGD at defending against the attack they were trained with (since ALP, LRM, and the PGD defenses were all trained on PGD attacks), but that SPSA is sufficiently different to erase most of this difference, with LRM still slightly ahead. We also note that this turned out to be a relatively unlucky data sample for ALP and LRM, as  their 1000-step PGD performance on it is about 1.5% worse than on the whole test set, while PGD is only 0.15% worse, so it seems likely that both are actually slightly better than represented here. We will add these experiments and discussion to the next revision. Thank you for your suggestion!</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Sye38MYpnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Evaluation with many random restarts</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bylj6oC5K7&amp;noteId=Sye38MYpnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Marius_Mosbach1" class="profile-link">Marius Mosbach</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018</span><span class="item">ICLR 2019 Conference Paper833 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Another suggestion would be to try PGD attack with many random restarts (keeping a large number of iterations). As we find in our paper <a href="https://arxiv.org/abs/1810.12042" target="_blank" rel="nofollow">https://arxiv.org/abs/1810.12042</a> this can again lead to a drop in the accuracy of both ALP models and models trained with plain adversarial training. 

Since you already find a difference in the robustness of your model when evaluated against SPSA compared to when evaluated against PGD, we would expect to see further drops in accuracy here when evaluating for many random restarts.  This might change the relative ranking between the three methods (PGD training, ALP, LRM).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_BkxdXoXmsX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Confusing about the Eq. 10 and the final loss function</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bylj6oC5K7&amp;noteId=BkxdXoXmsX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Oct 2018</span><span class="item">ICLR 2019 Conference Paper833 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, it seems to be an error in Eq. 10:

Should the "\left \|  f( g(x^{(i)}) \right \|_2" be modified to "\left \|  g(x^{(i)}) \right \|_2"? 

In addition, the formula "h(f(x(i)), f(g(x(i))))" has an extra closing parenthesis. 

One more thing, what is the final loss function for LRM in training ?
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1x7Lv4ViQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Eq. 10 clarification</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bylj6oC5K7&amp;noteId=B1x7Lv4ViQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper833 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Oct 2018</span><span class="item">ICLR 2019 Conference Paper833 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your interest in our work! Eq. 10 is actually correct as written, up to one missing closing parenthesis in the last term -- the regularization term \left \|  f( g(x^{(i)})) \right \|_2 provides regularization on the logits f() of the adversarial example g(x^{(i)}).

The final loss function for LRM is Eq. 10 combined with the regular adversarial training loss (label smoothing and paired-example data augmentation can optionally be included in the loss or alternatively considered as preprocessing steps).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_r1l29CdbqQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Logit pairing is not robust</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bylj6oC5K7&amp;noteId=r1l29CdbqQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper833 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I wonder if the authors have seen this work: <a href="https://arxiv.org/abs/1807.10272" target="_blank" rel="nofollow">https://arxiv.org/abs/1807.10272</a>  which shows that logit pairing does not lead to any increased robustness of the model to adversarial attacks.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJxDFbaOsm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>That's not quite a correct summary of the paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bylj6oC5K7&amp;noteId=SJxDFbaOsm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Oct 2018</span><span class="item">ICLR 2019 Conference Paper833 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">While there are problems with the ALP evaluation, your summary of the linked paper isn't quite correct.

The linked paper evaluated a low-cost version of the ALP model that was prepared separately for a contest. The model was intended to run fast to make it easier for contestants to run baseline experiments before starting to work on their on submissions.

There's never been a real ALP code release and I don't know of a serious third party re-implementation so no one really knows.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJeBTYR_im" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Incorrect critique</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bylj6oC5K7&amp;noteId=BJeBTYR_im"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">21 Oct 2018</span><span class="item">ICLR 2019 Conference Paper833 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Your statement here:

&gt; The linked paper evaluated a low-cost version of the ALP model that was prepared separately for a contest. The model was intended to run fast to make it easier for contestants to run baseline experiments before starting to work on their on submissions.

Does not make sense as an explanation of the lack of robustness of the model; evaluation of pre-trained models (which is what the authors of the linked paper use in their evaluation) is always fast.

Given that the ALP authors have not released a truly robust model corroborating the claims in their paper, we can assume that the claims made in "Evaluating and Understanding the Robustness of Adversarial Logit Pairing" are true. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJe2GIgmjQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>ImageNet vs CIFAR-10</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bylj6oC5K7&amp;noteId=rJe2GIgmjQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper833 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Oct 2018</span><span class="item">ICLR 2019 Conference Paper833 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have seen that paper, which focuses on targeted white-box attacks on ImageNet. However, in our experiments on CIFAR-10 we have seen clear and reproducible benefits of logit pairing, one of the several methods we study.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJgDzgn0sQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Breaking ImageNet is enough</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bylj6oC5K7&amp;noteId=BJgDzgn0sQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018</span><span class="item">ICLR 2019 Conference Paper833 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Considering logit pairing is built upon adversarial training, it is enough to invalidate it on ImageNet. The advantage of logit pairing is it can be extended to ImageNet compared to adversarial training. Adversarial training solely is good enough for CIFAR-10. 

Therefore, the benefits of logit pairing on CIFAR-10 are questionable, especially if you do not evaluate it against the attack from <a href="https://arxiv.org/abs/1807.10272" target="_blank" rel="nofollow">https://arxiv.org/abs/1807.10272</a> </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJge1jKsh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>ImageNet is quite special for adversarial robustness</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bylj6oC5K7&amp;noteId=HJge1jKsh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Maksym_Andriushchenko1" class="profile-link">Maksym Andriushchenko</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018</span><span class="item">ICLR 2019 Conference Paper833 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I would disagree with the statement that results on ImageNet are enough. Three arguments:

1. ImageNet is quite special for adversarial robustness. 
As <a href="https://arxiv.org/abs/1802.00420" target="_blank" rel="nofollow">https://arxiv.org/abs/1802.00420</a> point out, some classes are too close to each other, and thus evaluation with an untargeted attack is not very meaningful. That's why they recommend applying an untargeted attack (e.g. with a random target class). So we can see that on ImageNet evaluation scheme is already different from other datasets, and thus final conclusion regarding robustness might also differ.

2. Moreover, this also makes unclear which attack (targeted or untargeted) to use for adversarial training.
The use of a targeted attack for adv. training is criticized in https://arxiv.org/abs/1807.10272 :
"a defense that is only robust to targeted attacks is weaker than one robust to untargeted attacks"
However, the use of an untargeted attack might be problematic because there are too close classes. Therefore, this can also influence final conclusions.

3. Finally, drawing conclusions based on just one dataset (despite the most challenging) may be unreliable. That's why people tend to evaluate a new method on 2-3 datasets.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SyemC9bu27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: Breaking ImageNet is enough</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bylj6oC5K7&amp;noteId=SyemC9bu27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper833 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018</span><span class="item">ICLR 2019 Conference Paper833 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The attack used in <a href="https://arxiv.org/abs/1807.10272" target="_blank" rel="nofollow">https://arxiv.org/abs/1807.10272</a> is a PGD attack with up to 1,000 iterations. We did an informal evaluation of up to 200 PGD iterations in our paper (see Sec. 5.3), and furthered that to 1,000 iterations with a formal analysis elsewhere in the discussion on this page. As is common practice for CIFAR-10 (e.g. agreeing with https://arxiv.org/abs/1802.00420), we focused on untargeted adversarial examples, which is also noted in the linked paper as harder to defend against.

We respectfully disagree that adversarial training solely is "good enough" for CIFAR-10, since even the most robust models are only roughly 50% accurate for a relatively limited range of adversarial examples. Whether ALP works on ImageNet or not is out of scope for this paper and discussion, and we merely note that the question doesn't yet appear to be completely settled one way or the other -- in the linked paper, the authors note that "unreleased models were used to generate the results [from the ALP paper]" and that "the authors are currently investigating these models".</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_S1xuvSP-qQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Possible contradiction with another submission?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bylj6oC5K7&amp;noteId=S1xuvSP-qQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper833 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">This paper appears to contradict another submission: <a href="https://openreview.net/pdf?id=BJlr0j0ctX" target="_blank" rel="nofollow">https://openreview.net/pdf?id=BJlr0j0ctX</a>

This paper states "Very surprisingly, using only label smoothing can result in a model that is nearly as robust as models trained with PGD-based adversarial training" whereas the other paper states "Our experimental results suggest that simple regularizers [including label smoothing] can hurt adversarial robustness".

Is there some subtle difference between these two papers' experiments that explains this difference?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJx_c9eXoQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Some differences</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bylj6oC5K7&amp;noteId=SJx_c9eXoQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper833 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Oct 2018</span><span class="item">ICLR 2019 Conference Paper833 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comment, the referenced work is interesting and very related to portions of ours. Besides the difference the other commenter notes (which applies to their black-box evaluations), there are a few other differences:

-In our work, we also used standard forms of data augmentation (noted in Sec. 5.1). It's not clear if this was also used in their work, but if not, that could explain some of the difference, serving to replace some of the gaussian data augmentation they used.
-Most of their evidence for the claim that simple regularizers hurt robustness comes from MNIST, while we focus on CIFAR-10.
-Some of their analysis (e.g. Figures 3 and 7) uses a linear approximation (from Sec. 1.1). This may actually be a fine assumption -- we note it only as a difference.
-In Table 2 of their work, increasing the amount of label smoothing using a fixed amount of gaussian data augmentation (from alpha = 0.2 to alpha = 0.95, with gaussian augmentation of sigma = 0.3) actually does increase robustness slightly. In our work, we compare using only label smoothing to the vanilla setting of no label smoothing or gaussian augmentation, where we saw benefits.
-On CIFAR-10, they used the 'wide' ResNet model from Madry et al., while we used the smaller ResNet model (they differ by the number of filters in some of the layers). Anecdotally, we've seen that using a larger model can actually hurt adversarial robustness while only using label smoothing (label smoothing still helps, just to a lesser extent), and we'll add a more rigorous analysis of this to Section 5.3 to add to our other label smoothing investigations.

All that said, label smoothing does appear to have some counterintuitive properties (see e.g. Section 5.3 in our paper), so it's quite possible that these differences alone account for the differing conclusions.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Sklhj_dZ9Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Different source models</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bylj6oC5K7&amp;noteId=Sklhj_dZ9Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Oct 2018</span><span class="item">ICLR 2019 Conference Paper833 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I think the difference came from the source models used to generate adversarial examples. 

The paper you mentioned, <a href="https://openreview.net/pdf?id=BJlr0j0ctX," target="_blank" rel="nofollow">https://openreview.net/pdf?id=BJlr0j0ctX,</a> uses a naturally trained model as their source model while the target model is regularized. 

However, the author of this paper said " As is standard in most black-box evaluations of adversarial defenses, this is performed by generating adversarial examples with one model (the “Source”) and evaluating them on a separate independently trained model (the “Target”).". 

This is a more proper setting to evaluate robustness on black-box attacks.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>