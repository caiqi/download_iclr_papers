<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Bamboo: Ball-Shape Data Augmentation Against Adversarial Attacks from All Directions | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Bamboo: Ball-Shape Data Augmentation Against Adversarial Attacks from All Directions" />
        <meta name="citation_author" content="Huanrui Yang" />
        <meta name="citation_author" content="Jingchi Zhang" />
        <meta name="citation_author" content="Hsin-Pai Cheng" />
        <meta name="citation_author" content="Wenhan Wang" />
        <meta name="citation_author" content="Yiran Chen" />
        <meta name="citation_author" content="Hai Li" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1zW13R5tm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Bamboo: Ball-Shape Data Augmentation Against Adversarial Attacks..." />
      <meta name="og:description" content="Deep neural networks (DNNs) are widely adopted in real-world cognitive applications because of their high accuracy. The robustness of DNN models, however, has been recently challenged by..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1zW13R5tm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Bamboo: Ball-Shape Data Augmentation Against Adversarial Attacks from All Directions</a> <a class="note_content_pdf" href="/pdf?id=H1zW13R5tm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=huanrui.yang%40duke.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="huanrui.yang@duke.edu">Huanrui Yang</a>, <a href="/profile?email=jingchi.zhang%40duke.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="jingchi.zhang@duke.edu">Jingchi Zhang</a>, <a href="/profile?email=hc218%40duke.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="hc218@duke.edu">Hsin-Pai Cheng</a>, <a href="/profile?email=wenhanw%40microsoft.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="wenhanw@microsoft.com">Wenhan Wang</a>, <a href="/profile?email=yiran.chen%40duke.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="yiran.chen@duke.edu">Yiran Chen</a>, <a href="/profile?email=hai.li%40duke.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="hai.li@duke.edu">Hai Li</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Deep neural networks (DNNs) are widely adopted in real-world cognitive applications because of their high accuracy. The robustness of DNN models, however, has been recently challenged by adversarial attacks where small disturbance on input samples may result in misclassification. State-of-the-art defending algorithms, such as adversarial training or robust optimization, improve DNNs' resilience to adversarial attacks by paying high computational costs. Moreover, these approaches are usually designed to defend one or a few known attacking techniques only. The effectiveness to defend other types of attacking methods, especially those that have not yet been discovered or explored, cannot be guaranteed. This work aims for a general approach of enhancing the robustness of DNN models under adversarial attacks. In particular, we propose Bamboo -- the first data augmentation method designed for improving the general robustness of DNN without any hypothesis on the attacking algorithms. Bamboo augments the training data set with a small amount of data uniformly sampled on a fixed radius ball around each training data and hence, effectively increase the distance between natural data points and decision boundary. Our experiments show that Bamboo substantially improve the general robustness against arbitrary types of attacks and noises, achieving better results comparing to previous adversarial training methods, robust optimization methods and other data augmentation methods with the same amount of data points.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">DNN robustness, Adversarial attack, Data augmentation</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">The first data augmentation method specially designed for improving the general robustness of DNN without any hypothesis on the attacking algorithms.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Hyg6kb7_am" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Paper withdrawn before review was ready</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1zW13R5tm&amp;noteId=Hyg6kb7_am"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper962 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper962 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">As the paper was withdrawn, I am not proceeding with the review.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJeRhkn1T7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1zW13R5tm&amp;noteId=rJeRhkn1T7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper962 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper962 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJl6O3eY3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>random sphere sampling for data augmentation for training robust model against adversarial attacks - limited contribution and insufficient analysis</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1zW13R5tm&amp;noteId=SJl6O3eY3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper962 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper962 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposed a data augmentation training method to gain model robustness against adversarial perturbations, by augmenting uniformly random samples from a fixed-radius sphere centered at training data. Comparing to adversarial training methods that trained on specific attacks, the authors claim better universal robustness across different kinds of attacks, especially to Gaussian perturbations. 

However, this paper has severe flaws in terms of analysis and limited contributions. The details are summarized as follows.

1. Uniform/Gaussian data augmentation is known to have limited benefit to adversarial robustness - Uniform/Gaussian perturbed data augmentation has been a standard way to improve model robustness to input perturbations (not necessarily adversarial) in the computer vision literature. Recently, papers have shown that Gaussian data augmentation alone provided limited improvement for model robustness. For example, see the analysis of <a href="https://arxiv.org/abs/1707.06728" target="_blank" rel="nofollow">https://arxiv.org/abs/1707.06728</a> , https://arxiv.org/abs/1711.08478 and reference therein. The experimental results presented in this paper basically agrees with this trend - the proposed method is more effective against Gaussian perturbations (which is, strictly speaking, NOT a strong adversarial attack), while showing weak defense performance against PGD attack. Therefore, the contribution of this work is quite limited.

2. Why using the distortion of a particular attack to measure robustness? Throughout the entire paper, the authors use the L2 distortion from untargeted CW attack as a robustness metric. However, the motivation of this paper is towards an attack-agnostic data augmentation for robust model training. Therefore, the use of a distortion metric from a particular attack could be very biased, and it does not fully support the motivation of this work. The authors are suggested to use some attack-independent robustness metrics for making robustness claims, or using some certified robustness verification tools (such as https://arxiv.org/pdf/1711.00851.pdf) for justification. 

3. Insufficient analysis - The analysis and experiments in this paper are not sufficient to provide convincing results. For example, in Table 1, why the authors only show the distortion of CW attack but not their defense rates as the row of PGD. In Table 2, why did the authors only show the results of FGSM (a known weak attack)? What about PGD and CW attacks? Lastly, it will be more convincing if the authors directly show two plots to compare different robust models: (1) attack performance (success rate) vs attack strength;  (2) distortion of successful attack vs attack strength, where attacks should at least include PGD and CW. These plots will evidently justify the robustness claims. The authors are also encouraged to follow the comments of testing model robustness in the Discussion Section of https://arxiv.org/abs/1802.00420 for robustness validation.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Bye4ULm0sm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Table 1 questions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1zW13R5tm&amp;noteId=Bye4ULm0sm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018</span><span class="item">ICLR 2019 Conference Paper962 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">In Table 1 for MNIST, the CW distortion goes *down* when going from an undefended model to a PGD defended model. This seems incorrect. Can you explain what is going on here? Madry et al. claim 90% accuracy against PGD eps=0.3 but you claim only 25% accuracy. Are you using their code?

For CIFAR, the CW distance for an undefended model is 38.0 and the PGD defended model is 38.1. This appears incorrect: PGD adversarial training should increase distortion by more than just 0.2%. The header says e=0.3, N=50, however Madry et al. suggest using e=0.03, is this a typo?

Similarly, on CIFAR, your model only increases mean CW distortion by 1.8%, which seems small. Is this right?</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJx8sE7CsX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Comparison to past work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1zW13R5tm&amp;noteId=BJx8sE7CsX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018</span><span class="item">ICLR 2019 Conference Paper962 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Could you compare the approach in this paper to ( <a href="https://arxiv.org/abs/1707.06728" target="_blank" rel="nofollow">https://arxiv.org/abs/1707.06728</a> ), in particular 3.2 which proposes Gaussian Data Augmentation which appears similar to what you are doing?

This is especially relevant because this paper was shown to be ineffective by ( https://arxiv.org/abs/1711.08478 ).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Syg2dX7Ro7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>More attacks on ImageNet</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1zW13R5tm&amp;noteId=Syg2dX7Ro7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018</span><span class="item">ICLR 2019 Conference Paper962 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">This paper only applies FGSM to ImageNet citing a lack of attacks that work on ImageNet. CleverHans (<a href="https://github.com/tensorflow/cleverhans)" target="_blank" rel="nofollow">https://github.com/tensorflow/cleverhans)</a> and FoolBox (https://github.com/bethgelab/foolbox) both work with ImageNet.

FGSM is known to be a very weak attack (and even still the proposed defense only shows a +3% gain on robustness). Please consider applying stronger attacks using one of these libraries.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>