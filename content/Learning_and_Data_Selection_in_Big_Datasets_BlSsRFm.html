<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learning and Data Selection in Big Datasets | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learning and Data Selection in Big Datasets" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1l8SsR9Fm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learning and Data Selection in Big Datasets" />
      <meta name="og:description" content="Finding a dataset of minimal cardinality to characterize the optimal parameters of a model is of paramount importance in machine learning and distributed optimization over a network. This paper..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1l8SsR9Fm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning and Data Selection in Big Datasets</a> <a class="note_content_pdf" href="/pdf?id=B1l8SsR9Fm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019learning,    &#10;title={Learning and Data Selection in Big Datasets},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=B1l8SsR9Fm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Finding a dataset of minimal cardinality to characterize the optimal parameters of a model is of paramount importance in machine learning and distributed optimization over a network. This paper investigates the compressibility of large datasets. More specifically, we propose a framework that jointly learns the input-output mapping as well as the most representative samples of the dataset (sufficient dataset). Our analytical results show that the cardinality of the sufficient dataset increases sub-linearly with respect to the original dataset size. Numerical evaluations of real datasets reveal a large compressibility, up to 95%, without a noticeable drop in the learnability performance, measured by the generalization error.
</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Data selection, non-convex optimization, learning theory, active learning</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rygHpbqqh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Unclear if the approach will give practical benefits</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1l8SsR9Fm&amp;noteId=rygHpbqqh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper88 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper88 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
This paper proposes an algorithm for data compression which learns
the most representative samples of a dataset by solving a mixed
integer program. The paper then finds a stationary point of the
program by a greedy method. Empirical results show that compressing
a large fraction of samples using the method does not cause a
noticeable drop in learning performance.

The algorithm and theory in this paper are clearly presented. The
idea however is not quite novel. The impact of reducing samples on
learning problems have been studied in literature, e.g. [1][2][3].
Algorithms like sketching [2] and importance sampling [3] have been
shown both in theory and practice that can reduce dataset sample
size without a big drop in performance. Besides, both [2][3]
requires less computation time than solving the original problem.
However, the proposed algorithm takes more computation than
minimizing the loss.

Some applications of the proposed algorithm are presented in
appendix B. However, it would be better to also show the learning
or optimization problems in these applications since they can help
us understand how to apply the proposed algorithm in practice. Some
experiments on these applications are also preferred.

For experiments, both synthetic data and large-scale real data are
tested. However, it would be better comparing with other algorithms
like [1][2][3] to show the novelty of the proposed algorithms. In
section 4.2, the synthetic data experiments are conducted using
only one constructed dataset and multiple random noise. Generating
a random dataset each time will show that the performance of the
algorithm is not a special case for some specific datasets and will
also give insights on the robustness of the algorithm. The proposed
method can be applied to both regression and classification
problems. Some experiments on classification would help better
exploring this algorithm. The experiments on reduced sample size
and learning performance are comprehensive. However, I would like
to see experiments on running time, since running time represents
the key performance metric in real world applications.

Some questions: In Section 2.4: Please explain what robust learning
in your case is. In your experiments, how you choose your parameter
epsilon? What is the average loss in your experiments? Average loss
is more commonly used in practice. In your experiments, what if you
pick these selected samples and train them? Data compression is
also well studied in learning theory [4] (Part IV, Section 30). It
is also interesting to know if the proposed algorithm can reach the
theoretical bound.

[1] Understanding Black-box Predictions via Influence Functions
[2] sketching as a tool for numerical linear algebra
[3] Randomized algorithms for matrices and data
[4] understanding machine learning from theory to algorithms</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Skl3TwLU2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Dataset selection and compression: a work done without much motivation, in isolation, without an interesting theoretical conclusion.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1l8SsR9Fm&amp;noteId=Skl3TwLU2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper88 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper88 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper addresses the problem of selecting a subset of a labeled dataset that retains the performance of training on the full dataset. The techniques presented in the paper are able to select just 5% of the dataset with only a small loss in error.

The paper has several severe issues with it: motivation, related work, and theoretical novelty.

First of all, the problem setting is somewhat strange. In active learning, it is natural to select a subset of the dataset to label, for when the lableling cost is high. When there is too much labeled data, it is natural to select a subset for computational reasons (e.g. core-set selection). However, this work doesn't save on labeling cost or on computational costs. Instead, the paper claims the application is distributed learning where communication costs are expensive (and presumably where computation is not limited on the agents, which is unrealistic in some of the applications the paper mentions). However, the paper doesn't really address this problem in the experiments, algorithm, or theory they just mention it off-hand as an application. Why restrict the communication protocol to datasets rather than say, model parameters? How would "sufficient datasets" be combined and how would this change things?

Perhaps an even larger issue is the lack of comparison to other methods. This paper doesn't have a related work section or compare to any baselines (not even random sampling). As examples, core-set selection and machine teaching are very similar setups, but these aren't mentioned anywhere. This glaringly shows up in the experiments section when only results for the algorithm presented are shown.

Finally, from a theoretical perspective, the conceptual/theoretical novelty that datasets can be compressed sub-linearly for a given model is not really that interesting. Intuitively, once a model class has seen enough data, it won't be able to learn much more from further data. From theory, we expect that for a given model trained on a given dataset, there will be approximation error and estimation error. The approximation error is fixed (for the model family) while the estimation error typically goes down as 1/n or 1/sqrt{n}. For such large datasets and low-dimensional data, I do not find it very surprising that there is only a small decrease in estimation error. In fact, I wouldn't be suprised to see the same phenomenon with randomly sampled data on some of the datasets (probably not as good as their method, but qualitatively similar). Unfortunately, the paper does not compare to a random sampling baseline. 

As another point, for their "novel problem" (2a)-(2c), I think (2c) doesn't make much sense. Why require the dataset to be of a certain size if the generalization error is already required to be low? In fact, this constraint is in the opposite direction of what we want, I could see trying to minimize the size of the dataset. I suspect that this constraint is only to make the optimization technique presented in this paper work.

In summary, this paper solves an unmotivated problem, in isolation, with a theoretically uninteresting conclusion.

Small points:
f: X |-&gt; Y should be f: X -&gt; Y. 
The natural numbers (\mathbb{N}) are already positive so the + is unnecessary. Did you mean \mathbb{Z}^+?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hyg38QxOo7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1l8SsR9Fm&amp;noteId=Hyg38QxOo7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper88 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper88 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper is written reasonably clearly, and appears to be original.

This paper proposes a coupled optimization for simultaneously compressing a dataset (picking an informative subset) and learning. My major concern is that the problem itself is not formulated (nor solved) properly.

This is most easily uncovered by looking at the solution of the optimization given an oracle that provides h*. The algorithm would then pick z* by simply choosing the K lowest objective function values (indeed this is one of the block-coordinate steps). These are simply the data that the single candidate function h* fits best, and they may be very close / redundant with one another. There is no step in the procedure that forces the subset to capture the learning problem itself.

Further, each "F-step" requires solving a feasibility problem that is just as hard as solving the original learning problem on the full dataset. Even if I set aside that concern, there is still the matter of the arbitrary threshold epsilon, which the paper provides no guidance on. Indeed the F-step may not even be feasible if epsilon is set incorrectly. The authors incorrectly state the solution to the F-step for linear regression (for the same reason).

I am also not certain the main result (proposition 4 / corollary 1) provides any useful theoretical result. In particular, for K = 1 (given an appropriate choice of epsilon) it's not hard to pick an h* such that g(h*, z*) = 0 for all N. It's also very odd that the proposition requires a lower bound on K, when increasing K just increases the optimal objective g(h*, z*) -- having a lower bound seems to suggest increasing K should result in a better objective g. The proposition also provides no constraint / assumption on epsilon, which is also odd.

Finally, the paper does not compare its method to any other compression scheme -- even random subsampling. The experimental results on compression are meaningless without a baseline to say how easily compressed these datasets are.

In addition to the above major concerns, the paper does not cite any papers from the vast body of literature on coresets and model compression.


</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>