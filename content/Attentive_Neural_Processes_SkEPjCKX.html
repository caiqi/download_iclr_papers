<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Attentive Neural Processes | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Attentive Neural Processes" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SkE6PjC9KX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Attentive Neural Processes" />
      <meta name="og:description" content="Neural Processes (NPs) (Garnelo et al., 2018) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions. Each function models..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SkE6PjC9KX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Attentive Neural Processes</a> <a class="note_content_pdf" href="/pdf?id=SkE6PjC9KX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 14 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019attentive,    &#10;title={Attentive Neural Processes},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SkE6PjC9KX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=SkE6PjC9KX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Neural Processes (NPs) (Garnelo et al., 2018) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions. Each function models the distribution of the output given an input, conditioned on the context. NPs have the benefit of fitting observed data efficiently with linear complexity in the number of context input-output pairs, and can learn a wide family of conditional distributions; they learn predictive distributions conditioned on context sets of arbitrary size. Nonetheless, we show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on. We address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction. We show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Neural Processes, Conditional Neural Processes, Stochastic Processes, Regression, Attention</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A model for regression that learns conditional distributions of a stochastic process, by incorporating attention into Neural Processes.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SylBJkja3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper proposes to resolve this issue by adding an attention mechanism to the deterministic path.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkE6PjC9KX&amp;noteId=SylBJkja3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper308 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper308 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Neural process (NP) is a recent probablistic method for modeling distributions of functions. The authors claim that one substantial weakness of NP is the tendency of under-fitting. The authors give a hypoethesize: the under-fitting behaviour of NP is because the mean-aggregation step in the encoder acts as a bottleneck, as a result, it is difficult for the decoder to learn the relevant information for a give target prediction. This paper proposes to resolve this issue by adding an attention mechanism to the deterministic path. The experimental results show that the proposed method converge faster and give better results on various tasks.

One major concern about the paper is the lack of analysis of the true cause of under-fitting in NP. The authors give the hypoethesize about the potential cause of the under-fitting issue and proposes to resolve it with attention, however, without theoretical or empirical analyses, it is hard to understand the true cause of the under-fitting issue. Although the proposed method give better performance, it is not clear whether the better performance is due to the added complexity to the model (the attention mechanism) or truely resolving the under-fitting issue. Some analyses along this line can make the paper clearer and more convincing.

A lot of technical details are missing in the paper, which makes the method not reproducible. Please add more details about the proposed attention mechanism and how they are implemented into NP.

In the GP literature, there are also methods tackling meta-learning or multi-task learning or few shot learning. These works are known as multi-output / multi-tasks Gaussian processes. A few works on this topic are listed:
* Z Dai, MA Álvarez, ND Lawrence, Efficient Modeling of Latent Information in Supervised Learning using Gaussian Processes, NIPS 2017
* MA Alvarez, L Rosasco, ND Lawrence, Kernels for vector-valued functions: A review, Foundations and Trends® in Machine Learning 2012
* EV Bonilla, KM Chai, C Williams, Multi-task Gaussian process prediction, NIPS 2007

For the 1D regression experiments (Figure 1left, Figure 3right), it is not clear which fitting is better. It largely depends on the prior of kerel parameters. As the data points are generated from a GP, plotting the Gaussian process fit with the ground truth parameters can show what a ground truth fitting would look like.

The Bayesian optimization experiment is very nice and gives some good insights about the quality of the uncertainty of prediction. Maybe consider it to include it in the main text.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hylne3oKaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkE6PjC9KX&amp;noteId=Hylne3oKaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper308 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper308 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank you for your constructive criticism of the paper. Here are our responses:

“The authors give the hypoethesize about the potential cause of the under-fitting issue and proposes to resolve it with attention…. Although the proposed method give better performance, it is not clear whether the better performance is due to the added complexity to the model (the attention mechanism) or truely resolving the under-fitting issue”

Our reasoning that the mean-aggregation acts as a bottleneck is as follows. In NPs, the aggregated representations r_C and s_C needs to have information about the particular function (the realisation of the stochastic process) not only at the context x-values but for all x-values in the domain for the predictions to generalise well to unseen targets. When we compute these aggregated representations simply by taking the mean of representations of each context pair, it will be difficult for the resulting representation to model the entire function, and may require a large representation dimensionality. On the other hand, cross-attention allows the aggregated representation r* to be specific to the target x-value x*, so that it suffices for the representation r* to only be informative about the prediction at x* instead of all x-values in the domain. We claim that this inductive bias allows the ANP to resolve the undefitting issue, not just because of the added model capacity (i.e. more parameters) that attention introduces. As evidence for this, on the left of Figure 3 we show quantitative results for differing sizes of the bottleneck in NPs, ranging from d=128 to 1024. The figure shows that raising d (i.e. increasing model capacity) does help achieve better reconstructions, but there seems to be a limit in how much the reconstructions can improve, hence showing that naively adding capacity to the model is insufficient in addressing the underfitting behaviour. We show that multihead (and dot-product) attention with d=128 gives a much faster decrease in the reconstructions than NPs with any value of d, both against iteration and wall-clock time. We would also like to point out that the number of parameters in the NP model for bottleneck size d is approximately 10d^2 (ignoring smaller order terms), whereas for multihead it is 15d^2 (so cross-attention introduces 5d^2 extra parameters). So the number of parameters of NP with d=1024 is much higher than that of multihead with d=128, yet we can get noticeably better performance with multihead attention. Hence the improvement in performance is mostly due to the attention mechanism rather than the increase in model complexity (i.e. number of parameters).


“A lot of technical details are missing in the paper, which makes the method not reproducible.”

In addition to the experimental and model architecture details in Appendix A, we have included Figure 8 detailing the precise architecture for the different models. We believe this is sufficient to reproduce the method, but if you think anything is missing please do let us know and we revise the experimental details accordingly.


“In the GP literature, there are also methods tackling meta-learning or multi-task learning or few shot learning.”

Thank you for pointing out these references. We agree that these are relevant work and have added them to the related work section of the paper.


“For the 1D regression experiments (Figure 1left, Figure 3right), it is not clear which fitting is better. It largely depends on the prior of kerel parameters. As the data points are generated from a GP, plotting the Gaussian process fit with the ground truth parameters can show what a ground truth fitting would look like.”

In the revised version of the paper, we have included Figure 9 showing the predictive mean and variance of the oracle GP from which the contexts were drawn for comparison in Appendix C. Note that the data was generated from a GP with small observation noise (sigma_n = 0.02), so we do want the predictive mean to pass through the contexts with predictive variance small near the context points and large away from them. It is clear that the Multihead ANP is much closer to the ground truth than the NP with very accurate predictive mean, despite there being evidence of underestimating the variance away from the contexts - one possible explanation for this is that variational inference usually leads to underestimates of predictive variance.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1lKdp2qnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The authors propose an extension to the recently established framework of Neural Processes by adding an attention-based conditioning mechanism which allows the model to better capture dependencies in its conditioning set. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkE6PjC9KX&amp;noteId=S1lKdp2qnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper308 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper308 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper is a joy to review, as it is clearly written and has a crisp idea that the authors try to motivate consistently.
It extends the framework of neural processes and conditional neural processes by an incremental seeming idea: self attention on the conditioning set and cross attention. What this means in practice is that the model is able to learn a more detailed and structured 'kernel' between query and past data which allows it to identify and model conditional structure better.

The authors try three main prongs of such attention mechanisms with the multi-head attention appearing to be the most successful one in the experiments.

Regarding experiments, the authors show a 1d function gitting example and various conditional image generation ones, similar to the original examples in the paper. While I find the function fitting exampole quite unconvin cing, it arguably also contains less interesting structure for the model to pick up.
In the image generation examples both he quantitative and the qualitative illustrations appear to indicate that a very rich conditioning apparatus (stacked multi head attention) manages to give the model more detailed generative abilities.
While introducing all this machinery seems a bit over-engineered at times, the results do show a benefit. 

Overall I find the exposition of the effects of the attention mechanism very well executed and the paper clearly positioned and written. My main complaint would be the incremental nature of the work, as the contributions here are not as significant advances as some preceding ideas that have gone into this work, but still steadily improve on the vision of NP and appear to be necessary steps to push the model forward giving this work validity on its own.
The authors discuss a similar mechanism for generation, which while more involved would be a very exciting change from the current framework.  I would have enjoyed seeing more of that in this paper to discuss input and output attention jointly.


</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HklNVhjK67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkE6PjC9KX&amp;noteId=HklNVhjK67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper308 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper308 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank you for your positive review and thoughtful comments. Here are our responses:

“My main complaint would be the incremental nature of the work … but still steadily improve on the vision of NP and appear to be necessary steps to push the model forward giving this work validity on its own.”

Regarding the nature of the work, we show that attention addresses underfitting, which is a fundamental drawback of NPs, so this simple change makes a large difference. We show empirically that attention leads to large improvements in training time, expressiveness of the model, and solves the underfitting issue so that ANPs can be used reliably for tasks such as mapping images from one resolution to another. Hence as you pointed out, we believe that ANPs are a notable improvement to NPs and is valid work in its own right.


“The authors discuss a similar mechanism for generation, which while more involved would be a very exciting change from the current framework.”

We gladly agree that the incorporation of self-attention in the decoder is an interesting direction for future research. We are currently investigating this avenue as future work.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkgOpjMKnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Official review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkE6PjC9KX&amp;noteId=rkgOpjMKnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper308 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper308 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
The authors extend neural processes by incorporating two types of attention processes: self-attention for enriching the features of the context points and cross-attention for producing a query-specific representation. By replacing MLPs and mean pooling with these attention processes, the authors resolve the underfitting problem of NPs. The experimental results show that ANPs converge better and faster than NPs.

Overall, I had fun to read the paper and have not much to complain. Below are some comments and questions.

1. It is intuitive and reasonable that the cross-attention process makes ANPs fit with smaller predictive uncertainty for those regions with many context points. This is well illustrated in the qualitative results in the experiment section.

2. I would like to see an ablation study with the two separate techniques (self- and cross-attention processes) on NPs since the two techniques aim to improve different aspects of NPs. More specifically, I wonder the results of just adding cross-attention with the vanilla MLPs for feature encoding and just replacing the MLPs with self-attention modules while keeping using mean pooling.

3. While the dot product improves the performance significantly, the gain of Laplace is much lower. Also, qualitatively it fails to overcome the underfitting problem. Do you have any intuition about why performs worse than other models?

4. Do you have any specific application in mind? I just wonder some example tasks where contexts are given as inputs.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1x1unoFTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkE6PjC9KX&amp;noteId=S1x1unoFTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper308 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper308 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank you for your thoughtful review with constructive criticism. Here are our responses to your suggestions and questions.

“I would like to see an ablation study with the two separate techniques (self- and cross-attention processes) on NPs”

Regarding the ablation study for using just cross-attention, all the ANP results for 1D GP regression use cross-attention but not self-attention (i.e. have MLP encodings of each context pair). Hence the improvements here show the contribution of cross-attention only. Figure 4b also shows quantitative results for the CelebA image regression with only cross-attention, denoted Multihead ANP in the legend. The counterpart results for MNIST can be found in Figure 13b of Appendix E of the revised version of the paper. Qualitative comparisons of the NP, Multihead ANP and Stacked Multihead ANP can be found in Figures 14 and 15. It would indeed be helpful to show results for the case when we just have self-attention in the encoder without any cross-attention. We are currently running these experiments and will include the results in the next version of the paper.


“While the dot product improves the performance significantly, the gain of Laplace is much lower… Do you have any intuition about why performs worse than other models?”

Laplace attention is parameter-free (so keys and queries are just x-values) whereas for dot-product attention, the keys and queries are parameterised representations of the x-values (output of learned MLP that takes x-values as inputs). So the dot-product similarities are computed in a learned representation space, whereas for Laplace the similarities are computed based on L1 distance in the x-space. Hence it is expected that dot-product attention will be able to fit the contexts better than Laplace. We have made this clear in the revised version of the paper.


“Do you have any specific application in mind?”

Bayesian Optimisation (BO) is one notable application of (A)NPs since the predictive mean and uncertainty can be used for finding the minimum of a test function drawn from a stochastic process whose realisations can be used to train the (A)NP (toy experiment results shown in Appendix C, referred to in the last paragraph of the section on 1D experiments in main text). The image data experiments also show promise for applying ANPs to arbitrary pixel inpainting, bottom half prediction and mapping images between arbitrary resolutions.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>