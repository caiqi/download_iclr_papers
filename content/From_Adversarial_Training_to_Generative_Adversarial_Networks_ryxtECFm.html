<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>From Adversarial Training to Generative Adversarial Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="From Adversarial Training to Generative Adversarial Networks" />
        <meta name="citation_author" content="Xuanqing Liu" />
        <meta name="citation_author" content="Cho-Jui Hsieh" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ryxtE3C5Fm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="From Adversarial Training to Generative Adversarial Networks" />
      <meta name="og:description" content="In this paper, we are interested in two seemingly different concepts: \textit{adversarial training} and \textit{generative adversarial networks (GANs)}. Particularly, how these techniques work to..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ryxtE3C5Fm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>From Adversarial Training to Generative Adversarial Networks</a> <a class="note_content_pdf" href="/pdf?id=ryxtE3C5Fm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=xqliu%40cs.ucla.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="xqliu@cs.ucla.edu">Xuanqing Liu</a>, <a href="/profile?email=chohsieh%40cs.ucla.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="chohsieh@cs.ucla.edu">Cho-Jui Hsieh</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">In this paper, we are interested in two seemingly different concepts: \textit{adversarial training} and \textit{generative adversarial networks (GANs)}. Particularly, how these techniques work to improve each other. To this end, we analyze the limitation of adversarial training as a defense method, starting from questioning how well the robustness of a model can generalize. Then, we successfully improve the generalizability via data augmentation by the ``fake'' images sampled from generative adversarial network. After that, we are surprised to see that the resulting robust classifier leads to a better generator, for free. We intuitively explain this interesting phenomenon and leave the theoretical analysis for future work.
Motivated by these observations, we propose a system that combines generator, discriminator, and adversarial attacker together in a single network. After end-to-end training and fine tuning, our method can simultaneously improve the robustness of classifiers, measured by accuracy under strong adversarial attacks, and the quality of generators, evaluated both aesthetically and quantitatively. In terms of the classifier, we achieve better robustness than the state-of-the-art adversarial training algorithm proposed in (Madry \textit{et al.}, 2017), while our generator achieves competitive performance compared with SN-GAN (Miyato and Koyama, 2018).</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">adversarial training, conditional GAN</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We found adversarial training not only speeds up the GAN training but also increases the image quality</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">5 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rkeG5_gOT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxtE3C5Fm&amp;noteId=rkeG5_gOT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1469 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1469 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1e4WhIkp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This work in a very preliminary stage.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxtE3C5Fm&amp;noteId=r1e4WhIkp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1469 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1469 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper is concerned with improving generalization within GANs by employing adversarial training techniques. The authors propose an architecture that augments the AC-GAN architecture with a PGD attack module that generates adversarial examples for the discriminator. The authors report that an adversarially trained GAN is more robust to PGD attacks than state of the art adversarial discriminative models. Moreover, adversarial training seems to improve convergence rates for training. The authors offer some intuition behind why these phenomena take place. 

The investigation of possible synergies between adversarial attacks and generative models is definitely an intriguing endeavor, however, the evidence offered in the paper (in its current form) is not solid enough to build a convincing argument. 

Some major issues relating to the main claims of this paper:

* ideas of combining adversarial training and GANs have already appeared in the literature, and the authors even cite one such paper (Defense-GANs, Samangouei et al.). The citation appears amongst a long list of references that are summarily considered to provide "an illusion of safety". This to some degree challenges the novelty of the claims.

* it is very hard to evaluate the generality of the claims based on what is essentially a relatively small portion of ImageNet. Especially when arguing about losses and architectures. 

* it is not clear whether the improved convergence rate is actually worth it -- the computational cost of running the adversarial example generation is not reflected in the epoch plot. 

Apart from that, the paper contains a significant number of typos, and the exposition is confusing even though the arguments are not heavily technical. 
For this paper to be considered for publication, one would expect either a stronger theoretical analysis or a more rigorous empirical evaluation that shows that the reported phenomena persist across different problems/datasets.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rygRGBFsh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting approach to improve generalization ability of adversarial training by using GAN.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxtE3C5Fm&amp;noteId=rygRGBFsh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1469 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1469 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper touches on the limitation of adversarial training and propose a method that combines generator, discriminator, and adversarial attacker to improve the robustness and generalization ability of the network. The paper reads well (through there are a number of typos) and the source code is publicly available on the github. 

Pros:
- It provides insights on why adversarial training may not improve robustness on the test data set. 
- It demonstrates that one can improve robustness on the test data set by using GAN (to predict the real data distribution) and plugging it into adversarial training.
- Potential improvements on robustness and convergence speed of GAN.

Cons:
- lack of theoretical analysis.
- needs comparison with a number of relevant work in the literature that use GAN for adversarial training, more specifically, the following works are related to this paper:
     * Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon. Generative adversarial examples, 2018
     * Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, and Dawn Song. Generating adversarial
        examples with adversarial networks, 2018

Other comments:
- There is a contradiction between Table 1 and the text on Page 1 (the contribution section). As per table 1, the performance of the proposed approach drops to 30.25%, whereas on page 1 it is mentioned it is dropped to 36.4%.
- Figure 4 needs to be elaborated. The symbols are also not introduced.
- It also helps if the authors provide a pseudocode specifying all the steps they took to generate the results. 
- Figure 6 is hard to read and interpret. It could help if the authors show fewer images and discuss some of the differences.
- There are several typos in the paper. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ByxUTNM52m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Using GAN for data augmentation is not new and there lacks theoretical insights.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxtE3C5Fm&amp;noteId=ByxUTNM52m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1469 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1469 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Authors propose to apply GAN to generate the adversarial examples efficiently when training the robust model. 
Major issue:
1.	Using GAN as the generator for sampling adversarial examples is popular these years. This work simply applies it for the data augmentation where the novelty is limited.
2.	There are many strategies for data augmentation as adversarial training, and authors should include some in comparison, e.g., 
ICLR’18: Certifying Some Distributional Robustness with Principled Adversarial Training
3.	The experiments seem not convincing. The performance on CIFAR-10 without attacks is much worse than the-state-of-the-art. Authors claim the proposed strategy is efficient, but they didn’t provide the results on the imagenet-1k data set.
Minor issue:
The first three subfigures in Fig. 4 is not clear.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1gW98FR2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The contributions of our paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxtE3C5Fm&amp;noteId=r1gW98FR2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1469 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1469 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your useful comments and suggestions! We will improve our paper based on your feedbacks.

Since the reviewer mainly questioned on the novelty of this paper, below we address the contributions of our paper:

1. We think the value of this paper is much more than data augmentation, as we mentioned in the title and abstract, this paper reveals for the first time that "adversarial training and generative adversarial networks are closely related". Not just as trivial as "use fake images generated by GAN to do data augmentation". Instead, we found that the commonly used (and the state-of-the-art) adversarial training method suffers severely from large generalization gap (see Fig. 1), and hence the testing accuracy under adversarial perturbation is unsatisfactory. At the same time, it is known that GAN training is highly unstable and slow to converge, we show that by inserting an adversarial attacker, we can solve these two problems simultaneously: the generalization ability is stronger (Fig. 8), and the converge rate of GAN is better (Fig. 7).

2. We are aware of the ICLR'18 paper, it is true that this paper is a good example of "data augmentation as adversarial training", however: A) this paper only suggests to find adversarial examples based on original training data, while we suggest to make use of the fake images generated from GAN. B) The ICLR'18 paper you mentioned mainly discusses the the generalization bound of the adversarial training scheme, showing that the standard adversarial training techniques (like PGD adversarial training in our experiments) are able to generalize TO SOME EXTENT. As we said, the standard adversarial training methods suffer from large generalization gap (Fig. 1), our paper gives a method that greatly shrinks the generalization gap (Fig. 8)

3. It is very common to see a non-negligible accuracy drop by doing adversarial training (see [1] below). In Table 1 we compare our result with the standard PGD-adversarial training method (proposed in [1]), and our accuracy is only 0.35% lower than theirs without attack, and ~6% higher if adversarial attack exists (when perturbation=0.08).

As to the lacking of experiments on ImageNet-1k: adversarial training on ImageNet-1k is very hard, few papers have results on that. To the best of our knowledge, only [2] has some experiments, but [3] challenges the effectiveness of [2].
If the reviewer has other good examples, we would be very happy to read them.

Minor issue: We will add more annotations and explanations to Fig. 4 to make it clearer, thanks for pointing out.

We are eager to see more feedbacks from reviewer, since we believe the contributions of our paper are largely ignored. Again, we thank the reviewer for your helpful suggestions.


==========
References
[1] Madry et al. Towards Deep Learning Models Resistant to Adversarial Attacks. In ICLR'18
[2] Kannan et al. Adversarial Logit Pairing. In NIPS'18
[3] Engstrom et al. Evaluating and Understanding the Robustness of Adversarial Logit Pairing, Arxiv.


</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>