<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJluy2RcFm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Janossy Pooling: Learning Deep Permutation-Invariant Functions for..." />
      <meta name="og:description" content="We consider a simple and overarching representation for permutation-invariant functions of sequences (or set functions). Our approach, which we call Janossy pooling, expresses a..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJluy2RcFm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs</a> <a class="note_content_pdf" href="/pdf?id=BJluy2RcFm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019janossy,    &#10;title={Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BJluy2RcFm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=BJluy2RcFm" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We consider a simple and overarching representation for permutation-invariant functions of sequences (or set functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with k-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">representation learning, permutation invariance, set functions, feature pooling</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose Janossy pooling, a method for learning deep permutation invariant functions designed to exploit relationships within the input sequence and tractable inference strategies such as a stochastic optimization procedure we call piSGD</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SJxGsKnlA7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Question about the second experiment.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJluy2RcFm&amp;noteId=SJxGsKnlA7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1000 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you. I also really enjoyed reading the paper. 
For the vertex classification, without the aggregating function, minibatch sampling approaches (e.g. FASTGCN) are also getting almost similar or better accuracy than exact mean-pool (GRAPHSAGE) according to the paper. FASTGCN samples a few nodes to learn the graph neural network, so it is already fast and can avoid the additional computational burden of the aggregator. This means that the need for aggregation function is not clear for the GCN-baed models. I think more analysis with variance or other better measures are needed to show why it is meaningful for the vertex classification task as well. 

 It may be useful to test it with the work (Moore &amp; Neville, 2017).

(Chen, Ma, and Xiao, 2018) Jie Chen, Tengfei Ma, Cao Xiao. FASTGCN: Fast Learning with graph convolutional networks via importance sampling.
(Moore &amp; Neville, 2017) John Moore and Jennifer Neville. Deep collective inference.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HklWWHbbT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Emergency review for Janossy Pooling</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJluy2RcFm&amp;noteId=HklWWHbbT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1000 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1000 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">I have found the ideas proposed in the paper very insightful and interesting. The paper, in general, is written very well and is accessible.  My most important concern is 

 The whole development seems not as effective as k =1 in Table.2 (BTW, there is a typo there). One wonders, why for k =2, k =1 is not included? That is, can the formulation be changed in a way that \downarrow operator represents l \in {1 \cdots k} projections?  In the end, the method creates k tuples and feed them through specific fs so why not having smaller tuples?

The rest of my review below hopefully can help improving the paper;


- Is there any reason as to why higher order Janossy poolings do not perform as good as k =1 for the sum experiment? 

- Can you report the number of parameters for the developments (Janossy -k)? Some examples according to the experiments help.

- I am a bit lost to grasp the paragraph below Eq.4, can you rephrase it and possibly provide references?

- When it comes to testing, how do you use Eq.13? Do you sample a few permutation and compute 13? If yes, how many in practice? 

- In preposition 2.1,  n seems confusing, why not |h|

- In P6, x_i is a sequence. this needs to be mentioned 

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1eC2L8J0m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer4</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJluy2RcFm&amp;noteId=B1eC2L8J0m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1000 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1000 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your positive comments. We address your concerns below.

"- Is there any reason as to why higher order Janossy poolings do not perform as good as k =1 for the sum experiment?" 

The sum task is an easy task, designed for k=1. Our revised manuscript shows sum task results with more runs and more epochs and the difference is not statistically significant. 

“- The whole development seems not as effective as k =1 in Table.2....”

Theorem 2.1 shows that Janossy Pooling (JP) with k-ary dependencies includes and is more expressive than JP with (k-1)-ary dependencies, but there will be tasks where it is sufficient to let k=1 (and also easier to optimize).  This is especially true for easy tasks like the sum task which do not require exploiting dependencies within the input sequence.  Our revised manuscript now considers the harder task of computing the variance of a sequence of numbers. For this harder task, full-sequence Janossy (k = |h|) is significantly more accurate than k = 1,2,3, by using pi-SGD to train the model (which optimizes \doublebar{J} rather than \doublebar{L}). In the range task, full Janossy (k = |h|) + GRU + pi-SGD also shows significant gains over k=1,2,3. For all other tasks, Janossy k =|h| + GRU + pi-SGD performs as well as the other approaches. 

"- One wonders, why for k =2, k =1 is not included? That is, can the formulation be changed in a way that \downarrow operator represents l \in {1 \cdots k} projections?  In the end, the method creates k tuples and feed them through specific fs so why not having smaller tuples?"

Theoretically it is not necessary (by Theorem 2.1) but is an interesting direction for future work that could help in practice. It is clear, however, that Janossy k = |h| with GRU + pi-SGD is hard to beat in more challenging tasks.

"-Can you report the number of parameters for the developments (Janossy -k)? Some examples according to the experiments help. "

We have added the number of parameters in the Supplementary Material (Table 7 and Table 9) together with more details about our experimental setting. We have also tested k=2,3 with more complex models for \arrow{f}, the Supplementary Material shows the improved results.

"- I am a bit lost to grasp the paragraph below Eq.4, can you rephrase it and possibly provide references?"

Thank you, we rephrased our observations to simplify the exposition. We also considered the pros and cons of including a proof that Eq.4 captures any permutation-invariant function with an expressive-enough set of permutation-sensitive functions: the proof is straightforward as one can simply add all possible asymmetries (that cancel out when summing over all permutations) to the set of all permutation-invariant functions and make this a set of permutation-sensitive functions. It could be useful as a Proposition but, given the page limit, we have chosen to omit this straightforward proof in favor of other observations.

“- When it comes to testing, how do you use Eq.13? Do you sample a few permutation and compute 13? If yes, how many in practice?”

We have rewritten our experimental section to clarify how Eq.13 is used. We recommend looking at the new Table 1 which now more clearly defines "infr samples" to describe how many samples we use to estimate Eq.13. 

" - In preposition 2.1, n seems confusing, why not |h| "

That was a typo, we have changed to |h|. Thank you!

- In P6, x_i is a sequence. this needs to be mentioned 

Thank you. We have made changes in the notation to clarify that x(i) is the i-th sequence from the training (test) data.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1ey5ZDp2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A few of comments and request for clarifications</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJluy2RcFm&amp;noteId=B1ey5ZDp2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1000 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1000 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this paper, the authors presented a new pooling method called Janossy Pooling (JP), which is designed to better capture high-order information by addressing two limitations of existing works - fixed pooling function and fixed-size inputs. The studied problem is important and the motivation is clear, where the inputs are sets of objects such as values or vectors and how we can learn a good aggregation function to maximally preserve the information in the original sequence. The authors attacked this problem by firstly formally formulating this problem and introducing a general approach as well as a few of approximation methods to realize it in practice. They also discussed the connections of this work and some existing works such as deep set, which I found is quite useful. 

In general, JP was proposed to learn permutation-invariant function for aggregating the information of the input sequence. The basic idea of JP is to simply take all generated order of sequences from the original sequence input, which however I found is not new since it has been conceptually discussed already in the literature.  Since this approach is computationally prohibitive, there are several ways of approximations to approach the solution. As the authors are aware of the existing works in the literature, these approaches were discussed before either in the same context or in some particular learning tasks. From this perspective, the proposed solutions are not novel either. 

The experimental results are particularly weak. It is little interesting on the first toy problem and the results on graph embedding are not promising. In Table 2, it is clearly shown that the LSTM aggregation functions on the randomly sampled sequences are really beating the simple mean aggregation function. I think the authors need much more experiments to demonstrate why we need LSTM based pooling for realizing JP in terms of both the final accuracy and computational cost. 

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1x2XvP107" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJluy2RcFm&amp;noteId=r1x2XvP107"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1000 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1000 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comments. We address the two issues, experimental evaluation and novelty, below.

1) "Experiments weak"

(1.a) "Toy problems:" Our arithmetic tasks are more challenging than those of Deep Sets, whose experimental methodology we extend. That paper evaluated the model on a task that adds a set of digits. While summation does not require exploiting dependencies among elements in the sequence, we consider tasks such as “range” where doing so is imperative.  Our updated submission adds the task of computing the variance of a sequence of 10 integers. This update also makes our former preliminary results part of the main paper with a discussion of the new insights (here summarized in points 1.b and 1.c.)

Overall, our work is focused on generalizing today's pooling methods rather than any specific task. With that view, we make our tasks as simple as they can be to avoid spurious effects, but not so simple that the effects of increased pooling expressiveness do not apply. We welcome suggestions of ways to improve them.

(1.b) "No significant gains". "LSTM does not beat mean pooling."  The new variance task shows pi-SGD + GRUs + MLP \rho significantly outperforms other methods, including sum-pooling (variance requires better modeling of high-order interactions). Overall, pi-SGD + GRUs + MLP \rho  yields equal or superior performance across all arithmetic tasks. In general, using RNNs has the benefits of accepting variable-length sequences and seamlessly exploiting dependencies within the sequence.

(1.c) "Graph tasks": We followed the tasks found in Hamilton et al. (2017), which we found are quite easy. Our main interest was in evaluating differences between the different JP approaches (different choices of k and the impact of proper inference) on a task distinct from the arithmetic ones, and the results confirmed the anticipated benefits of using better inference at test time. In particular, proper inference of the \pi-SGD + LSTM model via Remark 2.2 can yield performance gains "for free" simply by averaging over forwarded permutations of the input sequence at test time.  

2) "Novelty": "The basic idea of JP is to simply take all generated order of sequences from the original sequence input, which however I found ... has been conceptually discussed already in the literature." We will try to answer this in a few different ways.

(2.a) "There is prior modeling work summing permutations in pooling layers". To the best of our knowledge, our pooling framework is the first that generalizes pooling, unless the reviewer is aware of other work we haven't cited. As we answer next, Hamilton et al. (2017) and Moore &amp; Neville (2017) performed pi-SGD in ad hoc manner, at the time it was not clear that it was a sound optimization procedure. Our work provides the theoretical underpinnings for their approach. 

(2.b) "\pi-SGD is not novel because it has already been tried". Hamilton et al. (2017) and Moore &amp; Neville (2017) did not provide a theoretical justification for their approach, and it was not obvious how to extend it. Our framework provides a theoretical justification for why and how pi-SGD works (SGD on the aforementioned ideal), as well as a characterization of the *correct* way to do inference at test time (missing in Hamilton et al. (2017)).

(2.c) "Novelty of k-ary Janossy pooling". To the best of our knowledge, k=3,... in full generality has not been tried. Deep Sets (with k=1) shows that sum-pooling is a universal approximator to permutation-invariant functions if the upper layers are universal approximators. We show that if the upper layers are not universal approximators (or if the universal approximation is hard to learn), k-ary Janossy pooling (k &gt; 1) is more powerful. Moreover, we show that this pooling approach is equivalent to summing over permutation-sensitive functions and achieves tractability via a restricted model class (functions with k inputs) rather than an approximate algorithm. Thus, our framework links two views of pooling: Inductive biases imposed on the model to capture dependencies in the sequence is inextricably linked with tractability strategies and present a tradeoff with learnability. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1xNEC4q2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting demonstration that standard pooling methods are insufficiently flexible</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJluy2RcFm&amp;noteId=S1xNEC4q2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1000 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1000 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">I really enjoyed this paper. It takes an idea which at first glance seems to be obviously bad (if you want permutation invariance, build a model that considers all permutations) and uses it to make the important point that the universal approximation results contained in Deep Sets [Zaheer et al. 2017] are not the last word on pooling. Janossy Pooling is intractable for most problems of interest (because it sums over all n! permutations of the input set) so the authors suggest 3 tractable alternatives: canonical orderings, k-ary dependencies and SGD / sampling-based approaches. Only the latter two are explored in detail, so I’ll focus on them:

K-ary dependencies
Functions that are restricted to k-ary dependencies in Janossy Pooling require summing over only |h|! / (|h| - k)! terms - that is they sum over the permutations of subsets of h of length k. In the experimental section, the authors show that this recovers some of the performances lost by using sum / mean pooling (as in Deep Sets), but this suggests the natural question: is it the fact that you’re explicitly modelling higher-order interactions that improves performance? Or is it that you’re doing Janossy pooling over the higher order interactions (i.e. summing over permutations of non-invariant functions)? 

These two effects could be separated by comparing to invariant models that allow higher order interactions. E.g. you could compare against Santoro et al. [2017] who explicitly model pairwise interactions (or similarly any of the graph convolutional models [Kipf and Welling 2016, Hamilton et al 2017, etc.] with a fully connected graph would do the same); similarly Hartford, et al. [2018] allow for k-wise interactions by extending Deep Sets to exchangeable tensors - the permutation invariant analog of k-ary Janossy Pooling. All of these approaches model k-wise interactions through sum-pooling over permutation invariant functions so this lets you address the question - is it the permutation invariance that’s the problem (necessitating k-ary Janossy pooling) or is it the lack of higher-order interaction terms? 

SGD approaches:
I think that the point that the sampling-based approaches are bias with respect to the Janossy sum is important to make and I liked the discussion around it, but I don’t follow the relevance of Proposition 2.2? I see that it gives conditions under which we can expect \pi-SGD to converge, but we aren’t provided with any guidance about how likely those conditions are to be satisfied? Furthermore - these conditions don’t seem to be specific to \pi-SGD - any SGD algorithm with “slightly biased” gradients that satisfy these conditions would converge. The regularization idea is interesting, but it isn’t evaluated so we’re left with theory that doesn’t provide guidance and isn’t evaluated.

Summary:
There are two ways to read this paper:
 1. Janossy pooling as a framework &amp; proposed pooling approach implemented in one of the two ways discussed above.
 2. Janossy pooling as an intractable upper bound on what we might want from a pooling method (with approximations in the form of the LSTM approaches) and a demonstration that our current invariant pooling methods are insufficient.

I liked the paper based on reading (2). Janossy pooling clearly demonstrates limitations to sum / mean pooling which is widely used in practice which shows the need for better alternatives and it is on this basis that I’m arguing for it’s acceptance. My view is that the experimental section is too limited to support reading (1) which asserts that k-ary pooling or LSTM + sampling approaches are the right solution to this problem. 

[Zaheer et al. 2017] - Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and
Alexander Smola. Deep Sets
[Santoro et al. 2017] - Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Tim Lillicrap. A simple neural network module for relational reasoning.
[Kipf and Welling 2016] - Thomas N. Kipf and Max Welling. Semi-Supervised Classification with Graph Convolutional Net- works
[Hamilton et al 2017] - William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive Representation Learning on Large Graphs
[Hartford, et al. 2018] - Jason S. Hartford, Devon R. Graham, Kevin Leyton-Brown, and Siamak Ravanbakhsh. Deep models of interactions across sets</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1gpO5L10X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJluy2RcFm&amp;noteId=B1gpO5L10X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1000 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1000 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your positive comments.  We also see Janossy Pooling as simultaneously providing theory that highlights limitations of existing methods and an overarching framework for developing pooling functions.  We also agree that more experiments (as always) are beneficial; our revision includes a more thorough experiments section upon which we elaborate below.

(1) In the experimental section, the authors show that [k-ary Janossy Pooling] recovers some of the performances lost by using sum / mean pooling...Is it the fact that you're explicitly modelling higher-order interactions that improves performance? Or is it that you're doing Janossy pooling over the higher order interactions (i.e. summing over permutations of non-invariant functions)?

Our development of k-ary Janossy Pooling (JP) demonstrates that the increased performance associated with k&gt;1 does not rely upon using permutation-sensitive Janossy functions \harrow{f}. Indeed, our proof that (k-1)-ary JP is less expressive than k-ary JP constructs a permutation-invariant k-ary Janossy function (\harrow{f}) which cannot be expressed by any (k-1)-ary Janossy function, permutation-sensitive or otherwise.  We modeled \harrow{f} as permutation-sensitive in our experiments since basic neural network building blocks are permutation-sensitive. 

(2) I don't follow the relevance of Proposition 2.2? I see that it gives conditions under which we can expect \pi-SGD to converge, but we aren't provided with any guidance about how likely those conditions are to be satisfied? Furthermore - these conditions don't seem to be specific to \pi-SGD - any SGD algorithm with ``slightly biased'' gradients that satisfy these conditions would converge.

We sought to reassure the reader of the appropriateness of randomly sampling and forwarding just one permutation of the sequence during training -- which at first glance may appear inappropriate.  We agree that this can be achieved simply by pointing out the similarity to ''typical'' SGD and we have revised our paper accordingly and moved the detailed proof to the appendix.

(3) My view is that the experimental section is too limited to support reading (1) which asserts that k-ary pooling or LSTM + sampling approaches are the right solution to this problem. 

While we agree that reading (2) is our preferred reading too, we have added experiments to the revised version which provide further support of the power of proposed JP models.  These include (a) the addition of a more complex \rho (the function composed with the output of pooling) to all models for the arithmetic tasks, which presents a more competitive baseline, (b) the addition of a harder arithmetic task -- computing the variance of a sequence of integers -- and (c) further analysis of the impact of increasing the number of permutations sampled at test-time for prediction in a pi-SGD model.  The latter was performed on the PPI graph, a new dataset evaluated in this submission.

(a and b) Whereas \rho was previously a linear layer only, we have added results where \rho is an MLP with a single hidden layer.  Our results show that Janossy pooling architectures achieve superior or similar performance to the baseline of sum pooling across all tasks -- including the variance task -- for either choice of \rho.  Notice that the GRU model with an MLP \rho achieves a mean RMSE of 0.40 on the variance task, beating the sum-pooling baseline by a substantial margin.

(c) Our arithmetic tasks show a clear benefit of averaging over more permutations at test time; doing so either improved the mean performance or left it unchanged (especially when performance was already saturated).  We have also expanded our investigation of this phenomenon in the graphs tasks, where we plotted performance as a function of the number of permutations sampled  at test time across different models.  We saw that simply sampling just a few permutations led to consistent and statistically significant gains in performance.  These gains level off but do not degrade as more permutations are sampled.  

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJlOsS79pm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Unique sum and unique count require a multiset not a set</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJluy2RcFm&amp;noteId=HJlOsS79pm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1000 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1000 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I didn't pick this up in my initial review, but the "unique sum" and "unique count" tasks in the synthetic experiments go beyond the scope of the deep sets work since that paper refers to sets not multisets. "Unique" doesn't make sense for sets. These experiments should be removed (or at the very least this should be made clear). Similarly for the other tasks, sampling without replacement makes more sense to ensure the input is in fact a set.  </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1gYh_tiTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarification regarding 'unique count' and 'unique sum' tasks</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJluy2RcFm&amp;noteId=B1gYh_tiTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1000 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1000 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Our primary interest was in permutation-invariant functions, and we only used the term “set function” to follow Zaheer 2017.  Please note that Deep Sets performs the sum task on “sets” of integers {0, 1, 2, …, 9} of size 50 which must have duplicates.  In following their design, our input sequences also have duplicates.  

We also note that extensions of the Deep Sets theorem relating permutation-invariance and sum pooling was recently extended to include multisets in [Xu et al 2018].

We will add a line to the paper to clarify this.  Thanks.

[Xu et al 2018] Xu, Keyulu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. "How Powerful are Graph Neural Networks?." arXiv preprint arXiv:1810.00826 (2018).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>