<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>RoC-GAN: Robust Conditional GAN | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="RoC-GAN: Robust Conditional GAN" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Byg0DsCqYQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="RoC-GAN: Robust Conditional GAN" />
      <meta name="og:description" content="Conditional generative adversarial networks (cGAN) have led to large improvements in the task of conditional image generation, which lies at the heart of computer vision. The major focus so far has..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Byg0DsCqYQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>RoC-GAN: Robust Conditional GAN</a> <a class="note_content_pdf" href="/pdf?id=Byg0DsCqYQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019roc-gan:,    &#10;title={RoC-GAN: Robust Conditional GAN},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Byg0DsCqYQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=Byg0DsCqYQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Conditional generative adversarial networks (cGAN) have led to large improvements in the task of conditional image generation, which lies at the heart of computer vision. The major focus so far has been on performance improvement, while there has been little effort in making cGAN more robust to noise or leveraging structure in the output space of the model. The end-to-end regression (of the generator) might lead to arbitrarily large errors in the output, which is unsuitable for the application of such networks to real-world systems. In this work, we introduce a novel conditional GAN model, called RoC-GAN, which adds implicit constraints to address the issue. Our model augments the generator with an unsupervised pathway, which promotes the outputs of the generator to span the target manifold even in the presence of large amounts of noise. We prove that RoC-GAN share similar theoretical properties as GAN and experimentally verify that our model outperforms existing state-of-the-art cGAN architectures by a large margin in a variety of domains including images from natural scenes and faces.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">conditional GAN, unsupervised pathway, autoencoder, robustness</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We introduce a new type of conditional GAN, which aims to leverage structure in the target space of the generator. We augment the generator with a new, unsupervised pathway to learn the target structure. </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rJev0VNha7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>General comment - revision 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byg0DsCqYQ&amp;noteId=rJev0VNha7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper309 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper309 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">One of the crucial issues raised by the reviewers is the novelty of our approach. In this comment we address this general question and also outline the additional revisions (see also general response for the first part):

We argue that there is considerable novelty in our work; to our knowledge there has *not* been a study of robustness in dense regression. 

Machine learning is entering an era that is widely used in many diverse applications ranging from particle physics [1] to cyber-security [2] and from medical applications [3] to molecule generation [4]. In several applications and domains safety/robustness is critical, however the majority of the dense regression networks just report the best results ignoring the out of manifold or noise of real applications. Hence, we argue that robustness analysis should be introduced and performed in dense regression tasks. 

In this context we restate our contributions: 

* We introduce RoC-GAN that performs conditional image generation by leveraging structure in the target space. Neither the model has emerged before, nor the context of our analysis.

* We perform a robustness analysis through a series of experiments. We scrutinize the performance of the original cGAN and our model under different types of noise. We extend the adversarial perturbations in dense regression tasks. 

* We experimentally demonstrate how our method can be used with different architectures and tasks. We additionally show that RoC-GAN can be beneficial in semi-supervised learning task or how it performs with lateral connections from encoder to decoder.

To address the request of reviewer 2 for an alternative metric to SSIM, we compute a cumulative plot that measures the similarities of the identities in the face experiment. We employ the well-studied (and robust) recognition embeddings of FaceNet ([5]) to evaluate the similarity of the target image with the outputs of compared methods. For each pair of output and corresponding target image, we compute the cosine distance of their embeddings; the cumulative distribution of those distances is plotted. The plots illustrate that indeed RoC-GAN outputs are closer to the target images with respect to the identities. Please find the complete metric analysis in section D.4 in the appendix.

We consider that our work has become significantly stronger with the revised experimental results, thus we request the reviewers to reconsider their rating.


[1] de Oliveira, Luke et al. "Learning particle physics by example: location-aware generative adversarial networks for physics synthesis", Computing and Software for Big Science 2017.
[2] Ye, Guixin et al. "Yet Another Text Captcha Solver: A Generative Adversarial Network Based Approach", ACM SIGSAC Conference on Computer and Communications Security 2018.
[3] Wei, Wen et al. "Learning Myelin Content in Multiple Sclerosis from Multimodal MRI through Adversarial Training", arxiv 2018.
[4] De Cao, Nicola and Kipf, Thomas "MolGAN: An implicit generative model for small molecular graphs", arxiv 2018.
[5] Schroff, Florian et al. "Facenet: A unified embedding for face recognition and clustering", CVPR 2015.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJgoRH1vaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>General comment - revision 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byg0DsCqYQ&amp;noteId=BJgoRH1vaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper309 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper309 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We appreciate the constructive feedback of the reviewers. In response to their reviews, we have updated several sections in the paper:

* We add an experiment on the ImageNet dataset, in both sparse inpainting and denoising (appendix, sec. D1).
* We include a paragraph with the contributions in the introduction.
* We train a method to demonstrate the  maximum representational power of each network (this can be thought of as the upper bound of each experiment). Specifically, we train an adversarial autoencoder (in the target space) and utilize the reconstructed images for evaluation. 
* We include a visual example to illustrate how a projection in a linear subspace can promote the output to span the target subspace (appendix, sec. B). 

In summary, we believe that the aforementioned results strengthen our claims and improve the paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SyeeXquTh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Simple but effective method, methodological novelties are limited though!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byg0DsCqYQ&amp;noteId=SyeeXquTh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper309 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper309 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This manuscript proposes a robust version of conditional GAN (named RoC-GAN) that leverage the intrinsic structure in the output space. To achieve robustness, the authors replace the single pathway in the generator with two different pathways that partially share weights. The authors study the theoretical properties of RoC-GAN and prove that it shares the same properties as the vanilla GAN. For quantitative evaluations, the authors use two datasets of natural scenes and faces and evaluate denoising and sparse inpainting using the SSIM metric.
-	The idea is simple and seems to be working. The methodological novelties seem more-or-less limited, but the theoretical analysis and the intuitive (and well-motivated) modification over CGANs add merits to the paper. 
-	The theoretical analysis of the method relates RoC-GAN to the original GAN, rather than CGAN! What is the connection here? If RoC-GAN is very similar to CGAN from a theoretical point of view (which it seems to be), then all the analysis to relate it to traditional GAN seem useless.
-	The extensive experiments in the supplementary material are appreciated. But the authors only compare their method with one single previous work (i.e., Rick Chang et al. (2017)), while there are several similar related works (either based on adversarial training strategies or simple denoising AEs).
-	Also, ablation studies can further show how each component of the model contributes to the final results. What if we were to only use the two-path generator without adversarial training? Different components of the final loss function can be removed and analyzed one at a time!
-	What are the conditions for mode-collapse for the proposed GAN? There are no discussions on this.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HkewAY1Pam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byg0DsCqYQ&amp;noteId=HkewAY1Pam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper309 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper309 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">In addition to the general answer above, we answer each of the points raised below:

1) 'The idea is simple and seems to be working. The methodological novelties seem more-or-less limited, but the theoretical analysis and the intuitive (and well-motivated) modification over CGANs add merits to the paper. ': 

We thank the reviewer for the recognition of the work. 


2) 'The theoretical analysis of the method relates RoC-GAN to the original GAN, rather than CGAN! What is the connection here? If RoC-GAN is very similar to CGAN from a theoretical point of view (which it seems to be), then all the analysis to relate it to traditional GAN seem useless.': 

On the contrary, we prove that our RoC-GAN shares the same theoretical properties as GAN; this can be seen as a sanity check, conforming that our method shares some beneficial theoretical properties with well-studied methods. 
Similar proofs are provided in other extensions to cGAN, such as Zhe et al. ([1]).


3) 'The extensive experiments in the supplementary material are appreciated. But the authors only compare their method with one single previous work (i.e., Rick Chang et al. (2017)), while there are several similar related works (either based on adversarial training strategies or simple denoising AEs).': 

The goal of this work is not to propose a state-of-the-art network per se, but rather to present a method that is more robust to additional sources of noise. Our model is not architecture dependent. Specifically, RoC-GAN can be seen as a meta algorithm which can be used to augment any existing cGAN model to achieve additional robustness.
We scrutinize the robustness under:
i) similar types of noise, 
ii) types of noise not encountered during training, 
iii) adversarial perturbations. 
In addition to those, we also note that our method performs favorably when tested with samples similar as the training distribution. We have included an external  method to illustrate that even strong performing networks can have difficulty in such tasks.
However, if there are some specific works that the reviewers feel are particularly relevant, we are happy to evaluate their pre-trained models.


4) 'Ablation studies can further show how each component of the model contributes to the final results. What if we were to only use the two-path generator without adversarial training? Different components of the final loss function can be removed and analyzed one at a time!':

We appreciate the reviewer's proposal; indeed in the synthetic experiment we optimize only the generators to simplify the problem; please see sec. 3.4. In addition, removing one by one the losses is performed in sec. E.2 (appendix). 
Even though our experiments are not exhaustive, we consider that we have covered a wide range of choices; those demonstrate the merits or trade-offs of our RoC-GAN.


5) 'What are the conditions for mode-collapse for the proposed GAN? There are no discussions on this.': 

We follow the same strategy as popular methods in cGAN ([2], [3]). We agree with the reviewer that mode collapse is significant especially in original GAN training, however there are other works tackling this issue, e.g. [4-6].   


[1] Gan, Zhe, et al. "Triangle generative adversarial networks.", NIPS 2017.
[2] Isola, Phillip et al. "Image-to-Image Translation with Conditional Adversarial Networks", CVPR 2017.
[3] Zhu, Jun-Yan et al. "Toward multimodal image-to-image translation", NIPS 2017.
[4] Che, Tong et al. "Mode Regularized Generative Adversarial Networks", ICLR 2017.
[5] Anonymous, "Generative Adversarial Network Training is a Continual Learning Problem", under review ICLR 2019.
[6] Anonymous, "DISTRIBUTIONAL CONCAVITY REGULARIZATION FOR GANS", under review ICLR 2019.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SygbgUr527" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Well-written paper but the novelty and significance might be a weakness</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byg0DsCqYQ&amp;noteId=SygbgUr527"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper309 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper309 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">General:
In general, this is a well-written paper. This work focuses on the robustness of conditional GAN(RoC-GAN) when facing the noise. The authors claim the generator of RoC-Gan will span the target manifold, even in the presence of large amounts of noise. The main contribution of the paper is to introduce a two-pathway model, where one of them is used to perform regression as ordinary GAN while the other one helps the whole model span the target domain.

Strength:
1. The idea is simple and straightforward. The authors provide necessary theoretical analysis and empirical validation for their model. 
2. The proposed method seems technically correct to me. i.e. Although I am not very sure how well it works in practice, the idea is fine.

Possible Improvements:
1. I agree adding another auto-encoder as a helper may give better generation results by spanning the whole target space, but I don't think this constraint is strong enough in practice. 
2. In section 3.3, the time complexity of computing 'L_deconv' seems extremely large. From the perspective of numerical optimization, optimizing such a matrix will cause trouble if the dimension of weight matrices are large. i.e. optimizing the high-dimensional covariance matrix seems a problem to me.
3. The experiments looks good. The experiments could be more convincing if using more complex data sets(e.g. CIFAR10, ImageNet) besides CelebA. My concern for using such data sets(the resolution of images is low and the distribution is simple)  is that: although the noise seems to corrupt most of the image, the distribution of the image is not complex, so the generative model can recover it easily. Since this is a more empirical paper, the experiments should be more convincing.

Conclusion:
The author(s) are thoughtful and they put lots of work on this paper. The proposed method is simple. For novelty and significance, I think the idea is not very fancy to me. I am not very convinced by the method proposed in the paper. Although the paper demonstrates the robustness of their model with different experiments, most of them were not performed on deep neural networks and complicated data sets. As a conclusion, I vote for weak rejection.

Minor suggestion:
Increase the resolution of the figures.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hkl9noywp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byg0DsCqYQ&amp;noteId=Hkl9noywp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper309 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper309 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for recognizing the effort and contribution of our method. In addition to the general answer above, we answer each of the improvement points below: 

1) 'I agree adding another auto-encoder as a helper may give better generation results by spanning the whole target space, but I don't think this constraint is strong enough in practice. ': 

We respectfully disagree with the reviewer; we have demonstrated in a series of experiments how this modification is beneficial. We provide the intuition, the synthetic experiment, a linear analogy analysis, and several experiments. 
In the revision we add a visual example for the linear subspace (appendix, sec. B). We demonstrate how one corrupted and one clean image can have similar reconstructions from a PCA model. 


2) 'In section 3.3, the time complexity of computing '$L_deconv$' seems extremely large. From the perspective of numerical optimization, optimizing such a matrix will cause trouble if the dimension of weight matrices are large. i.e. optimizing the high-dimensional covariance matrix seems a problem to me.':

In the implementation details (section 4; page 7), we mention that we use $L_deconv$ in the output of the encoders. Those layers include tensors of $batch x 1 x 1 x channels$ where the number of channels is typically up to 1024. In our experiments for 4 layer network channels=512, for the 5 and 6 layer networks channels=768. Cogswell et al. ([1]) include an analysis for deeper networks. In practice, we have not noticed a significant computational burden, but this can be further explored in the future.    


3) 'The experiments look good. The experiments could be more convincing if using more complex data sets(e.g. CIFAR10, ImageNet) besides CelebA. My concern for using such data sets(the resolution of images is low and the distribution is simple)  is that: although the noise seems to corrupt most of the image, the distribution of the image is not complex, so the generative model can recover it easily. Since this is a more empirical paper, the experiments should be more convincing.': 

We have conducted the requested experiments on Imagenet (appendix, sec. D.1). The results are in line with those reported in the main paper -- particularly the natural scenes case and confirm the advantages of our method. That is, RoC-GAN outperform the cGAN in both denoising and sparse inpainting while the difference is increased when evaluated with additional noise.  We note that the same hyper-parameters (as the rest of the paper) are used; additional tuning per experiment might be beneficial however for avoiding confusion all the hyper-parameters remain the same.


4) 'Although the paper demonstrates the robustness of their model with different experiments, most of them were not performed on deep neural networks and complicated data sets.': 

In the revised version we have included an experiment on Imagenet (appendix, sec. D.1). Our model is not architecture dependent. Specifically, RoC-GAN can be seen as a meta algorithm which can be used to augment any existing cGAN model to achieve additional robustness. We have made our best effort to demonstrate that with i) similar types of noise, ii) types of noise 'unseen' during training, iii) adversarial perturbations. 

5) 'Minor suggestion: Increase the resolution of the figures.':

We will add new figures to reflect the AAE method added and improve older ones the next few days. We appreciate the proposal.

[1] Cogswell, Michael, et al. "Reducing overfitting in deep networks by decorrelating representations.", ICLR 2016.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1x_dKfthX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Explanation is not clear and experiments are weak</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byg0DsCqYQ&amp;noteId=B1x_dKfthX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper309 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper309 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Authors propose to augment a conditional GAN model with an unsupervised branch for spanning target manifold and show better performance than the conditional GAN in natural scene generation and face generation.

However the novelty is limited and not well explained.
1.Similar idea of using an autoencoder as another branch to help image generation has been proposed in Ma et al.â€™s work. 
Liqian Ma, Qianru Sun, Stamatios Georgoulis, Luc Van Gool, Bernt Schiele, Mario Fritz. Disentangled Person Image Generation, CVPR 2018.

2. In the paper authors claim that skip connection makes it harder to train the longer path, which is kind of contradictory to what is commonly done in tasks of image classification, semantic segmentation and depth estimation. Can authors explain this claim?
In addition, it is not clear why maximizing the variance can address the challenge of training longer path.

3. Covariance is computed for decov loss but it is not clear which layerâ€™s representation is used to compute covariance.

4. In Table 1, the improvement over baselines is small in case of sparse inpaint setting.

5. In Figure 4, the fourth row is more blurry than the third row although with less artifacts like black dots.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJxl7bVhpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer 2 - part 1 of 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byg0DsCqYQ&amp;noteId=SJxl7bVhpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper309 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper309 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We appreciate the constructive feedback of the reviewer; in addition to the general comments on top that summarize our revisions, we answer to the reviewer's points below:

1) 'The novelty is limited and not well explained.':

Even though we might not have emphasized the novelty enough, we believe that our paper makes several contributions. We have pointed them out in the revised text (please consult the general comment above). 
The essence of our contributions is studying robustness of cGAN in dense regression tasks. To the authors' knowledge this has not been studied at all for cGAN, i.e. a widely used framework for dense regression. If the reviewer has noticed it *anywhere*, we are happy to reconsider the positioning of our manuscript. 


1) 'Similar idea of using an autoencoder as another branch to help image generation has been proposed in Ma et al.â€™s work ([1])':

We disagree with the reviewer; the two works differ significantly in their use of AE:
   a) Ma et al. utilize an autoencoder (AE) with a different goal than RoC-GAN. They use three specialized AE to obtain the latent representations (embeddings) and not to leverage structure in the target (e.g. image) space. 
   b) They devise a well-thought and heavily engineered pipeline for the task of person image generation (Fig. 2, 3 of their paper). Several of their modules, e.g. Region of interest boxes, are task-specific. Our goal is the extension of *any* cGAN to a more robust model. 
   c) The AE in [1] is learned separately (and then is fixed), while the AE pathway in our work is not fixed, but *jointly* optimized with the regression pathway.
   d) Their loss functions are different. In particular, in [1] the authors include different losses in each step of their two-stage pipeline, while ours is a generic loss that can differ per task.  
In that sense the two works are orthogonal by the use of AE. Despite the differences we consider that some ideas can be used to extend RoC-GAN, e.g. using a discriminator to match the latent representations (as done for the embeddings in [1]). We have added this as future work in the manuscript. 


2a) 'In the paper authors claim that skip connection makes it harder to train the longer path, which is kind of contradictory to what is commonly done in tasks of image classification, semantic segmentation and depth estimation. Can authors explain this claim?':

To the best of our knowledge, there has not been much study of how the longer path is optimized in cGAN setting. However, in the broader community of deep learning, several papers report the issue with the longer path training, please check [2-3]. As widely reported, the skip connections might help convergence, but they also enable the network to trivially copy the representations of previous layers and might shatter the meaningful representation learning in the longer path. 

A more intuitive explanation in our case: a trivial solution for the network would be to copy the representation to the decoder. The option of copying the meaningful representations through the shortcut is more 'attractive' in our case due to the latent loss Llat. If we do not regularize the longer path representations, the network has less incentive to learn meaningful representations in the longer path, which defeats the concept of including the latent loss. On the contrary, including by decorrelating the weights we encourage both pathways to learn meaningful representations in the longer path.

[1] Ma, Liqian, et al. "Disentangled Person Image Generation", CVPR 2018.
[2] Rasmus, Antti, et al. "Semi-supervised learning with ladder networks.", NIPS 2015.
[3] Zhang, Yuting, et al. "Augmenting supervised neural networks with unsupervised objectives for large-scale image classification", ICML 2016.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HklhCxV2a7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer 2 - part 2 of 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byg0DsCqYQ&amp;noteId=HklhCxV2a7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper309 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper309 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">2b) 'In addition, it is not clear why maximizing the variance can address the challenge of training longer path.': 

Reducing the correlations of the weights has several advantages that are well-studied in both computer vision and machine learning. Several methods for decorrelating the weights has been used in deep networks, for instance [6-10]. 

An intuitive idea about why we have included this loss: By reducing the correlations we encourage our method to explore different 'principal' directions in different layers, which is beneficial for training the network. Similar observations and experiments for the benefit of exploring different directions during training have been explored in [4-6].


3) 'Covariance is computed for decov loss but it is not clear which layerâ€™s representation is used to compute covariance.':

We have actually included the requested information in the original submission (sec. 4, page 7 in the revised manuscript).


4) 'In Table 1, the improvement over baselines is small in case of sparse inpainting setting.': 

We appreciate the comment; we have performed a similar analysis in sec D.2 (appendix). In short: the improvement in the additional noise experiments (sparse inpainting task) is not marginal but quite significant (up to 15%). Furthermore, we believe the experiments that we have conducted cover several cases and the results are always consistent, i.e. RoC-GAN *always* improve the baseline, while in the regions of more extreme noise or adversarial perturbations the difference is substantial.   


5) 'In Figure 4, the fourth row is more blurry than the third row although with less artifacts like black dots.': 

We argue that the black dots the reviewer mentions make the images unrealistic. Such irregularities can have detrimental effect for higher level tasks accepting those images as input.  Nevertheless, to demonstrate with quantitative metrics the difference, we have added a new metric for the experiments of faces. The metric is focused on the similarity of the identities of the facial images. We have measured the distance between the methods' outputs and the target images and prepared the cumulative plot. Please find the complete metric analysis in section D.4 in the appendix. 


[4] Jia, Kui et al. "Improving training of deep neural networks via singular value bounding", CVPR 2017.
[5] Miyato, Takeru et al. "Spectral normalization for generative adversarial networks", ICLR 2018. 
[6] Bansal, Nitin et al. "Can We Gain More from Orthogonality Regularizations in Training Deep CNNs?", Arxiv.
[7] Cohen, Taco and Welling, Max "Group equivariant convolutional networks", ICML 2016.
[8] Cogswell, Michael, et al. "Reducing overfitting in deep networks by decorrelating representations.", ICLR 2016.
[9] Rodriguez, Pau et al. "Regularizing cnns with locally constrained decorrelations", Arxiv.
[10] Ozay, Mete and Okatani, Takayuki "Optimization on Submanifolds of Convolution Kernels in CNNs", Arxiv.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>