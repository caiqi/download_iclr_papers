<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Continual Learning via Explicit Structure Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Continual Learning via Explicit Structure Learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ryxsS3A5Km" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Continual Learning via Explicit Structure Learning" />
      <meta name="og:description" content="Despite recent advances in deep learning, neural networks suffer catastrophic forgetting when tasks are learned sequentially. We propose a conceptually simple and general framework for continual..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ryxsS3A5Km" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Continual Learning via Explicit Structure Learning</a> <a class="note_content_pdf" href="/pdf?id=ryxsS3A5Km" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 18 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019continual,    &#10;title={Continual Learning via Explicit Structure Learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=ryxsS3A5Km},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=ryxsS3A5Km" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Despite recent advances in deep learning, neural networks suffer catastrophic forgetting when tasks are learned sequentially. We propose a conceptually simple and general framework for continual learning, where structure optimization is considered explicitly during learning. We implement this idea by separating the structure and parameter learning. During structure learning, the model optimizes for the best structure for the current task. The model learns when to reuse or modify structure from previous tasks, or create new ones when necessary. The model parameters are then estimated with the optimal structure. Empirically, we found that our approach leads to sensible structures when learning multiple tasks continuously. Additionally, catastrophic forgetting is also largely alleviated from explicit learning of structures. Our method also outperforms all other baselines on the permuted MNIST dataset in continual learning setting.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">continuous learning, catastrophic forgetting, architecture learning</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rygRL-QRTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Paper revision summary</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxsS3A5Km&amp;noteId=rygRL-QRTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1572 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1572 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank all reviewers for providing constructive feedback that further improves the paper. We have highlighted the changes in text by using blue color. Minor editing changes are not marked. In this update revision we did following changes.

1) We added more analysis on forgetting, which we think provides more insights into the method. In addition to use simple L-2 based regularization we finetuned our model without using any regularization, and we still obtained interesting result where the forgetting is minimal. This further suggests the importance of structure learning when learning continual tasks.

2) As all reviewers suggested, we added more comparisons to more recent, existing methods. In particular, we compared ours with the more recent methods such as  dynamically expandable network, incremental moment matching, progressive network, hard attention to task, etc on permuted MNIST dataset. We show that our method is performs competitive or better as compared to all these method.

3) Provided more details in appendix

4) Corrected editorial errors as pointed out by reviewers.

Due to the time limit, we only completed experiments on permuted MNIST. Additionally we are also running experiment on split MNIST so that we have more comparisons, and we will update another version with those results before the deadline.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BkekeLd52X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review of "Continual Learning via Explicit Structure Learning"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxsS3A5Km&amp;noteId=BkekeLd52X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1572 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1572 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper considers the problem of sequential learning where data access for the previous tasks is completely prohibited. Authors propose a conceptually simple framework to learn structures (it is the selection of reusing, adapting previously learned layers or training new layers) as well as corresponding parameters in the sequential learning.

The paper is potentially interesting and providing possibly important framework for life-long learning. It is well written in most of cases and easy to follow (however I got the impression that the paper was rushed in the last minute; there are some trivial typos and very low resolution images etc.)

However, I have a huge concern about the empirical evaluations.  This area is really huge and has attracted lots of interest from many researchers, meaning that we lots of methods to compare. Nevertheless, authors only focus on providing insights on effects of different components of the propose model. This is also critical but comparing against state-of-the-arts is also very important. Especially, comparing against Lee et al 2017 seems essential. I can see the difference against that paper from the authors' argument in the related work, but that is the difference not comparison. It would be great to compare the performances as well as the number of increased memory sizes as the number of task increases.

Moreover, the details should be provided; for instance provide the explicit form of R(s). </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryxJKZm067" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxsS3A5Km&amp;noteId=ryxJKZm067"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1572 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1572 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your feedback. We have added additional results comparing our method with other more recent and relevant methods. Please refer to the updated manuscript.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1e--EHYnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxsS3A5Km&amp;noteId=B1e--EHYnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1572 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1572 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
This paper proposes a new approach to mitigate the catastrophic forgetting for continual learning. The model is composed to the neural architecture search and parameter learning based on the intuition that largely different tasks should allow to use different network structure to train them. In structure learning, they introduce three candidate to decide network architecture, reuse, adaptation and new. In the experiments, they show that their model outperforms SGD and EWC.

Basically, the intuition of structure learning and the validation of that is straight forward and easy to follow. However, I’m not sure that the proposed model can outperform the recent continual learning methods, such as IMM(Lee et al, 2017), DEN or  RCL(Ju Xu et al, 2018). There is only a relatively weak(and old) comparison with l2, and EWC.

-	In the equation (4), I wonder that, in the model, the hyperparameter(lambda_i or beta_i) of regularizer looks different according to the task, is it correct?
-	As shown in the Fig. 2) three choice-reuse, adaptation, and, new, is decided in the layer level. But with a semantic intuition, such that two different task can share specific features and simultaneously each of them requires the different neural space to learn discriminative ones at layer l, it seems better if the model could search structure much flexible. Is there some of experimental trial or plan about these kind of joint-adoption?
-	What is the main contribution of adaptation? I wonder that only reuse and new can work well including the role of adaptation, or not.
-	Is there any experiments to compare the recent continual learning methods(as I mentioned), in terms of AUC(or accuracy) and the network capacity?

Minor remarks,
Page 3: 	“is been” -&gt; is
	“unlikely”-&gt; unlike
Page 4: 	“sharealbe” -&gt; shareable
Page 5: 	“, After” -&gt; , after
	“permuated” -&gt; permuted
Page 6:	“Fig. 5” -&gt; Fig. 4

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Skx9o-X0TQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxsS3A5Km&amp;noteId=Skx9o-X0TQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1572 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1572 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your feedback. We have added additional results comparing our method with other more recent and relevant methods. Please refer to the updated manuscript.

Regarding the questions:
-	In the equation (4), I wonder that, in the model, the hyperparameter(lambda_i or beta_i) of regularizer looks different according to the task, is it correct?

Yes they can be different for each tasks, this is more of a design choice. However, in our implementation and experiments, to make things easier, we just used the same hyperparameter for all tasks.

-	As shown in the Fig. 2) three choice-reuse, adaptation, and, new, is decided in the layer level. But with a semantic intuition, such that two different task can share specific features and simultaneously each of them requires the different neural space to learn discriminative ones at layer l, it seems better if the model could search structure much flexible. Is there some of experimental trial or plan about these kind of joint-adoption?

This is a very good point. Ideally we would like to be able to do more finer grained search, and that is definitely desired. In practice, we could only make the search space more restricted so that the search can be done in a more efficient manner. Of course one is not restricted to use only the options that we provided in our implementation. More finer grained and search is definitely possible, for example, learning to share at filter/neuron level instead of layer level. This is more of a balance between training efficiency and final performance. The current implementation highlights the importance of taking structure into account. However, one should not limit themselves with only the options that we demonstrated. As long as the search space is reasonably sized and operations are plausible, it could be incorporated in our framework. This leads to interesting future work directions.

-	What is the main contribution of adaptation? I wonder that only reuse and new can work well including the role of adaptation, or not.

The role of adaptation is to strike a balance between number of parameters and performance. As mentioned in the end of section 3.1, we have different cost for select each option. Adaptor provides a way of using and modifying previous representation without incurring any forgetting by adding a relatively small amount of parameter overhead.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HkeheEjO3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea, but needs a stronger experimental justification</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxsS3A5Km&amp;noteId=HkeheEjO3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1572 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1572 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The proposed approach aims to mitigate catastrophic forgetting in continual learning (CL) problems by structure learning: determining whether to reuse or adapt existing parameters, or initialise new ones, when faced with a new task. This is framed as an architecture search problem, applying ideas from Differentiable Architecture Search (DARTS). The approach is verified on the Permuted MNIST dataset and evaluated on the Visual Decathlon, showing an improvement.

I think this is an interesting idea with potential, and is worth exploring, and the paper is well-structured and easy to follow.

Unfortunately, I feel the paper fails to consider recent work on CL, both in terms of discussion and benchmarking. The only previous work that is compared is EWC, on permuted MNIST, and the Visual Decathlon performance is only compared to simple baselines (such as adding an adapter or fine tuning) which makes it difficult to gauge the contribution.
There are recent works, some with better results on more difficult problems, such as Variational Continual Learning [1], Progress and Compress [2], or (Variational) Generative Experience Replay [3][4].
Given the approach is based on dynamically adding parameters or modules, Progressive Networks and Dynamically Expandable Networks (both cited) are especially relevant and should be compared (I believe the former may be related to the “adapter” baseline, but this should be made explicit).

I have some questions / discussion points:
- What's the intuition behind implementing the “adapt” operator as additive bias over the previous weights, rather than just copying the previous weights and fine tuning?
- In the general case, if the architecture search is a continuous relaxation (softmax combination of operators), why is the "adapt" operator necessary? Wouldn't this already be a linear combination of new and old parameters? (In the example case of a 1×1 adaptor it makes sense, but this is a special restricted case which adapts with a smaller set of parameters)
- How is the structure regulariser backpropagated into the parameters of each layer? As I understand, it is composed of a constant discrete term z (number of parameters in each option), multiplied by architecture softmaxes alpha; the gradient with respect to each alpha is a constant, and so this has the effect of scaling the gradients of each operator.
- For the "reuse - tuned" case, isn’t the model effectively maintaining a new network for each task?

I also have a number of other comments:
- Reference to figure in page 6 should be figure 4, not 5.
- I think the readability of the paper would benefit from another few proofreads; there are a number of grammatical issues throughout, and several sentence fragments, eg. in the top para of page 2: “..., it has the potential to encourage information sharing. Since now the irrelevant part can be handled…”.

I would encourage the authors to strengthen the experimental comparison by incorporating stronger, external baselines, and improving some of the minor writing issues.

[1] Nguyen, Cuong V., et al. "Variational Continual Learning." ICLR, 2018.
[2] Schwarz, Jonathan, et al. "Progress &amp; Compress: A scalable framework for continual learning." ICML, 2018.
[3] Shin, Hanul, et al. "Continual learning with deep generative replay." NIPS, 2017.
[4] Farquhar, Sebastian, and Yarin Gal. "Towards Robust Evaluations of Continual Learning." arXiv, 2018.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJxiRZ7A6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxsS3A5Km&amp;noteId=SJxiRZ7A6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1572 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1572 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your feedback. We have added additional results comparing our method with other more recent and relevant methods. Please refer to the updated manuscript.

Regarding the questions:
- What's the intuition behind implementing the “adapt” operator as additive bias over the previous weights, rather than just copying the previous weights and fine tuning?

The role of adaptor is to strike a balance between number of parameters and performance. As mentioned in the end of section 3.1, we have different cost for select each option. Adaptor provides a way of using and modifying previous representation without incurring any forgetting by adding a relatively small amount of parameter overhead.

- In the general case, if the architecture search is a continuous relaxation (softmax combination of operators), why is the "adapt" operator necessary? Wouldn't this already be a linear combination of new and old parameters? (In the example case of a 1×1 adaptor it makes sense, but this is a special restricted case which adapts with a smaller set of parameters)

In the adaptor case, when searching the combination of the old parameters with 1x1 conv forms an option. For example, in case we have two options, reuse and adaptor, the softweight is over the original parameter and the original parameter plus adaptor combined, so here the second part is treated as one option. To some extend what you are suggesting is true, however, this does not exactly corresponds to what is happening (as we explained above).

- How is the structure regulariser backpropagated into the parameters of each layer? As I understand, it is composed of a constant discrete term z (number of parameters in each option), multiplied by architecture softmaxes alpha; the gradient with respect to each alpha is a constant, and so this has the effect of scaling the gradients of each operator.

In our implementation, the structure regularizer does not backprop to the parameters of each layer. Instead, the regularizer serves as a penalty for different choices, and thus has effect on the magnitude of alphas. Since alpha controls the weight for different options, this would influence the choice of different options during structure learning.

- For the "reuse - tuned" case, isn’t the model effectively maintaining a new network for each task?

No. When the model is reused, the parameters are tuned, and the tuned parameter is used both for current tasks that it is finetuned on as well as all previous tasks.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>