<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>FUNCTIONAL VARIATIONAL BAYESIAN NEURAL NETWORKS | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="FUNCTIONAL VARIATIONAL BAYESIAN NEURAL NETWORKS" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rkxacs0qY7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="FUNCTIONAL VARIATIONAL BAYESIAN NEURAL NETWORKS" />
      <meta name="og:description" content="Variational Bayesian neural networks (BNN) perform variational inference over weights, but it is difficult to specify meaningful priors and approximating posteriors in a high-dimensional weight..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rkxacs0qY7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>FUNCTIONAL VARIATIONAL BAYESIAN NEURAL NETWORKS</a> <a class="note_content_pdf" href="/pdf?id=rkxacs0qY7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019functional,    &#10;title={FUNCTIONAL VARIATIONAL BAYESIAN NEURAL NETWORKS},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rkxacs0qY7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=rkxacs0qY7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Variational Bayesian neural networks (BNN) perform variational inference over weights, but it is difficult to specify meaningful priors and approximating posteriors in a high-dimensional weight space. We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions. We prove that the KL divergence between stochastic processes is equal to the supremum of marginal KL divergences over all finite sets of inputs. Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator. With fBNNs, we can specify priors which entail rich structure, including Gaussian processes and implicit stochastic processes. Empirically, we find that fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and can scale to large datasets.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">functional variational inference, Bayesian neural networks, stochastic processes</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We perform functional variational inference on the stochastic processes defined by Bayesian neural networks.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">5 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SyeUaWdyaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Summary of Contributions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkxacs0qY7&amp;noteId=SyeUaWdyaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper573 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper573 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1. We present the theorem to compute KL divergence between stochastic processes. Stochastic processes have various implications in machine learning research, including Gaussian Processes, Bayesian neural networks and so on. Furthermore, generative models including VAEs (Kingma et al, 2013) and GANs (Goodfellow et al, 2014) can also be viewed as the discretization of generated functions (of stochastic processes). Therefore, our presented theorem lays the foundation and opens up new views for many research areas.

2. With KL divergence of stochastic processes, we present two practical functional variational inference methods: the adversarial functional VI, which is closely related to the popular GAN-like training procedures; the sampling-based functional VI, which provides an uniformly maximization objective and avoids the instability of adversarial training. Our functional variational inference methods are quite general and ready to be adopted in many scenarios, including BNNs, VAEs and etc.

3. We pin out the mis-specified weight-space prior problem in BNNs. We show that BNNs can perform unpredictably even with the exact posterior inference. Alternatively, we propose fBNNs to perform functional VI over the outputs of a BNN. To the best of our knowledge, we are the first to apply functional priors and perform functional VI over BNNs. 

4. Empirically, we conduct multiple experiments to demonstrate our fBNNs’ superiority over standard BNNs
	1) Reliable uncertainties (see contextual bandits, Bayesian optimization and Figure 1).
	2) Sensible extrapolations (see extrapolating periodic structures, piecewise structures and grid texture patterns). 
	3) Predictive abilities (see regression experiments). 
	4) Scalability (see large scale regression).
	5) Model-agnostic (see the texture pattern experiment where we used CNNs).

5. Comparisons to GPs
	1) fBNNs enable fast inferences compared to GPs.
	2) fBNNs enable richer prior structures, including explicit priors such as student-t process, and implicit priors such as piecewise priors.
	3) fBNNs enable sampling coherent functions (at global level) since the weights of fBNNs can be viewed as global latent variables.


However, as a pioneering work, we don’t expect fBNNs to solve all problems, and we don’t expect fBNNs to totally outperform GPs in all aspects, which has been more than 20 years’ fruitful research. However, we pioneered a new promising research area that connects closely between deep learning networks and statistical theories. Overall, our fBNNs push forward the frontier of BNNs research. We show that it is more desirable to work on the functional-space in view of stochastic processes than working on the weight-space. It also provides new techniques to the research of more general stochastic processes.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1gT7Ic0hX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Summary of new revision</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkxacs0qY7&amp;noteId=B1gT7Ic0hX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper573 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper573 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1, We add theoretical results on the sampling-based functional VI as in Theorem 3.
2. We add the KL divergences between two conditional stochastic processes as in Appendix B.1.
3, We add multiple citations as pointed by the reviewers, including  
    (Ma et al, 2018) Variational Implicit Processes 
    (Matthews et al, 2015) On Sparse variational methods and the Kullback-Leibler divergence between stochastic processes.
    (Gal et al, 2016) Dropout as a bayesian approximation: Representing model uncertainty in deep learning.
    (Gal et al, 2017) Concrete dropout.

Later we will address reviews' concerns accordingly.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1lgR6DAnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper concerns the fitting of variational Bayesian Neural Network approximations in functional form and considering matching to a stochastic process prior implicitly via samples.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkxacs0qY7&amp;noteId=B1lgR6DAnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper573 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper573 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Overall Thoughts:

I found this paper to be very interesting and to address a topic that I think will be of interest to the community. The gap between theoretically advantageous stochastic processes like the GP and the computational efficiency of finite BNNs is a topic not yet fully understood and I believe this paper has some useful points to make. I would be very interested and grateful to hear the authors’ thoughts on the comments/questions below.

Specific Comments/Questions:

I would prefix this discussion with the fact that this is quite a dense (and long) paper on a number of topics so while I hope that I have understood the essence of the approach I apologise if I have missed something and hope the authors will be able to correct me.

I follow the point that it is less clear how finite variational deep BNNs relate to GPs and would agree that finding such an agreement would be a topic of interest. Is the approach taken in the paper not quite close conceptually to the variational sparse GP of Titsias? In that paper, effectively a functional bound is also being taken (i.e. an approximation of a full GP with another GP). So a stochastic process is approximating a full GP. In addition, the approximating GP is defined by a set of samples (the pseudo-input locations) that are optimised as variational parameters. The variational bound is defined in the function domain. There is also alignment with the Stein gradient estimation - the pseudo-input locations are used to define a Nystrom approximation to the full GP kernel. This would seem equivalent to the approximation being performed in (2) as long as the kernel used for the eigenfunctions is the same as that of the full GP. 

Following from the above, I would be very interested to directly contrast the differences to the Titsias approach - would it be possible to add it as a baseline to the experiments? In particular, there are potentially differences due to differences between the Stein kernel and the full GP kernel (it might be best to have both kernels the same if they are not already the same in all experiments - sorry, I couldn’t tell). In the variational-GP, the pseudo-input locations are optimised directly under the bound and so whilst they are sensitive to initialisation, the optimisation is stable and guaranteed to converge to a local optimum. It is unclear to me how stable the adversarial problem in Sec3.2 is. In the proposed sampling variant, it isn’t clear to me that it is a safe procedure to follow - taking a random subset from the training data and weights on c seems rather heuristic - are there any guarantees? 

There would also seem to be some connections with the recent approaches on (conditional) neural processes - perhaps the authors might like to comment on this?

For a number of the GP priors in the experiments, it might be quite hard for a BNN with ReLu activations to match the posteriors? Would it be worth trying with other (more smooth) activation functions?

Overall the results are interesting - would it be possible to include comparisons to the variational-GP (at least for the small-scale experiments)? It would be interesting to contrast their complexity as well if they get stuck on the large-scale ones. For some of the experiments would it be possible to include histograms rather than just error bars to check that the error distributions are similar?

For the appendix BayesOpt experiment - would it be possible to use Thompson sampling as the acquisition function? To my mind this would evaluate the predictive density more directly since it mitigates the effect of a particular choice of acquisition function on the performance. Also would a comparison with the full GP not be appropriate?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Ske9nqYFhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting paper on functional variational inference but more analysis is needed re the approximations</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkxacs0qY7&amp;noteId=Ske9nqYFhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper573 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper573 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a new variational inference algorithm for Bayesian neural network models where the prior is specified functionally (i.e. through a stochastic process) rather than via a prior (e.g.) over weights. The paper is motivated by the maximization of the evidence lower bound defined on stochastic processes, which is itself motivated as the minimization of the KL between the approximate posterior and the true posterior processes. 

The paper relies heavily on Th 1, which is proved in the appendix, which states that the KL divergence between two stochastic processes is equivalent to the supremum of the marginal KL divergence over all finite sets of input locations. This yields a GAN-like objective where the ELBO is minimized wrt the input subsets and maximized wrt the approximate posterior. Obviously, as the former minimization is unfeasible, the authors proposed two additional approximations: (1) Restrict the size of the subset to search for; (2) replace the mimimization step with a sampling/average procedure. From the theoretical standpoint, I believe ths is the major defficiency of the paper, as these approximations are not justified and it is not clear, theoretically, how they relate to the original objective. In fact, for example on the case of Gaussian process priors, it looks too good to be true that one can have a KL-divergence over low-dimensional distributions instead of handling N-dimensional (fully coupled) distributions. It is unclear what is lost here (whereas in well-known sparse variational methods such as that of Titsias, one knows how the sparse model relates to the original one). 

Only the first experiment compares to a GP model, where it is shown that the solution given by fBNN (which was seeded with the GP solution) is not better (if not slightly worse than the GP’s). As recommended by Matthews et al (2018), all the experiments should compare to a base GP model.

Other Comments:
The paper claims that the method estimates reliable uncertainties. However, there is not an objective evaluation of this claim (as in the predictive posteriors are well-calibrated). 
Why aren’t hyper-parameters estimated using the ELBO?
In Figure 2, why are the results so bad for BBB? This is very surprising.
How does the approach relate Variational Implicit Processes (Ma et al, 2018)?
Most of the experiments in the paper assume 1 hidden layer. In the case of deeper architectures, how can one specify a prior over functions that is “meaningful”?
Most (all?) the experiments are specific to regression. Is there any limitation for other likelihood models?
How does the approach compare to inference in implicit models?
In the intro “practical variational BNN approximations can fail to match the predictions of the corresponding GP”. Any reference for this?
I believe the paper should also relate to the work of Matthews et al (2015)


References
(Ma et al, 2018) Variational Implicit Processes 
(Matthews et al, 2018) Gaussian process behavior in wide deep neural networks
(Matthews et al, 2015) On Sparse variational methods and the Kullback-Leibler divergence between stochastic processes

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1leNQqy6m" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkxacs0qY7&amp;noteId=r1leNQqy6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper573 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJxDojhXh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting and timely contributions hampered by rushed submission</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkxacs0qY7&amp;noteId=BJxDojhXh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper573 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper573 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">-- Paper Summary --

The primary contribution of this paper is the presentation of a novel ELBO objective for training BNNs which allows for more meaningful priors to be encoded in the model rather than the less informative weight priors featured in the literature. This is achieved by way of introducing a KL measure over stochastic processes which allows for priors to take the form of GP priors and other custom variations. Two approaches are given for training the model, one inspired by GANs, and a more practical sampling-based scheme. The performance of this training scheme is validated on a variety of synthetic and real examples, choosing Bayes by Backprop as the primary competitor. An experiment on contextual bandit exploration, and an illustrative Bayesian optimisation example  provided in the supplementary material showcase the effectiveness of this method in applications where well-calibrated uncertainty is particularly pertinent.

-- Critique --

This paper makes important strides towards giving more meaningful interpretations to priors in BNNs. To the best of my knowledge, the KL divergence between stochastic processes that gives rise to an alternate ELBO has not been featured elsewhere, making this a rather interesting contribution that is supplemented by suitable theorems both in the main text and supplementary material. The introductory commentary regarding issues faced with increasing the model capacity of BNNs is particularly interesting, and the associated motivating example showing how degeneracy is countered by fBNN is clear and effective.

The GAN-inspired optimisation scheme is also well-motivated. Although the authors understandably do not pursue that scheme due to the longer computation time incurred (rendering its use impractical), it would have been interesting to see whether the optimum found using this technique is superior to the sampling based scheme used throughout the remainder of the paper. The experimental evaluation is also very solid, striking an adequate balance between synthetic and real-world examples, while also showcasing fBNNs’ effectiveness in scenarios relying on good uncertainty quantification.

In spite of the paper’s indisputable selling points, I have several issues with some aspects of this submission. For clarity, I shall distinguish my concerns between points that I believe to be particularly important, and others which are less significant:

- Monte Carlo dropout (Gal &amp; Ghahramani, 2016), and its extensions (such as concrete dropout), are widely-regarded as being one of the most effective approaches for interpreting BNNs. Consequently, I would have expected this method to feature as a competitor in your evaluation, yet this method does not even get a cursory mention in the text.

 - The commentary on GPs in the related work paints a dour picture of their scalability by mostly listing older papers. However, flexible models such as AutoGP (Krauth et al, 2017) have been shown to obtain very good results on large datasets without imposing restrictions on the choice of kernels.

 - The regression experiments all deal with a one-layer architecture, for which the proposed method is shown to consistently obtain better results. In order to properly assess the effectiveness of the method, I would also be interested in seeing how it compares against BBB for deeper architectures on this problem. Although the authors cite the results in Figure 1 as an indicator that BBB with more layers isn’t particularly effective, it would be nice to also see this illustrated in the cross-dataset comparison presented in Section 5.2.

 - Furthermore, given that all methods are run for a fixed number of iterations, it might be sensible  to additionally report training time along with the results in the table. This should reflect the pre-processing time required to optimise GP hyperparameters when a GP prior is used. Carrying out Cholesky decompositions for 1000x1000 matrices 10k times (as described in Section 5.2.2) does not sound insignificant.

- The observation regarding the potential instability of GP priors without introducing function noise should be moved to the main text; while those who have previously worked with GPs will be familiar with such issues, this paper is directed towards a wider audience and such clarifications would be helpful for those seeking to replicate the paper’s results. On a related note, I would be keen on learning more about other potential issues with the stability of the optimisation procedure, which does not seem to be discussed upfront in the paper but is key for encouraging the widespread use of such methods.

- The paper contains more than just a handful of silly typos and grammatical errors - too many to list here. This single-handedly detracts from the overall quality of the work, and I highly advise the authors to diligently read through the paper in order to identify all such issues.

 - The references are in an absolute shambles, having inconsistent citation styles, arXiv papers cited instead of conference proceedings, etc. While this is obviously straightforward to set right, I’m nonetheless disappointed that this exercise was not carried out prior to the paper’s submission.

 - The theory presented in Appendix A of the supplementary material appears to be somewhat ‘dumped’ there. Given that this content is crucial for establishing the correctness of the proposed method, linking them more clearly to the main text would improve its readability and give it a greater sense of purpose. I found it hard to follow in its current state.

** Minor **

 - In the introduction there should some mention of deep Gaussian processes which are implicitly a direct competitor to BNNs, and can now also be scaled to millions and billions of observations (Cutajar et al. 2017; Salimbeni et al. 2017). The former is particularly relevant to this work since the architecture can be assimilated to a BNN with special structure for emulating certain kernels.

 - Experiment 5.1.1 is interesting, and the results in Figure 2 are convincing. I would also be interested in seeing how fBNN performs when the prior is misspecified however, which may be induced by using a less appropriate GP kernel. This would complement the already provided insight on using tanh vs ReLU activations.

 - The performance improvement for the experiment on large regression datasets is quite subdued, so it might be interesting to see how both methods compare against each other when deeper BNN architectures are considered. 

- With regards to Appendix C.2, which order arccosine kernel is being used here? One can easily draw similarities between the first order arccosine kernel and NN layers with ReLUs, so perhaps it would be useful to specify which order is being used in the experiment.  

- Given that the data used for experiments in Appendix C.3 effectively has grid structure, I would be interested in seeing how KISS-GP performs on this task. There should be easily accessible implementations in GPyTorch for testing this out. Given how GPs tend to not work very well on image completion tasks due to smoothness in the kernel, this comparison may also be in fBNNs favour.

- Restating the basic architecture of the BNN being used for the contextual bandits experiment in the paper itself would be helpful in order to avoid having to separately check out Riquieme et al (2018) to find such details.

- I wonder if the authors have already thought about the extendability of their proposal to more complex BNN architectures such as Bayesian ConvNets?


-- Recommendation --

Whereas several ICLR submissions tend heavily towards validation by way of empirical evaluation, I find that the theoretic contributions presented in this paper are by themselves interesting and well-developed, which is very commendable. However, there are multiple telling signs of this being a rushed submission, and I am less inclined to argue ardently for such a paper’s acceptance. Although the paper indeed has its strong points, both in terms of novelty and varied experimental evaluation, in view of this overall lack of finesse and other concerns listed above, I think that the paper is in dire need of a thorough clean-up before being published.

Pros/Cons summary:

+   Interesting concepts that extend beyond empirical fixes.
+   Defining more interpretable priors is a very pertinent topic in the study of BNNs.
+   The presented ideas could potentially have notable impact.
+   Illustrative experiments and benchmark tests are convincing.
-   Not enough connection to MC dropout.
-   Choice of experiments and description of stochastic processes overly similar to other recent widely-publicised papers. It feels on trend, but consequently also somewhat reductive.
-   More than a few typos and grammatical errors.
-   Presentation is quite rough around the edges. The references are in a particularly dire state.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>