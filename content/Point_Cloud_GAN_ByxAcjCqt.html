<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Point Cloud GAN | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Point Cloud GAN" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ByxAcjCqt7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Point Cloud GAN" />
      <meta name="og:description" content="Generative Adversarial Networks (GAN) can achieve promising performance on learning complex data distributions on different types of data. In this paper, we first show a straightforward extension..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ByxAcjCqt7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Point Cloud GAN</a> <a class="note_content_pdf" href="/pdf?id=ByxAcjCqt7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019point,    &#10;title={Point Cloud GAN},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=ByxAcjCqt7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Generative Adversarial Networks (GAN) can achieve promising performance on learning complex data distributions on different types of data. In this paper, we first show a straightforward extension of existing GAN algorithm is not applicable to point clouds, because the constraint required for discriminators is undefined for set data. We propose a two fold modification to GAN algorithm for learning to generate point clouds (PC-GAN). First, we combine ideas from hierarchical Bayesian modeling and implicit generative models by learning a hierarchical and interpretable sampling process. A key component of our method is that we train a posterior inference network for the hidden variables. Second, PC-GAN defines a generic framework that can incorporate many existing GAN algorithms. We further propose a sandwiching objective, which results in a tighter Wasserstein distance estimate than the commonly used dual form in WGAN. We validate our claims on ModelNet40 benchmark dataset. PC-GAN trained by the sandwiching objective achieves better results on test data than the existing methods by comparing with true meshes quantitatively. We also conduct studies on several tasks, including generalization on unseen point clouds, latent space interpolation, classification, and image to point clouds, to demonstrate the versatility of the proposed PC-GAN.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Point Cloud, GAN</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a GAN variant which learns to generate point clouds. Different studies have been explores, including tighter Wasserstein distance estimate,  conditional generation, generalization to unseen point clouds and image to point cloud.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_H1e8c5rL3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The paper may contain interesting ideas, but lacks clarity, and might have issues with experimental evaluation.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxAcjCqt7&amp;noteId=H1e8c5rL3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper579 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper579 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
This paper introduces a generative model for 3D point clouds. Authors aim at theoretically showing the difficulties of using existing generative models to learn distributions of point clouds, and propose a variant that supposedly solves the issues.

Pros:
+ The problem of designing generative models for 3D data is important.

Cons: 
- Paper is often hard to follow, and contains a significant number of typos. 
- Authors claim to identify a fundamental problem with the existing generative models for point clouds, yet Section 2 tries to show that a _specific version_ that uses DeepSet does not satisfy theoretical guarantees. What if we use e.g. a recurrent network instead? As is, the counter example proof itself is quite confusing: it would really help if the proof was more formal.
- Jointly learning an inference network (Q) has certainly been done before, and I am not sure authors provide an elaborate enough explanation of what is the difference with adversarially learned inference /  adversarial feature learning.
- It is not clear why authors did not follow the evaluation protocol of [Achlioptas’17] or [Wu’16] more closely. In particular, evaluation for the classification task should be compatible with the proposed model, which would give a much better picture of the learned representations.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SyeS7CQ83m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Generative model for point clouds, based on local-global latent variables.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxAcjCqt7&amp;noteId=SyeS7CQ83m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper579 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper579 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
This paper proposes a generative point cloud model based on adversarial learning and definitti’s representation theorem of exchangeable variables.
The main focus in experiments and the exposition is on 3D point clouds representing object shapes (seems the surface, but could also be the interior of objects, please clarify). 
The main idea is to represent a point cloud using a global latent variable that captures the overall shape, and a collection of local latent variables that code for the position of a point on the shape.
The model consists of thee components:
(i) an “encoder” that takes a point cloud as input and maps it to a (point estimate of) the global latent variable of the shape represented by the input cloud, a point-net architecture is used here
(ii) a “decoder” that takes the estimated global latent variable, and a local latent variable, and maps it to an “output” point in the cloud to be produced by the model. 
(iii) a “discriminator” network that aims to distinguish points from a *given* shape, and the points produced by pipe-lining the encoder and decoder. Critically different from conventional GANs, the discriminator is optimized *per shape*, ie each point cloud is considered as a *distribution* over R^3 specific to that shape. 
(iv) a “shape prior” that, once the encoder-decoder model from above is trained, is used to model the distribution over the global latent variables. This model is trained, presumably, in a conventional GAN style using the global latent variable representations inferred across the different training point clouds.

As compared to prior work by Achiloptas et al (2017), the proposed approach has the advantage to allow for sampling an arbitrary number of points from the target shape, rather than a fixed pre-defined number. 

In addition, the authors propose to minimize a weighted average of a lower bound and upper bound on the Wasserstein distance between the distributions of points corresponding to given shapes. 
This approach translates to improved quantitative evaluation measures, 

Experiments are conducted on a simple toy data set, as  a proof of concept, and on data from ModelNet10 and ModelNet40. 
Two performance metrics are introduced to assess the auto-encoding ability of the model: to what extent does the encoder-decoder pipeline result in point clouds similar to the shape from which the input point-cloud is generated. 

Overall I find the idea of the paper interesting and worth publishing, but the exposition of the paper is less than ideal and needs further work. 
The experimental validation of the proposed approach can also be further improved, see more specific comments below. 

Specific comments:

- The counter example at the bottom of page 2 is limited, in the sense that the oracle assumption seems highly non-realistic, casting doubt on the relevance of the argument.

- The notation in section 3 (before 3.1) is rather sloppy. 
For example, 
- please define P and G, the elements of the divergence D(P||G) that appears in the first paragraph of section 3.
- it is not defined in which space theta lives, it is not clear what the authors intend with the notation G_theta(u) \sim p(theta). 
- what prior distributions p(z) and p(u) are used? What is the choice based on?

- abbreviation IPM is referred several times in the paper, but remains undefined in the paper until end of page 4, please define earlier. 

- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?

- Lack of clarity in the following passage: “In our setting, each point xi in the point cloud can be considered to correspond to single images when we train GANs over images”

- The notion of divergence D(P|G) is not made concrete in section 3 and 3.1, which makes the notation of rather little use.

- The following paper merits a discussion in the related work section: 
“TOWARDS A NEURAL STATISTICIAN”, ICLR’17, <a href="https://openreview.net/pdf?id=HJDBUF5le" target="_blank" rel="nofollow">https://openreview.net/pdf?id=HJDBUF5le</a>

- The manuscript contains many typos. For example
 “vedio” op page 4, “circile” on page 5, “condct” on page 8, etc.
Please proof read your paper and fix these.
The refenence to  Bengio 2018 is incomplete: what do you refer to precisely?

- There seems to be no mention of the dimension of the “local” latent variables z_i. 
Please comment on the choice, and its impact on the behavior of the model.

- The quantitative evaluation in table 1 is interesting and useful. 
It is limited, however, in the sense that it (only) measures auto-encoding capabilities: to what extent can the shape be reproduced given a sample point cloud from the given shape. 
Quantitative evaluation of generative modeling performance is unfortunately missing from this paper, as it is in much of the GAN literature. 
Could you please comment on how this can/will be fixed?

- The toy data set experiments could be dropped  to make room for experiments suggested below.

- An experimental study of the effect of the mixing parameter “s” would be useful to include. 
For example, by taking s on a grid from 0 to 1, one could plot the coverage and distance-to-face measures.

- Experimental evaluation of auto-encoding using a variable number of input points is interesting to add: ie how do the two evaluation measures evolve as a function of the number of points in the input point cloud?

- Similar, it is interesting to evaluate how auto encoding performs when non-uniform decimation of the input cloud is performed, eg what happens if we “chop off” part of the input point cloud (eg the legs of the chair), does the model recover and add the removed parts? This is potentially useful to practitioners which have to deal with incomplete point clouds acquired by range scanners. 

- Analysis of shapes with different genus and dimensions would be interesting. 
Does the model manage to capture that some shapes have holes, or consists of a closed 2D surface (ball) vs an open surface (disk),  despite a simple prior on the local latent variables z?

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1eZXehVnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A variant of WGAN, suppressing bias but leading to worse stability when estimating Wasserstein distance</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxAcjCqt7&amp;noteId=H1eZXehVnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper579 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper579 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Authors provide a variant of WGAN, called PC-GAN, to generate 3D point clouds. The drawback of a vanilla GAN with a DeepSet classifier is analyzed. The rationality that decoupling the point generator with the object generator is also discussed. 
A sandwiching objective function is proposed to achieve a better estimation of Wasserstein distance. 
Compared with AAE and the simplified variants of the proposed PC-GAN, the proposed PC-GAN achieves incremental results on point cloud generation.

Comments:
1. Authors calculate W_U in a primal form via solving an assignment programming problem. Have authors ever tried Sinkhorn iteration? To my knowledge, sinkhorn iteration is a very popular method to solve OT problem effectively. It would be nice if authors can provide some reasons and comparisons for their choice on the optimizer of W_U. 

2. Authors proved that the sandwiching object W_s is closer to the real Wasserstein distance, but it increases the variance of the loss function. Specifically, the dynamics of W_U, and W_L, according to lemma1, is (epsilon2-epsilon1)*w(P, G) while the dynamics of W_s is 2*epsilon1 * w(P, G), and 2epsilon1 &gt; epsilon2 - epsilon1 (according to the assumption in lemma 1). Does it mean that the W_s is not as stable as W_L or W_U during training?  Additionally, authors combined W_U with W_L with a mixture 20:1, i.e., the s in Eqs(6, 13, 14) is smaller than 0.05. In such a situation, both the value and the dynamics of W_s will be very close to that of W_U. Does it mean that W_L is not so important as W_U? Authors should analyze the stability of their method in details.

Essentially, the proposed method is a variant of WGAN, which estimates Wasserstein distance with lower bias but may suffer from worse stability. In the experiments, both the setting and the experimental results show that the proposed W_s will be very close to W_U. As a result, the improvement caused by the proposed method is incremental compared with its variants. 

Typos:
- The end of the 2nd line of lemma 1: P, G should be \mathbb{P}, \mathbb{G}
- The 3rd line of lemma 1: epsilon1 -&gt; epsilon_1
- Page 14, Eq(14), \lambda should be s
- Page 14, Eqs(13, 14), w(\mathbb{P}, \mathbb{G}) should appear on the right.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BygRsWmBcQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Comment on pg 2 counter example</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxAcjCqt7&amp;noteId=BygRsWmBcQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Oct 2018</span><span class="item">ICLR 2019 Conference Paper579 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I don't think the counter example presented at the bottom of page 2 supports the conclusion that: "simply using DeepSets classifier without any constraints in simple GAN in order to handle sets does not lead to a valid generative model."

Specifically, introducing the deterministic transformation T({x_i}) artificially precludes the discriminator from observing vital information about how points are distributed within each set of points passed to the discriminator. By construction, this transform removes all information from the point sets aside from the object parameter(s) theta which could have generated each point in the set. But, to determine if the sets of points faithfully match the conditionals p(x|theta) in the true joint p(x, theta)=p(theta)p(x|theta), the discriminator requires finer-grained information about the points in these sets.

I agree that, with access only to the "source object" theta for each point in a set, the discriminator can only force the generator to generate point sets that (i) include points which are mapped to the same theta by T({x_i}), and (ii) match the true marginal p(theta) over objects. I disagree that this failing similarly affects a general DeepSets-style discriminator that isn't constrained to observe the point sets through this T({x_i}). At least, this point is not made by the argument as presented.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bye2CkeKqQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Great question</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxAcjCqt7&amp;noteId=Bye2CkeKqQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper579 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Oct 2018</span><span class="item">ICLR 2019 Conference Paper579 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank you for raising an excellent question about counter example. The purpose of the counter example is only to show that there exists some spurious solutions to GANs with general DeepSets-style discriminator for point clouds. We agree that setup we selected is destined to fail, but it was done on purpose to illustrate the presence of spurious solutions. A good generator and discriminator would definitely be a solution as well. However, solutions during optimization might not always correspond to such good solutions and can also correspond to the demonstrated spurious solutions. This is precisely the point made by counter example argument, that simple DeepSets classifier does not ensure proper generative model. Moreover, we found empirically that GAN with simple DeepSet-like discriminator most of the times fails to learn to generate point clouds even after converging. However, sometimes it does results in reasonable generations (although worse than proposed PC-GAN), which may correspond to the case you describe. So the message here is that we need additional constraints for GANs with simple DeepSet-like discriminator to exclude such bad solutions and lead to a more stable training.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ryeUxneAY7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>previous works on geometric deep learning</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxAcjCqt7&amp;noteId=ryeUxneAY7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Michael_Bronstein1" class="profile-link">Michael Bronstein</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Sep 2018</span><span class="item">ICLR 2019 Conference Paper579 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The authors should be aware of a large amount of works done on the intersection of ML, CG and CV fields on geometric deep learning on graphs and manifolds that are directly related to their work. In [1], the first intrinsic CNN-like architecture was proposed for 3D shapes, further extended in [2-4]. In [5], the first intrinsic autoencoder for 3D shapes was proposed. Most recent works applying graph-based deep learning techniques to point clouds are [7-8]. Finally, the authors may refer to a review paper [8] on geometric deep learning methods. 


1. Geodesic convolutional neural networks on Riemannian manifolds, ICCV Workshops 2015. 

2. Learning shape correspondence with anisotropic convolutional neural networks, NIPS 2016. 

3. Anisotropic diffusion descriptors, Computer Graphics Forum 35(2):431-441, 2016.

4. Geometric deep learning on graphs and manifolds using mixture model CNNs, CVPR 2017. 

5. Deformable shape completion with graph convolutional autoencoders, CVPR 2018.

6. Dynamic Graph CNN for learning on point clouds, arXiv:1712.00268

7. Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs, 1711.09869

8. Geometric deep learning: going beyond Euclidean data, IEEE Signal Processing Magazine, 34(4):18-42, 2017
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BylnKVcMqQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for your comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxAcjCqt7&amp;noteId=BylnKVcMqQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper579 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Oct 2018</span><span class="item">ICLR 2019 Conference Paper579 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank you for pointing out some of the recent results of applying deep learning on point cloud for classification and segmentation by leveraging graph convolution. Main focus of our work is to design a generative model for point cloud and thus many of these papers were purposefully not included in related works for being concise. However, it might be a good idea to provide a more exhaustive background of on geometric deep learning and we would definitely discuss the related papers you mention above in next revision of the paper.

Moreover, some of the architectures presented in the mentioned papers, like the point cloud autoencoder with graph convolution may be an alternative choice of Q in addition to DeepSets in the proposed PC-GAN framework, but again is not the focus of our paper. We want to develop a generic framework for generation of point clouds or set data in general, thus employed the most simplistic components like DeepSets. Using the more advanced architectures (e.g. Wang et al., (2018) or PointNet++) should bring in improvements and can be an interesting future direction. We will discuss these aspects in the next revision of the paper. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>