<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Unsupervised Document Representation using Partition Word-Vectors Averaging | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Unsupervised Document Representation using Partition Word-Vectors Averaging" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HyNbtiR9YX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Unsupervised Document Representation using Partition Word-Vectors..." />
      <meta name="og:description" content="Learning effective document-level representation is essential in many important NLP tasks such as document classification, summarization, etc. Recent research has shown that simple weighted..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HyNbtiR9YX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Unsupervised Document Representation using Partition Word-Vectors Averaging</a> <a class="note_content_pdf" href="/pdf?id=HyNbtiR9YX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019unsupervised,    &#10;title={Unsupervised Document Representation using Partition Word-Vectors Averaging},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HyNbtiR9YX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=HyNbtiR9YX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Learning effective document-level representation is essential in many important NLP tasks such as document classification, summarization, etc. Recent research has shown that simple weighted averaging of word vectors is an effective way to represent sentences, often outperforming complicated seq2seq neural models in many tasks. While it is desirable to use the same method to represent documents as well, unfortunately, the effectiveness is lost when representing long documents involving multiple sentences. One reason for this degradation is due to the fact that a longer document is likely to contain words from many different themes (or topics), and hence creating a single vector while ignoring all the thematic structure is unlikely to yield an effective representation of the document. This problem is less acute in single sentences and other short text fragments where presence of a single theme/topic is most likely. To overcome this problem, in this paper we present PSIF, a partitioned word averaging model to represent long documents. P-SIF retains the simplicity of simple weighted word averaging while taking a document's thematic structure into account. In particular, P-SIF learns topic-specific vectors from a document and finally concatenates them all to represent the overall document. Through our experiments over multiple real-world datasets and tasks, we demonstrate PSIF's effectiveness compared to simple weighted averaging and many other state-of-the-art baselines. We also show that PSIF is particularly effective in representing long multi-sentence documents. We will release PSIF's embedding source code and data-sets for reproducing results.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Unsupervised Learning, Natural Language Processing, Representation Learning, Document Embedding</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A simple unsupervised method for multi-sentense-document embedding using partition based word vectors averaging that achieve results comparable to sophisticated models.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rJxruoSypm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A very well written paper with solid technical contribution.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyNbtiR9YX&amp;noteId=rJxruoSypm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper421 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper421 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">A very well written paper with solid technical contribution. The impact to the community might be incremental.

Pros:
1. I enjoyed reading the paper, very well written, clean, and organized.
2. Comprehensive literature survey, the authors provided both enough context for the readers to digest the paper, and well explained how this work is different from the existing literature.
3. Conducted extensive experiments.

Cons (quibbles):
Experiments:
The authors didn't compare the proposed method against topic model (vanilla LDA or it’s derivatives discussed in related work). Because most topic models could generate vector representation for document too, and it's interesting to learn additional benefit of local context provided by the word2vec-like model.

Methodology:
1. About hyperparameters:
a. Are there principled way/guideline of finding sparsity parameters k in practice?
b. How about the upper bound m (or K, the authors used both notation in the paper)?

2. About scalability:
How to handle such large sparse word vectors, as it basically requires K times more resource compared to vanilla word2vec and it's many variants, when it’s used for other large scale downstream cases? (see all the industrial use cases of word vector representations)

3. A potential alternative model: The motivation of this paper is that each word may belong to multiple topics, and one can naturally extend the idea to that "each sentence may belong to multiple topics". It might be useful to apply dictionary learning on sentence vectors (e.g., paraphrase2vec) instead of on word vectors, and evaluate the performance between these two models. (future work?)

Typos:
The authors mentioned that "(Le &amp; Mikolov, 2014) use unweighted averaging for representing short phrases". I guess the authors cited the wrong paper, as in that paper Le &amp; Mikolov proposed PV-DM and PV-DBOW model which treats each sentence as a shared global latent vector (or pseudo word).
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyxrQI1ga7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author response to AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyNbtiR9YX&amp;noteId=SyxrQI1ga7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper421 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper421 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Apologies for the delay in response. We would like to thank the reviewers for evaluating our manuscript. We have tried to address all the reviewers’ concerns in a proper way and believe that our paper has improved. We would be happy to make further corrections and look forward to hearing from you soon. We respond to the questions and concerns in the following points:

Experiments: We have compared our approach with the LDA topic modelling representation for the 20NewsGroup dataset (see Table 5). We will add the LDA baseline for the Reuters dataset as well.

Methodology: The method is sensitive to the total number of clusters (K), which we tune using cross-validation. However, it is not very sensitive to k (#non-zero entry parameter) as the number of senses of words is limited. For our experiments, we take k equal to the total_clusters of clusters (K) divided by 10, which is a good approximation. The upper bound depends on two things a) average length of documents in the corpus b) number of diverse different themes (or topics) in the corpus. For both the long document corpus and for the diverse themes corpus, we require larger K. For all our experiments we tune the value of the hyper-parameter K.

Scalability: The dimensions of the word topic vectors are K x d (K= #topics, d= #word2vec dimensions), which is larger than the vanilla word vector dimension (d). However, for each word only k (sparsity parameter) values are active, therefore our representation is very sparse. Thus, we can store them in a sparse format and the can perform a sparse operation for speed up. For downstream tasks where a continuous low dimensional representation is required, we can perform a lower dimension manifold learning over the learnt word topic vectors. We are currently delving into the details of the effective approaches to learn this lower dimensional manifold for higher dimension sparse vectors.

A potential alternative model: We thank the reviewer for proposing an alternative model. We will investigate more about the proposed idea. Some immediate drawbacks in the proposed model are a) the assumption that each sentence consists of only single topics and b) each sentence is represented in the same d dimension vector space as the vanilla word vectors. In P-SIF we did not make these assumptions. As suggested, we will compare our results with the proposed baseline

We have corrected the typos.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SyxGNMhchQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The work is incremental even though the experimental results are good and the method is well presented. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyNbtiR9YX&amp;noteId=SyxGNMhchQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper421 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper421 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
Pros:
The paper shows that we could have a better document/sentence embedding by partitioning the word embedding space based on a topic model and summing the embedding within each partition. The writing and presentation of the paper are clear. The method is simple, intuitive, and the experiments show that this type of method seems to achieve state-of-the-art results on predicting semantic similarity between sentences, especially for longer sentences. 

Cons:
The main concern is the novelty of this work. The method is very similar to SCDV (Mekala et al., 2017). The high-level flow figure in appendix H is nearly identical as the Figure 1 and 2 in Mekala et al., 2017. The main difference seems to be that this paper advocates K-SVD (extensively studies in Arora et al. 2016) as their topic model and SCDV (Mekala et al., 2017) uses GMM. 
However, in the semantic similarity experiments (STS12-16 and Twitter15), the results actually use GMM. So I suppose the results tell us that we can achieve state-of-the-art performances if you directly combine tricks in SIF (Arora et al., 2017) and tricks in SCDV (Mekala et al., 2017).
In the document classification experiment, the improvement looks small and the baselines are not strong enough. The proposed method should be compared with other strong unsupervised baselines such as ELMo [1] and p-mean [2].

Overall:
The direction this paper explores is promising but the contributions in this paper seem to be incremental. I suggest the authors to try either of the following extensions to strengthen the future version of this work. 
1. In addition to documentation classification, show that the embedding is better than the more recent proposed strong baselines like ELMo in various downstream tasks.
2. Derive some theories. One possible direction is that I guess the measuring the document similarity based on proposed embedding could be viewed as an approximation of Wasserstein similarity between the all the words in both documents. The matching step in Wasserstein is similar to the pooling step in your topic model. You might be able to say something about how good this approximation is. Some theoretical work about doing the nearest neighbor search based on vector quantization might be helpful in this direction.

Minor questions:
1. I think another common approach in sparse coding is just to apply L1 penalty to encourage sparsity. Does this K-SVD optimization better than this L1 penalty approach? 
2. How does the value k in K-SVD affect the performances?
3. In Aorora et al. 2016b, they constrain alpha to be non-negative. Did you do the same thing here?
4. How important this topic modeling is? If you just randomly group words and sum the embedding in the group, is that helpful?
5. In Figure 2, I would also like to see another curve of performance gain on the sentences with different lengths using K-SVD rather than GMM.
 
Minor writing suggestions:
1. In the 4th paragraph of section 3, "shown in equation equation 2", and bit-wise should be element-wise
2. In the 4th paragraph of section 4, I think the citation after alternating minimization should be Arora et al. 2016b and Aharon et al. 2006 rather than Arora et al., 2016a
3. In the 2nd paragraph of section 6.1, (Jeffrey Pennington, 2014) should be (Pennington et al., 2014). In addition, the author order in the corresponding Glove citation in the reference section is incorrect. The correct order should be Jeffrey Pennington, Richard Socher, Christopher D. Manning.
4. In the 3rd paragraph of section 6.1, "Furthermore, Sentence"
5. In the 6th paragraph of section 6.1, I thought skip-thoughts and Sent2Vec are unsupervised methods.
6. In Table 2 and 3, it would be easier to read if the table is transposed and use the longer name for each method (e.g., use skip-thought rather than ST)
7. In Table 2,3,4,5, it would be better to show the dimensions of embedding for each method
8. Table 10 should also provide F1
9. Which version of GMM is used in STS experiment? The one using full or diagonal covariance matrix? 


[1] Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., &amp; Zettlemoyer, L. (2018). Deep contextualized word representations. NAACL
[2] Rücklé, A., Eger, S., Peyrard, M., &amp; Gurevych, I. (2018). Concatenated p-mean Word Embeddings as Universal Cross-Lingual Sentence Representations. arXiv preprint arXiv:1803.01400.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rklLsLyeaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author response to AnonReviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyNbtiR9YX&amp;noteId=rklLsLyeaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper421 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper421 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Apologies for the delay in response. We would like to thank the reviewer for evaluating our manuscript. We have tried to address the reviewer concerns in a proper way and believe that our paper has improved. We would be happy to make further corrections and look forward to hearing from you soon. We respond to the questions and concerns in the following points:

1. We agree that the methods draw inspiration from the SCDV (Mekala et al., 2017). However, there are major differences between SCDV (Mekala et al., 2017) and this work:

a) SCDV (Mekala et al., 2017) uses GMM whereas P-SIF uses k-svd (Arora et al. 2016) for topic modelling. It should be noted that we did not apply manual hard thresholding over final documents representation as the method in SCDV did. Because of k-svd we implicitly put the sparsity constraint during clustering. This yields several other benefits such as sparser documents thus reducing space and time complexity. For single sentence datasets (STS 12-16 and Twitter15), we found out that both GMM and k-svd work really well. GMM works slightly better as it is much easier to optimize compared to k-svd. We also noted for these datasets the total number of clusters is small. However, for datasets containing multiple sentences such as 20NewsGroup and Reuters (200-500-word documents), k-svd outperforms GMM a.k.a P-SIF outperforms SCDV. Additionally, using k-svd leads to a fewer number of total clusters and hence fewer dimensions of document vectors for better representations. As a result, the feature formation, training and prediction time are faster. We will add time and space complexity results in the paper as well.

b) We used the SIF weighting and common component removal in P-SIF, whereas SCDV used tf-idf. SIF (Arora et. al. 2017) has shown the benefit of using such weighting and common component removal. We have successfully generalized SIF (Arora et. al. 2017) using the ideas from the SCDV paper. Additionally, SCDV used SGNS-initialised word vectors, whereas P-SIF used Doc2VecC-initialised word vectors. Also, in Doc2VecC the averaging is based on sentence representation and training of word vectors is done jointly with corruptions. This kind of training with corruption results in zeroing of common words' word vectors. However, since our approach is about partition based averaging such as zeroing, we can yield more robust document representation. We will add more downstream tasks which were requested by the reviewer as well. It should be noted that we have more thorough experiments on 26 STS similarity and 2 text classification datasets of the SCDV paper (28 in total). 

The proposed baselines in document classification are taken from a very recently published paper (2017-2018). As suggested by reviewer we will add more baselines such as the Elmo [1] and p-mean [2] for text classification. We didn't find the baselines for our reported datasets, but we have found the code to run on our datasets.

Minor Questions:
1. Yes, but directly optimising the L1 is an NP-hard objective. So k-svd does an alt-min (Arora et al. 2016b and Aharon et al. 2006) between clustering and thresholding to achieve the require sparsity.
2. We keep the k small, unlike other normal k-svd applications as we know that in text each word has a very limited number (&lt; 5) of total senses. We use the procedure similar to (Arora et al. 2016b) to choose the optimal k. For our experiments, we take k equal to the total_clusters of clusters (K) divided by 10, which is a good approximation.
3. Yes, if we randomly average words in same clusters and analyze the top dominant words in the clusters, we observe words with similar meanings are close to each other. A similar observation was reported by (Mekala et al., 2017) and (Arora et al. 2016b).
4. We will plot the point. Our institution is that initially for small length documents GMM and K-svd perform equally well (GMM may be slightly better due to easier optimization) but later for long length documents k-svd will easily outperform GMM with a fewer lesser number of clusters as reported in the text classification. 
5. We used the full covariance matrix of the GMM in our paper.

We thank the reviewer for providing helpful directions for theoretical derivations 
We were having doubts about how to proceed with the theoretical analyzes. The idea of measuring the document similarity based on P-SIF embedding and viewing it as an approximation of Wasserstein similarity between all the words in both documents seem interesting. As stated, the matching step in Wasserstein is indeed similar to the pooling step in our topic model. We will also delve into the theoretical work on doing the nearest neighbour search on vector quantizations. Thanks for your suggestion. We pinned down atleast one relevant paper: <a href="http://proceedings.mlr.press/v37/kusnerb15.pdf" target="_blank" rel="nofollow">http://proceedings.mlr.press/v37/kusnerb15.pdf</a> .

Minor writing corrections
We made the correction suggested in the revised version. We didn't transpose the table due to page limitation.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJxkA-Zea7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Sounds good</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyNbtiR9YX&amp;noteId=HJxkA-Zea7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper421 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper421 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the responses.

I think one of my concerns is not addressed very well. When you concatenate the average word embedding from different clusters, your number of dimension increases. If you have many clusters, your number of dimension might be much larger than other types of embedding. It might not be very fair to compare the performance without showing the number of dimensions in each table.
My 4th question in the minor questions section is also about this concerns, but I do not understand your response. My question is that if you do not perform clustering, you just randomly assign/hash each word to a group/bin and then average the words inside the group and bin. If this simple baseline could also improve the performance on those STS or multiclass classification problem, it might imply that P-SIF improves from SIF mostly because it uses more dimensions.

By the way, I think you also forgot to reply my 3rd minor question (the non-negativity constraint).

Overall:
If you could clarify the above dimension concern and either
1) have some significant theoretical contributions or 
2) show that P-SIF as pretraining is better than ELMo and p-mean on document classification in Table 4 and 5, and on 1 more downstream task (do not include semantic similarity prediction), 
I will vote for acceptance. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkl_3oB5Tm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author responce to AnonReviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyNbtiR9YX&amp;noteId=rkl_3oB5Tm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper421 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper421 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Apologies for the delay in response. Thanks for your patience. 

It is true that the average size of our embedding is much larger than many other embeddings in the baseline but P-SIF embedding and other baselines embeddings are all derived from similar lower dimensional (200D) unsupervised trained word vectors. The difference in performance is due to the difference in composition operation rather than higher dimensionality. The reported baseline is the best performance embedding taken from the original paper which is reported after fine-tuning. We did not achieve any improvements in the baselines with further increase in the embedding dimensions, e.g. the tf-idf weighted average word vectors with 2000 dimensions only yields 0.2% improvement on 20NewsGroup dataset.

If we randomly assign/hash each word to a random group/bin (with equal bin probability) and then average the words inside each group/bin, we wouldn't get any better results than the averaging of the Doc2VecC initialized word vectors directly without any hashing (reported as the Doc2veC baseline in the paper). 

We present results for three random runs on the 20NewsGroup dataset. The dimensions of the embedding are similar to our embedding, i.e., 200 x 40 = 8000. You can access the results here: <a href="https://goo.gl/KbGR81" target="_blank" rel="nofollow">https://goo.gl/KbGR81</a> (Table 14)

As expected, the result is similar to that of the Doc2VecC average reported in our paper. 

Our main idea of clustering the word-vectors was to ensure that similar meaning words get mapped/hashed/assigned to a similar cluster. Averaging of any linear combination of word-vectors within this cluster will always keep the resultant vector close to the original cluster. If we randomly assign/hash each word to a group/bin and then average the words inside each bin, this means that we will average very dissimilar words' word-vectors with different meanings. The produced average vectors will not necessarily fall in the same cluster. Thus, we end up with the averaging of random words' word vectors which results in random word vectors in which the meanings are dissimilar to the original words. 

We did not set any explicit constraint for the positivity of the alpha_i. Arora et. al. 2016b also did not put any non-negativity constraints. In fact, we observe sparse coding procedures assigned non-negative values to most coefficients alpha_i, even if they are unrestricted. Perhaps, this is because of the fact that the appearances of a word are best explained with the help of the theme/topic being used to generate it, rather than the theme/topic is not being used. A similar observation was reported by Arora et. al. 2016b, please refer to sec 5 Experiments with Atom of Discourse 5th paragraph. We have mentioned this point in our paper, please refer sec 5 PSIF Discussion, Sparse Dictionary Learning vs. Fuzzy Clustering. 

As suggested by the reviewer, we ran experiments on 20NewsGroup dataset for the ELMO and P-mean Univeral Embedding baselines. For p-mean we tried various values and combinations of p (as reported), we also checked the effect of z-normalization and tf-idf weighting.  You can access the results here: https://goo.gl/KbGR81 (Table 15)

For EMLO, we ran the experiments on 20NewsGroup using the allennlp package (https://allennlp.org/elmo) with the original model. We used the default options and weight file (code package: https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md)

We tried averaging and concatenation of multiple layers. We also run the experiments by only using the top layer. In addition, we tried our experiments based on the whole document words as context as well as each sentence as a context.  You can access the results here: https://goo.gl/KbGR81 (Table 16)

We are still running ELMO and P-means experiments on the Reuters dataset. It is taking more time than what was expected because we are employing a one vs rest multi-label classification approach (~90 classifiers) and the documents also have more words than 20NewsGroup.

For the STS 12-16 task, we will include and discuss some more baseline in Table 2 from the following two recent survey papers: https://goo.gl/aLTiS5 and https://goo.gl/PjjBRD . P-SIF is able to outperform most of these baselines. We were able to outperform most of the baseline and perform similarly to State-of-the-art u-SIF from http://aclweb.org/anthology/W18-3012 .  You can access the results here: 
 https://goo.gl/KbGR81 (Table 17)

We will update the submission draft with new results in the next revision.

We are working on a strong and significant proof while taking inspiration from the generative gaussian random walk model in (Arora et. al. 2016b) and simple random walk model in (Arora et. al. 2016a) for word vectors representations and its application to prove SIF averaging model as in (Arora et. al. 2017).  We are still improving the proof. You can access our proof sketch here: https://goo.gl/fHn1rN .</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJlh3pvcpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The experimental results look promising but theoretical analysis is not significant yet</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyNbtiR9YX&amp;noteId=SJlh3pvcpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper421 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper421 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">If I understand it correctly, the proof sketch basically just says that we can assign each word to cluster with some probability (Arora et. al. 2016b). After fixing the probability, we could view the document as K bags of words and use the same analysis in Arora et. al. 2017 to justify the weighted average behavior within each bag. This is not significant. One example of significant results is to show that this two-stage process (estimating co-efficient and estimating embedding) actually optimizes a unified objective function (in an approximated way?). 

Minor things in your proof sketch:
I think your notations are not consistent and confusing. For example, you sometimes use m to represent K. alpha(w) in (6,7) and alpha seems to be the same thing, but alpha_(w,j) is different. Your p(w) seems to mean global frequency of w in the whole document sometimes and the frequency of w in each bag/partition sometimes. Do arrow{c_j} in (9,10,12) and arrow{V_{c_j}} mean the same thing? Your footnote 11 comes from Arora et. al. 2017, but I think it makes more sense to use argmax instead of max here.


P-SIF is much better than ELMo and p-mean in 20NewsGroup, which is pretty nice. I look forward to the results on Reuters and 1 more downstream task.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_Hkeha2EqnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>review of Unsupervised Document Representation using Partition Word-Vectors Averaging </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyNbtiR9YX&amp;noteId=Hkeha2EqnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper421 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper421 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Paper overview: The paper extends the method proposed by Arora 2017 for sentence embeddings to longer document embeddings. The main idea is that, averaging word embedding vectors mixes all the different topics on the document, and therefore is not expressive enough. Instead they propose to estimate the topic of each word (using dictionary learning) through the $\alpha$ weights (see page 4).These weights give "how much" this word belongs to a certain topic. For every topic we compute the $\alpha$-weighted vector of the word and  concatenate them (see word topic vector formation). Finally, we apply SIF (Arora 2017) using these word embeddings on all the document.   

Questions and remarks:
     1) How sensitive is the method to a change in the number of topics (k)?
    2) Please provide also the std instead of just the average performance, so that we can understand if the differences between methods are significantly meaningful
 
Points in favor: 
   1) Good results and thorough tests 
    2) Paper is easy to read and follow 

Points against:
A very similar method was already proposed by Mekala 2017, as the authors acknowledge in section 7. The main difference between the two methods is that Mekala et al use GMM and the authors of the present paper use sparsity method K-SVD to define the topics. 


The novelty of the paper is not enough to justify its acceptance at the conference.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJgH7vkxam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author response to AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyNbtiR9YX&amp;noteId=HJgH7vkxam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper421 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper421 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Apologies for the delay in response. We would like to thank the reviewers for evaluating our manuscript. We have tried to address all the reviewers’ concerns in a proper way and believe that our paper has improved. We would be happy to make further corrections and look forward to hearing from you soon. We respond to the questions and concerns in the following points:

1. The method is sensitive to the total number of clusters (K), which we tune using cross-validation. However, it is not very sensitive to k (#non-zero entry parameter) as the number of senses of a word is limited.

2. We will provide the std instead of only providing the average performance so that we can understand if the differences between these methods are significantly meaningful. From our preliminary experiments, we found out that std-values are in 10^-3 order. Thus the results are robust.

1. We agree that the methods draw inspiration from the SCDV (Mekala et al., 2017). However, there are significant differences between SCDV (Mekala et al., 2017) and this work:

a) SCDV (Mekala et al., 2017) uses GMM whereas P-SIF uses k-svd (Arora et al. 2016) for topic modelling. It should be noted that we did not apply manual hard thresholding over final documents representation as the method in SCDV did. Because of k-svd we implicitly put the sparsity constraint during clustering. This yields several other benefits such as sparser documents thus reducing space and time complexity. For single sentence datasets (STS 12-16 and Twitter15), we found out that both GMM and k-svd work really well. GMM works slightly better as it is much easier to optimize compared to k-svd. We also noted for these datasets the total number of clusters is small. However, for datasets containing multiple sentences such as 20NewsGroup and Reuters (200-500-word documents), k-svd outperforms GMM a.k.a P-SIF outperforms SCDV. Additionally, using k-svd leads to a fewer number of total clusters and hence fewer dimensions of document vectors for better representations. As a result, the feature formation, training and prediction time are faster. We will add time and space complexity results in the paper as well.

b) We used the SIF weighting and common component removal in P-SIF, whereas SCDV used tf-idf. SIF (Arora et. al. 2017) has shown the benefit of using such weighting and common component removal. We have successfully generalised SIF (Arora et. al. 2017) using the ideas from the SCDV paper. Additionally, SCDV used SGNS-initialized word vectors, whereas P-SIF used Doc2VecC-initialized word vectors. Also, in Doc2VecC the averaging is based on sentence representation and training of word vectors is done jointly with corruptions. This kind of training with corruption results in zeroing of common words' word vectors. However, since our approach is about partition based averaging such as zeroing, we can yield more robust document representation. We will add more downstream tasks which were requested by the reviewer as well. It should be noted that we have more thorough experiments on 26 STS similarity and two text classification datasets of the SCDV paper</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>