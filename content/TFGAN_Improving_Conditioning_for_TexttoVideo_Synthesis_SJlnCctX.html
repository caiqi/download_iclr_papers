<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>TFGAN: Improving Conditioning for Text-to-Video Synthesis | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="TFGAN: Improving Conditioning for Text-to-Video Synthesis" />
        <meta name="citation_author" content="Yogesh Balaji" />
        <meta name="citation_author" content="Martin Renqiang Min" />
        <meta name="citation_author" content="Bing Bai" />
        <meta name="citation_author" content="Rama Chellappa" />
        <meta name="citation_author" content="Hans Peter Graf" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SJl11nCctX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="TFGAN: Improving Conditioning for Text-to-Video Synthesis" />
      <meta name="og:description" content="Developing conditional generative models for text-to-video synthesis is an extremely challenging yet an important topic of research in machine learning. In this work, we address this problem by..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SJl11nCctX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>TFGAN: Improving Conditioning for Text-to-Video Synthesis</a> <a class="note_content_pdf" href="/pdf?id=SJl11nCctX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=yogesh%40cs.umd.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="yogesh@cs.umd.edu">Yogesh Balaji</a>, <a href="/profile?email=renqiang%40nec-labs.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="renqiang@nec-labs.com">Martin Renqiang Min</a>, <a href="/profile?email=bbai%40nec-labs.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="bbai@nec-labs.com">Bing Bai</a>, <a href="/profile?email=rama%40umiacs.umd.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="rama@umiacs.umd.edu">Rama Chellappa</a>, <a href="/profile?email=hpg%40nec-labs.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="hpg@nec-labs.com">Hans Peter Graf</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Developing conditional generative models for text-to-video synthesis is an extremely challenging yet an important topic of research in machine learning. In this work, we address this problem by introducing Text-Filter conditioning Generative Adversarial Network (TFGAN), a GAN model with novel conditioning scheme that aids improving the text-video associations. With a combination of this conditioning scheme and a deep GAN architecture, TFGAN generates photo-realistic videos from text on very challenging real-world video datasets. In addition, we construct a benchmark synthetic dataset of moving shapes to systematically evaluate our conditioning scheme. Extensive experiments demonstrate that TFGAN significantly outperforms the existing approaches, and can also generate videos of novel categories not seen during training.
</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Conditional GAN, Video Generation, Text-to-Video Synthesis, Conditional Generative Models, Deep Generative Models</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">An effective text-conditioning GAN framework for generating videos from text</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HJenoM0saQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJl11nCctX&amp;noteId=HJenoM0saQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper946 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper946 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1xv0aW0nX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Want to see the generated videos</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJl11nCctX&amp;noteId=H1xv0aW0nX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper946 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper946 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper discusses how to synthesize videos from texts using a text-conditioning scheme. The results are shown on a synthetic moving shape dataset, the Kinetics human action dataset, and CUB bird dataset.

The proposed algorithm is quite reasonable to me; however, I hope the experimental results could be more convincing.  More specifically,

- There is no comparison against previous works (e.g. T2V by Li et al 2018) in the moving shape experiment. 

- The Kinetics experiment is most exciting. However, the submission should provide generated videos for the readers and reviewers. It is a paper of generating videos, so the audience would want to watch these videos.

- The CUB bird dataset is irrelevant since it is all images, not videos.

At last, it will be nice to provide the optimization details and conduct other ablation experiments using different hyperparameters.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ByxTiwNT3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>GAN is extended to video with a video-level discriminator to make the frame generation smooth</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJl11nCctX&amp;noteId=ByxTiwNT3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper946 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper946 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a GAN-based method for video generation conditioned on text description. A given text description is first encoded, then combined with latent noise, and finally input to the generator for generating video frames. The frame discriminator discriminates between the generated frames and real frames conditioned on the encoded text description. The discriminator ensures that the frame generation is smooth.

Positives: 
- Easy-to-read paper
- New conditioning method that generates convolution filters from the encoded text, and uses them for a convolution in the discriminator
- New toy dataset for evaluation
- High-quality generated videos on Kinetics dataset


Negatives and suggestions:

- Novelty is limited. While the paper presents interesting empirical results for computer vision, they may not be of broad interest to the ICLR community. The main contribution seems to be in combining the encoded text with visual features by using the convoluted operation instead of the common concatenation.  

- Clarity can be improved. In section 3, there is no description about the text encoder T (section 3.1). Clarity could be improved by providing a figure of the overall architecture of D, and how to generate a set of convolved f_i from text features (they are described in the appendix, which makes the text hard to read and understand). 

- Experiments: 
 (a) It not clear how to generate fake pairs (v, t). (By shuffling ground-truth pair or fake video or fake text?) 
(b) It is not clear how to construct the text description from 5 control parameters. 
(c) There are no qualitative results for the Shape-v1 dataset and  the Shape-v2 datasets with background. One suggestion for Shape-v2 is that the background should include color. Otherwise, it is a very easy setting, because the background is a grayscale image while the objects are colored.
(d) The metric for classifier accuracy seems strange, because the classifier is not perfect. We cannot trust the classifier for evaluating the generated results.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BylmenAq3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>With a combination of several similar techniques, text-to-video/image performance is improved</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJl11nCctX&amp;noteId=BylmenAq3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper946 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper946 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
This paper proposed conditional GAN models for text-to-video synthesis:
(1) Text-feature-conditioned CNN filters is developed; (2) A moving-shape dataset is constructed; (3) Experimental results on video/image generation are shown with improved performance.


Originality: 

(0) The overall model can be considered as MoCoGAN [0], but with the text-conditioned information.  Note that [0] also has a conditional version, but based on labels.
(1) Generating CNN filters conditioned on other feature is not new for video generation, for example, Dynamic Filter Networks in [1], where the CNN filters to produce next frame are generated.
(2) The idea of creating moving shape datasets is explored and used in the previous papers [2]. This paper creates similar but a bit more comprehensive ones.

The above three points to the existing work should be made CLEARLY, to reflect the contributions of this submission. 


Clarity: 
A. The current paper only reports the overall performance for a method with several "new" modifications: (a) TF, (b) ResNet-style architecture (c) Regularization. Please do some ablation study, and show which component helps solve the problem text-to-video.

B. Following MoCoGAN [0], the latent feature consists of two parts: one controls the global video (content in [0]) and one controls the frame-wise dynamics (motion in [1]).  It is okay to follow [0]. But, did the authors verify the two parts of latent codes can really control the claimed generation aspects? Or, is it really necessary to have to the two separated parts for the problem? if the goal is just to have text-controllable video?



Significance: 
Text-to-video is a very challenging task, it is encouraging to see the attempt on this problem. However, I hope each contribution to this important problem are concrete and solid. Therefore, it will much more convincing to study each part of "contributions" the more comprehensively (one significant contribution would be enough), rather than put several minor/existing techniques together.


Questions: 
A. The generated frames (Figure 5) for one video are so similar, it raise a questions: does this model really generate video (which captures the dynamics)? or it is just a model for the static image generation, with small perturbations? If the answer is the ground-truth frames (the employed training dataset) are very similar, I would suggest to change the dataset at the first place, the employed dataset is not a proper dataset for text-to-VIDEO generation. 

B. [3] is also a text-to-video paper, the comparison to it is missing. Why?


References:
[0] MoCoGAN: Decomposing motion and content for video generation, 2018
[1] Dynamic Filter Networks, 2016
[2] Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks, 2016
[3] To create what you tell: Generating videos from captions, 2017</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>