<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Automatically Composing Representation Transformations as a Means for Generalization | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Automatically Composing Representation Transformations as a Means for Generalization" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1ffQnRcKX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Automatically Composing Representation Transformations as a Means..." />
      <meta name="og:description" content="How can we build a learner that can capture the essence of what makes a hard problem more complex than a simple one, break the hard problem along characteristic lines into smaller problems it knows..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1ffQnRcKX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Automatically Composing Representation Transformations as a Means for Generalization</a> <a class="note_content_pdf" href="/pdf?id=B1ffQnRcKX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019automatically,    &#10;title={Automatically Composing Representation Transformations as a Means for Generalization},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=B1ffQnRcKX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=B1ffQnRcKX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">How can we build a learner that can capture the essence of what makes a hard problem more complex than a simple one, break the hard problem along characteristic lines into smaller problems it knows how to solve, and sequentially solve the smaller problems until the larger one is solved?  To work towards this goal, we focus on learning to generalize in a particular family of problems that exhibit compositional and recursive structure: their solutions can be found by composing in sequence a set of reusable partial solutions.  Our key idea is to recast the problem of generalization as a problem of learning algorithmic procedures: we can formulate a solution to this family as a sequential decision-making process over transformations between representations.  Our formulation enables the learner to learn the structure and parameters of its own computation graph with sparse supervision, make analogies between problems by transforming one problem representation to another, and exploit modularity and reuse to scale to problems of varying complexity. Experiments on solving a variety of multilingual arithmetic problems demonstrate that our method discovers the hierarchical decomposition of a problem into its subproblems, generalizes out of distribution to unseen problem classes, and extrapolates to harder versions of the same problem, yielding a 10-fold reduction in sample complexity compared to a monolithic recurrent neural network. We further show that our method can compose learned spatial transformations to recover canonical MNIST digits from transformed ones.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">compositionality, deep learning, metareasoning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We explore the problem of compositional generalization and propose a means for endowing neural network architectures with the ability to compose themselves to solve these problems.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">11 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_r1gbHlap3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Well-written paper; second experiment could be made stronger.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1ffQnRcKX&amp;noteId=r1gbHlap3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1340 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1340 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: This paper is about trying to learn a function from typed input-output data so that it can generalize to test data with an input-output type that it hasn't seen during training. It should be able to use "analogy" (if we want to translate from French to Spanish but don't know how to do so directly, we should translate from French to English and English to Spanish). It should also be able to generalize better by learning useful "subfunctions" that can be composed together by an RL agent. We set up the solution as having a finite number of subfunctions, including "HALT" which signifies the end of computation. At each timestep an RL agent chooses a subfunction to apply to the current representation until "HALT" is chosen. The main idea is we parameterize these subfunctions and the RL agent as neural networks which are learned based on input -output data. RL agent is also penalized for using many subfunctions. The algorithm is called compositional recursive learner (CRL). Both analogy and meaningful subfunctions should arise purely because of this design.

Multilingual arithmetic experiment. I found this experiment interesting although it would be helpful to specify that it is about mod-10 arithmetic. I was very confused for some time since the arithmetic expressions didn't seem to be evaluated correctly. It also seems that it is actually the curriculum learning that helps the most (vanilla CRL doesn't seem to perform very well) although authors do note that such curriculum learning doesn't help the RNN baseline. It also seems that CRL with curriculum doesn't outperform the RNN baseline that much on test data with the same length as training data. The difference is larger when tested on longer sequences. However here, the CRL learning curve seems to be very noisy, presumably due to the RL element. The qualitative analysis illustrates well how the subfunctions specialize to particular tasks (e.g. translation or evaluating a three symbol expression) and how the RL agent successively picks these subfunctions in order to solve the full task.

Image transformations experiment. This experiment feels a bit more artificial although the data is more complicated than in the previous experiment. Also, in some of the examples in Figure 2, the algorithms seems to perform translation (action 2) twice in a row while it seems like this could be achieved by only one translation. How does this perform experimentally in comparison to an RNN (or other baseline)?

I found this paper to be well-written. Perhaps it could be stronger if the "image transformations" experiment quantitatively compared to a baseline. I'm not an expert in this area and don't know in detail how this relates to existing work (e.g. by Rosenbaum et al; 2018).</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Byg3F7Qap7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1ffQnRcKX&amp;noteId=Byg3F7Qap7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1340 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1340 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank Reviewer 3 for their constructive review, which helped us improve the paper in various aspects. We would be happy to incorporate any other suggestions Reviewer 3 may have for the paper. We would like to make the following clarifications:

1. We have clarified in Section 4.1 that arithmetic problems are modulo-10.

2. With regards to how CRL compares to the RNN on test data with the same length as the training data, Figure 2b shows that there is a substantial difference between CRL (red curve) and RNN (purple curve). It is only with 10x more data does the RNN (yellow curve) reach comparable performance with CRL.

3. Reviewer 3 noted that in the right half of Figure 4, the top-two examples showed that CRL performs transformation twice, when in fact this can be achieved by only translation. This is true. For simplicity, we had fixed the number of transformations to two transformations. That CRL finds alternate ways of achieving the same end representation (using two translations instead of one) illustrates a core feature of the CRL framework: that it is possible to solve a problem (e.g. a large translation) by composing together partial solutions (two small translation).

4. We will have the baseline experiments Reviewer 3 requested in time for the final, and will endeavor to add these in to the paper during the discussion period.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJl1SmLj3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Trying to learn composition</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1ffQnRcKX&amp;noteId=BJl1SmLj3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1340 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1340 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This is a good review paper. I am not sure how much it adds to the open question of how to learn representation with high structure. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1eifeX9n7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting approach to compositionally</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1ffQnRcKX&amp;noteId=B1eifeX9n7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1340 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1340 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">==== Summary ====

This paper proposes a model for learning problems that exhibit compositional and recursive structure, called Compositional Recursive Learner (CRL). The paper approaches the subject by first defining a problem as a transformation of an input representation x from a source domain t_x to a target domain t_y. If t_x = t_y then it is called a recursive problem, and otherwise a translational problem. A composite problem is the composition of such transformations. The key observation of the paper is that many real-world problems can be solved iteratively by either recursively transforming an instance of a problem to a simpler instance, or by translating it to a similar problem which we already know how to solve (e.g., translating a sentence from English to French through Spanish). The CRL model is essentially composed of two parts, a set of differential functions and a controller (policy) for selecting functions. At each step i, the controller observes the last intermediate computation x_i and the target domain t_y, and then selects a function and the subset of x_i to operate on. For each instance, the resulting compositional function is trained via back-propagation, and the controller is trained via policy gradient. Finally, the paper presents experiments on two synthetic datasets, translating an arithmetic expression written in one language to its outcome written in another language, and classifying MNIST digits that were distorted by an unknown random sequence of affine transformations. CRL is compared to RNN on the arithmetic task and shown to be able to generalize both to longer sequences and to unseen language pairs when trained on few examples, while RNN can achieve similar performance only using many more examples. On MNIST, it is qualitatively shown that CRL can usually (but not always) find the sequence of transformations to restore the digit to its canonical form.

==== Detailed Review ====

I generally like this article, as it contains a neat solution to a common problem that builds on and extends prior work. Specifically, the proposed CRL model is a natural evolution of previous attempts at solving problems via compositionally, e.g. Neural Programmer [1] that learns a policy for composing predefined commands, and Neural Module Networks [2] that learns the parameters of shared differential modules connected via deterministically defined structure (found via simple parse tree). The paper contains a careful review of the related works and highlights the similarities and differences from prior approaches. Though the experiments are mostly synthetic, the underlying method seems to be readily applicable to many real-world problems.

However, the true contributions of the paper are somewhat muddied by presenting CRL as more general than what is actually supported by the experiments. More specifically, the paper presents CRL as a general method for learning compositional problems by decomposing them into simpler sub-problems that are automatically discovered, but in practice, a far more limited version of CRL is used in the experiments, and the suggested translational capabilities of CRL, which are important for abstract sub-problem discovery, are not properly validated:

1. In both experiments, the building-block functions are hand-crafted to fit to the prior knowledge on the compositionally of the problem. For the arithmetic task, the functions are limited to operate each step just on a single window of encompassing 3 symbols (e.g., &lt;number&gt; &lt;op&gt; &lt;number&gt;,  &lt;op&gt; &lt;number&gt; &lt;op&gt;) and return a distribution over the possible symbols, which heavily forces the functions to represent simple evaluators for simple expressions of the form &lt;number&gt; &lt;op&gt; &lt;number&gt;. For the distorted MNIST task, the functions are limited to neural networks which choose the parameter of predetermined transformations (scaling, translation, or rotation) of the input. In both cases, CRL did not *found* sub-problems for reducing the complexity of the original instance but just had to *fine tune* loosely predefined sub-problems. Incorporating expert knowledge into the model like so is actually an elegant and useful trick for solving real problems, and it should be emphasized far clearly in the article. The story of “discovering subproblems” should be left for the discussion / future research section, because though it might be a small step towards that goal, it is not quite there yet.
2. The experiments very neatly show how recursive transformations offer a nice framework for simplifying an instance of a problem. However, the translation capabilities of the model are barely tested by the presented experiments, and it can be argued that all transformations used by the model are recursive in both experiments. First, only the arithmetic task has a translation aspect to it, i.e., the task is to read an expression in one language and then output the answer in a different language. Second, this problem is only weakly related to translation because it is possible to translate the symbols independently, word by word, as opposed to written language that has complex dependencies between words. Third, the authors report that in practice proper translation was only used in the very last operation for translating the computed value of the input expression to the requested language, and not as a method to translate one instance that we cannot solve into another that we can. Finally, all functions operate and return on all symbols and not ones limited to a specific language, and so by the paper’s own definition, these are all recursive problems and not translational ones.

In conclusion, I believe this paper should be accepted even with the above issues, mostly because the core method is novel, clearly explained, and appears to be very useful in practice. Nevertheless, I strongly suggest to the authors to revise their article to focus on the core qualities of their method that can be backed by their current experiments, and correctly frame the discussion on possible future capabilities as such.

[1] Reed et al. Neural Programmer-Interpreters. ICLR 2016.
[2] Andreas et al. Neural Module Networks. CVPR 2016.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1ei6m7TaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1ffQnRcKX&amp;noteId=H1ei6m7TaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1340 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1340 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank Reviewer 2 for their constructive review, which helped us improve the paper in the following aspects. We would be happy to incorporate any other suggestions Reviewer 2 may have for the paper.

1. We have revised Section 3.1 and the introductory paragraph of Section 3 to be more precise about the domain-specific assumptions CRL makes about the problem distribution. In particular, we included a discussion about restricting the representational vocabulary and the functional form of the modules as a way to incorporate as an inductive bias domain-specific knowledge of the problem distribution. 

2. We agree with Reviewer 2 that the “recursive”/”translational” terminology should be clearer. Therefore, we have revised the “Problems” and “The goal” paragraphs in Section 2 to remove the discussion on translational problems and only focus on recursive problems, where the input and output representations are drawn from the same vocabulary.

3. Further, we agree with and appreciate Reviewer 2’s analysis that our paper is only a first step towards the full general problem of discovering subproblem decomposition. Accordingly we have revised the end of Section 6 (Discussion) to acknowledge this. We also revised “The challenge” paragraph in Section 2 to be more precise that we are not solving the general subproblem decomposition problem, but rather solving the problem of learning to compose partial solutions to subproblems when the general form of the subproblem decomposition of a task distribution is known.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJlzO5a1nm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Relation of the Compositional Recursive Learner to Routing Networks</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1ffQnRcKX&amp;noteId=HJlzO5a1nm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">26 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1340 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I have read the paper "Automatically Composing Representation Transformations as a Means for Generalization" with great pleasure. I particularly enjoyed how the paper tries to link compositionality to analogical reasoning. I think an architecture for compositional reasoning that can solve even complex tasks elegantly is of great value.  
I do though have some concerns about the relationship between the "Compositional Recursive Learner" (CRL) and "Routing Networks" (RN).  Specifically, it seems to me that the CRL is an example of a single agent recursive routing network, as described in (Rosenbaum et al, ICLR 2018). In particular, the design of a compositional computation and learning framework that combines trainable function blocks with a reinforcement learning meta learner (as described in section 3.2 and 3.3) is highly similar (section 3.2) or nearly identical (section 3.3) to the formulation in the routing networks paper.
The main difference is that while (Rosenbaum et al) focused on a limited-horizon recurrence (see pages 1, 3, 4, 7, and particularly 14 in the appendix), CRL uses an infinite-horizon recurrence.
Surprisingly, this relationship is not discussed in the paper in any detail. Routing Networks are more closely examined in the appendix only. Additionally, there are two stated assumptions (on p. 15) on routing networks that I do not think are true: (1) Routing Networks necessarily have a separate controller per computation step and (2) Routing Networks necessarily use a different set of functions per computation step.  The idea of an RN with a single controller applied across computation steps is discussed on page 5 of (Rosenbaum et al).  The idea of re-using function blocks across computation steps is discussed on pages 1, 3, 4, 7 and 14.

Given the obviously close relationship between these two works, I feel that the connection should be more emphasized and the comparison more central to the paper. And indeed, the results shown for routing networks are somewhat hard to believe (at least for smaller problems as routing networks are not expected to scale to inputs of the same size). Is the routing networks implementation compared to actually also recurrent? Does the routing network receive the same curriculum learning strategy training?

The link to Rosenbaum et al in ICLR 2018: <a href="https://openreview.net/forum?id=ry8dvM-R-" target="_blank" rel="nofollow">https://openreview.net/forum?id=ry8dvM-R-</a></span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hkx25xk6hQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to "Relation of the Compositional Recursive Learner to Routing Networks"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1ffQnRcKX&amp;noteId=Hkx25xk6hQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1340 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1340 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We are grateful to the Anonymous Commenter (OP) for their detailed and insightful comment.

It is true, as OP points out, that there is a close connection to Routing Networks (RN), an important and interesting paper that seeks to mitigate task interference in multi-task learning by routing through the modules of a convolutional neural network. Like RN, a feature of our work is that the learner creates and executes a different computation graph for different inputs, where this computation graph consists of a series of functions applied according to a controller. Therefore, it is possible to see CRL as taking a step beyond the single-controller (which they refer to as “single-agent” in Rosenbaum et al.) version of RN by incorporating several algorithmic improvements that make the single controller version not only effective for solving the task (c.f. Figure 4 and Figure 5 of Rosenbaum et al.) but also effective for extrapolation, a problem domain that Rosenbaum et al. does not consider. 

We will follow OP’s recommendation and make the comparison with RN more salient in the experiments and related work section. However, we would like to emphasize that the problem that RN tackles (mitigating task interference in multi-task learning) is not the central focus of the paper. That CRL and RN started from significantly different motivations and problem domains but converged to a similar architecture design serves as encouraging evidence in support of an old idea that exploiting modularity and encapsulation yield help more efficiently capture the modalities of a task distribution, and we are excited that both we and Rosenbaum et al. are actively pushing this front.

We thank OP for pointing out it is indeed true that 1) RN does not necessarily have a separate controller per time step and 2) RN does not necessarily use a different set of functions per computation step; we will follow OP’s recommendation and clarify this in the next version of the paper to avoid potential misunderstanding. One source for our misunderstanding is that the exposition of RN in section 3 of Rosenbaum et al. (e.g. “If the number of function blocks differs from layer to layer in the original network, then the router may accommodate this by, for example, maintaining a separate decision function for each depth” (page 4, Rosenbaum et al.) and “The approximator representation can consist of either one MLP that is passed the depth (represented in 1-hot), or a vector of d MLPs, one for each decision/depth” (page 5, Rosenbaum et al.)) seems to heavily suggest the two assumptions we made on page 15 of our manuscript, so we thought that the single-controller or shared function cases were included in Rosenbaum et al. mostly for the sake of comparison. The reason that our submission discussed points (1) and (2) was not intended to misrepresent RN. Rather it was because we interpreted Figure 4, Figure 5, Table 3, Table 4 of Rosenbaum et al. as claiming the routing-all-fc (one-agent-per-task, separate controller per depth, different functions-per-layer) as the flag bearer of their results. To make the comparison that most fairly represents RN’s claims, we had conducted our comparison based on the best version of RN reported in Rosenbaum et al. (routing-all-fc), which uses a separate controller per depth and a different set of functions per depth (according to Table 3 and 4 in Rosenbaum et al.).</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJeiNE7TaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Comparison with Routing Networks</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1ffQnRcKX&amp;noteId=BJeiNE7TaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1340 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1340 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Based on OP’s suggestions, we have included a paragraph in Section 3.4 (“Discussion of Design Choices”) that features a discussion that compares CRL with Routing Networks.

To avoid misrepresenting Routing Networks, we have revised the wording of the experiment of Appendix D.2 to compare with a mixture-of-expert- inspired baseline, rather than Routing Networks, because as OP points out, 1) RN does not necessarily have a separate controller per time step and 2) RN does not necessarily use a different set of functions per computation step. The purpose of this experiment is to show the benefits of reusing modules across computation steps and to show the benefit of allowing a flexible computation horizon.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SJg_qTBs3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Update to concerns above</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1ffQnRcKX&amp;noteId=SJg_qTBs3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1340 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Now that the review period is officially over, I was hoping to get a response to the issues raised above. I ask the authors to address the following questions in particular:
1. Do the authors agree with the assessment that the CRL is in effect a Routing Network? (I might point out that the authors even hint at that in the arxiv version of this paper)
2. Do the authors agree that the only two minor differences (apart from the training schedule) are (1) that the CRL has infinite horizon recurrence, while RNs only have limited horizon recurrence, and (2) the RL algorithm chosen? (this implies a mischaracterization of RNs on the authors part) 
3. In light of the previous two points, why do the authors claim that their architecture is novel? (this critique does not extend to the other parts of their paper) </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1x-kZyT2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to "Update to concerns above"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1ffQnRcKX&amp;noteId=r1x-kZyT2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1340 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1340 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Question 1

Although we have acknowledged the similarities in "Response to Relation of the Compositional Recursive Learner to Routing Networks", we respectfully disagree with OP that “CRL is in effect a Routing Network.” To make such a statement would be to mischaracterize the difference between the generative nature of CRL and the routing-based nature of RN and to ignore the respective problem domains that CRL and RN tackles.

We focus on the extrapolation problem (Sec 2 and 3), for which learning on multiple tasks is a means to this end, whereas Rosenbaum et al. focus on task interference, for which multi-task learning is the end itself (see abstract of Rosenbaum et al.). Because our focus is on subproblem decomposition, CRL restricts the representation space such that harder problems can be expressed in the same vocabulary as easier problems. RN do not focus on subproblem decomposition, so it is not clear whether their modules learn any interpretable atomic functionality or whether their representations capture semantic boundaries between subproblems that comprise a larger problem. Therefore, RN does not have the inductive bias for extrapolation problems that require the learner to re-represent the new problem in terms of problems the learner has seen during training.

The key methodological difference between CRL and RN lies in the generative nature of CRL and the routing-based nature of RN. RN and other work such as PathNet (Fernando et al. 2017) route input-dependent paths through a large fixed architecture. In contrast, the extrapolation problem necessitates CRL be generative, meaning that it incrementally builds module on top of module without a fixed computational horizon. This is necessary for the problem domain we consider, in which we want to train and extrapolate to different problems that require various computation depths. Therefore, variable-length computation horizon, the restrictions on the representational vocabulary, and the emergent semantic functionality of its submodules as solutions to subproblems within a larger problem (see Figure 3) are crucial design considerations for the capability of CRL that RN does not incorporate in their approach.

Question 2

Based on the crucial difference between the generative nature of CRL and the routing-based nature of RN, the variable computation horizon is a crucial feature of CRL, not a minor difference, as we discussed above and in the Related Work. Because of the variable computation horizon, it is not possible to have a separate controller at each timestep/depth because the number of time steps of computation unknown; therefore this is also not a minor difference. 

We agree with OP that the particular RL algorithm (PPO vs MARL-WPL) is not particularly relevant to the central focus of our paper, which is extrapolation in compositionally structured problems, and we indeed did not claim so. Nevertheless, our work represents an algorithmic improvement that does make the single controller architecture more effective (above 90% extrapolation accuracy for multilingual arithmetic) than Rosenbaum et al.’s architecture (Figure 4 and Figure 5 of Rosenbaum et al. shows &lt; 50% accuracy, whereas their best method achieves around 60%).

CRL’s focus on capturing interpretable atomic functionality in its modules and using representations capture semantic boundaries between subproblems that comprise a larger problem are important ingredients for CRL’s analogical reasoning: literally re-representing a problem in terms of problems it has already seen. This is another key difference between RN and CRL, because the architectural design of RN do not have the inductive bias (restrictions on the modules and representations) that encourage it re-represent problems in literally terms of previously-seen problems.

Question 3: Novelty

The novelty of our work (with respect to RN) lies in the generative nature of CRL because we reframe of the extrapolation problem as a problem of learning algorithmic procedures over transformations between representations, as discussed in the abstract, intro, and discussion. CRL generates function composition, in contrast to how RN routes through function paths. As shown in the experiments section, the transformations CRL learns have interpretable, atomic functionality and the representations capture semantic boundaries between subproblems that comprise a larger problem. These features of the CRL architecture crucially differentiate it from other routing-based architectures, including RN and PathNet.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>