<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Monge-Amp\`ere Flow for Generative Modeling | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Monge-Amp\`ere Flow for Generative Modeling" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rkeUrjCcYQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Monge-Amp\`ere Flow for Generative Modeling" />
      <meta name="og:description" content="We present a deep generative model, named Monge-Amp\`ere flow, which builds on continuous-time gradient flow arising from the Monge-Amp\`ere equation in optimal transport theory. The generative map..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rkeUrjCcYQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Monge-Amp\`ere Flow for Generative Modeling</a> <a class="note_content_pdf" href="/pdf?id=rkeUrjCcYQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019monge-amp\`ere,    &#10;title={Monge-Amp\`ere Flow for Generative Modeling},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rkeUrjCcYQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We present a deep generative model, named Monge-Amp\`ere flow, which builds on continuous-time gradient flow arising from the Monge-Amp\`ere equation in optimal transport theory. The generative map from the latent space to the data space follows a dynamical system, where a learnable potential function guides a compressible fluid to flow towards the target density distribution. Training of the model amounts to solving an optimal control problem. The Monge-Amp\`ere flow has tractable likelihoods and supports efficient sampling and inference. One can easily impose symmetry constraints in the generative model by designing suitable scalar potential functions. We apply the approach to unsupervised density estimation of the MNIST dataset and variational calculation of the two-dimensional Ising model at the critical point.  This approach brings insights and techniques from Monge-Amp\`ere equation, optimal transport, and fluid dynamics into reversible flow-based generative models. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">generative modeling, Monge-Amp\`ere equation, dynamical system, optimal transport, density estimation, free energy calculation</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A gradient flow based dynamical system for invertible generative modeling</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_B1geQk2Ah7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A Related Paper by Mesa, et al</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkeUrjCcYQ&amp;noteId=B1geQk2Ah7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper90 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">A similar idea of using the optimal transport for MNIST generation has been considered in this paper:
<a href="https://arxiv.org/pdf/1801.08454.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1801.08454.pdf</a>

The author(s) need to address this.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HklT8WB927" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Continuous time flows with symmetries motivated from the Monge-Ampere equation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkeUrjCcYQ&amp;noteId=HklT8WB927"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper90 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper90 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
This paper introduces a continuous-time flow, which is motivated from continuous-time gradient flow in the Monge-Ampere equation in optimal transport theory. They relate the resulting generative model to a dynamical system with a learnable potential function that controls a compressible fluid (representing the probability density), towards the target density. The resulting set of differential equations for the evolution of the samples and the density is solved through a fourth-order Runge-Kutta ODE solver. By imposing symmetries on the scalar potential function, symmetries of the resulting target distribution can also be enforced, a property that is desirable in many applications.

The scalar potential is modeled using a fully connected MLP with a single hidden layer. Forward propagating of samples requires obtaining the gradient of the scalar potential (output of MLP) with respect to its input (the sample). Forward propagation of the log density requires computation of the Laplacian (not the hessian) of the scalar potential. Both of these quantities can easily be computed with automatic differentiation (in O(D) where D is data dimension). The potential is kept constant over time, although this is not necessary.

The proposed method is evaluated on density estimation for MNIST, and variational inference with respect to the Boltzmann distribution of the 2D Ising model at the critical temperature. 
On MNIST, comparison is done with respect to MADE, MAF and realNVP. Monge-Ampere flows outperforms the baselines. On the variational inference task one baseline is used, and the result is compared to the exact known free energy. Monge-Ampere flows are reported to approximate the exact solution to comparable accuracy as a baseline. As the authors show that they can easily enforce symmetries, it would be very informative to see the performance of Monge-ampere flows with and without these symmetries enforced on for instance the Ising model. Have the authors looked at this?

It is not clear from the paper how much the ODE solvers used in the forward pass, as well as backpropagating through it with respect to model parameters, will influence the run time. I suspect the training time of models like MAF to be significantly shorter than that of Monge-Ampere flows. For sampling, the comparison would also be interesting. Where sampling from MAF is O(D), with D the data dimension, sampling from the Monge-Ampere flows requires propagating through an ODE solver. Can the authors comment on the runtimes for these settings?
 
The experimental validation is not extensive, but the proposed method is well motivated and as far as I can tell original. It is a useful contribution to the field of normalizing flows/invertible networks. The ability to easily enforce symmetries into the density seems to be promising and could lead to interesting future work on permutation invariant systems.

See below for comments and more questions:

Quality
The paper is well structured. The experimental validation is not extensive, and perhaps even on the low side.

Clarity
The paper is overal clearly written. One small nuisance is that the citations are not in brackets in sentences, even if they are not part of the actual sentence itself. This interrupts reading. It would be greatly appreciated if the authors could change this. The authors leave out some details with regards to the experiments, but with code available this should be sufficient for reproducibility. 

Originality
To my knowledge the idea of using the Monge-Ampere equation for continuous normalizing flows is new. Note that it is also significantly different from a concurrent ICLR submission entitled ‘Scalable Reversible Generative Models with Free-form Continuous Dynamics’, which also discussed continuous normalizing flows with ODE’s.

Significance
This work is of use to the research community. The method is memory efficient and appears to perform well. Especially the ability to enforce symmetries seems very appealing. If the authors can comment on the runtime in comparison to other flow methods, both in terms of training time and sampling, this would enable a better view on practical use. 

Detailed questions/comments:

1. In Fig. 3a, the train and test error during training are shown to follow each other very closely. How long was the model trained, and did the train and test curve at some point start to diverge?
2. In Section 4.2, the results are said to be of comparable accuracy as the baseline by Li &amp; Wang. It would be informative to actually state the result of Li &amp; Wang, so that the reader can judge too if this is comparable. 
3. Out of curiosity, did the authors also consider using other activation functions that have higher order derivatives, such as tanh?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkxAz5z9hQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting but needs more work: clarifications to existing work and ablations would make this paper more appealing</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkeUrjCcYQ&amp;noteId=rkxAz5z9hQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper90 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper90 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a continuous-time gradient flow as an alternative normalizing flow. The formulation is motivated from optimal transport and the Monge-Ampere equation. Symmetry constraints motivated by the use of a potential function can be enforced during training.

I'm surprised [1] was mentioned only for their backpropagation scheme. Much of this paper is similar to theirs, such as Eqs (2) and (3) being the "instantaneous change of variables" in [1], the mention of efficient reversibility, experimenting with forward and reverse KL objectives, and parameter efficiency. 

Given the different angle of approach in this work, I'm willing to believe some of this is independently done. This work contains interesting derivations and a different parameterization, with enough contributions to potentially be interesting in its own right. However, I firmly believe in proper attribution and believe [1] should at least be mentioned in the introduction and/or theoretical background. 

Pros:
 - The potential function and the resulting gradient flow parameterization is interesting.
 - Parameterizing a potential function motivates some symmetry constraints.
 - Interesting application of normalizing flows to the Ising model.
 
Cons:
 - Paper presentation needs some work on clarity.
 - Relation to existing work needs to be clarified.
 - Experiments lack ablations and proper comparisons. e.g. the effect of using symmetry constraints, the effect of using a gradient flow parameterization.
 - If I understood correctly, the symmetry "constraints" are really data augmentation during the training phase, rather than hard constraints on the model.

Main questions:
- It seems the potential function plays a similar role to the negative log-likelihood in [2]. 
- Does having symmetry constraints lead to a better model when the constraints are justified? ie. can you provide comparisons for the Ising model experiment in 4.2?
- What are the set of constraints you can specify using a potential function? Permutation of the variables is very briefly mentioned in the experiment section, but this could be clarified much earlier. 
- I may have missed this, but what exactly are the symmetry conditions that were used in the experiments?
- It seems that the proposed permutation constraints could be part of the training algorithm rather than the model. How different would it be if you permute the data samples and use an existing normalizing flow algorithm? ie. can you provide comparisons where randomly permuted data samples are also used during training with existing algorithms?
- Since you used a fixed-step size solver, do you have some guarantees, theoretical or empirically, that the numerical integration has low error? e.g. what is the reconstruction error from doing a forward and reverse pass, and what would the error be if compared to a much smaller step size?

Minor:
- The potential function is parameterized directly but is not integrated to infinity. Since the resulting gradient flow is time-invariant, how this would affect the expressivity of the flow? Could a time-variant potential function be used?
- Eq (5) is also the Liouville equation, which I think should be mentioned.
- MNIST digits have completely black backgrounds, so I don't understand why Figure 3 samples have grey backgrounds. Could this have something to do with numerical issues in reversing the numerical integration?
- It's awkward that Figure 3 contains the loss over training for Monge-Ampere flows but only the final loss for the rest. Table 1 sufficiently summarizes this figure, so unless you can show the loss over training for all methods I think this figure is redundant.
- Equations are referenced both with and without parenthesis. It'd be best if this is consistent across text.
- There are quite a few grammar mistakes, especially around important digression. (e.g. top of page 3 "experienced by someone travels with the fluid" -&gt; "experienced by someone traveling with the fluid".)
- Please use citep and citet properly. Many references should be done using citep (with brackets around the author-year), when the author is a not a part of the sentence.

[1] Chen, Tian Qi, et al. "Neural Ordinary Differential Equations."
[2] Tabak, Esteban G., and Eric Vanden-Eijnden. "Density estimation by dual ascent of the log-likelihood."</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJekJlednX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Proposes a novel parameter-efficient generative modeling approach based on the Monge-Ampere equation. Applications section is not convincing enough.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkeUrjCcYQ&amp;noteId=rJekJlednX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper90 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper90 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a novel parameter-efficient generative modeling approach that is based on the Monge-Ampere equation. In the proposal, a feed-forward neural network is trained as an ODE integrator which solves (2) and (3) for a fixed time interval $[0,T]$, so that the distribution $p(x,t)$ at time 0 is a simple base distribution such as a Gaussian, and that at time $T$ mimics the target distribution.

[pros]
- The proposal provides a parameter-efficient approach to generative modeling, via parameter sharing in the depth direction.
- I think that the idea itself is quite interesting and that it is worth pursuing this direction further.

[cons]
- The Applications section is not convincing enough to demonstrate usefulness of the proposal as an approach to generative modeling.
- How the gradient-based learning in the proposal behaves is not discussed in this paper.

[quality]
How the gradient-based learning in the proposal behaves is not discussed. I understand that the non-convex nature of the loss function poses problems already in the conventional back-propagation learning of a multilayer neural network. On the other hand, in the proposal, the loss function (e.g., (4)) is further indirectly parameterized via $\varphi$. It would be nice if the parameterization of the loss in terms of $\varphi$ is regular in some sence.

[clarity]
Description of this paper is basically clear. In the author-date citation style employed in this paper, both the author names and publication year are enclosed in parentheses, with exception being the author names incorporated in the text. This paper does not follow the above standard convention for citation and thus poses strong resistance to the reader. For example, in the first line of the Introduction section, "Goodfellow et al. (2016)" should read "(Goodfellow et al., 2016)".

[originality]
The idea of considering the Monge-Ampere equation in its linearized form to formulate generative modeling seems original.

[significance]
In the experiment described in Section 4.1, it is not clear at all from the description here whether the learned system is capable of successfully generating MNIST-like fake images, which would question the significance of the proposal as a framework for generative modeling. It is well known that the KL divergence $D(P\|Q)$ tends to put more penalty when $P$ is large and $Q$ is small than the opposite. One can then expect in this experiment that it tolerates the model, appearing as $Q$ in $D(P\|Q)$, to put weights on regions where the data are scarce, which might result in generation of low-quality fake images. It would be nice if the authors provide figures showing samples generated via mapping of Gaussian samples with the learned system.
Also, in the experiment described in Section 4.2, I do not see its significance. It is nice to observe in Figure 4 that the loss function approaches the true free energy as well as that the snapshots generated by the model seem more or less realistic. My main concern however is regarding what the potential utilities of the proposal are in elucidating statistical-physical properties of a system. For example, it would be nice if the proposal could estimate the phase-transition point more easily and/or more accurately compared with alternative conventional approaches, but there is no such comparison presented in this paper, making the significance of this paper obscure.

Minor points:

The reference entitled "A proposal on machine learning via dynamical systems" would be better cited not as "E (2017)" but rather as "Weinan (2017)".

Page 6, line 10: the likelihoods of these sample(s)</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>