<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learning Discriminators as Energy Networks in Adversarial Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learning Discriminators as Energy Networks in Adversarial Learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=By40DoAqtX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learning Discriminators as Energy Networks in Adversarial Learning" />
      <meta name="og:description" content="We propose a novel framework for structured prediction via adversarial learning. Existing adversarial learning methods involve two separate networks, i.e., the structured prediction models and the..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_By40DoAqtX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning Discriminators as Energy Networks in Adversarial Learning</a> <a class="note_content_pdf" href="/pdf?id=By40DoAqtX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019learning,    &#10;title={Learning Discriminators as Energy Networks in Adversarial Learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=By40DoAqtX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We propose a novel framework for structured prediction via adversarial learning. Existing adversarial learning methods involve two separate networks, i.e., the structured prediction models and the discriminative models, in the training. The information captured by discriminative models complements that in the structured prediction models, but few existing researches have studied on utilizing such information to improve structured prediction models at the inference stage. In this work, we propose to refine the predictions of structured prediction models by effectively integrating discriminative models into the prediction. Discriminative models are treated as energy-based models. Similar to the adversarial learning, discriminative models are trained to estimate scores which measure the quality of predicted outputs, while structured prediction models are trained to predict contrastive outputs with maximal energy scores. In this way, the gradient vanishing problem is ameliorated, and thus we are able to perform inference by following the ascent gradient directions of discriminative models to refine structured prediction models. The proposed method is able to handle a range of tasks, \emph{e.g.}, multi-label classification and image segmentation.  Empirical results on these two tasks validate the effectiveness of our learning method.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">adversarial learning, structured prediction, energy networks</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a novel adversarial learning framework for structured prediction, in which discriminative models can be used to refine structured prediction models at the inference stage. </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Syl4I9Uq3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Adversarial learning framework for energy-based structured prediction </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=By40DoAqtX&amp;noteId=Syl4I9Uq3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper313 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper313 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Building on the work of (Gygli et al., 2017), this paper introduces a training algorithm for energy-based models for structured prediction. Similar to Gygli et al. (2017), they train an energy-based discriminator, which matches the energy value of structured outputs with their target values assigned by a value function.
The authors present the learning algorithm in an adversarial learning framework by describing a structured prediction model G and a discriminator D. However, it is very confusing for me to understand the proposed formulation as an adversarial framework. In an adversarial framework, at the equilibrium, G could be used as a final prediction model, however, the predicted output of G are still low quality. For example, considering Table 1, the performance of G is exactly the same as NN baseline, which suggests that only the first term of Eq. 6  participates in the training of G (because L_g(G, y*) is the exact objective of the NN baseline).
What that I can easily relate to, however, is that this training algorithm is similar to Gygli et al. (2017), but uses G to get an initial point for gradient-based inference. We want this initial point to be close to the target value Eq. 6. We use the initial value (prediction of G) and the ground truth as the matching constraints (Eq. 5) (as well as the other samples that construct Eq. 10). This actually describes why D can refine the output of G (because it looks at it as an initial point that needs refinement), but the discriminators in the other adversarial frameworks can't refine that much (since they have reached the equilibrium). I would love to hear authors comments on my concern regarding the proposed adversarial framework.

Other comments:
1) Lg is a surrogate loss, not the task-loss. Task-loss could be F1, IOU, BLEU, etc, which is the ultimate performance measure on a task. 
2) The authors refer to G as a structured prediction model but starting from Section 4, they have switched to call it a classifier, which is confusing.
3) "Gygli et al. (2017) found that the key to learning energy-based models is generating proper training data.":  Is this a general statement for every energy-based model? I understand its effect when matching values, but is it still true for other training algorithms such as structural SVM training (Belanger and McCallum, 2016)? Do you have evidence to support it?
4) "In the experiment, we adopt the fully convolutional network (FCN) (Long et al., 2015) baseline model proposed in (Gygli et al., 2017) as our segmentation network. It consists of three 5 × 5 convolutional layers and two deconvolution layer.": The text from Gygli et al. (2017) says three convolutional layers and two fully connected layers. Are you using the same architecture? If not, can you describe the architecture in more details? 
5) The qualitative results for Gygli et al. (2017) appear in <a href="https://gyglim.github.io/deep-value-net/." target="_blank" rel="nofollow">https://gyglim.github.io/deep-value-net/.</a> The reported output for DVN row is significantly worse than the segmentation results of the same horses specifically for columns 4 and 8, while the overall reported IOU in Table 2 is exactly the same. Can you describe the source of this disagreement?
6) Is having a continuous domain for value function v essential for the proposed training algorithm? 

 </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ryeGl4s_nX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Useful but incremental improvement over previous work; Clarity of writing needs improvement</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=By40DoAqtX&amp;noteId=ryeGl4s_nX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper313 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper313 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">- The writing and structure of the paper can be improved. It is difficult to read without first reading Gygli et al. 2017, and this paper should be more self-contained. There are also many parts that are not clear: 
  1. What is the model structure of G? Is it another neural network, or other structured prediction approaches such as graphical models? 
  2. The GAN-based approaches listed in the experiments section are originally designed for learning generative models. What are the adaptations required to turn them into structured prediction models? This is not clear at all. 

- The convergence of GAN training is an ongoing research problem and in practice also affects the quality of results. Yet in this paper I don't see any details on how these adversarial networks are trained jointly (e.g., heuristics to balance the progress on G and D). The authors should give more details on these.   

- The main difference of this paper compared to Gygli et al. 2017 seems to be the joint learning of a prediction model G. Instead of relying only on the valuation network D and starts iterative gradient ascent on the initial prediction of a vector y^(0) of all zeros, the authors start the iterative gradient ascent with the prediction from G (equation 7). Otherwise the paper looks very similar to Gygli et al. 2017, including the training sample generation methods. So to me the main message of this paper is that you can improve deep value networks by providing a better starting point in inference with G. The improvement is somewhat small though, between 1-2% on the datasets shown in the experiments section. 

- Overall this paper gives a useful but incremental improvement over the deep value network proposed by Gygli et al. 2017. However, the writing should be substantially improved to make the paper more self-contained and to include missing experiment details. 


</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1xC2G4Ihm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting ideas, but needs a few more justifications for certain modeling choices over others</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=By40DoAqtX&amp;noteId=B1xC2G4Ihm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper313 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper313 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
This paper effectively learns a variant of a Deep Value Network (Gygli et al 2017), a model consisting of an energy network that assigns scores to input-output tuples that is trained to mimic a task-specific loss. The primary differences between the model presented in this work (titled LDRSP) and DVNs are twofold: first, the initial label prediction used at test time for inference is the output of a model rather than being initialized to all zeros. Second, a GAN-inspired loss is used to train both the scoring function and the initial prediction estimator. This new setup is compared against a variety of recent structured prediction methods on the tasks of multilabel classification, semantic segmentation, and 3-class face segmentation.

Comments:
I think the ideas presented in this paper are interesting, but I think their presentation could be a bit clearer. As mentioned in the summary, what you’re presenting is still more or less a deep-value network with some additions - however, you don’t refer to it as such in the body of the paper anywhere I saw. The first addition is the use of a learned model to produce the initial prediction; this is a natural extension to Deep Value Networks, and on its own is somewhat incremental in nature. I do not think you adequately explained why you chose to use a GAN-like loss to learn these models. Another baseline that would have helped justify its use would be to train your G model to predict structured outputs in the standard way (max-margin or cross-entropy loss) and then train your energy function in the DVN way. 

The experimental settings are somewhat small in scope but follow the precedent set by previous structured prediction papers, which is fine. You make appropriate comparisons against previous structured prediction models as well as against different types of GAN-like losses. But, as I mentioned before, I think you needed to have more comparisons against different ways of training these networks that do not follow a GAN-inspired framework. 

Overall, I like the new ideas in this paper but I think a few more experimental settings are required before they should be published.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJlfxvBAsQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>question about Mean-IOU and Global-IOU</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=By40DoAqtX&amp;noteId=BJlfxvBAsQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018</span><span class="item">ICLR 2019 Conference Paper313 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I have a question about the evaluation metrics: mean-IOU and global IOU. I cannot find the metric global IOU online, and I am not in the CV field.  Could you please give a formal definition, or tell us how do you compute it ? 

In the paper, you mention that the mean-IOU is mean image IOU, and global-IOU is IOU over whole test dataset. For mean-IOU, do you mean to compute the foreground IOU for each test image, and then take the average ?  Could you please explain these two metrics more clearly? Thanks a lot. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1eHsdAAim" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>mean IOU and global IOU</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=By40DoAqtX&amp;noteId=S1eHsdAAim"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper313 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018</span><span class="item">ICLR 2019 Conference Paper313 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your interest in our paper. The mean IOU is computing the foreground IOU for each test image, and then taking the average [2]. The global IOU is  |Y \cap Y^| / |Y \cup Y^|, where Y and Y^ are the sets of ground truth and predicted foreground pixels [1].

[1] Yang, Jimei, Safar, Simon, and Yang, Ming-Hsuan. Maxmargin boltzmann machines for object segmentation. CVPR, 2014.
[2] Y. Li, D. Tarlow, and R. Zemel. Exploring compositional high order pattern potentials for structured output learning. In CVPR, 2013.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1eycNwkhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Mean IOU</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=By40DoAqtX&amp;noteId=S1eycNwkhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018</span><span class="item">ICLR 2019 Conference Paper313 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your reply. 

However, I find that the mean-IOU should be defined as the average over IOUs of all classes [1]. For the case of horse experiment, should the background be counted as one class ? 

[1]. Long, Jonathan, Evan Shelhamer, and Trevor Darrell. "Fully Convolutional Networks for Semantic Segmentation."</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJx5Pv0k27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Mean IOU</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=By40DoAqtX&amp;noteId=rJx5Pv0k27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper313 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">26 Oct 2018</span><span class="item">ICLR 2019 Conference Paper313 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">For the horse segmentation, former works [1,2] only reported the foreground IOU. To make a fair comparison, we followed their settings and only reported the foreground IOU.

[1] Y. Li, D. Tarlow, and R. Zemel. Exploring compositional high order pattern potentials for structured output learning. In CVPR, 2013.
[2] Yang, Jimei, Safar, Simon, and Yang, Ming-Hsuan. Maxmargin boltzmann machines for object segmentation. CVPR, 2014.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_B1xPQoAycX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>you may be interested in our related paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=By40DoAqtX&amp;noteId=B1xPQoAycX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Kevin_Gimpel1" class="profile-link">Kevin Gimpel</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Oct 2018</span><span class="item">ICLR 2019 Conference Paper313 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Nice paper! We wanted to bring to your attention our ICLR 2018 paper (Tu and Gimpel, "Learning Approximate Inference Networks for Structured Prediction", <a href="https://arxiv.org/abs/1803.03376" target="_blank" rel="nofollow">https://arxiv.org/abs/1803.03376</a> ) which similarly combines structured prediction and a GAN-like learning formulation with separate networks for scoring and inference. There are some differences in the actual objectives used (we use hinge losses with a value (cost) function while you use regression to predict the value function directly if I understand correctly), and we use inference networks for amortized/distilled inference while you additionally use gradient descent on the output of your generator. But in general, glad to see more evidence that adversarial learning frameworks are effective for structured prediction!</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>