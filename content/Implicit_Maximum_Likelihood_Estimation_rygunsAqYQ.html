<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Implicit Maximum Likelihood Estimation | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Implicit Maximum Likelihood Estimation" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rygunsAqYQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Implicit Maximum Likelihood Estimation" />
      <meta name="og:description" content="Implicit probabilistic models are models defined naturally in terms of a sampling procedure and often induces a likelihood function that cannot be expressed explicitly. We develop a simple method..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rygunsAqYQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Implicit Maximum Likelihood Estimation</a> <a class="note_content_pdf" href="/pdf?id=rygunsAqYQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019implicit,    &#10;title={Implicit Maximum Likelihood Estimation},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rygunsAqYQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Implicit probabilistic models are models defined naturally in terms of a sampling procedure and often induces a likelihood function that cannot be expressed explicitly. We develop a simple method for estimating parameters in implicit models that does not require knowledge of the form of the likelihood function or any derived quantities, but can be shown to be equivalent to maximizing likelihood under some conditions. Our result holds in the non-asymptotic parametric setting, where both the capacity of the model and the number of data examples are finite. We also demonstrate encouraging experimental results. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">likelihood-free inference, implicit probabilistic models</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We develop a new likelihood-free parameter estimation method that is equivalent to maximum likelihood under some conditions</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">5 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rylhASZchX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Novel and interesting idea, but significant algorithmic and empirical concerns</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygunsAqYQ&amp;noteId=rylhASZchX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper726 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper726 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Two high-level points about my review before going into the details:
1. This paper was a thoroughly enjoyable and insightful read. Kudos to the authors for attempting such a comprehensive overview of likelihood-based vs. likelihood-free learning.
2. I’ll be more than happy to revise my current rating if my concerns are addressed by the authors.

With regards to the technical assessment of this work, the idea of using a nearest neighbors objective for learning a generative model is both intriguing and appealing. What makes this work even more interesting are its connections with maximum likelihood estimation. Novelty aside, I believe there are major theoretical, algorithmic, and empirical concerns in the current work which I discuss below:

Theorem 1 
- The third condition is true for location-scale family of distributions e.g., Gaussian. But the distribution learned by a generative model p_theta is far from Gaussian or other location-scale distributions. 
- More importantly, I don’t think the upper bound is tight in practice because the likelihoods can vary significantly across the dataset. Take MNIST for example. Compare the log-likelihoods of an autoregressive model or ELBOs of a VAE across the different classes of digits. Straight digits (like 1s) have much higher log-likelihoods on average than curved digits. 

Algorithm
- While significant advancements have indeed been made for nearest neighbor evaluation as the authors highlight, it’s hard to believe without any empirical evidence that nearest neighbor evaluation is indeed efficient in comparison to other methods of likelihood evaluation.
- Similarly, I was a bit disappointed by the choice of Euclidean distance in a pixel space as the choice of distance metric. The argument that you do not want to use “auxiliary sources of labelled data or leverage domain-specific prior knowledge” is indeed necessary for fair comparisons, but also points to a limitation of the current approach.

Empirical evaluation 
- Seems too outdated both in terms of baselines and metrics. The authors are clearly aware of the current research in generative modeling but the current work provides almost no strong evidence to consider this work as an alternative to other approaches.
- While it is arguably well-established that Parzen window estimates are misleading (Theis et al.), that’s the only quantitative estimate in this work (Table 1). Hard to think of any recent published work (last 1-2 years) in generative modeling that even reports these estimates.
- The baselines in Table 1 are all from 2013-15. Clearly, much has happened in the last 3 years that merit the inclusion of more recent baselines.
-  Even for sample quality, there has been a lot of research in designing and improving metrics. E.g., Inception scores, Frechet Inception Distance, Kernel Inception Distance. I am not looking for state-of-the-art numbers, showing heavily zoomed out samples without any of these metrics is slightly disingenuous.
- As mentioned before, reporting the computation time/per iteration and number of iterations for convergence for the proposed algorithm in comparison with other approaches  is important.
- Similarly, the argument about the method avoiding even the other GAN problems (e.g., vanishing gradients, stability in training) can and should be supported by empirical evidence.

Analysis and discussion
- One family of generative models that is crucially missing from this work is normalizing flow models. 
- This is somewhat debatable, but I do not agree that the tradeoff between likelihoods and sample quality is due to model capacity. As far as I can tell, the cited work of Grover et al., 2017 provides evidence contrary to what the authors claim. The prior work trained the same normalizing flow model via maximum likelihood and adversarial training, and observed vastly different results on likelihood and sample quality metrics. So model capacity isn't necessarily the key differentiating factor (which is same for both training algorithms in their experiments), it's more about the choice of the objective function and the optimization procedure.

Minor points for improving presentation:
- Section 3 can be made more concise and to the point. I’d be especially interested if the precision and recall discussion in this section and elsewhere can be formalized.
- Use numbered lists instead of bullets for assumptions in Theorem 1, so that the discussion of the assumptions right after the theorem statement are easy to follow.
- The citation of Grover et al. seems outdated? The current title is Flow-GAN: Combining maximum likelihood and adversarial learning in generative models.
- In general, avoid making somewhat hard assertions that are speculative. Some of them I’ve highlighted earlier in my review (e.g., some of the theorem assumptions being typically true, comparison of likelihood and sample quality based on model capacity etc.).</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJxKXwZYnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea, but there is room for improving the presentation and the strength of the results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygunsAqYQ&amp;noteId=HJxKXwZYnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper726 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper726 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a new algorithm for implicit maximum likelihood estimation based on a fast Nearest Neighbor search. The algorithm can be used to implicitly maximize the likelihood of models for which the former quantity is not intractable but for which sampling is easy which is typically the case for implicit models.  The paper shows that under some conditions the optimal solution of the algorithm corresponds to the MLE solution and provides some experimental evidence that the method leads to a higher likelihood. However, The paper lacks clarity and the experiments are not really convincing. Here are some remarks:
Experiments:
- The estimated likelihood was reported on table 1 using parson window which is known to have bad scaling behavior with the dimension of data. In the end, the table compares methods that maximize different objectives and are evaluated with an unreliable metric. Here are two possible experiments that could be more informative:
- Consider toy examples for which the likelihood can be evaluated and the MLE obtained easily and then compare with the proposed method. This would already give a good sense of how well the algorithm behaves in simple cases.
- Another possibility is to use generative models like Real-NVP for which the likelihood can also be computed in closed form. This would allow comparing the proposed algorithm to direct likelihood maximization on more complicated datasets as done in [1].
It seems like having experiments of this nature is far more convincing than a long justification for why the results are not necessarily state-of-the-art.
- There are way too many samples on the figures so it is very hard to perform any visual assessment.

Theory:
- More discussions of the assumptions are needed, concrete examples for which these assumptions hold or not would be very useful. 
- Lemma 2 is a direct consequence of the following result: if p is continuous at x_0 then x_0 is a Lebesgue point.

General remarks on the paper:
- What complexity is the nearest neighbor algorithm? Since it is crucial for the proposed method to be scalable it is worth presenting this algorithm at a high level in the main paper.
- The discussion in section 3 could be much more concise if concrete examples and figures were provided. Most of the facts discussed in that section are generally well understood, so conciseness is very appreciated in this case.
- «  A secondary issue that is more easily solvable is that samples presented in papers are sometimes cherry-picked; as a result, they capture the maximum sample quality, but not necessarily the mean sample quality. » Could you please provide an example of such paper? I would be very interested in having a closer look.
- In the last paragraph of section 5, it is said that although the samples may not be state of the art in terms of precision, other methods which achieve better precision «  may » have less recall. It would be good to have empirical evidence to back this claim.


[1] I. Danihelka, B. Lakshminarayanan, B. Uria, D. Wierstra, and P. Dayan. Comparison of Maximum Likelihood and GAN-based training of Real NVPs.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1xeMzFO3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice Theory, Questionable Practicality</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygunsAqYQ&amp;noteId=S1xeMzFO3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper726 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper726 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:

This paper proposes a nearest-neighbor-based algorithm for implicit maximum likelihood.  Samples are produced by the generator network and then a nearest neighbors algorithm is run to match the samples with their nearest data point.  The generator is then updated using the Euclidean distance between samples and neighbors as the optimization objective.  Six conditions are then provided, and if they are met, then the authors show that this method is performing maximum likelihood on the implied density.  Experiments report Parzen window density estimates, samples from the model, and latent-space interpolations for MNIST, Toronto Faces, and CIFAR-10.

Pros: 

The primary contribution of this paper is an algorithm for implicit likelihood maximization with theoretic guarantees.  As far as I’m aware, this is a novel and noteworthy contribution.  Moreover, as each sample must be paired with an observation, it does seem like the algorithm would be somewhat robust to the notorious mode collapse problem.

Cons:

My primary critique of the paper is that there is very little experimental investigation of the crucial details of the algorithm.  Firstly, running the nearest neighbors algorithm seems like it could be a computational bottleneck.  The authors acknowledge this, but then say “this is no longer the case due to recent advances in nearest neighbor search algorithms (Li &amp; Malik 2016; 2017)” (p 3-4).  No other justification is given, from what I can tell.  A simulation showing how the runtime scales with dimensionality or number of data points would be very useful for knowing the scalability and practicality of the algorithm.  In the same vein, showing that the algorithm works well even with a relaxation such as approximate neighbors or random projections would make the algorithm more attractive to adopt.  

Moreover, I found it frustrating that the paper teases a fix to several well-known GAN issues: “The proposed method could sidestep the three issues mentioned above: mode collapse, vanishing gradients and training instability” (p 3).  But the paper never experimentally investigates if the proposed approach indeed is better in these aspects.  I was disappointed since, intuitively, the algorithm does seem like it could be robust to mode collapse.  In addition to this lack of experimental focus, the only quantitative result is the Parzen window estimates in Table 1.  The proposed method does best the others but the other reported results are quite old---all from 2015 or earlier. 

Minor points: 

The paper is at 10 pages, and while it is well-written, the writing is verbose and could be use tightening.  


Evaluation:  This paper presents an interesting contribution: an implicit likelihood estimation algorithm amenable to theoretical analysis.  Moreover, the theory seems not too divorced from practice (but I didn't check every detail).  However, the evaluation of this method is where the paper falters.  A big issue (that the authors note themselves) is the practicality of performing repeated nearest neighbor iterations.  No runtimes are report, nor are any approximations considered.  Rather, samples and interpolations are given the most discussion.  Furthermore, there is no demonstrations of training stability or quantitative analysis of mode collapse.  Due to these experimental deficiencies, I recommend rejection, weakly.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJgIh9jniX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The method is almost identical to "Generative Latent Optimization"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygunsAqYQ&amp;noteId=SJgIh9jniX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">23 Oct 2018</span><span class="item">ICLR 2019 Conference Paper726 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">As far as I understand, the only difference between the proposed method and the Generative Latent Optimization(<a href="https://arxiv.org/abs/1707.05776" target="_blank" rel="nofollow">https://arxiv.org/abs/1707.05776</a> , published at ICML 2018) is a very minor detail. The GLO optimizes for the latent codes whereas the submission keeps them fixed. The rest of the method is exactly same. Am I missing something?

If I am correct; the authors should discuss the differences properly, cite the paper and provide an empirical comparison. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJljEH0Aim" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>It's actually quite different</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rygunsAqYQ&amp;noteId=BJljEH0Aim"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper726 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018</span><span class="item">ICLR 2019 Conference Paper726 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">That's actually not correct. There are two major differences:

(1) GLO is not a probabilistic model, because there is no probability density associated with the latent vectors z. Instead it simply learns a mapping from some latent space to the output space, in the hope that the latent space is easier to model than the original output space (in this sense, it is more closely related to dimensionality reduction methods like PCA and autoencoders). In order to generate novel examples, one still needs to fit a probabilistic model to the learned latent vectors. In the case of GLO, a Gaussian is used for this purpose. In contrast, the proposed method trains a stand-alone probabilistic model, and so there is no need to fit another probabilistic model. 

(2) GLO enforces a 1-1 mapping between the latent vectors z and the data examples, whereas the proposed method does not. Not enforcing a 1-1 mapping is critical for showing equivalence to maximum likelihood, because of the asymmetric nature of KL-divergence. More specifically, maximum likelihood corresponds to minimizing D_KL(data || model), and it is well-known that minimizing D_KL(model || data), which swaps the data and the model distributions, is *not* equivalent to maximum likelihood. Now, suppose that we enforce a 1-1 mapping between the latent vectors and the data examples, then swapping the latent vectors and data examples in the proposed loss function would not change the loss function. This shows that if we were to enforce a 1-1 mapping, minimizing the loss cannot be equivalent to maximum likelihood.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>