<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Temporal Gaussian Mixture Layer for Videos | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Temporal Gaussian Mixture Layer for Videos" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Hyfg5o0qtm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Temporal Gaussian Mixture Layer for Videos" />
      <meta name="og:description" content="We introduce a new convolutional layer named the Temporal Gaussian Mixture (TGM) layer and present how it can be used to efficiently capture longer-term temporal information in continuous activity..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Hyfg5o0qtm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Temporal Gaussian Mixture Layer for Videos</a> <a class="note_content_pdf" href="/pdf?id=Hyfg5o0qtm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 10 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019temporal,    &#10;title={Temporal Gaussian Mixture Layer for Videos},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Hyfg5o0qtm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=Hyfg5o0qtm" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We introduce a new convolutional layer named the Temporal Gaussian Mixture (TGM) layer and present how it can be used to efficiently capture longer-term temporal information in continuous activity videos. The TGM layer is a temporal convolutional layer governed by a much smaller set of parameters (e.g., location/variance of Gaussians) that are fully differentiable. We present our fully convolutional video models with multiple TGM layers for activity detection. The experiments on multiple datasets including Charades and MultiTHUMOS confirm the effectiveness of TGM layers, outperforming the state-of-the-arts.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_B1g-RVbO3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An Official Review: Temporal Gaussian Mixture Layer for Videos</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hyfg5o0qtm&amp;noteId=B1g-RVbO3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper504 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper504 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents Temporal Gaussian Mixture (TGM) layer, efficiently capturing longer-term temporal dependencies with smaller number of parameters. The authors apply this layer to the activity recognition problem, claiming state-of-the-art performance compared against several baselines.

Strong points of this paper:
 - The authors clearly described the proposed layer and model step by step, first explaining TGM layer, followed by single layer model, then generalizing it to multi-layer model.
 - The authors achieved state-of-the-art performance on multiTHUMOS dataset. The results look great in two aspects: the highest MAP scores shown in Table 1, and significantly smaller number of parameters shown in Table 2 to achieve the MAP scores.

Questions:
 - Basically, the idea in this paper is proposing to parameterize conv layers with Gaussian mixtures, with multiple pairs of mean and variance. Although Gaussian mixtures are powerful to model many cases, it might not be always to perfectly model some datasets. If a dataset is highly multi-modal, Gaussian mixture model also needs to have large M (number of mixture components). It is not clear how the authors decided hyper-paramter M, so it will be nicer for authors to comment the effect of different M, on various dataset/task if possible.
 - Same for the hyper-parameter L, the temporal duration. It will be nicer to have some experiments with varied L, and to discuss how much this model is sensitive to the hyper-parameter.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1e-_cpm67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Added experiments to the appendix</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hyfg5o0qtm&amp;noteId=B1e-_cpm67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper504 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper504 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review and the suggestions. Please find our answers below to address your concerns. We also have updated the paper.

1) Effect of different M values on different datasets:

We conducted a new set of experiments for this, and added them to the appendix. Appendix B shows experimental results comparing different values for M and C_out, which are the parameters that determine the number of Gaussians and mixtures. On both Charades and MultiTHUMOS, we find that M=16 and C_out&gt;=16 performs best. Larger values of M lead to slightly lower performance, likely because they introduce more parameters and end up learning a kernel similar to a standard 1-D convolution. Smaller values of M limit the possible temporal kernels, which reduces performance quite a bit. Larger values of C_out do not impact performance, likely learning redundant information.

2) Effect of different L:

Similarly, we added multiple experimental results comparing different values of L for models with 1 TGM layer, 3 TGM layers, and one 1-D Conv layer. We find that on MultiTHUMOS, setting L=15 (1 layer)/5 (3 layers) performs best (which was our default setting in the paper), as this captures about 8 seconds of information (i.e., +- 4 seconds) and activities are on average 3.3 seconds in MultiTHUMOS. However, on Charades, we found that larger values of L performed better. The average activity length is 12.8 seconds, and we found setting L=30 with 3 TGM layers performed best. This increased the overall performance of our {two-stream I3D + 3 TGM layers} model from 21.8 to 22.3. Since the number of parameters in our TGM layer is independent of L, we were able to increase the performance by using larger L values, while 1-D conv needs to learn many more parameters for larger L and it decreases performance. Thank you for suggesting these experiments.

We have also added a figure in the appendix (Figure 6)  illustrating the learned TGM kernels which visually confirms that the TGM captures longer temporal information for Charades and shorter intervals for MultiTHUMOS.

Please let us know if this answers your questions and if you have any others.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1gsmbhDnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>TGMs are an interesting idea but need better justification and comparisons to standard 1D convolutions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hyfg5o0qtm&amp;noteId=S1gsmbhDnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper504 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper504 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
This paper proposes a temporal convolution layer called Temporal Gaussian Mixture (TGM) for video classification tasks. Usually, video classification models can be decomposed into a feature extraction step, in which frames are processed separately or in local groups, and a classification step where labels are assigned considering the full video features/longer range dependencies. Typical choices for the classification step are Recurrent Neural Networks (RNNs) and variations, pooling layers and 1D temporal convolutions. TGMs are 1D temporal convolutions with weights defined by taking samples from a mixture of Gaussians. This allows to define large convolutional kernels with a reduced number of parameters -the mean and std of the gaussians- at the cost of having reduced expressiveness. The authors experiment replacing 1D temporal convolutions with TGMs in standard video classifications models and datasets and report state-of-the-art results.

Strengths:
[+] The idea of defining weights as samples from a Mixture of Gaussians is interesting. It allows to define convolutional kernels that scale with the number of channels (equivalent to the number of Gaussians) instead of the number of channels and receptive field.

Weaknesses:
[-] The explanation of the TGM layers is very confusing in its current state. 

Certain aspects of TGMs that would help understand them are not clearly stated in the text. For example, it is not clear that 1) TGMs are actually restricted 1D convolutions as the kernel is expressed through a mixture of gaussians, 2) In a TGM the number of mixtures corresponds to the number of output channels in a standard 1D convolution and 3) despite TGMs are 1D convolutions, they are applied differently by keeping the number of frame features (output channels of a CNN) constant through them. Section 3 does not explain any of these points clearly and induces confusion.

[-] The comparisons in the experimental section are unfair for the baselines and do not justify the advantages of TGMs. 

TGMs are 1D convolutions but they are applied to a different axis - 1D convolutions take as input channels the frame features. Instead, TGMs add a dummy axis, keep the number of frame features constant through them, and finally use a 1x1 convolution to map the resulting tensor to the desired number of classes. There is no reason why TGMs can’t be used as standard 1D convolutions taking as the first input channels the frame features - in other words, consider as many input mixture components as frame features for the first TGM. This is a crucial difference because the number of parameters of TGMs and the baselines is greatly affected by it.

The implicit argument in the paper is that TGMs perform better than standard 1D convolutions because they have less parameters. However, in the experiments they compare to 1D convolutions that have smaller temporal ranges – TGMs have a temporal kernel of 30 timesteps or 10 per layer (in a stack of 3 TGMs), whereas the 1D convolutions either have 5 or 10 timestep kernels according to section 4. Thus, it is impossible to know whether TGMs work better because 1) in these experiments they have longer temporal range – we would need a comparison to 1D convolutions applied in the same way and with the same temporal range to clarify it, 2) because they are applied differently – we would need a comparison when TGMs are used the same way as 1D convolutions, 3) because the reduced number of parameters leads to less overfitting – we would need to see training metrics and see the generalization gap when compared to equivalent 1D convolutions or 4) because the reduced number of parameters eases the optimization process. To sum up, it's true that TGMs would have a reduced number of parameters compared to the equivalent 1D convolutions with the same temporal range, but how does that translate to better results and, in this case, why is the comparison made with non-equivalent 1D convolutions? What would happen if the authors compared to 1D convolutions used in the same way as TGMs?

[-] To claim SOTA, there are comparisons missing to recently published methods.
In particular, [1] and [2] are well-known models that report metrics for the Charades dataset also used in this paper.

Rating:
At this moment I believe the TGM layer, while a good idea with potential, is not sufficiently well explained in the paper and is not properly compared to other baselines. Thus, I encourage the authors to perform the following changes:

-Rewrite section 3, better comparing to 1D convolutions and justifying why TGMs are not used in the same manner but instead using a dummy axis and a linear/1x1 conv layer at the end and what are the repercussions of it.
-Show a fair comparison to 1D convolutions as explained above.
-If claiming SOTA results, compare to [1] or [2] which use different methods to the one proposed in these paper but outperform the baselines used in the experiments.

[1] Wang, Xiaolong, et al. "Non-local neural networks." CVPR 2018
[2] Zhou, Bolei et al. "Temporal Relational Reasoning in Videos" ECCV 2018</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJgDnqTm6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Revised section 3 and added more comparisons in the appendix</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hyfg5o0qtm&amp;noteId=SJgDnqTm6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper504 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper504 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your detailed comments. We have updated the paper to address your concerns.

1) Explanation of the TGM layers:

We have revised Section 3 to clarify our approach and better compare to 1-D convolution and justify our design decisions. In addition, we added a new figure in the appendix (Figure 7), to help the understanding of the difference between the TGM layer, the standard 1D convolution, and other forms of using Gaussian mixtures. Please let us know if you find the revised version helpful.

2) Comparison to more baselines and clarification of our L setting:

As we mentioned in the previous comment, we revised the paper to clarify our L setting. All the baselines and the TGM layers use the identical default L setting of L=30/10 (for InceptionV3) and L=15/5 (for I3D), for 1-layer/3-layers, unless otherwise mentioned. We revised the paper to make this explicit.

In Appendix C we added more experiments comparing different forms of the Gaussian mixture layers as suggested (Table 9). These include learning of temporal Gaussian mixtures to be convolved in the standard 1-D convolution fashion without temporal channel axis (Figure 7b-c), and a temporal convolutional layer with channel combination but without Gaussians (Figure 8). We found that none of these alternatives performed as well as our TGM layer, confirming that both the Gaussian mixture and added temporal channel axis are important for the performance.

3) Comparison to Non-local neural networks and Temporal Relational Reasoning networks:

We believe there is a misunderstanding/confusion and we want to clarify it.

We want to emphasize that we are not claiming state-of-the-art accuracy on activity 'classification', but claiming the state-of-the-art  accuracy on temporal activity 'detection' from continuous videos. Classification and detection (often also called 'localization') are relevant but different problems. Both non-local neural networks and Temporal Relational Reasoning focuses on the activity classification problem and they only report their performances on the classification datasets. To our knowledge, there is no prior work reporting better detection accuracy than ours on the continuous video public datasets we used: MultiTHUMOS and the Charades_v1_localize setting of Charades.

To be more specific, non-local neural networks and Temporal Relational Reasoning networks both test on the 'classification' setting of Charades, while we are focusing on the 'localization' setting (i.e., dense labeling). Note that Charades has two different evaluation settings and maintain the results of these two settings separately.

One may argue to extend such classification works for the detection, but there are limitations. Due to the design of the non-local neural network, it is non-trivial to extend it to variable length, continuous videos as the non-local layer requires reshaping to a fixed THW tensor. This does not support variable length videos, which is necessary for dense-labeling detection tasks with variable length, continuous videos. Temporal Relation Reasoning networks were only evaluated on the classification setting and provided lower performance than the I3D we use as a baseline (25.2 for TRN vs. 34.4 for I3D). If it were extended to the localization setting, we believe it would become a weaker baseline than I3D.

Further, notice that we can always apply TGM layers on top of any baseline CNNs (e.g., I3D or anything better) to increase the performance by capturing longer temporal information.

Please let us know if this answers your questions and if you have any others.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJxVQYuypX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Regarding the comment on the experimental comparison being unfair due to TGM kernels using larger temporal size</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hyfg5o0qtm&amp;noteId=BJxVQYuypX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper504 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper504 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the detailed comments. We will get back to you later with more detailed answers/revisions to address your concerns. Here, we would like to clarify one important confusion/misunderstanding as soon as possible.

- Regarding the comment on the experimental comparison being unfair due to TGM kernels using larger temporal size:

In all our experiments, the TGM layers, standard 1D convolutional layers, and temporal pyramid pooling layers used the filters with the identical temporal length (i.e., L). To be more specific, when using InceptionV3 as the base architecture (obtained at 8fps), our L was 30 for 1-layer models and 10 for 3-layer models. When using I3D as the base architecture (obtained at 3fps), our L was 15 for 1-layer models and 5 for 3-layer models. This was the case for both the TGM layers and the standard 1D conv layers.

We will revise the paper to clarify this more explicitly. The current version of the paper describes that L=30 or 10 in Section 4.1 (we meant for InceptionV3) while also indicating that L=10 for InceptionV3 and L=5 for I3D in Appendix A, which we agree is confusing.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1gd1mL8n7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good paper, need more justification on the learned features by TGM</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hyfg5o0qtm&amp;noteId=H1gd1mL8n7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper504 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper504 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper introduces a new convolutional layer named the Temporal Gaussian Mixture (TGM) layer, and present how it can be used for activity recognition. The kernels of the new layer are controlled by a set of (temporal) Gaussian distribution parameters, which significantly reduce learnable parameters. The results are complete on four benchmarks and show consistent improvement. I just have minor comments. 

1. I am curious what the learned feature look like. As the author mentioned, "The motivation is to make each temporal Gaussian distribution specify (temporally) ‘where to look’ with respect to the activity center, and represent the activity as a collection/mixture of such temporal Gaussians convolved with video features." So does the paper achieve this goal? 

Another thing is, can the authors extract the features after TGM layer, and maybe perform action recognition on UCF101 to see if the feature really works? I just want to see some results or visualizations to have an idea of what TGM is learning. 

2. How long can the method actually handle? like hundreds of frames? Since the goal of the paper is to capture long term temporal information. 

3. It would be interesting to see an application to streaming video. For example, surveillance monitoring of human activities. 



</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJlu1iaQpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Added visualizations to the appendix</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hyfg5o0qtm&amp;noteId=SJlu1iaQpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper504 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper504 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review. We have updated the paper to address your questions.

1) Filter visualization:

We have added a figure to the appendix (Figure 6) illustrating examples of the learned TGM kernels. The most interesting aspect of this figure is that it shows the kernels focus on short temporal intervals on MultiTHUMOS even if we make the filters long, as the activities are an average of 3.3 seconds long. On Charades, the TGM kernels learn to capture much longer intervals, as the activities are an average of 12.8 seconds long. We believe that this suggests TGMs are learning to capture information from the important necessary intervals.

TGMs aren't really suitable for datasets like UCF101, as those videos are quite short (2-3 seconds on average). CNNs like I3D already capture 2-3 seconds of temporal information, so there isn't much benefit to applying TGMs there. 

2) Our approach is able to handle continuous (streaming) videos of any duration by its design. Average lengths of videos in MultiTHUMOS is around 2.5 minutes and those in Charades is around 30 seconds. Within these videos, the TGM layers are able to capture the longer temporal structure than prior works. Using L=30 in the 3 layer model on Charades with I3D, we are able to capture ~800 frames (roughly +-15 seconds from each frame) of temporal information. In contrast, I3D only captures 99 frames (+- 2 seconds).

3) As our TGM layer is (temporally) convolutional, it can easily be applied to streaming video in an online, convolutional fashion.

Please let us know if this answers your questions and if you have any others.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>