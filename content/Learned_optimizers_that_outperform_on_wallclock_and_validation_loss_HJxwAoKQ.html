<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learned optimizers that outperform on wall-clock and validation loss | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learned optimizers that outperform on wall-clock and validation loss" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HJxwAo09KQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learned optimizers that outperform on wall-clock and validation loss" />
      <meta name="og:description" content="Deep learning has shown that learned functions can dramatically outperform hand-designed functions on perceptual tasks. Analogously, this suggests that learned update functions may similarly..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HJxwAo09KQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learned optimizers that outperform on wall-clock and validation loss</a> <a class="note_content_pdf" href="/pdf?id=HJxwAo09KQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019learned,    &#10;title={Learned optimizers that outperform on wall-clock and validation loss},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HJxwAo09KQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Deep learning has shown that learned functions can dramatically outperform hand-designed functions on perceptual tasks. Analogously, this suggests that learned update functions may similarly outperform current hand-designed optimizers, especially for specific tasks. However, learned optimizers are notoriously difficult to train and have yet to demonstrate wall-clock speedups over hand-designed optimizers, and thus are rarely used in practice. Typically, learned optimizers are trained by truncated backpropagation through an unrolled optimization process. The resulting gradients are either strongly biased (for short truncations) or have exploding norm (for long truncations). In this work we propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer performance. This allows us to train neural networks to perform optimization faster than well tuned first-order methods. Moreover, by training the optimizer against validation loss, as opposed to training loss, we are able to use it to train models which generalize better than those trained by first order methods. We demonstrate these results on problems where our learned optimizer trains convolutional networks in a fifth of the wall-clock time compared to tuned first-order methods, and with an improvement</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Learned Optimizers, Meta-Learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We analyze problems when training learned optimizers, address those problems via variational optimization using two complementary gradient estimators, and train optimizers that are 5x faster in wall-clock time than baseline optimizers (e.g. Adam).</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Byetapx9nQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting method, but oversold results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxwAo09KQ&amp;noteId=Byetapx9nQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper900 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper900 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper tackles the problem of learning an optimizer, like "learning to learn by gradient descent by gradient descent" and its follow-up papers. Specifically, the authors focus on obtaining cleaner gradients from the unrolled training procedure. To do this, they use a variational optimization formulation and two different gradient estimates: one based on the reparameterization trick and one based on evolutionary strategies. The paper then uses a method from the recent RL literature to combine these two gradient estimates to obtain a variance that is upper-bounded by the minimum of the two gradients' variances. 

While the method for obtaining lower-variance gradients is interesting and appears useful, the application to learn optimizers is very much oversold: the paper states that the comparison is to "well tuned hand-designed optimizers", but what that comes down to in the experiments is Adam, SGD+Momentum, and RMSProp with a very coarse grid of 11 learning rates and *no regularization* and *no learning rate schedule*. The authors' proposed optimizer is just a one-layer neural net with 32 hidden units that gets as input basically all the terms that the hand-designed optimizers compute, and it has everything it needs to simply use weight decay and learning rate schedules -- precisely what you need for the authors' contributions (speed and generalization). This is a fundamental flaw in the experimental setup (in particular the choice of baselines) and thus a clear reason for rejection.

Some details:

- While the authors' method is optimized by training 5 x 0.5 million, i.e. 2.5 million (!) full inner optimization runs of 10k steps each, the hand-designed optimizers get to try 11 values for the learning rate, which are logarithmically spaced between 10^{-4} and 10 (i.e., very coarsely, with sqrt{10} difference between successive values; even just for this fixed learning rate one would want to space factors by as little as 1.1 or so in the optimal region). 

- The lack of any learning rate schedule for the baselines is highly problematic; it is common knowledge that learning rate schedules are important. This is precisely why one would want to do research on learning optimizers to set the learning rate! Of course, without learning rate schedules one will not obtain a very efficient optimizer and it is easy to show large speedups over that poor baseline (the authors' first stated contribution in the title).

- The authors' second stated contribution is that their learned optimizers generalize better than the baselines. But they pass their optimizers all information required to learn arbitrary weight decay, while the baselines are not allowed to use any weight decay. Thus, the second stated contribution in the title also does not hold up.

- There are many details in the experiments that would be hard to reproduce truthfully. Given the reproducibility crisis in machine learning, I would trust the results far more if the authors made their code available in anonymized form during the review period. If the authors did this I could also evaluate it against properly tuned baseline optimizers myself. In that case I would lean towards increasing my score since the availability of code for this line of work would be very useful for the community.

- Page 4 didn't print for me; both times I tried it came out as a blank page. 

- Several issues on page 4: 
 - I don't see why the unnumbered equation necessarily leads to an exponential increase; H^{(j)} can be different for each j, such that there isn't a single term being exponentiated. Or am I mistaken?
 - The problem in Figure 3a is not the problem discussed in the text
 - The global minimum of the function is not 0.5 as stated in the caption
 - It is not stated what sort of MLP there is in Figure 3d (again, code availability would fix things like this)

- Section 5 is extremely dense. This is the paper's key methodological contribution, and it is less than a page! I would suggest that the authors describe these methods in more detail (about another page) and save space elsewhere in the paper.

The paper is written well and the illustrations of the issues of TBPTT, as well as the authors' fix are convincing. It's a shame, but unfortunately, the stated contributions for the learned optimizers do not hold up.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJlea51tTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxwAo09KQ&amp;noteId=SJlea51tTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper900 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper900 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your thoughtful review! We will take these under consideration for a future submission. Comments addressed below.

Lack of good baseline:
You raise a good point. We will update the paper to include a more extensive hyperparameter search for the baselines, including a denser learning rate search, and a search over regularization and learning rate decay parameters. We will also soften the claims surrounding speedups over hand designed optimizers.

However, we would like to reemphasize that learned optimizers have not previously been shown to beat, or even match, standard first order optimizers on wall clock time. We would also like to reemphasize that previous approaches to meta-training learned optimizers required many tricks, and a lucky choice of random seed. A primary aim of our experiments was to provide proof that the proposed meta-training method works well and is reliable enough to train a simple mlp meta-optimizer without the complex tricks employed in other works. We believe that this work represents a significant step forward in training learned optimizers, and we are gratified to hear that you thought our analysis sections and proposed fixes were convincing.

"I don't see why the unnumbered equation necessarily leads to an exponential increase; H^{(j)} can be different for each j, such that there isn't a single term being exponentiated. Or am I mistaken?"

You are correct that H^{(j)} can be different for each training iteration in the general case. In a quadratic setting, H is constant, and the outer-gradient will explode if the learning rate is too large. In the non-quadratic setting (any time the Hessian is changing) it is possible for this equation to be stable depending on the sequence of Hessians encountered. Empirically, we find that the optimal meta-parameters are right on the edge of instability, so it is common to enter the unstable regime, causing outer-gradients to then grow exponentially with the number of steps.

"The global minimum of the function is not 0.5 as stated in the caption":
This is a typo and will be fixed. Should be ~3.5 (the actual global min). Thank you for spotting this.

"The problem in Figure 3a is not the problem discussed in the text."
This is discussed in the last paragraph of page 4.

Reproducibility: We provide more detailed information about experiments in the appendix (page 11-12). Additionally, we are looking into options as to how to release evaluation code containing demonstrative problems and the weights of the learned optimizer.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkl4p-n6sm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting paper too condensed and difficult to understand</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxwAo09KQ&amp;noteId=rkl4p-n6sm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper900 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">24 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper900 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Review:

	This paper proposes a method to learn a neural network to perform optimization. The idea is that the neural network will receive as an input several parameters, including the weights of the network to be trained, the gradient, and so on, and will output new updated weights. The neural network that is used to compute new weights can be trained through a complicated process called un-rolled optimization. The authors of the paper show two problems with this approach. Namely, the gradients tend to explode as the number of iterations increases. Truncating the gradient computation introduces some bias. To solve these problems the authors propose a variational objective that smooths the objective surface. The proposed method is evaluated on the image net dataset showing better results than first order methods optimally optimized.

Quality: 

	The quality of the paper is high. It addresses an important problem of the community and it seems to give better results than first other methods.

Clarity: 

	The clarity of the paper is low. It is difficult  to follow and includes many abstract concepts that the reader is not familiar with. I have had problems understanding what the truncation means. Furthermore, it is not clear at all how the validation data is used as a target in the outer-objective.  It is also unclear how the bias problem is addressed by the method proposed by the authors. They have said nothing about that, yet in the abstract they say that the proposed method alleviates the two problems detected.

Originality: 

	As far as I know the idea proposed is original and very useful to alleviate, at least, one of the problems mentioned of exploding gradients.

Significance:

	It is not clear at all that the method is evaluated on unseen data when using the validation data for outer-training. This may question the significance of the results.

Pros:

	- Interesting idea.

	- Nice illustrative figures.

	- Good results.

Cons:

	- Unclear points in the paper with respect to what truncation means.

	- The validation data is used for training and there is no left-out data, which may bias the results.

	- Unclear how the authors address the bias problem in the gradients.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1ewyjyFpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxwAo09KQ&amp;noteId=r1ewyjyFpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper900 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper900 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your thoughtful review. We will take these under consideration for a future submission. We have addressed your comments below.

Truncation: Due to space limitations, we were unable to include a more comprehensive introduction to truncated backpropagation through time (TBTT). We would emphasize though that TBTT is a standard approach in training RNNs, and that it has an identical meaning in the case of backpropagation through many timesteps of unrolled optimization as it does in backpropagation through many timesteps of RNN dynamics. We will update the text to further emphasize this correspondence.

Unseen / validation data: We agree that the distinction between validation data on train tasks and validation data on *test* tasks was unclear. We have updated the text to clarify this. To answer your question: we never see any test data before test time. This includes both the training and validation/test images. We split the Imagenet dataset by class (700 for train, 300 for test) and outer-train on the training set (using both train and validation data from those 700 classes). When evaluating our model, we use the alternate set (the remaining 300 test classes), and only use the training images for training those models.

Combating biased gradients: Previous work was unable to train with longer truncations because of exploding gradients. In this work, we can simply make the truncation length longer which reduces the bias in the gradients. By unrolling longer, we drop fewer terms from the true gradient, thus lowering bias. We will make this connection clearer in the text.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Byl8hoXzom" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good idea, but needs more work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxwAo09KQ&amp;noteId=Byl8hoXzom"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper900 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper900 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
The paper presents a method for "learning an optimizer"(also in the literature Learning to Learn and a form of Meta-Learning) by using a Variational Optimization for the "outer" optimizer loss. The mean idea of the paper is to combine both the reparametrized gradient and the score-function estimator for the Variational Objective and weight them using a product of Gaussians formula for the mean. The method is simple and clearly presented. The paper also presents issues with the standard "learning to learn" optimizers, one being the short-horizon bias and as credited by the authors has been observed before in the literature, and the second one is what is termed the "exponential explosion of gradients" which I think lacks enough justification as currently presented (see below for details). The ideas are clearly stated, although the work is not groundbreaking, but more on combining several ideas into a single one. 

Experiments: 
The authors evaluate their method on a single task which consists of optimizing a 3-layer convolutional neural network on downsampled images from ImageNet. A key idea, not new to this work, is to optimize the meta-optimizer with respect to the validation dataset rather than the training, which seems to be crucial for any meaningful training to happen. Although the experiments do show so promising results, they seem to be somewhat limited (see below for details). There is also a small ablation study on how do different features presented to the optimizer affect its performance. Given the still small-scale experiments, I'm not sure this is a significant result for the community. 

Conclusion:
As a whole, I think the idea in the paper is a good one and worth investigating further. However, the objections I have on section 2.3 and the experiments seem to indicate that there needs to be more work into this paper to make it ready for publication. 


On section 2.3 and the explosion of gradients:

There is a mistake in the equation on page 4 regarding the "gradient with respect to the learning rate". Although the derivation in Appendix A is correct, the inner product in the equation starts wrongly from j=0, where it should in fact start at j = i + 1. To be more clear the actual enrolled equation for dw^T/dt for 3 steps back is:

dw^T/dt = (I - tH^{T-1})(I - tH^{T-2})(I - tH^{T-3}) dw^{T-3} - (I - tH^{T-1})(I - tH^{T-2}) g^{T-3} - (I - tH^{T-1}) g^{T-2} - g^{T-1} 

Hence the product must start at j = i + 1. 
It is correct that in this setting the equation is a polynomial of degree T of the Hessian, however, there are several important factors that the authors have not discussed. Namely, if the learning rate is chosen accordingly such that the spectral radius of the Hessian is less than 1/t then rather than the gradient exploding the higher order term will vanish. However, even if they do vanish for large T since the Hessian plays with smaller and smaller power to more recent gradients (after correcting the mistake in the equation) than the actual T-step gradient will never vanish (in fact even if tH = I then dw^T/dt = g^{T-1}). Hence the claims of exploding gradients made in this section coupled with the very limited theoretical analysis seem to unconvincing that this is nessacarily an issue and under what circumstances they are. 

The toy example with l(w) = (w - 4)(w - 3) w^2 is indeed interesting for visualizing a case where the gradient explosion does happen. However, surprisingly here the authors rather than optimizing the learning rate, which they analyzed in the previous part of the section, they are now optimizing the momentum. The observation that at high momentum the training is unstable are not really surprising as there are fundamental reasons why too high momentum leads to instabilities and these have been analyzed in the literature. Additionally, it is not mentioned what learning rate is used, which can also play a major role in the effects observed. 

As a whole, although the example in this section is interesting, the claims made by the authors and some of the conclusions seem to lack any significant justifications, in addition to the fact that usually large over-parameterized models behave differently than small models. 


Experiments:

I have a few key issues with the experimental setup, which I think need to be addressed:

1. The CNN being optimized is quite small - only 3 layers. This allows the authors to train everything on a CPU. The key issue here, as well with previous work on Learning to Learn, is that it is not clear how scalable is this method to very Deep Networks. 

2. Figure 1 - The setup is to optimize the problem for 10000 iterations, however, I think it is pretty clear even to the naked eye that the standard first-order optimizers (Adam/RMS/Mom) have not fully converged on this problem. Hence I think its slightly unfair to compare their "final performance" after this fixed period. Additionally using the curriculum the "meta"-optimizer is trained explicitly for 10000 iterations. Hence, it is also unclear if it retains its stability after letting it run for longer. From the text it is also unclear whether the authors have optimized the parameters of the first-order methods with respect to their training or validation performance - I hope this is the latter as that is the only way to fairly compare the two approaches. 

3. Figure 6 - the results here seem to indicate that the learned optimizer transfers reasonably well, achieving similar performance to first-order methods (slightly faster validation reduction). Given however that these are plots for only 10000 iterations it is still unclear if this is scalable to larger problems. 

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1gjUikY6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxwAo09KQ&amp;noteId=r1gjUikY6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper900 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper900 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your thoughtful review! We will take these under consideration for a future submission. Comments addressed below.

j=i+1: You are indeed correct. This was an unfortunate typo we realized just after submitting. Good catch and thank you for giving our work such a thorough review!

Your points as to exploding or vanishing gradients are correct. If the optimal learning rate is known ahead of time (requiring knowledge of the hessian at each iteration) it is possible to not have exploding gradients. In practice, however, it is rarely possible to find useful bounds on the eigenvalues of the Hessian in neural network training. This, coupled with the fact that the optimal learning rate is often at the edge of unstable dynamics, can lead to learning rates in the unstable regime and thus exploding gradients.

We appreciate the comment about vanishing gradients. We will update this section discussing that gradients do not vanish -- only explode. (Note that the outer-parameters are used at every training iteration -- so even if the backpropagated outer-gradient shrinks exponentially with respect to unrolled optimization steps, it does not vanish with respect to the outer-parameters.)

" However, surprisingly here the authors rather than optimizing the learning rate, which they analyzed in the previous part of the section, they are now optimizing the momentum."

Here, we use momentum as it more clearly maps to a physical phenomenon, in the hope that it provides better intuition. Similar behavior holds for learning rate -- it is possible to take a step that is either just under or just over a local maximum, resulting in diverging trajectories.

"in addition to the fact that usually large over-parameterized models behave differently than small models."
We use these toy problems as a tool to build intuition as understanding the non-convex setting is very complex. That being said, we have done additional work (not included) around exploring these effects on larger problems. In particular, we are able to find multiple saddle points / paths to descend a loss function resulting in discontinuous trajectories, and exploding gradients (as in the toy case). We do this by taking 2d slices through the inner problems, and sweeping meta-parameters. We observe discontinuous jumps in final location and trajectory. We will look into adding, likely in the appendix, some examples of this behavior in large networks. We emphasize that Figure 3e already shows a slice through the outer-loss landscape for a neural network task and a neural network optimizer, and that pathological behavior in the unrolled loss landscape is visible in this figure.

Scale: As of now, these methods are quite expensive. As a result, the field mostly explores meta-training on small scale tasks. Despite the expense, this work operates on considerably larger models than almost all prior work. We train conv-nets (as opposed to small MLPs) and train for 10k inner-iterations -- around 2 orders of magnitude longer than most existing work. We are extremely interested in pushing these methods further -- applying to even larger problems, but instability in meta-training has previously been a major obstacle to such scaling.

"Hence I think its slightly unfair to compare their "final performance" after this fixed period.": When evaluating on training loss, we show optimization speed. With regard to validation loss, you are correct that we do not provide sufficient detail for the "better final performance" claim. In practice, we find overfitting occurs in under 20k inner-iterations (Shown in the dashed line on the figures). We will modify the text / figures to show the training step where existing optimizers begin to overfit.

" From the text it is also unclear whether the authors have optimized the parameters of the first-order methods with respect to their training or validation performance"
We do the latter, and will update the text to better emphasize this.

"Figure 6 - the results here seem to indicate that the learned optimizer transfers reasonably well, achieving similar performance to first-order methods (slightly faster validation reduction). Given however that these are plots for only 10000 iterations it is still unclear if this is scalable to larger problems."
Transfer to larger problems was not the focus of this work. We targeted stable training of task specific optimizers. In this context, 10k inner iterations is enough to achieve the best performance on these tasks (with learned optimizers). We agree that transfer to larger problems is critical for broad applicability, and are actively working on this.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>