<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>How Powerful are Graph Neural Networks? | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="How Powerful are Graph Neural Networks?" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ryGs6iA5Km" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="How Powerful are Graph Neural Networks?" />
      <meta name="og:description" content="Graph Neural Networks (GNNs) for representation learning of graphs broadly follow a neighborhood aggregation framework, where the representation vector of a node is computed by recursively..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ryGs6iA5Km" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>How Powerful are Graph Neural Networks?</a> <a class="note_content_pdf" href="/pdf?id=ryGs6iA5Km" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019how,    &#10;title={How Powerful are Graph Neural Networks?},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=ryGs6iA5Km},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Graph Neural Networks (GNNs) for representation learning of graphs broadly follow a neighborhood aggregation framework, where the representation vector of a node is computed by recursively aggregating and transforming feature vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs in capturing different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">graph neural networks, theory, deep learning, representational power, graph isomorphism, deep multisets</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We develop theoretical foundations for expressive power of GNNs and design a provably most powerful GNN.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">29 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rkl2Q1Qi6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Problem with Equation (4.1)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=rkl2Q1Qi6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper835 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I do not think that Equation (4.1) is as powerful as the 1-WL. Consider the two labeled graphs 

r -- g
|    |
g -- r

and 

r -- g
|    |
r -- g

with node color "g" and "r". Clearly, the 1-WL can distinguish between these two graphs. Howeover, when using (4.1) with an 1-hot encoding of the labels, both graphs will end up with the same two features. The set of node features will always be the same. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1gkUYX76Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Dataset problem</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=H1gkUYX76Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper835 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">A comment on the dataset. I think current dataset is very limited for evaluating different graph learning algorithms. A new paper showed that using very simple degree statistics already can perform on par with the state-of-the-art graph neural networks and graph kernel. Imagenet Like dataset is strongly needed for evaluating different algorithms fairly.

Reference:
A simple yet effective baseline for non-attribute graph classification <a href="https://arxiv.org/abs/1811.03508" target="_blank" rel="nofollow">https://arxiv.org/abs/1811.03508</a> </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1xLcPaKpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Dataset problem</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=B1xLcPaKpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Christopher_Morris1" class="profile-link">Christopher Morris</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper835 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">There are already larger real-world datasets available, see e.g., [1].

[1] <a href="http://moleculenet.ai/datasets-1" target="_blank" rel="nofollow">http://moleculenet.ai/datasets-1</a></span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SklgY8Ky07" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>But we still don't have a dataset that contain many large networks</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=SklgY8Ky07"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper835 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for pointing out the dataset. But I believe those datasets contain many small graphs. A dataset of many large graphs is still missing.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_rkeW9FDnnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>One of the better GNN papers; would improve a lot with more careful discussion/analysis</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=rkeW9FDnnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper835 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper835 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This papers presents an interesting take on Weisfeiler-Lehman-type GNNs, where it shows that a WL-GNNs classification power is related to its ability to represent multisets. The authors show a few exemplar networks where the mean and the max aggregators are unable to distinguish different multisets, thus losing classification power. The paper also proposes averaging the node representation with its neighbors (foregoing the “concatenate” function) and using sum pooling rather than mean pooling as aggregator. All these observations are wrapped up in a GNN, called GIN. The experiments on Table 1 are inconclusive, unfortunately, as the average accuracies of the different methods are often close and there are no confidence intervals and statistical tests to help guide the reader to understand the significance of the results.

My chief concern is equating the Weisfeiler-Lehman test (WL-test) with Weisfeiler-Lehman-type GNNs (WL-GNNs). The WL-test relies on countable set inputs and injective hash functions. Here, the paper is oversimplifying the WL-GNN problem. After the first layer, a WL-GNN is operating on uncountable sets. On uncountable sets, saying that a function is injective does not tells us much about it; we need a measure of how closely packed we find the points in the function’s image (a measure in measure theory, a density in probability). On countable sets, saying a function is injective tells us much about the function. Moreover, the WL-test hash function does not even need to operate over sets with total or even partial orders. As a neural network, the WL-GNN “hash” ($f$ in the paper) must operate over a totally ordered set (\mathbb{R}^n, n &gt; 0). Porting the WL-test argument of “convergence to unique isomorphic fingerprints” to a WL-GNN requires a measure-theoretic analysis of the output of the WL-GNN layers, and careful analysis if the total order of the set does not create attractors when they are applied recursively. 

To illustrate the above *attractor* point, let’s consider the construct of Theorem 1 of (Xu et al., 2018), where the WL-GNN “hash” ($f$) is (roughly) described as the transition probability matrix of a random walk on the input graph. Under well-known conditions, the successive application of this operator ("hash" or transition probability matrix P in this case) can go towards an attractor (the steady state). Here, we need a measure-theoretic analysis of the “hash” even if it is bijective: random walk mixing. The random walk transition operator can be invertible (bijective), but we still say the random walker will mix, i.e., the walker forgets where it started, even if the transition operation can be perfectly undone by inversion (P^{-1}). In a WL-GNN that only uses the last layer for classification, this would manifest itself as poor performance in a WL-GNN with a large number of layers, and vanishing gradients. Of course, since (Xu et al., 2018) argued to revert back to the framework of (Duvenaud et al., 2015) of using the embeddings of all layers, one can argue that this mixing problem is just a problem of “wasted computation”.

The matrix analysis of the last paragraph also points to another potential problem with the sum aggregator. GIN needs to be shallow. With ReLU activations the reason is simple: for an adjacency matrix $A$, the value of $A^j$ grows very quickly with $j$ (diverges). With sigmoid activations, GIN would experience vanishing gradients in graphs with high variance in node degrees.

The paper should be careful with oversimplifications. Simplifications are useful for insight but can be dangerous if not prefaced by clear warnings and a good understanding of their limitations. I am not asking for a measure-theoretic analysis revision of the paper (it could be left to a follow-up paper). I am asking for a *relatively long* discussion of the limitations of the analysis.

Suggestions to strengthen the paper:
•	Please address the above concerns.
•	Table 1 should have confidence intervals (a statistical analysis of significance would be a welcome bonus).
•	Please mention the classes of graphs where the WL-test cannot distinguish two non-isomorphic graphs. See (Douglas, 2011), (Cai et al., 1992) and (Evdokimov and Ponomarenko, 1999) for the examples. It is important for the WL-GNN literature to keep track of the more fundamental limitations of the method.
•	(Hamilton et al, 2017) also uses the LSTM aggregator, besides max aggregator and mean aggregator, which outperforms both max and mean in some tasks. Does the LSTM aggregator also outperforms the sum aggregator in the tasks of Table 1? It is important for the community to know if unusual aggregators (such as the asymmetric LSTM) have some yet-to-be-discovered class-distinguishing power.

Xu, K., Li, C., Tian, Y., Sonobe, T., Kawarabayashi, K., &amp; Jegelka, S. (2018). Representation Learning on Graphs with Jumping Knowledge Networks. In ICML.

Cai, J. Y., Fürer, M., &amp; Immerman, N. (1992). An optimal lower bound on the number of variables for graph identification. Combinatorica, 12(4), 389-410.

Douglas, B. L. (2011). The Weisfeiler-Lehman method and graph isomorphism testing. arXiv preprint arXiv:1101.5211.

Evdokimov, S., &amp; Ponomarenko, I. (1999). Isomorphism of coloured graphs with slowly increasing multiplicity of Jordan blocks. Combinatorica, 19(3), 321-333.


</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJgMSgUqhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice results on the expressive power of neighborhood aggregation mechanisms used in GNNs</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=HJgMSgUqhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper835 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper835 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=HJgMSgUqhQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The author study the expressive power of neighborhood aggregation mechanisms used in Graph Neural Networks and relates them to the 1-dimensional Weisfeiler-Lehman heuristic (1-WL) for graph isomorphism testing. The authors show that GCNs with injections acting on the neighborhood features can distinguish the same graphs that can be distinguished by 1-WL. Moreover, they propose a simple GNN layer, namely GIN, that satisfies this property. Moreover, less powerful GNN layers are studied, such as GCN or GraphSage. Their advantages and disadvantages are discussed and it is shown which graph structures they can distinguish. Finally, the paper shows that the GIN layer beats SOTA GNN layers on well-known benchmark datasets from the graph kernel literature.

Studying the expressive power of neighborhood aggregation mechanisms is an important contribution to the further development of GCNs. The paper is well-written and easy to follow. The experimental results are well explained and the evaluation is convincing.

However, I have some concerns regarding the main result in Theorem 3. A consequence of the theorem is that it makes no differences (w.r.t. expressive power) whether one distinguishes the features of the node itself from those of its neighbors. This is remarkable and counterintuitive, but not discussed in the article. However, it is discussed in the proof of Theorem 3 (Appendix) which suggests that the number of iterations must be increased for some graphs in order to obtain the same expressive power. Unfortunately, at this point, the proof is a bit vague. I would like to see a discussion of this differences in the article. This should be clarified in a revised version. 
----
Edit:
The counter example posted in a comment ( <a href="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=rkl2Q1Qi6X&amp;noteId=rkl2Q1Qi6X" target="_blank" rel="nofollow">https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=rkl2Q1Qi6X&amp;noteId=rkl2Q1Qi6X</a> ) actually shows that my concerns regarding Theorem 3 and its proof were perfectly justified. I agree that the two graphs provide a counterexample to the main result of the paper. Therefore, I have adjusted my rating. I will increase my rating again when the problem can be resolved. However, this appears to be non-trivial.
----
Moreover, the novelty of the results compared to the related work, e.g., mentioned in the comments, should be pointed out.


Some further questions and remarks:

(Q1) Did you use a validation set for evaluation? If not, what kind of stopping criteria did was use?

(Q2) You use the universal approximation theorem to prove Theorem 3. Could you please say something about the needed width of the networks?

(R1) Could you please provide standard deviations for all experiments. I suspect that the accuracies on the these small datasets fluctuates quite a bit.

(R2) In the comments it was already mentioned, that some important related work, e.g., [1], [2], are not mentioned. You should address how your work is different from theirs.


Minor remarks:

- The colors in Figure 1 are difficult to distinguish



[1] https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4703190
[2] https://people.csail.mit.edu/taolei/papers/icml17.pdf</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">1: Trivial or wrong</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJgIGNTP27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reviewer comment</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=BJgIGNTP27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper835 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper835 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a very interesting investigation of the expressive capabilities of graph neural networks, in particular focusing on the discriminative power of such GNN models, i.e. the ability to tell that two inputs are different when they are actually different.  The analysis is based on the study of injective representation functions on multisets.  This perspective in particular allows the authors to distinguish different aggregation methods, sum, mean and max, as well as to distinguish one layer linear transformations from multi-layer MLPs.  Based on the analysis the authors proposed a variant of the GNN called Graph Isomorphism Networks (GINs) that use MLPs instead of linear transformations on each layer, and sum instead of mean or max as the aggregation method, which has the most discriminative power following the analysis.  Experiments were done on node classification benchmarks to support the claims.

Overall I quite liked this paper.  The study of the expressive capabilities of GNNs is a very important problem.  Given the popularity of this class of models recently, theoretical analysis for these models is largely missing.  Previous attempts at studying the capability of GNNs focus on the function approximation perspective (e.g. Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction by Hertiz et al. which is worth discussing).  This paper presents a very different angle focusing on discriminative capabilities.  Being able to tell two inputs apart when they are different is obviously just one aspect of representation power, but this paper showed that studying this aspect can already give us some interesting insights.

I do feel however that the authors should make it clear that discriminative power is not the only thing we care, and in most applications we are not doing graph isomorphism tests.  The ability to tell, for example, how far two inputs are, when they are not the same is also very (and maybe more) important, which such isomorphism / injective map based analysis does not capture at all.  In fact the assumption that each feature vector can be mapped to a unique label in {a, b, c, ...} (Section 3 first paragraph) is overly simplistic and only makes sense for analyzing injective maps.  If we want to reason anything about the continuity of the features and representations, this assumption does not apply, and the real set is not countable so such a mapping cannot exist.

In equation 4.1 describes the GIN update, which is proposed as “the most powerful GNN”.  However, such architecture is not really new, for example the Interaction Networks (Battaglia et al. 2016) already uses sum aggregation and MLP as the building blocks.  Also, it is said that in the first iteration a simple sum is enough to implement injective map, this is true for sum, but replacing that with mean and max can lose information very early on.  Another MLP on the input features at least for mean or max aggregation for the first iteration is therefore necessary.  This isn’t made very clear in the paper.

The training set results presented in section 6.1 is not very clear.  The plots show only one run for each model variant, which run was it?  As the purpose is to show that some variants fit well, and some others overfit, these runs should be chosen to optimize training set performance, rather than generalization.  Also the restrictions should be made clear that all models are given the same (small) amount of hidden units per node.  I imagine if the amount of hidden units are allowed to be much bigger, mean and max aggregators should also catch up.

As mentioned earlier I quite liked the paper despite some restrictions anc things to clarify.  I would vote for accepting this paper for publication at ICLR.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJgofotjs7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The role of discriminative power for graph classification</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=HJgofotjs7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">23 Oct 2018</span><span class="item">ICLR 2019 Conference Paper835 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I understand that GIN provably has more discriminative power than other variants of GNN. But the ability to differentiate non-isomorphic graphs does not necessarily imply better graph classification accuracy, right? Would it be possible to strong discriminative power will backfire for the graph classification? After all, we don't need to solve graph isomorphism here.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJgd4DhjiQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>More powerful GNNs can better capture discriminative substructures of graphs</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=BJgd4DhjiQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper835 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">23 Oct 2018</span><span class="item">ICLR 2019 Conference Paper835 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">As we have pointed out in the experiment section, although stronger discriminative power does not directly imply better generalization, it is reasonable to expect that models that can accurately capture graph structures of interest also perform well on test set. In particular, with many existing GNNs, the discriminative power may not be enough to capture graph substructures that are important for classifying graphs. Therefore, we believe strong discriminative power is generally advantageous for graph classification. In our experiments, we empirically demonstrated that our powerful GIN has better generalization as well as better fitting to training datasets compared to other GNN variants. GINs performed the best in general, and achieved state-of-the-art test accuracy. We leave further theoretical investigation of generalization to our future work.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rklL6jDwjm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Some questions about the paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=rklL6jDwjm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Oct 2018</span><span class="item">ICLR 2019 Conference Paper835 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi！I'm writing to ask some questions.

1. In Section 3, you said that "Intuitively, the most powerful GNN maps two nodes to the same location only if they have identical subtrees structures with identical features on the corresponding nodes".  However, in my opinion, a powerful model should map nodes with different labels into different locations instead of features, since there may be some noise in features. 

2. In the paper, you said that GIN is the most powerful model. But you only reported experimental results on graph classification. Have you validated the proposed model on node classification tasks? Based on my understanding, it's also important to consider the performance on node classification when judging the power of a GNN model?

3. Instead of Mean/Max aggregators in GCN and GraphSAGE, MLP is used as the aggregator in each layer. Have you compared the parameter complexity with other baselines?

Thank you!</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HkeLiClijQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Answers</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=HkeLiClijQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper835 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">22 Oct 2018</span><span class="item">ICLR 2019 Conference Paper835 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your interest. Answers to your inquiries: 

1. Note that being powerful entails “being able to” map nodes with different subtrees to different representations. If a model is not capable of achieving this, then it’s intrinsically less powerful in distinguishing different graphs. In addition, to combat noise, we can simply regularize the mapping function to be locally smooth (e.g., by using Virtual Adversarial Training [1]). Nonetheless, in many graph classification applications including those in our experiments, the node features have specific meanings (e.g. an atom of certain types) and are not noisy. 

2. Note that our paper focuses on expressive power of GNNs, and there are two main reasons why it is not very interesting for us to conduct node classification experiments to validate our claim.
First, as we have emphasized in Section 5 and 5.3, in many node classification applications, node features are rich and diverse (e.g. bag-of-words representation of papers in a citation network), so GNN models like GCN and GraphSAGE are often already able to fit the training data well. Second, many node classification tasks assume limited training labels (semi-supervised learning scenario); thus, the inductive bias of GNNs also plays a key role in empirical performance. For example, as we discussed in Section 5.3, the statistical and distributional information of neighborhood features may provide a strong signal for many node classification tasks. 

Our GINs may potentially perform well on node classification tasks. However, due to our explanations above, the performance on node classification tasks are less directly explained by our theory of representational power, so we leave the experiments for future work. We believe our experiments on graph classification are sufficient and great for validating our theoretical claim on expressive power of GNNs. 

3. We set the numbers of hidden units and output units of MLP to be same. So the parameter complexity of Sum-MLP is roughly two times as many as that of Sum-Linear. However, note that with more hidden units, the performance of models with 1-layer perceptrons usually decreases. 

[1] <a href="https://arxiv.org/abs/1704.03976" target="_blank" rel="nofollow">https://arxiv.org/abs/1704.03976</a></span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJgxQah1sQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thoughtful and provocative work! Future directions?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=HJgxQah1sQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Oct 2018</span><span class="item">ICLR 2019 Conference Paper835 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the thoughtful and provocative work! The paper answered some questions I have been thinking about. Graph convolution that many people talk about was motivated by Fourier transform of graph Laplacian and analogy with computer vision, yet I thought it’s not quite the same as vision. I was curious what are the more natural explanations. The view of “capturing graph structures with powerful aggregators” sounds much more natural to me and also natural to graphs problems. Very provocative!

I wonder what possible good future directions look like for graphs? Many great works these years apply theoretical computer science techniques to machine learning, e.g. Prof Sanjeev Arora group from Princeton and Prof. Aleksander Madry group from MIT. Do you see similar directions for graphs?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJltX0els7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our thoughts.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=HJltX0els7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper835 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Oct 2018 (modified: 15 Oct 2018)</span><span class="item">ICLR 2019 Conference Paper835 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your interest in our work! 

Great that you found the framework presented in our paper intuitive/natural for understanding graph representations. We think the spectral perspectives [1] [2] also provide a very valuable and important angle. It would be interesting to understand how to connect and relate the different perspectives. Regarding future directions, besides what we have mentioned in our conclusion, we do not have further comments at this moment. Combining and applying techniques from many other communities indeed sounds very interesting and promising. Ideas from graph minor theory [3] and spectral graph theory [4] [5] may be interesting and are not fully explored in the current message passing frameworks, although we do not have detailed suggestions at the moment.

[1] Bruna, J., Zaremba, W., Szlam, A., and LeCun, Y. Spectral networks and locally connected networks on graphs. International Conference on Learning Representations (ICLR), 2014.
[2] Bronstein, M. Bruna, J., Szlam, A., LeCun, Y. and Vandergyst, P. Geometric Deep Learning: going beyond Euclidean Data IEEE Sig. Proc. Magazine, 2017 
[3] <a href="https://www.birs.ca/workshops/2008/08w5079/report08w5079.pdf" target="_blank" rel="nofollow">https://www.birs.ca/workshops/2008/08w5079/report08w5079.pdf</a>
[4] http://www.cs.yale.edu/homes/spielman/561/
[5] http://courses.csail.mit.edu/6.S978/</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJlxQFl4i7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you so much for the reply! </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=SJlxQFl4i7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Oct 2018</span><span class="item">ICLR 2019 Conference Paper835 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you so much for providing possible ideas for future directions! The materials you referenced look very helpful and I will take a look at graph minor theory and spectral graph theory.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_r1xAhRX05X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Related work, difference to older work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=r1xAhRX05X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Oct 2018</span><span class="item">ICLR 2019 Conference Paper835 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">There is an article from 2009 [1] which has a similar theoretical contribution. Could you please comment on the differences.

[1] <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4703190" target="_blank" rel="nofollow">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4703190</a></span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1lpE_ACc7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Connections</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=H1lpE_ACc7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Oct 2018</span><span class="item">ICLR 2019 Conference Paper835 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I am also curious if there is any connections here. From my understanding, one difference is that Scarselli et al. (2009) focus on a specific type of GNN (with a recurrent contraction aggregator), so the analysis probably doesn't apply to mordern GNN architectures like GCN.  On the other hand, this paper provides a general framework that gives insight to a number of GNN architectures.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Byx_XQZJjQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We provide a general framework for analyzing and rethinking a large amount of Graph NNs in the literature.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=Byx_XQZJjQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper835 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Oct 2018</span><span class="item">ICLR 2019 Conference Paper835 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank both Anonymous 1 and Anonymous 2 for your interest in our work! 

To Anonymous 1:  Thanks for bringing up this early work! We will comment on the differences below. We would like to refer to Anonymous 2’s comment first, which made a very good point. 

To Anonymous 2: Thank you for the insightful comment! Indeed, [1] analyzes a specific model with recurrent contraction maps, but our analysis framework applies to general GNNs with message passing/neighbor aggregation. Regarding the connection and differences of contraction, recurrent maps and more general aggregators, the talk/paper by Yujia Li et al [3][4] provide some very good explanations and insights! Highly recommended!

More detailed explanations on the differences: 

1) As Anonymous 2 pointed out, the 2009 paper [1] analyzes a specific architecture designed in [2] that uses contraction maps and the same aggregator in all layers. Although [1] proves [2] can capture rooted subtree structures, it has been observed e.g. in [3][4], that it does not perform ideally in practice, thus leading to the surge of a large amount of modern GNN architectures like Gated GNN, GCN, GraphSAGE etc. Our architecture GIN is shown to perform well in practice. To Anonymous 2: in our preliminary experiments, we also tried sharing the same aggregator across all layers of GNN, but the training accuracy was fairly low (usually &lt; 80%), possibly due to optimization or capacity issues.

2) While [1] focuses on the specific GNN in [2], we provide a general framework for characterizing the expressive power of many different GNN variants proposed so far in the literature. Our results are not only applicable to [2], GIN etc. but also applicable to almost all modern GNN architectures like GCNs and GraphSAGE.

3) We made an explicit comparison of different GNN variants both theoretically and empirically so that we can have better understandings of their theoretical properties. Specifically,  we characterized what graph substructures different aggregation schemes can capture, and discussed how that might affect empirical performance. We also made it clear that injectiveness of the aggregation function is the key to achieving high expressive power in GNNs. 

Therefore, we believe our work plays an important role in rethinking and structuring the 10-year literature of GNNs from the viewpoint of expressive power, despite some similarity to [1] in terms of capturing rooted subtree structures. We will also discuss [1] and [2] in our updated version.


[1] Scarselli, Franco, et al. "Computational capabilities of graph neural networks." IEEE Transactions on Neural Networks 20.1 (2009): 81-102.
[2] Scarselli, Franco, et al. "The graph neural network model." IEEE Transactions on Neural Networks 20.1 (2009): 61-80.
[3] <a href="https://www.cs.toronto.edu/~yujiali/files/talks/iclr16_ggnn_talk.pdf" target="_blank" rel="nofollow">https://www.cs.toronto.edu/~yujiali/files/talks/iclr16_ggnn_talk.pdf</a>
[4] Li, Yujia, et al. "Gated graph sequence neural networks." arXiv preprint arXiv:1511.05493 (2015).</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkg-9GNXi7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>More important related work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=Bkg-9GNXi7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Christopher_Morris1" class="profile-link">Christopher Morris</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Oct 2018</span><span class="item">ICLR 2019 Conference Paper835 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I think you also miss other important related work [5], which shows that the features computed by GNNs lie in the same Hilbert space as WL.


[5] <a href="https://people.csail.mit.edu/taolei/papers/icml17.pdf" target="_blank" rel="nofollow">https://people.csail.mit.edu/taolei/papers/icml17.pdf</a></span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1xDl1Ovim" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you everyone.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=S1xDl1Ovim"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper835 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Oct 2018</span><span class="item">ICLR 2019 Conference Paper835 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank everyone for interest and many inquiries about our work. 

To Anonymous 3: Thanks for bringing up this related work. Graph representation learning is an increasingly popular research topic with a surge of many wonderful works. We will make sure to add all the relevant references in our updated version. To emphasize the difference with the related work, [5] shows their proposed architecture lies in the RKHS of graph kernels, but does not tell anything about which graphs can actually be discriminated by the network. In contrast, we address the question of which graphs can be distinguished, and provide a framework for addressing this representational question in a general way, settling the representational power of a broad class of GNNs.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_H1grV6yjcQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>regarding lemma 4</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=H1grV6yjcQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Oct 2018</span><span class="item">ICLR 2019 Conference Paper835 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The proof of Lemma 4 assumes the graphs have a constant degree bound (|X|&lt;N). Is the statement true even in general (i.e., finite |X|, but not bounded by a constant)? E.g., in inductive setting test graphs could have high degree. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BygugHlo9Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The node degrees can be arbitrarily large </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=BygugHlo9Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper835 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Oct 2018</span><span class="item">ICLR 2019 Conference Paper835 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your interest! The finite node degrees |X| can be arbitrarily large, and we can always find an N that works (we do not have to put an upper bound on N). Note that Lemma 4 only shows the existence of injective functions, and in practice, we need our neural networks to learn these functions from data. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1g3L4zh5X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>clarification</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=r1g3L4zh5X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Oct 2018</span><span class="item">ICLR 2019 Conference Paper835 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Could you clarify how you can always find an N that works without an upper bound? My understanding is that N should be at least as large as the largest degree you would encounter in the set of all training + testing graphs, for the function to be injective in all of these graphs. Please correct me if I am wrong.

If the set of training + testing graphs are bounded in size, sure I can pick a large constant for N and that should work. But it's possible the distribution of graphs includes graphs of unbounded size (e.g., number of nodes drawn from a geometric distribution). What N should I pick then? 

In practice, of course, all graphs have bounded size and it doesn't matter. But I want to understand what is the precise theoretical statement to be made here.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Syl6WEQn9m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=Syl6WEQn9m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper835 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Oct 2018</span><span class="item">ICLR 2019 Conference Paper835 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">You are right; we can simply pick sufficiently large N that is bigger than the size of any graphs of interest. Also, all graphs of our interest are of bounded sizes, and we explicitly stated in our Lemma 4 that we dealt with finite multiset*; thus, your second question does not make sense to us.

*<a href="https://en.m.wikipedia.org/wiki/Finite_set" target="_blank" rel="nofollow">https://en.m.wikipedia.org/wiki/Finite_set</a></span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_HJe9n0eFcQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Remarks regarding the use of ReLU as non-linearity</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=HJe9n0eFcQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Oct 2018</span><span class="item">ICLR 2019 Conference Paper835 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for this very interesting work which gives a lot of insight into graph neural networks and structures the large amount of related work out there.

I have some remarks/opinions regarding the use of non-linearities in this work.

1) Regarding section 5.1 and lemma 5: I do not think that more than 1 layer is necessary. The ReLU non-linearity does only show its full potential when used together with a bias. In most literature, the bias term is (unfortunately) omitted in the paper but still used in the implementation. ReLU without bias separates based on a hyperplane which always goes through the origin, which is why the example in the proof of Lemma 5 works. All values lie in one piece-wise linear subspace of the functions range. When using a bias, the non-linear point can be shifted to separate both examples in a non-linear fashion and the example that proves Lemma 5 does not work anymore. I am not sure though if there is another example that works if a bias is present. I suspect though, that one layer with a "working non-linearity", e.g. ReLU with bias, should be enough.

Therefore, I guess the insight here is: We need a (working) non-linear mapping before doing the feature aggregation (assuming no one-hot encoding), otherwise, we lose injectivity and therefore, discriminative power. In many current GNN models (including GCNs), this is not the case.

2) Further, I suspect that depending on how the COMBINE operation is defined, the discriminative power of WL can also be obtained by stacking 2 layers in the following way: 
Assuming COMBINE to be \sigma ( W_1*x + W_2*y + b), with x being the result of neighbourhood aggregation and y the last current node feature. Further, in the first layer, let the features from the neighbourhood aggregation get discarded (W_1 = 0), resulting in a node-wise fully connected layer with nonlinearity (or "1x1-convolution" or however it might be called). 
Then, the second layer receives features which went through a non-linear function before aggregation. Since the network could learn W_1 = 0, those two layers should have the same discriminative power as the WL.

3) I think the formulation of GCN in Equation 2.2 is not correct. The original GCN aggregates first and applies the non-linearity afterwards. 
It should be noted that since GCN does not have individual W's for the root node and the neighbourhood (W_1 and W_2 in the equation above) the mentioned construction from 2) does not work here.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HyxT16oFqm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for the discussion!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=HyxT16oFqm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper835 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Oct 2018 (modified: 10 Oct 2018)</span><span class="item">ICLR 2019 Conference Paper835 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your interest and positive comments on our work! Let us try to answer your questions. There are many GNN formulations. So it is always interesting to understand the power of different variants!

1) Thanks for this insightful comment! With sufficiently large dimensionality of output units, ReLU with bias might indeed be able to distinguish different multisets (larger output dimensionality is generally needed as we have more multisets to distinguish). In our experiments, we actually had the bias term, and we empirically observed that under-fitting still sometimes occurred for models with 1-layer perceptrons (with bias) (see Figure 4). We think it could be due to the limited number of output units or optimization.

We would like to emphasize that with MLPs, we can enjoy universal approximation of multiset functions. This allows Sum-MLP (GIN) to go beyond just distinguishing different multisets and to learn suitable representations that are useful for applications of interest. In fact, Sum-MLP outperformed Sum-1-layer in 7 out of 9 datasets (comparable in the other 2) in terms of test accuracy!

We will further discuss these points and practical implications in our updated version.

2) There can certainly be other GNN architectures with the same discriminative power as GIN (as long as they satisfy conditions in our Theorem 3). Your proposed formulation with COMBINE could potentially also work, although we do not fully understand your description. It would be great future work to investigate other powerful GNN models with potentially better generalization and optimization.

3) (2.2) is indeed not exactly the same as the original GCN. Our emphasis here was that MEAN aggregation was used in GCN. We used the formulation (2.2) to share the same framework with GraphSAGE (MAX aggregation) to save space. We will include the exact formulation of GCN in the updated version. Also, we mentioned after (2.2) that GCN does not have a COMBINE step and aggregates a node along with its neighbors. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJgCBjM5cm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarification</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=HJgCBjM5cm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Oct 2018</span><span class="item">ICLR 2019 Conference Paper835 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the discussion!

I'd like to clarify my point 2) further (it is an observation, not criticism):

Assuming we have a COMBINE operation like described above:   \sigma ( W_1*x + W_2*y + b)
If we now stack n layers (NO weight sharing over time) and assume W_1 = 0 for the first n-1 of them, we arrive exactly at the formulation where we have an MLP with n-1 layers, followed by a normal GNN layer.

The point I wanted to make: There are architectures in current literature that already achieve injectivity (maybe by "accident") through this construction. Maybe it can be said: As long as there is an individual W for the self-connection, the condition can be fulfilled through stacking.

Examples are:
Defferrard et al.: Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering, 2016 (individual parameter for k=0 neighbourhood)
Gilmer et al.: Neural Message Passing for Quantum Chemistry, 2017 (depending on implementation, i guess)

</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1giX8SqqQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We develop theory to turn “by accident” into “common practice”</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=r1giX8SqqQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper835 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Oct 2018 (modified: 10 Oct 2018)</span><span class="item">ICLR 2019 Conference Paper835 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">That’s a good observation. Indeed, there are great stuff in the nature possibly found by accident, e.g. rare grasses in Chinese medicine. Here, our goal is to study and develop theory to understand the underlying principles, so that we can appreciate the great stuff, and that in the future, with the insight of our theory, we can build even better graph deep learning models!
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_S1evjSRPc7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>So why GIN still outperforms WL kernel on some dataset?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=S1evjSRPc7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Oct 2018</span><span class="item">ICLR 2019 Conference Paper835 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Since the GIN is developed to achieve as strong expressive power as WL graph isomorphism test, why does it still has much better result on reddit-binary and reddit-5K than WL subtree Kernel? Do you also tried on larger dataset such as reddit-12K?

</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyguON1Ocm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Because GIN can capture similarity between different subtrees.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryGs6iA5Km&amp;noteId=SyguON1Ocm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper835 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Oct 2018</span><span class="item">ICLR 2019 Conference Paper835 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your questions!

As we mentioned in Section 4 right after Theorem 3, GIN generalizes the WL graph isomorphism test by learning to embed the subtrees to continuous space. This enables GIN to not only discriminate different structures, but also to learn to map similar graph structures to similar embeddings and capture dependencies between graph structures. Such learned embeddings are particularly helpful for generalization when the co-occurrence of subtrees is sparse across different graphs or there are noisy edges (Yanardag &amp; Vishwanathan, 2015).

Regarding the dataset, we did not try reddit-12K at this moment.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>