<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Exploiting Invariant Structures for Compression in Neural Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Exploiting Invariant Structures for Compression in Neural Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rkl85oRqYX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Exploiting Invariant Structures for Compression in Neural Networks" />
      <meta name="og:description" content="Modern neural networks often require deep compositions of high-dimensional nonlinear functions (wide architecture) to achieve high test accuracy, and thus can have overwhelming number of..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rkl85oRqYX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Exploiting Invariant Structures for Compression in Neural Networks</a> <a class="note_content_pdf" href="/pdf?id=rkl85oRqYX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019exploiting,    &#10;title={Exploiting Invariant Structures for Compression in Neural Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rkl85oRqYX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Modern neural networks often require deep compositions of high-dimensional nonlinear functions (wide architecture) to achieve high test accuracy, and thus can have overwhelming number of parameters. Repeated high cost in prediction at test-time makes neural networks ill-suited for devices with constrained memory or computational power. We introduce an efficient mechanism, reshaped tensor decomposition, to compress neural networks by exploiting three types of invariant structures: periodicity, modulation and low rank. Our reshaped tensor decomposition method exploits such invariance structures using a technique called tensorization (reshaping the layers into higher-order tensors) combined with higher order tensor decompositions on top of the tensorized layers. Our compression method improves low rank approximation methods and can be incorporated to (is complementary to) most of the existing compression methods for neural networks to achieve better compression. Experiments on LeNet-5 (MNIST), ResNet-32 (CI- FAR10) and ResNet-50 (ImageNet) demonstrate that our reshaped tensor decomposition outperforms (5% test accuracy improvement universally on CIFAR10) the state-of-the-art low-rank approximation techniques under same compression rate, besides achieving orders of magnitude faster convergence rates.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Neural Network Compression, Low Rank Approximation, Higher Order Tensor Decomposition</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Compression of neural networks which improves the state-of-the-art low rank approximation techniques and is complementary to most of other compression techniques. </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Hke6l1hlAX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkl85oRqYX&amp;noteId=Hke6l1hlAX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper537 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper537 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SyejueHAhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The submission suggests a methodology compressing the parameters of neural network (NN) architectures but the novelty of the method is limited.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkl85oRqYX&amp;noteId=SyejueHAhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper537 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper537 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The submission suggests a methodology compressing the parameters of neural network (NN) architectures, by first converting them to higher-order tensors and then factorizing them with various tensor decompositions. The main assumption of this work is that there may be structure in lower-order tensors (e.g., matrices or low-order tensors) contained as parameters of a NN such as: periodicity (e.g., the vector [1,2,3,1,2,3,1,2,3]) or modularity (e.g., the vector [1,1,1,2,2,2,3,3,3]). The main argument of this work is that such invariant structures in lower-order objects often go unexploited in previous low-rank approximation techniques tackling the problem of compressing the parameters of a deep neural network. The authors argue that such structures may be revealed by reshaping lower-order tensors into higher-order ones and factorizing the resulting tensors.
Despite the fact that the results seem promising, the paper suffers from the following issues:
1. Tensorizing to a higher-order tensor with lower mode sizes and factorizing through tensor network approaches is not a new approach, as suggested in the main contributions. The authors should more clearly position this part of the work with respect to existing literature. I provide more details on this point below.
2. There is no experiment or discussion to back up the assumption that: 1) the invariant structures suggested by the authors do appear within the parameters of a neural network architecture; 2) such invariant structures are captured by the higher-order tensor decomposition proposed. As such, it seems that there is no solid intuition presented to justify why the approach works. Even if the authors stress out in the experiments that: "These results confirm the existence of extra invariant structure in the parameter space of deep neural networks", this is really not clear/well-justified.
3. The experiment setup is missing crucial discussions on the hyper-parameter search. For example, as the tensor order increases, there are many more potential choices for setting the low rank parameters of a tensor train decomposition, as well as the corresponding mode sizes of the input tensor to be decomposed. How does this increased space of hyper-parameters to search for affects performance seems a crucial performance factor, but it is not discussed at all.
4. The description of the sequential fine-tuning procedure seems really hand-waving. Despite the lengthy appendix, there is not even a pointer from the sequential fine-tuning section so that the reader can go over this process in more detail with precise notation. The lack of any detailed description of this step raises doubts regarding this step's novelty/methodological significance.
Additional comments w.r.t. first issue: The idea of reshaping a lower-order object into a higher-order tensor with low mode sizes and then factorize it does exist in the literature. Especially, in combination with tensor train decomposition, the framework has been called as the Quantized-Tensor Train (QTT). For example, the authors could check Section 2.9 of the survey: Grasedyck, Lars, Daniel Kressner, and Christine Tobler. "A literature survey of low‐rank tensor approximation techniques." GAMM‐Mitteilungen 36.1 (2013): 53-78. and references therein. 
I think the authors should mention the connections existing with this line of work and argue about how their approach differs. Admittedly, it seems that this is the first effort of applying those ideas in the context of compressing neural network (NN) parameters, as well as deriving the update rules w.r.t. a NN; still, appropriate credit should be given to the QTT line of work.
Another issue w.r.t. comparing with related work is that the authors should definitely try to include the comparison and discussions with other low-rank approximation-based NN compression in the main text, rather than the appendix. 
Minor comments: Tensor partial outer product should be more clearly defined in the main text. Currently, it is really not clear to the reader what this operation signifies.
Also, it is not clear on how to interpret CP tensor network diagrams as they are presented by the authors. It seems that the current presentation does not take into account the diagonal core tensor that is present in CP.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJgJ77632m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The contribution looks rather incremental and is not motivated enough neither in theory nor in experiments.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkl85oRqYX&amp;noteId=HJgJ77632m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper537 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper537 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors address the neural networks compression problem and propose to amend the previously known approximations based on different tensor decompositions (CP, Tucker, or tensor train) with a reshaping step, where a tensor is preliminary reshaped to a higher order tensor. However, I do not see why would such modification be beneficial compared to the previously known approximations; this is also not clarified by the experiments. 

The paper contains detailed overview of the related literature. However, I think it would be more interesting to see a detailed elaboration of the proposed idea. Most importantly, the description of the proposed approach -- reshaped tensor decomposition -- looks rather superficial. In particular, I can not see where the improvement (compared with similar approximations without the reshaping) in memory and/or runtime would come from. Let me clarify with an example. Given a matrix A of size N x M and assume that this matrix is actually rank-1, i.e. there exist vectors a and b such that A = a’b. Let's reshape this matrix into, say, an order-4 tensor T with dimensions n x m x k x p. Assume that this tensor T is also rank-1 and there exist vectors x1, x2, x3, x4 such that T is equal to their outer product. Since N+M = n+m+k+p, why would vectors x1, x2, x3, x4 need less memory than vectors a and b? Moreover, would such reshaping transformation preserve the structure of the original tensor, e.g., its low-rank representation? That is, if A is a rank-1 matrix, is the tensor T guaranteed to still be rank-1? Isn't it more difficult to factorize a tensor of higher order?

I am also confused by the author's choice of the baseline for their experiments. First of all, wouldn't it be more informative to compare with the other known compression methods based on tensor decompositions (Lebedev, et al, 2015; Jaderberg, et al, 2014; Kim, et al, 2016)? Although indeed these approaches can be seen as particular cases of the proposed reshaped decomposition, such comparison would demonstrate whether this newer approach (RTD) is better than already existing methods. In particular, Kim et al experimentally demonstrate that their approximation, based on Tucker decomposition, leads to nearly lossless compression (they perform their experiments with publicly available pre-trained networks which shouldn't be difficult to compare to). Why the experimental results presented in this paper are so different? Does the loss of accuracy result from the compression with the Tucker decomposition or is it due to the reshaping step or are there other reasons? I am also not sure what does plain tensor decomposition stand for and why is it considered to be state-of-the-art (references?)? 

Minor comments: 
figure 2: b is a particular case of a
figure 2: d - shouldn’t the outer product of two 3-dimensional tensors result in a 6-dimensional tensor?
might want to use \citep to add brackets around citations
typo: element-wisely -&gt; element-wise</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJgoEpF3jQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Three critical problems</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkl85oRqYX&amp;noteId=rJgoEpF3jQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper537 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">23 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper537 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this paper, a compressive representation of convolutional neural networks is proposed. The main idea is reshaping the kernel of a convolutional layer into a higher order tensor (e.g. 9x9x9 =&gt; 3x3x3x3x3x3) and applying tensor decomposition to reduce the number of parameters. Experiments with standard data sets (CIFAR10, MNIST, ImageNet) are conducted. 

The upside of this paper is that the technical details such as the gradient formulations are sufficiently covered in Appendix. This is nice in terms of self-contentedness. 

However, I found this paper contains three critical problems. 

1. Prior work is not explicitly referred. The core idea of reshaping &amp; tensor decomposition was originally proposed by Novikov et al. (2015). However, this fact is only mentioned in Appendix and in the main paper, there is no reference. This is unfair because some reader can misunderstand that the main idea is purely invented in this paper. 

2. Contributions are not enough. In Appendix A, it is claimed that Novikov et al. (2015) proposed the core idea for fully connected layers and the authors of this paper (and Wang et al. (2018)) extends for convolutional layers. However, it seems Garipov et al. (2016) already addressed this direction. What is the difference from their approach? Furthermore, even if the extension for convolutional layers is new, I feel the technical contribution is incremental (just changing the decomposing tensor from a fully-connected weight to convolution kernel) and not enough as an ICLR publication.

3. The experiments are not exhaustive &amp; results are not particularly good. Only tensor decomposition methods are used as baselines and no other compression method is compared. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>