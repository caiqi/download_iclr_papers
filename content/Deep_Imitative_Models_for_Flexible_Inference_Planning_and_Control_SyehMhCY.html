<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Deep Imitative Models for Flexible Inference, Planning, and Control | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Deep Imitative Models for Flexible Inference, Planning, and Control" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SyehMhC9Y7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Deep Imitative Models for Flexible Inference, Planning, and Control" />
      <meta name="og:description" content="Imitation learning provides an appealing framework for autonomous control: in many tasks, demonstrations of preferred behavior can be readily obtained from human experts, removing the need for..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SyehMhC9Y7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Deep Imitative Models for Flexible Inference, Planning, and Control</a> <a class="note_content_pdf" href="/pdf?id=SyehMhC9Y7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 14 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019imitative,    &#10;title={Imitative Models: Perception-Driven Forecasting for Flexible Planning and Control},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SyehMhC9Y7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=SyehMhC9Y7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Imitation learning provides an appealing framework for autonomous control: in many tasks, demonstrations of preferred behavior can be readily obtained from human experts, removing the need for costly and potentially dangerous online data collection in the real world. However, policies learned with imitation learning have limited flexibility to accommodate varied goals at test time. Model-based reinforcement learning (MBRL) offers considerably more flexibility, since a predictive model learned from data can be used to achieve various goals at test time. However, MBRL suffers from two shortcomings. First, the model does not help to choose desired or safe outcomes -- its dynamics estimate only what is possible, not what is preferred. Second, MBRL typically requires additional online data collection to ensure that the model is accurate in those situations that are actually encountered when attempting to achieve test time goals. Collecting this data with a partially trained model can be dangerous and time-consuming. In this paper, we aim to combine the benefits of imitation learning and MBRL, and propose imitative models: probabilistic predictive models able to plan expert-like trajectories to achieve arbitrary goals. We find this method substantially outperforms both direct imitation and MBRL in a simulated autonomous driving task, and can be learned efficiently from a fixed set of expert demonstrations without additional online data collection. We also show our model can flexibly incorporate user-supplied costs at test-time, can plan to sequences of goals, and can even perform well with imprecise goals, including goals on the wrong side of the road.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">imitation learning, forecasting, computer vision</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Hybrid Vision-Driven Imitation Learning and Model-Based Reinforcement Learning for Planning, Forecasting, and Control</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_S1echIHR3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper combines Imitation Learning (IL) and Model Base Reinforcement Learning (RL) to come up with a novel algorithm that can take in user-defined targets while maintaining expert like behaviors.   This  promising approach that combines the benefits of IL and RL but with result performed only in simulation.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyehMhC9Y7&amp;noteId=S1echIHR3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1300 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1300 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">- Does the paper present substantively new ideas or explore an under explored or highly novel
question? 

Yes, the paper combines two frameworks (Imitation Learning and Model Base
Reinforcement Learning) to incorporate target information while fitting to the expert distribution. Maybe, the idea is novel but experiments are only in simulation. 

- Does the results substantively advance the state of the art?

 No, the compared methods are not state-of-the-art.

-  Will a substantial fraction of the ICLR attendees be interested in reading this paper? 

 Yes a substantial fraction of ICLR attendees might be interested in reading the paper.

 - would I send this paper to one of my colleagues to read?

Yes. 


- Quality: 

The key point of this paper is that the proposed algorithm is novel and combines
the advantages of Imitation Learning and Model Base Reinforcement Learning. However, the
authors do not address the problem of IL when the stochasticity in the environment and/or model
results in trajectories outside of expert’s distribution. Additionally, all experiments are done in
simulation only and comparisons are made against components of the proposed algorithm instead
of the state-of-the-art.  This is definitely a limitation of the paper given recent works on imitation learning and model predictive control  as applied to real robotic systems in the task of agile off-road visual navigation. 

In addition, the paper does not provide any detail on the training procedure (Network architecture, cost
function, etc), which makes results hard to reproduce. In addition, the experiments only compare
the proposed algorithm to its components, namely proportional controller, IL only controller and
Model Basel RL only controller.

- Clarity: 

Easy to read. Thorough comparison with existing frameworks (Advantages compared to IL and model
based RL). 

Originality: 

– Novel algorithm presented with success in simulation. 

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJxcueXtpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>generalization, new experiments, updated details for reproducibility </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyehMhC9Y7&amp;noteId=SJxcueXtpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1300 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1300 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your helpful feedback.

Q1: “The authors do not address the problem of IL when the stochasticity in the environment and/or model results in trajectories outside of expert’s distribution.”
A1: In our original submission, we evaluated our model’s ability to control the agent in a held out test scene (Town02). This demonstrated our model’s ability to generalize its behavior beyond the behaviors observed in the data. As further evidence of generalization, we performed additional experiments designed to force the model to produce trajectories outside of the distribution of observed trajectories. In one, we added simulated potholes to the scene, which we modelled with a cost map. This forced our planning to produce trajectories that avoid the potholes. We found that the model could still complete most of its episodes, while avoiding most potholes, despite the fact that the agent was forced into situations not seen in the training data. Please see the revised paper for these results.

Q2: “the experiments only compare the proposed algorithm to its components, namely proportional controller, IL only controller and Model Basel RL only controller.” 
A2: We agree that relevant comparison is important. Our current IL comparison is not an ablation of our method, but rather a comparison to prior offline IL work. It most closely resembles the method of Codevilla, et al. "End-to-end driving via conditional imitation learning." ICRA, 2018. However, this prior method uses categorical command prediction, "turn left/turn right/go straight", for a learned lower-level controller, whereas our variant of this method regresses setpoints provided to a PID controller. We did not make the connection clear in the original paper, which we will fix. 
We also conducted additional experiments against the state-of-the-art with the “branched” network of Codevilla, et al. 2018, which we include in our revised comparison. We found this approach to slightly outperform the original IL baseline we included in our paper, but still underperform the MBRL method and our proposed method. Please see the updated paper for our quantitative comparison.

Q3: “the paper does not provide any detail on the training procedure (Network architecture, cost function, etc), which makes results hard to reproduce”
A3: In our updated version, we have simplified our explanation and expanded on additional details, including network architecture, cost function, etc. Please see Section 2.2 in the updated paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HklQf72q3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Elegant probabilistic formulation with limited experimental validation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyehMhC9Y7&amp;noteId=HklQf72q3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1300 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1300 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value"># Summary

This submission proposes a method to combine the benefits of model-based RL and Imitation Learning (IL) for navigation tasks. The key idea is to i) learn a prior over trajectory distributions from a fixed dataset of demonstrations, and ii) use this learned dynamical model for path planning via probabilistic inference. Reaching target waypoints is done by maximizing the trajectory likelihood conditioned on the planning goal. The prior is learned using R2P2 on LIDAR features and past positions. Experiments using the CARLA driving simulator show that this method can outperform standard control, IL, and model-based RL baselines, while flexibly incorporating test-time goals and costs thanks to its probabilistic formulation.


# Strengths

The method is an elegant way to get the best of both worlds in RL and IL, leveraging the recent R2P2 work to estimate a powerful sequential model used for planning via probabilistic inference. The flexibility of the method in considering test-time cost maps and user-defined goals (e.g. to avoid potholes) is appealing, especially since it does not require on-policy data collection.

The proposed planning-as-inference method can in theory handle the multi-modality present in human demonstrations by using a probabilistic model of the observed behaviors as prior over undirected expert trajectories.

The approach seem to outperform both model-based and imitation learning baselines on a simplified version of the CARLA benchmark, including on interesting fine-grained metrics (e.g., comfort based).


# Weaknesses

The main weakness of this submission lies in its experimental evaluation, especially the absence of any dynamic objects in the tested environment ("static world CARLA", section 1). It is unclear how this approach would generalize beyond just staying on the road. How would it handle traffic lights, pedestrians, other drivers, weather variations, and more complex driving tasks than waypoint following by traversing mostly free space? How does the prior generalize to more complex behaviors (e.g, by using more contextual information \phi)? How robust is the method to noise in the demonstrations, i.e. non-expert or suboptimal behavior? It seems that estimating the generative prior on human behavior might suffer from the same issues as behavior cloning, e.g., the sample inefficiency due to the combinatorial explosion of causal factors explaining complex human behaviors. It might be in fact even harder to estimate that generative model than use a direct discriminative approach (e.g., a modular pipeline), at the cost of reduced flexibility at test time of course. The currently reported sample efficiency (7000 training samples) and near perfect success rate seem to suggest that this (non-standard) version of the CARLA benchmark is too simple (no weather variations, no dynamic obstacles). Comparison to the state of the art (beyond the baselines implemented here) on the original CARLA benchmark seems needed (especially in the "Nav. dynamic" task).

The method is only described very succinctly in section 2. I do not believe there are enough details (especially around the learning algorithm, hyper-parameters, and other important technical elements) for reproducibility at this stage. Section 2.1 is also quite dense for people not familiar with the R2P2 paper. As the main contribution of the paper is to leverage that model for planning and control, it would be great to maybe discuss a bit deeper. Finally, the input modalities are not clear, especially for the baselines: the proposed method is using LIDAR and localization whereas the IL baseline seems to use vision (while the others just use the trajectory). This makes the fairness of the comparison really unclear (LIDAR is a much stronger signal for just staying on the road).

Minor remarks:
- Why use a proportional controller as a baseline instead of the standard PID one?
- Section 2.3 seems like it's missing the extension of equation 2 to the multi-goal case?
- Typos in section 3 ("trail-and-error"), section 4 ("autonmous", "knowledge to")


# Recommendation

Although the theoretical benefits of the method are well-motivated and clear (off-policy learning, probabilistic model, flexibility at test time), the experimental evaluation (custom simple CARLA test, unclear comparison to baselines) and lack of details impeding reproducibility seems to suggest that this submission needs a bit more work. First, adding more details as suggested above and clarifying the experimental protocol seem like a must, but can be easily addressed by an update to the text. Second, it would be ideal to evaluate the approach on the standard CARLA benchmark in order to compare fairly to the prior art. This is much more involved.

I personally like the approach, so although I think it is marginally below the acceptance threshold in its current form, I reserve my judgement for the time being and look forward to the authors' reply.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJgS9EXK6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>generalization experiments, SOA comparison, method and baseline clarifications</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyehMhC9Y7&amp;noteId=rJgS9EXK6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1300 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1300 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your helpful feedback. 

Q1: “Unclear how this approach would generalize beyond just staying on the road”
We agree that there are more sophisticated settings that could be used to test different generalization aspects of our method. In theory, expert behaviors could be modelled in such settings by including the relevant information in the context, as noted by the reviewer. However, we designed our original experiments to reduce the number of uncontrolled variables, in order to clearly isolate the benefits of our approach. In order to test other generalization capabilities, we have since conducted additional experiments in several settings: obstacles in the road that were unseen in the demonstrations (i.e. simulated potholes), and noise in the waypoints provided to the controller, which could occur in a real-world setting due to noisy localization. In the pothole experiment, we found that our model was able to navigate around simulated potholes by including them in the cost map, and compared it to our model that was not provided with a cost map of the potholes. This navigation demands the model generalize its planning to situations not observed in the training data, specifically, when the car must partially enter the opposing lane in order to avoid the obstacle. In the noisy waypoint experiment, we tested two different types of noise: high bias, low variance noise, and low bias, high variance noise. In the high variance setting, “decoy” waypoints are added to the set of possible waypoints. The decoys are obtained by significantly perturbing the original waypoints with Gaussian noise, sigma=8 meters. Successful navigation in this setting required the model’s ability to score its plans by  likeliest under the estimated expert’s distribution of behavior. In the high bias setting, all waypoints were provided on the wrong side of the road, which is modelled with a small amount of observation noise. We found that these waypoints were still sufficient to communicate high-level navigation directions, and that the model usually produced plans on the correct side of the road (where all expert demonstrations occurred). Please see the updated results for quantitative (Tables 2, 3) and qualitative comparisons (Figures 7, 8).

Q2: “Comparison to the state of the art (beyond the baselines implemented here) seems needed”
A2: Our problem motivation is, instead, that of completely offline learning, but state-of-the-art CARLA results require trial-and-error based data collection online (Codevilla, et al. 2018). Additionally, navigation performance isn’t the sole goal of our method; we also show that our model has flexibility to different test-time queries that require behavior not seen in the training data. However, we have since implemented the “branched” architecture of Codevilla, et al. 2018, and trained it with the same inputs and data used to train our method.  We found this approach to slightly outperform the original IL baseline we included in our paper, but underperform the MBRL comparison and our proposed method. Please see the updated results for our quantitative comparison (Table 1).

Q3: “The method is only described very succinctly in section 2”
We have included many more details about the method and the implementation in our updated version. Please see Section 2, and Section 2.2 in particular.

Q4: “the input modalities are not clear, especially for the baselines”
The input modalities are identical for all methods: they all receive the same waypoints, and observe the same LIDAR and past trajectory. We clarified this in the updated paper.

Q5: “Why use a proportional controller as a baseline instead of the standard PID one?
We tested added I+D terms, replacing the P-controller with a PID controller, and found no significant change -- the PID controller fundamentally cannot handle faraway waypoints. 

Q6: “Section 2.3 seems like it's missing the extension of equation 2 to the multi-goal case?”
We have generalized the mathematical explanation, from which all of our inference procedures can be derived. This includes the multigoal case, in Section 2.1 in the updated version. Additionally, we’ve included a qualitative demonstration of planning to sequential multi-goals (Figure 3). </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1eXBFr9h7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good paper with detailed experiments, but the idea seems lacking novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyehMhC9Y7&amp;noteId=H1eXBFr9h7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1300 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1300 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Major Contribution:
The paper introduces a method that combines the advantage and of model-based RL and imitation learning and offset their weakness. The method proposes a probabilistic inference approach to analyze the action of the model.

Organization/Style:
The paper is well written, organized, and clear on most points. 

Technical Accuracy:
I'm not an expert in RL. The method is obscure to me, but from my point of view, the experiments are done quite thoroughly and the results look good.

Presentation:
Good. 

Adequacy of Citations: 
The author should consider adding the related works include:
Bojarski, Mariusz, et al. "End to end learning for self-driving cars.": using CNNs to implement imitation learning for self-driving cars

Multimedia:
Videos are helpful to understand the method and are well composed.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">1: The reviewer's evaluation is an educated guess</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HylhHY7KTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>relevant citation, contribution clarification</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyehMhC9Y7&amp;noteId=HylhHY7KTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1300 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1300 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your helpful feedback. 

Q1: "The author should consider adding the related works"
A1: We have included your suggested reference. 

Q2: "Good paper with detailed experiments, but the idea seems lacking novelty"
A2: We have since given more evidence of the novelty of our method with additional experiments in a revision. Our main contribution is a novel hybridization of model-based RL and Imitation Learning. This enables high performance in CARLA without any trial-and-error learning, and additionally enables flexibility to tasks not observed in the training data, such as avoiding potholes (Table 2, Figure 7) and robust navigation in the presence of noisy goals (Table 3, Figure 8). </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>