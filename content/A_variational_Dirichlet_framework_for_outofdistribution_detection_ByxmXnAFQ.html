<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>A variational Dirichlet framework for out-of-distribution detection | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="A variational Dirichlet framework for out-of-distribution detection" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ByxmXnA9FQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="A variational Dirichlet framework for out-of-distribution detection" />
      <meta name="og:description" content="With the recently rapid development in deep learning, deep neural networks have been widely adopted in many real-life applications. However, deep neural networks are known to have very little..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ByxmXnA9FQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A variational Dirichlet framework for out-of-distribution detection</a> <a class="note_content_pdf" href="/pdf?id=ByxmXnA9FQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019a,    &#10;title={A variational Dirichlet framework for out-of-distribution detection},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=ByxmXnA9FQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">With the recently rapid development in deep learning, deep neural networks have been widely adopted in many real-life applications. However, deep neural networks are known to have very little control over its uncertainty for test examples, which can potentially cause very harmful and annoying consequences in practical scenarios. In this paper, we are particularly interested in designing a higher-order uncertainty metric for deep neural networks and investigate its performance on the out-of-distribution detection task proposed by~\cite{hendrycks2016baseline}. Our method is based on a variational inference framework where we interpret the output distribution $p(x)$ as a stochastic variable $z$ lying on a simplex of multi-dimensional space and represent the higher-order uncertainty via the entropy of the latent distribution $p(z)$. Under the variational Bayesian framework with a given dataset $D$, we propose to adopt Dirichlet distribution as the approximate posterior $F_{\theta}(z|x)$ to approach the true posterior distribution $p(z|D)$ by maximizing the evidence lower bound of marginal likelihood. By identifying the over-concentration issue in the Dirichlet framework, we further design a log-scaling smoothing function to avert such issue and greatly increase the robustness of the entropy-based uncertainty measure. Through comprehensive experiments on various datasets and architectures, our proposed variational Dirichlet framework is observed to yield state-of-the-art results for out-of-distribution detection.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">out-of-distribution detection, variational inference, Dirichlet distribution, deep learning, uncertainty measure</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A new framework based variational inference for out-of-distribution detection</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rJx-4Lt6nX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Bayesian reasoning about DNN outcome</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxmXnA9FQ&amp;noteId=rJx-4Lt6nX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1345 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1345 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary
=========
The paper describes a probabilistic approach to quantifying uncertainty in DNN classification tasks.
To this end, the author formulate a DNN with a probabilistic output layer that outputs a multinomial over the
possible classes and is equipped with a Dirichlet prior distribution.
They show that their approach outperforms other SOTA methods in the task of out-of-distribution detection.

Review
=========
Overall, I find the idea compelling to treat the network outputs as samples from a probability distribution and
consequently reason about network uncertainty by analyzing it.
As the authors tackle a discrete classification problem, it is natural to view training outcomes as samples from
a multinomial distribution that is then equipped with its conjugate prior, a Dirichlet.

However, the model definition needs clarification. In the classical NN setting, I find it misleading
to speak of output distributions (here called p(x)). As the authors point out, NNs are deterministic function approximators
and thus produce deterministic output, i.e. rather a function f(x) that is not necessarily a distribution (although can be interpreted as a probability).
One could then go on to define a latent multinomial distribution over classes p(z|phi) instead that is parameterized by a NN, i.e. phi = f_theta(x).
The prior on p(phi) would then be a Dirichlet and consequently the posterior is Dirichlet as well.
The prior distribution should not be dependent on data x (as is defined below Eq. 1).

The whole model description does not always follow the usual nomenclature, which made it at times hard for me to grasp the idea.
For instance, the space that is modeled by the Dirichlet is called a simplex. The generative procedure, i.e. how does data y constructed from data x and the probabilistic procedure, is missing.
The inference procedure of minimizing the KL between approximation and posterior is just briefly described and could be a hurdle to understand, how the approach works when someone is unfamiliar with variational inference.
This includes a proper definition of prior, likelihood and resulting posterior (e.g. with a full derivation in an appendix).

Although the authors stress the importance of the approach to clip the Dirichlet parameters, I am still a bit confused on what the implications of this step are.
As I understood it, they clip parameters to a value of one as soon as they are greater than one.
This would always degrade an informative distribution to a uniform distribution on the simplex, regardless whether the parameters favor a dense or sparse multinomial.
I find this an odd behavior and would suggest, the authors comment on what they mean with an "appropriate prior". Usually, the parameters of the prior are fixed (e.g. with values lower one if one expects a sparse multinomial).
The prior then gets updated through the data/likelihood (here, a parameterized NN) into the posterior.

Clipping would also lead to the KL term in Eq. 3 to be 0 often times, as the Dir(z|\alpha_c) often degrades to Dir(z|U).

The experiments are useful to demonstrate the application and usefulness of the approach. 
Outcome in table 3 could maybe be better depicted using bar charts, results from table 4 can be reported as text only, which would free up space for a more thorough model definition.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJghUGPw2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Not well motivated.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxmXnA9FQ&amp;noteId=HJghUGPw2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1345 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1345 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a new framework for out-of-distribution detection, based on variational inference and a prior Dirichlet distribution. The Dirichlet distribution is presented, and the way it is used withing the method is discussed (i.e. clipping, scaling, etc). Experiments on several datasets and comparison with the state of the art is reported and extensively discussed.

The motivation of the proposed approach is clear, and I agree with the attempt to regularize the network output. The choice of the Dirichlet distribution is quite natural, as each of its samples are the prior weights of a multinomial distribution. However, some other choices are less clear (clipping, scaling, decision, and the use of a non-informative prior). The overall inference procedure appears to be advantageous in the many experiments that the authors report (several datasets, and several baselines).

The first thought that came to my mind, is that out-of-distribution detection is to classification what outlier detection is to regression. Therefore, relevant and recent work on the topic deserves to be cited, for instance:
S. Lathuiliere, P. Mesejo, X. Alameda-Pineda and R. Horaud, DeepGUM: Learning Deep Robust Regression with a Gaussian-Uniform Mixture Model, In ECCV, 2018.

One thing that I found quite strange at first sight is the choice of clipping the parameters of the Dirichlet distribution. It is said that this is done in order to choose an appropriate prior distribution. However, the choice is not very well motivated, because what "appropriate" means is not discussed. So why do you clip to 1? What would happen if some of the alpha's go over 1? Is it a numerical problem, a modeling problem, a convergence issue?

I would also like the authors to comment on the use of a non-informative Dirichlet distribution within the KL-divergence. The KL divergence measures the deviation between the approximate a posteriori distribution and the true one. If one selects the non-informative Dirichlet distribution, this is not only a brutal approximation of the true posterior, but most importantly a distribution that does not depend on x, and that therefore cannot be truly called posterior.

It is also strange to take a decision based on the maximum alpha. On the contrary, the smallest alpha should be taken, since it is the one that concentrates more probability mass to the associated corner in the simplex.

Regarding the scaling function, it is difficult to grasp its motivation and effects. It is annouced that the aim of the smoothing function is to smooth the concentration parameters alpha. But in what sense? Why do they need to be smoothed? Is this done to avoid numerical/convergence problems? Is this a key part of the model? The same stands, by the way, for the form of the input perturbation.

The experiments are plenty, and I appreciated the sanity check done after introducing the datasets. However, I did not manage to understand why some methods appear in some tables and not in other (for example "Semantic"). I also have the feeling that the authors could have chosen CIFAR-100 in Table 2, since most of the metrics reported are quite high (the problems are not much challenging).

Regarding the parameter eta, I would say that its discussion right after Table 3 is not long enough. Specially, given the high sensitivity of this parameter, as reported in the Table of Figure 4. What is the overall interpretation of this sensitivity?

From a quantitative perspective, the results are impressive, since the propose methods systematically outperforms the baselines (at least the ones reported). However, since these baselines are not the same in all experiments, CIFAR-100 is not used, and the discussion of the results could be richer, I conclude that the experimental section is not too strong.

In addition to all my comments, I would say that the authors chose to exceed the standard limit of 8 pages. Even if this is allowed, the extra pages should be justified. I am affraid that there are many choices not well motivated, and that the discussion of the results is not informative enough. I am therefore not inclined to accept the paper as it is.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkxmjBjz3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>new method approximating the distribution of classification probability</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxmXnA9FQ&amp;noteId=rkxmjBjz3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1345 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1345 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper provides a new method that approximates the confidence distribution of classification probability, which is useful for novelty detection. The variational inference with Dirichlet family is a natural choice.

Though it is principally insightful to introduce the “higher-order” uncertainty, I do see the fundamental difference from the previous research on out-of-distribution detection (Liang, Lee, etc.). They are aimed at the same level of uncertainty.  Consider a binary classier, the only possible distribution of output y is Bernoulli- a mixture of Bernoulli is still Bernoulli.   

In ODIN paper,  the detector contains both the measurement of the extent to which the largest unnormalized output of the neural network deviates from the remaining outputs (U1 in their notation) and another measurement of the extent to which the remaining smaller outputs deviate from each other (U2 in their notation).  In this paper, the entropy term has the same flavor as U2 part?

I am a little bit concerned with the VI approach, which introduces extra uncertainty.  I do not understand why there is another balancing factor eta in equation 6, which makes it no longer a valid elbo. Is the ultimate goal to estimate the exact posterior distribution of p(z) through VI, or purely some weak regularization that enforces uniformity?  Could you take advantage of some recent development on VI diagnostics and quantify how good the variational approximation is?

In general, the paper is clear and well-motivated, but I find the notation sometimes confusing and inconsistent. For example, the dependency on x and D is included somewhere but depressed somewhere else.  alpha_0 appears in equation 4 but it is defined in equation 7. 

I am impressed by the experiment result that the new method almost always dominates best known methods, previously published in ICLR 2018. But I am not fully convinced why it works theoretically.  I would recommend a weak accept.  
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1eBylRZiX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Questions on ablation studies</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxmXnA9FQ&amp;noteId=S1eBylRZiX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1345 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, thanks for the nice paper. 

I have a questions on ablation studies in Table 4.

&gt;&gt;We also observe that concentration smoothing is playing a more important role than input perturbation.

I think it unfair to compare "concentration smoothing" with "input perturbation". Is it necessary to design the Dirichlet+Perturbation experiment for each data sets ?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Byl7LG6Vi7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your kindly remind</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxmXnA9FQ&amp;noteId=Byl7LG6Vi7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1345 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1345 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you so much for your careful and insightful comments. I totally agree with you, the current ablation set up is a bit problematic. Therefore, we add a few experiments to demonstrate the following results:

Dirichlet + Perturbation:
Model        OOD       FPR   Detection Error
VGG13       iSUN       16.8     8.5  
CIFAR10    LSUN       15.2    8.6
                   Tiny-IM    20.3   9.9

Dirichlet + Smoothing:
Model        OOD       FPR   Detection Error
VGG13       iSUN        14.4   7.9     
CIFAR10    LSUN        13.8   7.7
                   Tiny-IM    18.9   9.1

From the above observation, it's probably safer to claim that Dirichlet smoothing strategy is marginally more important than Input perturbation technique. Thank you for your kind remind, we will refine this part in the future revision.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HyxxfMDscX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Similar work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxmXnA9FQ&amp;noteId=HyxxfMDscX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Andrey_Malinin1" class="profile-link">Andrey Malinin</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1345 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hello!

Your work is similar to our work which is due to appear at NIPS 2018 -  <a href="https://arxiv.org/pdf/1802.10501.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1802.10501.pdf</a>  

Both our works parameterise a distribution over distributions using a DNN in order to derive measures of uncertainty in predictions for detection of out-of-distribution samples.

As far as I understand, the main differences between your work and ours are the following:

1. You interpret the the model to have latent variables which capture the distribution over distributions, while we interpret the model to be directly parameterizing a distribution over distributions. Though in the end the model architectures are essentially the same (DNN parameterises Dirichlet).
2. You train the model using ELBO using only in-domain data while we train the model using a contrastive KL-Divergence loss using in-domain and out-of-distribution data. 
3. You use additional heuristics, like clipping and smoothing the alphas, in order to get a well-behaved model.
4. You investigate only the Differential Entropy of the Dirichlet as a measure of 'higher level uncertainty' while we investigate a range of uncertainty measures, derivable from distributions over distributions, which capture uncertainty in predictions due to different sources of uncertainty (data/distributional/model uncertainty).

Please correct me if I have misunderstood anything. I find your paper to be very interesting. Specifically, I find it impressive that you are able to achieve very good empirical results without out-of-distribution training data! Smoothing the alphas also sounds like a good idea :) .

Best Regards,
Andrey Malinin</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Syxt4Id3cm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your insightful comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByxmXnA9FQ&amp;noteId=Syxt4Id3cm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1345 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1345 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi Andrey,

Thank you so much for your careful and insightful comments. Before I respond to your comment, I hope to clarify that I wasn't aware of the existence of your paper before our submission, it was only a few days after the deadline that I found your paper through the reference of "<a href="https://openreview.net/forum?id=H1gh_sC9tm" ."="" target="_blank" rel="nofollow">https://openreview.net/forum?id=H1gh_sC9tm".</a> After carefully reading your paper, I found it is really interesting and elegant, especially you have conducted comprehensive experiments on different uncertainty measures. I'm sorry that we didn't cite your paper in the current version, but we will definitely put your paper in our citation list in the future revision and compare against your results to gain a deeper understanding. 

Now I would like to answer your comments:
1. We indeed try to interpret the problem from two different angles, but we end up having the same architecture. I think it's probably due to the fact that Dirichlet seems to be the only "appropriate" choice for prior or posterior distribution (it has so many well-studied properties like entropy, variance, mean, etc).
2. I think this is one of the main differences between our papers. My method is based on ELBO, which only depends on the in-domain dataset and your method takes the contrastive loss, which depends on both in-domain and out-domain dataset (though the out-domain dataset can be synthesized). 
3. Yes, clipping and smoothing are the two weapons (tricks) to make our method work without providing any out-domain dataset. Because the model never gets the chance to see the adversarial (out-domain) examples under our setting, it is very inclined to put extremely high confidence in its own beliefs. In order to alleviate such an issue, we are inspired by the distribution calibration technique in the statistical theory (http://scikit-learn.org/stable/modules/calibration.html) to use transform function to re-adjust the model's belief in a more rational range. We experimented with several different smoothing techniques and end up having the simple log(x+1) as our calibration function. 
4. I really like your theoretical explanation for different uncertainty measures. In our framework, we actually experimented with some intuitive uncertainty measure like low-level entropy (over label) and max value, but their results turn out to be worse than our baseline (ODIN), therefore we just leave them out from the paper due to the page limit. Actually, we did compare against two different uncertainty measures (variational ratio (BNN) in table2, and evidential-Dirichlet uncertainty measure in table4). Anyway, I will list the results of other uncertainty measures in our future revision. 

Again, I'm really grateful for your helpful discussion!

Best regards.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>