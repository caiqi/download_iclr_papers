<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Neuron Hierarchical Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Neuron Hierarchical Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rylxrsR9Fm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Neuron Hierarchical Networks" />
      <meta name="og:description" content="In this paper, we propose a neural network framework called neuron hierarchical network (NHN), that evolves beyond the hierarchy in layers, and concentrates on the hierarchy of neurons. We observe..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rylxrsR9Fm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Neuron Hierarchical Networks</a> <a class="note_content_pdf" href="/pdf?id=rylxrsR9Fm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019neuron,    &#10;title={Neuron Hierarchical Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rylxrsR9Fm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">In this paper, we propose a neural network framework called neuron hierarchical network (NHN), that evolves beyond the hierarchy in layers, and concentrates on the hierarchy of neurons. We observe mass redundancy in the weights of both handcrafted and randomly searched architectures. Inspired by the development of human brains, we prune low-sensitivity neurons in the model and add new neurons to the graph, and the relation between individual neurons are emphasized and the existence of layers weakened. We propose a process to discover the best base model by random architecture search, and discover the best locations and connections of the added neurons by evolutionary search. Experiment results show that the NHN achieves higher test accuracy on Cifar-10 than state-of-the-art handcrafted and randomly searched architectures, while requiring much fewer parameters and less searching time.</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">By breaking the layer hierarchy, we propose a 3-step approach to the construction of neuron-hierarchy networks that outperform NAS, SMASH and hierarchical representation with fewer parameters and shorter searching time.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">neural network, architecture search, evolution strategy</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BygYDUs93X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>addressing an interesting problem but current contribution borderline</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylxrsR9Fm&amp;noteId=BygYDUs93X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper56 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper56 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a method for neural architecture search. It is observed that in standard layer-wise neural networks, there is mass redundancy in the model weights. The authors propose to go beyond the hierarchy of layers, prune low-sensitivity neurons and add new neurons to the network. The concept of layer is weakened, and the relation graph between individual neurons are highlighted. Overall, the proposed research problem is interesting.

However, one concern I have is the authors are not going as far as they claimed. The backbone of the neural network in Figure 2 is still a layered network structure. In a nutshell, a neural network is a computation graph. If the layers are not essential as claimed, why not go beyond such structures.

The method proposed for location and direction search is not entirely satisfactory. Given the typically large network structure, there has to be more informed methods/heuristics for selecting where to adjust the structure. Relying on genetic algorithms to find such locations and changes do not seem to be promising.

Finally the results are not so convincing. The authors claim to have obtained higher test accuracy than state of the art architectures. But their method's results were worse than several existing methods on the same datasets. The definition of "score" is rather ad hoc; no (sufficient) justifications are given as to why the particular choice/design. Even using "score", the proposed method is not really the best among competing methods.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SygMSV5c2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Official Review.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylxrsR9Fm&amp;noteId=SygMSV5c2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper56 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper56 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
This paper proposes a method to design architectures for neural networks. Their method has 3 steps.

1. Search for an architecture using an existing automatic model designing algorithm. ENAS (Pham et al 2018) is used in this work.

2. Prune some existing neurons in the model found in step (1), using a pruning algorithm. The algorithm (dynamic network pruning, DNP) comes from an Anonymous submission, so I don’t know what it is. However, I suspect that other algorithms can also be used here.

3. Add extra neurons and computations to the pruned network using evolutionary search.

The ultimate architecture resulting from the search indeed improves compared to the architecture found in step (1): having fewer parameters and achieving a smaller error rate on CIFAR-10.

Strengths:
The NHN method is well-motivated. Pruning of networks is an important technique, but there does not seem to be methods to undo over-prunning. NHN can be applied there.

The empirical results of the paper also demonstrated that (ENAS + pruning + NHN) is better than both ENAS and (ENAS + pruning). I think this is a strong point.

Weaknesses:
1. The paper is not very well-written. Followings are some points that confused me:
- Section 2.3: “the standard deviation thresholds are used to determine the optimal add-on directions”. Which stddev is it? Are you running the existing structure through a batch of data and measure the stddev of each neuron?

- Figure 3 is not very illustrative. My understanding of Figure 3 is as follows. Each foll. Please correct me if I’m wrong, but if I am correct, then I strongly suggest that you explain each circle in Figure 3 corresponds to a channel in the input and output layer.

- When you perform add-on search, how are the add-on neuron used? Are there outputs added to the existing output layers, or are they concatenated, followed by 1x1 convs etc.?

- In Section 2.2, the dynamic network pruning (DNP) algorithm is from a paper in submission. In this case, I suspect that the authors should take the responsibility to briefly explain how the algorithm works. At least, some intuitions should be given.

2. Experimental results of this paper are relatively weak.

First, the accuracy achieved by NHN is almost always behind that of block-based search methods, as reported in Table 3 (Zoph et al 2018; Pham et al 2018; Liu et al 2018). It has already been established that block-based search methods are stronger, since they have a smaller search space. Perhaps NHN should be applied on a cell found by a search-based method, rather than on a whole convnet found by ENAS.

Second, I am not a fan of the Score metric in Table 1. \sqrt{(e/2)^2 + (n/25)^2}, where e is the error rate and n is the number of parameters, is a strange metric. I wonder where do the numbers 2 and 25 come from, and had they been chosen differently (really, one can SGD-search for these numbers!), some other entries in Table 1 would have been bolded.

3. It’s unclear how much SmoothInit affects (i.e. improves) the performance of NHN. Can you comment on this?

4. Missing citations: Net2Net [1] also expands existing network architectures, albeit with methods to ensure the stability of the “add-on layers”, which can perfectly be applied to the search for add-on neurons in your method.

5. Nit-picking:
- Section 2.2: grouped convolutions where the number of groups is equal to the number of input (and output) channels is called “depthwise convolutions”. I think this is a less confusing name.

References.
[1] Net2Net: Accelerating Learning via Knowledge Transfer. <a href="https://arxiv.org/pdf/1511.05641.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1511.05641.pdf</a>
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1lR5iRI2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Ok, but not good enough</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rylxrsR9Fm&amp;noteId=B1lR5iRI2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper56 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper56 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper is more like a network pruning paper, where we first get an initial network structure by some method (ENAS in this case), prune unimportant connections and add some neurons back to remedy the performance degradation. The novelty lies on the third step, using evolutionary algorithm to insert the add-on neurons across layers. In this way, the paper got the state-of-the-art result on Cifar-10 with less parameters and less searching time. However, the paper has following problems.
1. As a general method, the proposed neuron search not only can work with structures found by ENAS, but also other network structures, for example ResNet. What are the effects on other common network structures? It would be better to also try other network architectures and evaluate on several datasets or tasks to show its generality, instead of just 12-layer CNN found by ENAS and on CIFAR 10. Without further experiments, the authors can only claim that their method is effective (to some extent) when coupled with ENAS.
2. The neuron adding procedure increases the accuracy from 96.20% to 96.52%, which is not significant.
3. The authors did not give enough details for others to reproduce their work. For example, the subroutines "Mutate", "GenerateLoc" and "GenerateDir" are not described in details. It is also not clear how the number of segments and the number genes in each segment are determined, and how they affect the performance. 
4. The paper says “Experiments show that the NHN outperforms the original ENAS architecture” (in Conclusion). But from Section 3 “Layer hierarchy search”, it says the ENAS achieves test accuracy of 96.55% on Cifar-10 with Cutout, while the final test accuracy of NHN is 96.52%. Apparently, NHN does not outperform the unpruned ENAS structure.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>