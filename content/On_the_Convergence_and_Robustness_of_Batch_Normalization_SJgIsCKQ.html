<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>On the Convergence and Robustness of Batch Normalization | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="On the Convergence and Robustness of Batch Normalization" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SJg7IsC5KQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="On the Convergence and Robustness of Batch Normalization" />
      <meta name="og:description" content="Despite its empirical success, the theoretical underpinnings of the stability, convergence and acceleration properties of batch normalization (BN) remain elusive. In this paper, we attack this..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SJg7IsC5KQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>On the Convergence and Robustness of Batch Normalization</a> <a class="note_content_pdf" href="/pdf?id=SJg7IsC5KQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 09 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019on,    &#10;title={On the Convergence and Robustness of Batch Normalization},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SJg7IsC5KQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=SJg7IsC5KQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Despite its empirical success, the theoretical underpinnings of the stability, convergence and acceleration properties of batch normalization (BN) remain elusive. In this paper, we attack this problem from a modelling approach, where we perform thorough theoretical analysis on BN applied to simplified model: ordinary least squares (OLS). We discover that gradient descent on OLS with BN has interesting properties, including a scaling law, convergence for arbitrary learning rates for the weights, asymptotic acceleration effects, as well as insensitivity to choice of learning rates. We then demonstrate numerically that these findings are not specific to the OLS problem and hold qualitatively for more complex supervised learning problems. This points to a new direction towards uncovering the mathematical principles that underlies batch normalization.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Batch normalization, Convergence analysis, Gradient descent, Ordinary least squares, Deep neural network</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We mathematically analyze the effect of batch normalization on a simple model and obtain key new insights that applies to general supervised learning.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BJl9bn0q2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting theoretical analysis for batch normalization </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJg7IsC5KQ&amp;noteId=BJl9bn0q2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper162 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper162 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper provides a theoretical analysis for batch normalization with gradient descent (GDBN) under a simplified scenario, i.e., solving an ordinary least squares problem. The analysis shows that GDBN converges to a stationary point when the learning rate is less than or equal to 1, regardless of the condition number of the problem. Some practical experiments are carried out to justify their theoretical insights. The paper is in general easy to follow. 

Pros:
This paper provides some insights for BN using the simplified model.
1. It shows that the optimal convergence rate of BN can be faster than vanilla GD.

2. It shows that GDBN doesn't diverge even if the learning rate for trainable parameters is very large. 

Cons:
1. In the main theorem, when the learning rate for the rescaling parameter is less than or equal to 1, the algorithm is only proved to converge to a stationary point for OLS problem rather a global optimal. 

2. To show convergence to the global optimal, the learning rate needs to be sufficiently small. But it is not specified how small it is. 

Overall, I think this paper provides some preliminary analysis for BN, which should shed some lights for understanding BN. However, the model under analysis is very simplified and the theoretical results are still preliminary.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rylDkM7mTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The convergence to global minimizers is proved in our revised paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJg7IsC5KQ&amp;noteId=rylDkM7mTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper162 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper162 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">In our revised paper (Theorem 3.3), we give some cases where the convergence to global minimizer for arbitrary learning rates for the weights is proved. We stress that the small learning rates requirement is to avoid saddles, but it is not required for the stability of the algorithm. The smallness is quantified by Lemma A.14. Nevertheless, if we take Theorem 3.3 (2)’s condition, we do not need this requirement on the learning rates.

We agree that we are analyzing BNGD on a specific problem, but we demonstrated through experiments that many insights hold generally. We believe it is appropriate to clearly analyze special and representative cases before attempting the general case, which is much more difficult.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HkgUrzR927" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting but unsure analogies hold</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJg7IsC5KQ&amp;noteId=HkgUrzR927"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper162 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper162 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper presents an analysis of the batch normalization idea on a simple OLS problem. The analysis is interesting as presented but several key questions remain, as described below. It is unclear that these questions are answered to the point where the insight gained can be considered transferable to BN in large Neural Network models.  

- The reason why the auxiliary variable 'a' is included in the formulation (7) is unclear. The whole reason for using BN is to rescale intermediate outputs to have an expectation of zero and variance of one. The authors claim that BN produces "order 1" output and so 'a' is needed. Can you please explain his better?

- The scaling proposition 3.2 is claimed to be important, but the authors don't provide a clear explanation of why that is so. Two different settings of algorithms are presented where the iterates should roughly be in the same order if input parameters of the formulation or the algorithm are scaled in a specific way. It is unclear how this leads to the claimed insight that the BN algorithm is yielded to be insensitive to input parameters of step length etc. due to this proposition. also, where is the proof of this proposition? I couldn't find it in the appendix, and I apologize in advance if that's an oversight on my part.

- The 'u' referred to in eqn (14) is the optimal solution to the original OLS problem, so has form H^{-1} g for some g that depends on input parameters. Doesn't this simplify the expression in (!4)? Does this lead to some intuition on how the condition number of H^* relates to H? Does this operation knock off the highest or lowest eigenvalue of H to impact the condition number?  
 
- Additionally, it is bad notation to use two-letter function names in a mathematical description, such as BN(z). This gets confusing very fast in theorems and proofs, though the CS community seems to be comfortable with this convention.  </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkxGw1BXT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Insights of BNGD for large neural networks</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJg7IsC5KQ&amp;noteId=rkxGw1BXT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper162 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper162 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">In our paper, we have done experiments on some neural networks and discovered that the following insights is transferable (see Figure 3):
(i) the BNGD allows large step sizes,
(ii) the optimal step size is insensitive,
(iii) the optimal performance is better than the optimal GD.

For general large scale neural networks, it is much more difficult to strictly analyse those observations. However, some basic properties remain exactly. In Lemma A.9, we show the scaling property for a more general case, and in fact, the same can be extended to general neural networks where BNGD is used. Moreover, one can check that the monotonicity of {|w_k|} also holds generally. Hence, in general, if one can establish convergence for |w_0|=1 and small step size \varepsilon, then one has the convergence of {|w_k|} for arbitrary learning rates, which will serve as a useful starting point to prove the convergence of {w_k}. 

Below is our comments on the questions that the reviewer addressed.

1) The reason why the auxiliary variable 'a' is included.

This is standard in the BN procedure and the reason of introducing `a` is explained in the original paper of BN (Ioffe and Szegedy, 2015). “Note that simply normalizing each input of a layer may change what the layer can represent.”. The variable `a` is there to make sure that the approximation power of the model remains the same.

2) The scaling property.

First, the scaling property ensures that equivalent configurations (as in Definition 3.1) must converge or diverge together, with the same rate up to a constant multiple. Hence, this property is used in the proof of the main convergence result (Theorem 3.3). The sketch of the proof is as follows: we first prove that the norm of the parameters {|w_k|} is a monotone increasing sequence, thus either converges to a finite limit or diverges. The scaling property is then used to exclude the divergent case -- if {|w_k|} diverges, then at some k the norm |w_k| should be large enough, and by the scaling property, it is equivalent to a case where |w_k|=1 and \varepsilon is small. But we can separately establish that for sufficiently small \varepsilon, the iterates converge from arbitrary initial condition |w_0|=1. This proves that {|w_k|} converges to a finite limit, from which the convergence of w_k and the loss can be established, after some work.  

The insensitivity of choosing step size is stated in Proposition 3.5 and proved in Appendix A.5. It is not a direct consequence of the scaling property, but rather is related to the dynamics of the effective learning rate \hat{\varepsilon}, defined in (11).

3)  Condition number of H^*.

The intuition here comes from the well-known Cauchy interlacing theorem. H^* can be regard as an orthogonal projection in the H-norm space. The proof of the eigenvalue property (Lemma A.1) is also similar to the proof of Cauchy interlacing theorem. H* does not knock out any eigenspace, but creates a zero-eigenvalue eigenspace with eigenvector u, thus it knocks out a bit from (in general) all eigenspaces and gives rise to the iterlacing property. This is the source of the acceleration.

Using $Hu=g$, we can express equation (14) as $H^* = H - g^T g/(u^T g)$ which contains more letters.  

4) We changed the notation ‘BN(z)’ to having BN on the subscript in revision.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1x_FLZ5nQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Relation to prior work is very unclear</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJg7IsC5KQ&amp;noteId=S1x_FLZ5nQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper162 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper162 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The author analyze the convergence properties of batch normalization for the ordinary least square (OLS) objective. They also provide experimental results on the OLS objective as well as small scale neural networks. First of all, understanding the properties of batch normalization is an important topic in the machine learning community so in that sense, contributions that tackle this problem are of interest for the community. However, this paper has a significant number of problems that need to be addressed before publication, perhaps the most important one being the overlap with prior work. Please address this point clearly in your rebuttal.

1) Overlap with Kolher et al. 2018: The authors erroneously state that Kolher et al. considered the convergence properties of BNGD on linear networks while after taking a close look at their analysis, they first derive an analysis for least-squares and then also provide an extension of their analysis to perceptrons. The major problem is that this paper does not correctly state the difference between their analysis and Kolher et al who already derived similar results for OLS. I will come back to this aspect multiple times below.

2) Properties of the minimizer
The authors should clearly state that Kolher et al. first proved that a^* and w^* have similar properties to Eq. 8. If I understand correctly, the difference seem to be that the algorithm analyzed in Kohler relies on the optimal a^* while the analysis presented here alternates between optimizing a and w. Is this correct? Is there any advantage in not using a^*? I think this would be worth clarifying.

3) Scaling property
I find this section confusing. Specifically,
a) The authors say they rely on this property in the proof but it is not very clear why this is beneficial. Can you please elaborate?
b) It seems to me this scaling property is also similar to the analysis of Kolher et al. who showed that the reparametrized OLS objective yields a Rayleigh quotient objective. Can you comment on this?
c) The idea of “restarting” is not clear to me, are you saying that one the magnitude of the vector w goes above a certain threshold, then one can rescale the vector therefore going back to what you called an equivalent representation? I don’t see why the text has to make this part so unclear. Looking at the proof of Theorem 3.3, this “property” seem to be used to simply rescale the a and w parameters.
d) The authors claim that “the scaling law (Proposition 3.2) should play a significant role” to extend the analysis to more general models. This requires further explanation, why would this help for say neural networks or other more complex models?

4) Convergence rate
It seems to me that the results obtained in this paper are weaker than previous known results, I would have liked to see a discussion of these results. Specifically,
a) Theorem 3.3 is an asymptotic convergence result so it is much weaker than the linear rate of convergence derived in Kolher et al. The authors require a sufficiently small step size. Looking at the analysis of Kolher et al., they show that the reparametrized OLS objective yields a Rayleigh quotient objective. Wouldn’t a constant step size also yield convergence in that case?
b) Proposition 3.4 also only provides a local convergence rate. The authors argue BNGD could have a faster convergence. This does seem to again be a weaker result. So again, I think it would be very beneficial if the authors could clearly state the differences with previous work.

5) Saddles for neural nets
The authors claim they “have not encountered convergence to saddles” for the experiments with neural networks. How did you check whether the limit point reached by BNGD was not a saddle point? This requires computing all the eigenvalues of the Hessian which is typically expensive. How was this done exactly?

6) Extension of the analysis to deep neural networks
The analysis provided in this paper only applies to OLS while Kolher et al. also derived an analysis for neural networks. Can the authors comment on extending their own analysis to neural nets and how this would differ from the one derived in Kolher et al.?

7) Experiments
How would you estimate the range of suitable step sizes (for both a and w) for BNGD for a neural network?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryxOLlVX67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The detailed difference with prior works (continued)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJg7IsC5KQ&amp;noteId=ryxOLlVX67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper162 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper162 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">4) Convergence rate

a) For the “asymptotic” convergence rate, by asymptotic we meant that for large k, (e_k sufficiently small). These are *not* results for asymptotically small learning rates, but rather for arbitrary learning rates. Thus, it is incorrect to say that we require small step size for these results. In this sense, none of our results are weaker than previous results include those in Kohler et al, who actually requires a modification of the BNGD algorithm with special learning rate schedules. In our revision, we no longer call it asymptotic, as this creates the potential confusion that this result is asymptotic in \varepsilon.

In fact, if one looks at Eq. (13) in our paper, if we modify the BNGD algorithm by setting the effective learning rate \hat{\varepsilon}_k to be 1/\lambda_{max} for every step, as is done in Kohler et al, then we would immediately obtain the same linear convergence rate (the convergence without this modification is much more difficult, and is the focus of the analysis in our paper). However, this requires the knowledge of \lambda_{max}, with which we can simply perform GD to get the same rate.

In contrast, we obtain a better convergence rate in Prop. 3.4 if we are allowed to set \hat{\varepsilon}_k=1/\lambda_{max}, due to the better spectral property of H^*. This is a better rate than GD, even with knowledge of \lambda_{max}.

Hence, our convergence results are stronger and not weaker than those in Kohler et al.
We stress that although these rates can be obtained by modifying the BNGD algorithm, this is not the focus of this paper, where we consider the much harder case where \hat{\varepsilon}_k is not artificially set to be a constant value at every iteration. As discussed in the opening comments, this distinction is important in the philosophy of studying simple models: we need to preserve the algorithm.

Finally, GD on the Rayleigh quotient objective is not shown to converge with arbitrary constant learning rates in Kohler et al, but we have showed this fact in Appendix B, where we also considered a similar simplifying modification. We do not discuss this in the main text because we would like to focus our discussion on the original BNGD, not a variant.

b) This result is local only in the sense that it requires e_k to be sufficiently small, but this must hold for all k large enough since we already proved convergence. We note that if we took the simplifying assumption that \hat{\varepsilon}_k can be set to a constant, then we would have a bound for all k. Thus, this is again not a weaker result, and instead is a stronger one. See 4)a)

5) Saddles

Here, we mean the OLS experiments. We will state it more clearly in revised paper.

6) Extension of the analysis to deep neural networks

We have discussed our approach in extending this to general cases (including neural networks) in answer to 3)d). The basic difference in our approach again is that we would like to consider the original BNGD algorithm with potentially large, constant learning rates, as we believe this is the regime of interest for analysis.

In this paper, we have done experiments in this case and discovered that the following insights translate to the general case (see Figure 3).
(i) the BNGD allows large step sizes
(ii) the optimal step size is insensitive
(iii) the optimal performance is better than the optimal GD (one of the reasons is that BN decreased the condition number).

However, the insights of Kohler et al. are totally different. Kohler et al. ascribed the acceleration of BN to the linear convergence (on specific models) while GD only achieves sublinear convergence. We think this argument is not convincing because GD’s sublinear convergence is for general objective functions (not for the special case they considered), and at worse it is sublinear, although many non-strongly-convex functions can achieve linear convergence using GD, for example, f(x,y) = x^2, or the BN on OLS.

7) Estimate the range of suitable step sizes.

It is a good question. Generally, we have no analytical estimates. Heuristically speaking, the step size for w_k can be arbitrarily taken and the maximal step size for $a$ only depends on the neural network (independent on the datasets). Hence the suitable step size for $a$ for one dataset could be used to other datasets.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hkge1xEQam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The detailed difference with prior works</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJg7IsC5KQ&amp;noteId=Hkge1xEQam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper162 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper162 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Here we give the comments on the problems that the reviewer addressed one-by-one.

1) Difference between our analysis and Kohler et al.

Here, for the linear networks, we meant to compare the OLS problem since this is the one we consider in this paper, thus we only mentioned the OLS part of Kohler’s analysis and its differences from ours (see opening comments and specific details furnished next). We acknowledge that analysis other than OLS is performed in Kohler et al, and we made this clear in the revision.

2) Properties of the minimizer

As stated in the introductory comments, Kohler et al considers both an adaptive learning rate schedule and no explicit gradient descent on $a$ (rather a chosen to satisfy d(loss)/da=0 at each step). In our case, the learning rate is constant and gradient descent is performed on $a$. As stated before, this is an important distinction since we are studying the original BNGD algorithm, whereas Kohler et al’s OLS analysis (and also Ma &amp; Klabjan, 17) is on a modified version of the BNGD algorithm. Note that we consider a version of a modified BNGD in the appendix B, where $a$ is also set according to d(loss)/da=0 (but learning rate is still constant). This tremendously simplifies the analysis but this is a different algorithm, and not the original BNGD that is widely used today.

One practical advantage to performing GD on $a$ instead of setting it to satisfy stationarity is that it is hard to do in the general case where the model is not linear. It is not efficient to perform a line-search type augmentation (as suggested in Kohler et al) in practice, when the number of out-put features are high.

In the revision, we have discussed this important distinction at some length in the second paragraph of the related work.

3) Scaling property

a) First, the scaling property ensures that equivalent configurations (as in Definition 3.1) must converge or diverge together, with the same rate up to a constant multiple.

This property is used in the proof of the main convergence result (Theorem 3.3). The sketch of the proof is as follows: we first prove that the norm of the parameters {|w_k|} is a monotone increasing sequence, thus either converges to a finite limit or diverges. The scaling property is then used to exclude the divergent case – if {|w_k|} diverges, then at some k the norm |w_k| should be large enough, and by the scaling property, it is equivalent to a case where |w_k|=1 and \varepsilon is small. But we can separately establish that for sufficiently small \varepsilon, the iterates converge from arbitrary initial condition |w_0|=1. This proves that {|w_k|} converges to a finite limit, from which the convergence of w_k and the loss can be established, after some work.  

b) The intuition of the scaling property follows from the Rayleigh quotient form, which we also consider in Appendix B, although this quotient form is not available for the original BNGD without the simplifications. This was not needed in Kohler et al. to prove their results, which are for small, dynamic learning rates. The scaling property is useful in controlling the stability of the system when large learning rates are explicitly allowed, as we have done in our work.

c) idea of “restarting”

Yes this is mostly right, except we emphasize that the rescaling is implicit, in the sense that equivalent configurations diverge or converge together. No actual rescaling happens and we are in fact analyzing the usual BNGD algorithm.

We have reworded the sketch of the proof of Theorem 3.3 to make this more clear. In essence, the scaling property allows us to reduce the case of large |w_k| to the case of |w_k| = 1 and \varepsilon small, which we have established separately that it converges. This allows us to control the cases where learning rates are large and |w_k| grows in the early iterates.

d) In Lemma A.9, we show the scaling property for a more general case, and in fact the same can be extended to general neural networks. Moreover, one can check that the monotonicity of {|w_k|} also holds generally. Hence, the same argument implies that in general, if one can establish convergence for |w_0|=1 and small \varepsilon, then one has the convergence of {|w_k|} for arbitrary learning rates, which will serve as a useful starting point to prove the convergence of {w_k}. This is work in progress.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1gGgammaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The key difference with prior works</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJg7IsC5KQ&amp;noteId=S1gGgammaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper162 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper162 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Since most of the reviewer’s criticisms are based on comparisons with Kohler et al, we believe it is necessary to start with some overall comments on the differences between our independent work and theirs, and how these differences are significant in the attempt to gain insights on BNGD.

The most important distinction is that in Kohler et al., the authors considered a modification of the BNGD on OLS in two ways: 1) The rescaling parameter is set to a value at every iteration to satisfy stationarity, instead of performing gradient descent; 2) the learning rate is chosen dynamically every step according to a rule which needs much more knowledge of the system. Thus, the analysis is fundamentally on a different algorithm from BNGD. In contrast, we analyse the original BNGD without these modifications, i.e. we consider a constant learning rate and also perform gradient descent on the rescaling parameters. We believe that this is an important distinction because the very goal of analyzing BNGD algorithm on a simplified model is to gain key insights into the *original* algorithm widely used in machine learning while circumventing the difficulties posed by a complex objective, say from a deep learning model.

Consequently, none of the results we derived for the OLS model are weaker than, or can be deduced from the analysis presented in Kohler et al. In fact, we outline in some specific comments below (in next comment) that some of our results are stronger than those in Kohler et al if we take the simplifying modifications of the BNGD described above. In particular, the linear convergence of this modified BNGD follows directly from Eq.(13) of our analysis, which is a consequence of a simple projection argument (Eq. (26)). However, we must stress that we do not discuss these results explicit at length because we believe it is not relevant to our approach to study the original BNGD algorithm using simplified models, as we described above.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>