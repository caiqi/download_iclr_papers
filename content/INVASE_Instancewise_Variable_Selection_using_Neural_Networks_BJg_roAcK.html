<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>INVASE: Instance-wise Variable Selection using Neural Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="INVASE: Instance-wise Variable Selection using Neural Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJg_roAcK7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="INVASE: Instance-wise Variable Selection using Neural Networks" />
      <meta name="og:description" content="The advent of big data brings with it data with more and more dimensions and thus a growing need to be able to efficiently select which features to use for a variety of problems. While global..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJg_roAcK7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>INVASE: Instance-wise Variable Selection using Neural Networks</a> <a class="note_content_pdf" href="/pdf?id=BJg_roAcK7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019invase:,    &#10;title={INVASE: Instance-wise Variable Selection using Neural Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BJg_roAcK7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">The advent of big data brings with it data with more and more dimensions and thus a growing need to be able to efficiently select which features to use for a variety of problems. While global feature selection has been a well-studied problem for quite some time, only recently has the paradigm of instance-wise feature selection been developed. In this paper, we propose a new instance-wise feature selection method, which we term INVASE. INVASE consists of 3 neural networks, a selector network, a predictor network and a baseline network which are used to train the selector network using the actor-critic methodology. Using this methodology, INVASE is capable of flexibly discovering feature subsets of a different size for each instance, which is a key limitation of existing state-of-the-art methods. We demonstrate through a mixture of synthetic and real data experiments that INVASE significantly outperforms state-of-the-art benchmarks.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Instance-wise feature selection, interpretability, actor-critic methodology</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BkehMR6shm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Instance-wise feature selection</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJg_roAcK7&amp;noteId=BkehMR6shm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper98 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper98 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes an instance-wise feature selection method, which chooses relevant features for each individual sample. The basic idea is to minimize the KL divergence between the distribution p(Y|X) and p(Y|X^{(s)}). The authors consider the classification problem and construct three frameworks: 1) a selector network to calculate the selection probability of each feature; 2) a baseline network for classification on all features; 3) a predictor network for classification on selected features. The goal is to minimize the difference between the baseline loss and predictor loss.

The motivation of the paper is clear and the presentation is easy to follow. However, I have some questions on the model and experiments:

1. How is Eq. (5) formulated? As the selector network does not impact the baseline network, an intuition regarding Eq. (5) is to maximize the predictor loss, which seems not reasonable. It seems more appropriate to use an absolute value of the difference in Eq. (5). Some explanation for the formulation of Eq. (5) would be helpful.

2. The model introduces an extra hyper-parameter, $\lambda$, to adjust the sparsity of selected features. I was curious how sensitive is the performance w.r.t. this hyper-parameter. How is $\lambda$ determined in the experiments?

3. After the selector network is constructed, how are the features selected on testing data? Is the selection conducted by sampling from the Bernoulli distribution as in training or by directly cutting off the features with lower probabilities?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hke-ey7oTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>RE: Instance-wise feature selection</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJg_roAcK7&amp;noteId=Hke-ey7oTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper98 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper98 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the insightful comments.

A1: Equation (5) is the difference between the cross-entropies of the predictor and baseline networks. The first term (-sum_y log f_i^\phi (x^(s), s)) is the cross-entropy of the predictor network and the second term (-sum y log f_i^\gamma (x)) is the cross-entropy of the baseline network. The loss in equation (5) is defined as the “first term – second term”. The selector network is trained to minimize this, not maximize it. Note that the baseline network is introduced to reduce the variance of this quantity, and not as a term that the selector network can change (this is a standard technique used in the actor-critic literature).

Also note that if the baseline network term (the second term) in equation (5) is removed, then we simply end up with the predictor loss defined in the “Predictor Network” section (l_1).

If instead we were to use absolute value, then when the baseline network loss is larger than the predictor network loss, the method would actually be trying to maximise the predictor network loss (which we do not want).

It is important to note that we are not trying to minimize the difference between the predictor and baseline losses - we are using the baseline to reduce the variance of the overall loss and we are simply trying to minimize the predictor loss.

A2: As can be seen in page 13 (subsection “Details of INVASE”), we explain that “We use cross-validation to select lambda among {0.1,0.3,0.5,1,2,5,10}”. We select the lambda which maximizes the predictor accuracy in terms of AUROC. We will clarify this in the revised manuscript. Below, we give the results for various values of lambda in the Syn4, Syn5, and Syn6 settings. More detailed results will be added to the revised manuscript.
--------------------------------------------------------------------------------------------------------------------------------------------------
               Datasets             |                   Syn4                   |                   Syn5                   |                   Syn6                   |
--------------------------------------------------------------------------------------------------------------------------------------------------
   Lambda / Metrics (%)  |         TPR        |      FDR        |         TPR        |      FDR       |         TPR        |      FDR        |         
--------------------------------------------------------------------------------------------------------------------------------------------------
                 0.1                      |       98.0         |      94.3        |         90.0        |      93.4      |         99.2        |        92.3     |
                 0.3                      |       93.7         |      87.9        |         84.2        |      88.9      |         96.9        |        86.7     |
                 0.5                      |       99.0         |      43.1        |         88.3        |      50.6      |         99.6        |        31.7     |
                   1                       |       66.3         |      40.5        |         73.2        |      23.7      |         90.5        |        15.4     |
                   2                       |         0.0         |       0.0         |         25.4        |       4.1       |         67.1         |         3.6      |
                   5                       |         0.0         |       0.0         |          7.5         |       2.7       |          7.6          |         2.5      |
                  10                      |         0.0         |       0.0         |          0.0         |       0.0       |          0.0          |         0.0      |         
--------------------------------------------------------------------------------------------------------------------------------------------------     

A3: As can be seen in the GitHub code (anonymously published on <a href="https://github.com/iclr2018invase/INVASE)," target="_blank" rel="nofollow">https://github.com/iclr2018invase/INVASE),</a> on testing data, we select the features whose selection probabilities are larger than 0.5. (see line 225 in INVASE-.py and line 274 in INVASE.py) We will clarify this in the revised manuscript.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1lgbw8K3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJg_roAcK7&amp;noteId=H1lgbw8K3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper98 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper98 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=H1lgbw8K3X" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a new instance-wise feature selection method, INVASE. It is closely related to the prior work L2X (Learning to Explain). There are three differences compared to L2X. The most important difference is about how to backpropagate through subset sampling to select features.  L2X use the Gumbel-softmax trick and this paper uses actor-critic models.

The paper is written well. It is easy to follow the paper. The contribution of this paper is that it provides a new way,  compared to L2X, to backpropagate through subset sampling in order to select features. The authors compare INVASE with L2X and several other approaches on synthetic data and show outperforming results. In the real-world experiments, the authors do not compare INVASE with other approaches. 

Regarding experiments, instance-wise feature selection is often applied on computer vision or natural language process applications, where global feature selection is not enough. This paper lacks experiments on CV or NLP applications. For the MAGGIC dataset, I expect to see subgroup patterns. The patterns that authors show in Figure 2 are very different for all randomly selected 20 patients. The authors do not explain why it is preferred to see very different feature patterns for all patients instead of subgroup patterns.

I have questions about other two differences from L2X, pointed by the authors. First, the selector function outputs a probability for selecting each feature \hat{S}^\theta(x). In the paper of L2X, it also produces a weight vector w_\theta(x) as described in section 3.4. I think the \hat{S}^\theta(x) has similar meaning as w_\theta(x) in L2X. In the synthetic data experiment, the authors fix the number of selected features for L2X so that it forces to overselect or underselect features in the example of Syn4. Did the author try to relax this constraint for L2X and use w_\theta(x) in L2X to select features as using \hat{S}^\theta(x) in INVASE? 

Second, I agree with the authors that L2X is inspired by maximizing mutual information between Y and X_S and INVASE is inspired by minimizing KL divergence between Y|X and Y|X_S. Both intuitions lead to similar objective functions that INVASE has an extra term \log p(y|x) and \lambda ||S(x)||. INVASE is able to add a l_0 penalty on S(x) since it uses the actor-critic models. For the \log p(y|x) term, as the author mentioned, it helps to reduce the variance in actor-critic models. This \log p(y|x) term is a constant in the optimization of S(x). In Algorithm 1, 12, the updates of \gamma does not depend on other parameters related to the predictor network and selector network. Could the authors first train a baseline network and use it as a fixed function in Algorithm 1? I don't understand the meaning of updates for \gamma iteratively with other parameters since it does not depend on the learning of other parameters. Does this constant term \log p(y|x) have other benefits besides reducing variance in actor-critic models?

I have another minor question about scaling. How does the scaling of X affect the feature importance learned by INVASE?

Note: I have another concern about the experiments. Previous instance-wise variable selection methods are often tested on CV or NLP applications, could the authors present those experiments as previous works?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SylXL-XspQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>RE1: Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJg_roAcK7&amp;noteId=SylXL-XspQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper98 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper98 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the insightful comments.

A1: We performed extensive experiments in the synthetic setting on all methods (we both reproduced and extended the settings from L2X). In addition to this, results for semi-synthetic data (where the underlying features are from real data but the label is generated synthetically) can be found in the Appendix on page 16. It is necessary to perform experiments on synthetic data if we wish to be able to compare the TPR and FDR of the different methods since we require knowledge of the ground truth relevant features.

For the real-world results, our focus was on qualitative results (believing we had already demonstrated the methods efficacy in the synthetic - and in the appendix the semi-synthetic - settings). We will move the semi-synthetic results to the main body of the paper to make clear that we have demonstrated the performance in this setting.

For the real-data experiment in which we report prediction performance, we have extended our results to include the other approaches. We use the same predictive model as the INVASE predictor network (to allow a fair comparison) but use only the selected features of each approach. As can be seen in the below table (for the PLCO dataset), INVASE does significantly outperform the other approaches. Detailed results will be added to the revised manuscript.

----------------------------------------------------------------------------------------------------
         Labels           |                  5-year                 |                  10-year                |
         Metrics         |      AUROC     |    AUPRC    |      AUROC     |     AUPRC    |
----------------------------------------------------------------------------------------------------
        INVASE           |     0.637         |     0.329      |       0.673        |       0.506    |
           L2X               |     0.558         |     0.170      |       0.583        |       0.365    |
          LIME             |     0.597         |     0.183      |       0.601        |       0.374    |
        Shapley          |     0.614         |     0.194      |       0.615        |       0.381    |
        Knockoff        |     0.619         |     0.230      |       0.658        |       0.475    |
           Tree             |     0.632         |     0.269      |       0.655        |       0.469    |
          SCFS             |     0.632         |     0.231      |       0.632        |       0.444    |
          LASSO          |     0.623         |     0.218      |       0.656        |       0.467    |
----------------------------------------------------------------------------------------------------

A2: Our method can definitely be applied to CV or NLP, though in the paper we focus on what we believe to be an equally important application where global feature selection is not enough, i.e. medicine.
We will provide qualitative results in the Appendix of the revised manuscript using the Kaggle Dog vs Cat dataset (<a href="https://www.kaggle.com/c/dogs-vs-cats)." target="_blank" rel="nofollow">https://www.kaggle.com/c/dogs-vs-cats).</a>

A3: The results shown in figure 2 for the MAGGIC dataset are entirely qualitative. We are not suggesting that the patterns shown are preferred (or expected) but rather showing that when we use INVASE to discover features for MAGGIC, we find that the patterns are different (though, if you look at, for example, patients 9, 10 and 11 we see a similar pattern for all 3). To us, this simply reinforces the fact that instance-wise feature selection is necessary - if MAGGIC did indeed only contain subgroup patterns then we would expect INVASE to pick these out (as it does in the synthetic and semi-synthetic experiments where, for example in Syn4, Syn5 and Syn6, there are two distinct subgroups).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1eVBWms6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>RE2: Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJg_roAcK7&amp;noteId=r1eVBWms6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper98 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper98 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
A4: The main problem in doing this for L2X is in the training stage (not the testing stage). As can be seen in the equations to compute V values (page 4 end of the left column of L2X paper), they must provide some k to train with which is, in general, unknown in real-world datasets (because we don’t know how many features are relevant in the real-world datasets). The weights w(X) are optimized according to a specific feature selection strategy during training, using them in a different strategy during testing would not make sense, as they are no longer optimized for this strategy. While intuitively possible, consider that, due to the way they’ve been trained, the weights w(X) are expected to “spit out” k features. Because of this, it might be that the weights for the unselected k features are essentially random (but lower than the selected k features). We have no reason to believe that the weights beyond the selected k features would be meaningful (since during training the method only ever selected precisely k features).

We have, however, conducted an experiment in the Syn4 and Syn5 settings with 100 featurs in which we directly use w(X) and threshold it to select features. As can be seen below, the results are significantly worse than for INVASE and the large increase in FDR is indeed consistent with the fact that the weights beyond the top k are not well-disciplined. We will clarify this in the revised manuscript. Note that the published code of the L2X paper is also forced to select k features in both the training and testing stages.
----------------------------------------------------------------------------------------
         Datasets             |               Syn4            |             Syn5              |
         Thresholds         |      TPR     |    FDR    |      TPR    |     FDR     |
----------------------------------------------------------------------------------------
      L2X      |      0.1      |      87.4     |     93.5   |     79.5     |     95.3    |
      L2X      |      0.3      |      69.9     |     83.8   |     77.2     |     77.1    |
      L2X      |      0.5      |      69.8     |     64.1   |     66.4     |     84.6    |
      L2X      |      0.7      |      59.1     |     61.2   |     54.4     |     65.7    |
      L2X      |      0.9      |      52.7     |     44.8   |     51.2     |     50.5    |
             INVASE           |       66.3    |      40.5   |     73.2     |     23.7    |
----------------------------------------------------------------------------------------

A5: The baseline network does not have to be trained iteratively with the other networks, however in actor-critic models it typically is. This is because the baseline is used in some sense to “normalize” the predictor network. For this reason, it is therefore good to have the baseline and predictor at a similar “level of convergence”. However, the performance differences are marginal between the two methods, and so we found that it was not important which training method we used.

A6: The scaling of X is not important. At no point do we multiply the feature vector (X) by the “importance weights”. The weights are used to obtain a binary mask vector which is then multiplied (element-wise) with the feature vector. As such, the unselected features end up being 0 and the selected features retain their original value.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Byl06-RnTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The authors addressed my concerns in the technical aspect</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJg_roAcK7&amp;noteId=Byl06-RnTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper98 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper98 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">First, I want to thank the authors for providing detailed replies to my questions. I have detailed comments for how I think of this paper in my original review. The authors have done a good work to address my concerns. I write my current opinions in short to support my updated score. I see the value of this work is proposing a new instance-wise feature selection method, INVASE, which has a tight relation with L2X. The most important value of INVASE compared to L2X is that one does not need to choose k, the number of relevant features, in advance. The authors have demonstrated that INVASE outperforms L2X and other methods for their synthetic data. They use MAGGIC medical dataset to show that INVASE is doing instance-wise feature selection qualitatively. The authors mention in the feedback that they will run experiments on Kaggle Dog vs Cat dataset and provide those results in the Appendix. I think the work has the value that it does not need to choose k in advance compared to L2X and the authors have detailed synthetic data results, so I tend to accept this work and updated my score. I do not see that instance-wise feature selection is useful for medical dataset MAGGIC in practice and I think that instance-wise feature selection is useful for CV and NLP applications. The lack of CV and NLP applications is a weak point. It is nice to include Kaggle Dog vs Cat dataset in the Appendix, the authors can consider applications in L2X and other previous works as well.

One can check my original questions and authors' feedback for experiments on synthetic dataset and MAGGIC dataset. I write a summary about my technical questions and authors' feedback. I think that they addressed my concerns. First question is whether one can use w_\theta(x) in L2X to select features. As can be seen in authors' feedback, w_\theta(x) in L2X has to depend on k and it does not work well. Second question is the meaning of the baseline network. Though it is a constant term in optimization, it is used to reduce variance in actor-critic models. Intuitively, the update of the baseline network can help to get predict network and baseline network at a similar "level of convergence" in each state. As the authors mentioned, if one trains an optimal baseline network and use it, the performance difference is marginal. The authors have addressed my technical concerns. I am satisfied with the feedback.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_rkxFiB2Tom" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Simple idea, but good results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJg_roAcK7&amp;noteId=rkxFiB2Tom"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper98 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">24 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper98 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In the paper, the authors proposed a new algorithm for instance-wise feature selection. In the proposed algorithm, we prepare three DNNs, which are predictor network, baseline network, and selector network. The predictor network and the baseline networks are trained so that it fits the data well, where the predictor network uses only selected features sampled from the selector network. The selector network is trained to minimize the KL-divergence between the predictor network and the baseline network. In this way, one can train the selector network that select different feature sets for each of given instances.

I think the idea is quite simple: the use of three DNNs and the proposed loss functions seem to be reasonable. The experimental results also look promising.

I have a concern on the scheduling of training. Too fast training of the predictor network can lead to the subotpimal selection network. I have checked the implementations in github, and found that all the networks used Adam with the same learning rates. Is there any issue of training instability? And, if so, how we can confirm that good selector network has trained?

My another concern is on the implementations in github. The repository originally had INVASE.py. In the middle of the reviewing period, I found that INVASE+.py has added. I am not sure which implementations is used for this manuscript. It seems that INVASE.py contains only two networks, while INVASE+.py contains three networks. I therefore think the latter is the implementation used for this manuscript. If this is the case, what INVASE.py is for?
I am also not sure if it is appropriate to "communicate" through external repositories during the reviewing period.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1xeFW7oT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>RE: Simple idea, but good results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJg_roAcK7&amp;noteId=S1xeFW7oT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper98 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper98 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the insightful comments.

A1: It is not true that fast training of the predictor network can lead to a suboptimal selector network. Even when the predictor network is fully trained after each selector network update, the selector network can converge optimally. However, because the input distribution of the predictor network changes with each update of the selector network, the predictor network will have to update after each selector update. It is therefore not possible for the predictor network to converge until after the selector network has converged. Therefore, there are no stability issues caused by using the same learning rates for each network.

A2: INVASE+.py is the code corresponding to the implementation found in this paper. INVASE.py corresponds to the same implementation but without the baseline (i.e. just the selector and predictor networks). In practice we found both to perform similarly, but the derivation of INVASE+ is a little more natural, and as such we used it for the paper.

We have since changed the names in the repository to INVASE and INVASE- (so that now INVASE is indeed the implemented method and INVASE- is the method without the baseline). We hope this alleviates the confusion.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>