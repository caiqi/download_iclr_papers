<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>On Generalization Bounds of a Family of Recurrent Neural Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="On Generalization Bounds of a Family of Recurrent Neural Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Skf-oo0qt7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="On Generalization Bounds of a Family of Recurrent Neural Networks" />
      <meta name="og:description" content="Recurrent Neural Networks (RNNs) have been widely applied to sequential data analysis. Due to their complicated modeling structures, however, the theory behind is still largely missing. To connect..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Skf-oo0qt7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>On Generalization Bounds of a Family of Recurrent Neural Networks</a> <a class="note_content_pdf" href="/pdf?id=Skf-oo0qt7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019on,    &#10;title={On Generalization Bounds of a Family of Recurrent Neural Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Skf-oo0qt7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Recurrent Neural Networks (RNNs) have been widely applied to sequential data analysis. Due to their complicated modeling structures, however, the theory behind is still largely missing. To connect theory and practice, we study the generalization properties of vanilla RNNs as well as their variants, including Minimal Gated Unit (MGU) and Long Short Term Memory (LSTM) RNNs. Specifically, our theory is established under the PAC-Learning framework. The generalization bound is presented in terms of the spectral norms of the weight matrices and the total number of parameters. We also establish refined generalization bounds with additional norm assumptions, and draw a comparison among these bounds. We remark: (1) Our generalization bound for vanilla RNNs is significantly tighter than the best of existing results; (2) We are not aware of any other generalization bounds for MGU and LSTM in the exiting literature; (3) We demonstrate the advantages of these variants in generalization.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Recurrent Neural Networks, MGU, LSTM, Generalization Bound, PAC-Learning</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SJgzW2Vqh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Marginal improvement over existing bounds; Incomplete comparison with existing works</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skf-oo0qt7&amp;noteId=SJgzW2Vqh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper597 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper597 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem. The problem is important to understand in the theoretical machine learning community. The paper is written well overall, clearly explaining the results obtained. I would like to raise several important points:

1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities. The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison.

2. Vacuous bounds in the regime \beta &gt;1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \beta &gt;1. A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous. They can indeed be subsumed by generalization bounds based on VC theory. The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting. 

3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3] 

4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the "model complexity" introduced upto numerical constants. It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice. 


[1] Koiran, Pascal, and Eduardo D. Sontag. "Vapnik-Chervonenkis Dimension of Recurrent Neural Networks." Discrete Applied Mathematics 86.1 (1998): 63-79.
[2] Dasgupta, Bhaskar, and Eduardo D. Sontag. "Sample complexity for learning recurrent perceptron mappings." Advances in Neural Information Processing Systems. 1996.
[3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SklswyoFnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The paper focuses on the generalization performance of RNNs and its variant in a theoretical perspective. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skf-oo0qt7&amp;noteId=SklswyoFnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper597 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper597 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper focuses on the generalization performance of RNNs and its variant in a theoretical perspective. Compared to the previous result (Zhang et al., 2018) for RNNs, this paper refines the generalization bounds for vanilla RNNs in all cases and fills the blank for RNN variants like MGU and LSTM. Specifically, in the work of Zhang et al. (2018), the complexity term quadratically depends on the layer (or say, current sequence length, denoted by t in original paper), making it less instructive. This paper improves it to at most linear dependence and can achieve at logarithmic dependence in some cases, which should be accredited. 

The key step in the proof is Lemma 2. In Lemma 2, the spectral norms of weight matrices and the number of weight parameters are decoupled. With Lemma 2, it is natural to construct a epsilon-net for RNNs and then upper bound the empirical Rademacher complexity by Dudleyâ€™s entropy integral, since such methodology is not so novel. Bartlett, et al. (2017) developed this technique to analyze the generalization bound for neural networks in a margin-based multiclass classification. However, it seems a little unexplainable to apply a technique developed from classification to analyze RNNs, since the main task of RNNs never should be classification. I wonder the motivation of analyzing generalization of RNNs by the techniques established by Bartlett. 

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BkxRG1XIhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>limited technical novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skf-oo0qt7&amp;noteId=BkxRG1XIhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper597 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper597 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
The authors provide new generalization bounds for recurrent neural networks.
Their main result is a new bound for vanilla RNNs, but they also have
bounds for gated RNNs.

They claim that their vanilla bound improves on an earlier
bound for RNNs in Section 6 of an ICML'18 paper by Zhang, et al.
It appears to me that the bounds are incomparable in strength, that
the new bound has an improved dependence on the size of the output
sequence, but a worse dependence on the number of hidden nodes.

I think that the root cause of this difference is that this paper,
at its core, adapts the more traditional analysis, used in Haussler's
1992 InfComp paper.  New analyses, like from the Bartlett, et al
NIPS'17 paper, strove for a weak dependence in the number of parameters,
but this proof technique appears to lead to a worse dependence on the
depth.  I think that, if you unwind the network, to view the function
from the first t positions of the input to output number t as a
depth t network, and apply Haussler's bound, you will get a qualitatively
similar result (in particular with bounds that scale polynomially with
d and t).  I think that Haussler's proof technique can be adapted to
take advantage of the weight sharing between layers in the unrolled
network.

It is somewhat interesting to note that the traditional bounds have
a better dependence on depth, with correspondingly better dependence
on the length of the output sequence of the RNN.

I also do not see that substantial new insight is gained through the
analysis that incorporates gating.

I do not see much technical novelty in this paper.



</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BkgrYFOH9Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>It's a density estimation not classification problem</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skf-oo0qt7&amp;noteId=BkgrYFOH9Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Oct 2018</span><span class="item">ICLR 2019 Conference Paper597 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">In real-world applications, RNNs are applied to sequential data analysis for a density estimation problem, but not a classification problem. This paper aims at a wrong target.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1gZ3ent9X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Classification problem and its generalization bound</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skf-oo0qt7&amp;noteId=S1gZ3ent9X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper597 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Oct 2018</span><span class="item">ICLR 2019 Conference Paper597 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We study seq2seq classification tasks since they have been widely used in real world applications. To name a few, in speech recognition, [1] hybridizes hidden Markov model with RNNs to label unsegmented sequence data; In computer vision, [2, 3] demonstrate scene labeling with LSTM and RNNs, achieving higher accuracy than baseline methods; In healthcare, [4] proposes a model, Doctor AI, to perform multiple label prediction (one for each disease or medication category). In addition, [5, 6] both apply RNNs to real-world healthcare datasets (MIMIC-III, PhysioNet, and ICU data) for mortality prediction and other multiple classifications tasks. We establish bounds for classification because it is typical in learning theory and is easy to compare with the existing literature.

On the other hand, for density estimation, our analysis applies as long as a suitable Lipschitz loss function is chosen (e.g. softmax with entropy loss). Specifically, Lemma 4 establishes an upper bound for empirical Rademacher complexity of general Lipschitz loss functions (the last line in Appendix A.4). By replacing the loss function in Lemma 1, we can derive generalization bounds for density estimation tasks, which can be covered as a regression problem. The bound is nearly identical to the established classification bound. The only difference is that we apply regression generalization bound and include the output range (see Chapters 3 and 10 in [7]). We will include a corresponding discussion in the next version.

References
[1] Graves, Alex, Santiago FernÃ¡ndez, Faustino Gomez, and JÃ¼rgen Schmidhuber. "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks." In Proceedings of the 23rd international conference on Machine learning, pp. 369-376. ACM, 2006.
[2] Byeon, Wonmin, Thomas M. Breuel, Federico Raue, and Marcus Liwicki. "Scene labeling with lstm recurrent neural networks." In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3547-3555. 2015.
[3] Socher, Richard, Cliff C. Lin, Chris Manning, and Andrew Y. Ng. "Parsing natural scenes and natural language with recursive neural networks." In Proceedings of the 28th international conference on machine learning (ICML-11), pp. 129-136. 2011.
[4] Choi, Edward, Mohammad Taha Bahadori, Andy Schuetz, Walter F. Stewart, and Jimeng Sun. "Doctor ai: Predicting clinical events via recurrent neural networks." In Machine Learning for Healthcare Conference, pp. 301-318. 2016.
[5] Che, Zhengping, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. "Recurrent neural networks for multivariate time series with missing values." Scientific reports 8, no. 1 (2018): 6085.
[6] Xu, Yanbo, Siddharth Biswal, Shriprasad R. Deshpande, Kevin O. Maher, and Jimeng Sun. "RAIM: Recurrent Attentive and Intensive Model of Multimodal Patient Monitoring Data." In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, pp. 2565-2573. ACM, 2018.
[7] Mohri, Mehryar, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2012.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1eG6BLE5m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>i.i.d data for RNN? Very Strong and Unrealistic Assumptions.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skf-oo0qt7&amp;noteId=S1eG6BLE5m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Oct 2018</span><span class="item">ICLR 2019 Conference Paper597 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I have quickly gone through this paper and find that the main part (including many technical details) of this paper is strongly based on the work of Peter Bartlett [1], which derives a margin bound for deep neural networks in the supervised multi-class classification setting. However, as we all know, when we derive the margin bound for supervised learning, we assume that the data are drawn i.i.d from some unknown distributions and therefore the generalization bound can be upper bounded by Rademacher Complexities plus a confidence term. 

However, for RNNs, the i.i.d. assumption (also for block-by-block i.i.d) is not the case, as the training data arrive sequentially and they have correlations to each other. Therefore, we cannot assume that the training data are i.i.d. and all results in supervised learning cannot be  directly applied to analyze the generalization property for RNNs. There are indeed some works that try to analyze the generalization property when learning with sequential data, such as sequential Rademacher Complexities [2]. I strongly suggest the author(s) expose sufficient related works in this paper.

Furthermore, perhaps more important, the analysis of this paper is quite incremental, by adapting the work in [1] from DNNs to RNNs. The technical contributions are really not enough to our machine learning theory community.



References:

[1] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. "Spectrally-normalized margin bounds for neural networks." Advances in Neural Information Processing Systems. 2017.

[2] Rakhlin, Alexander, Karthik Sridharan, and Ambuj Tewari. "Online learning via sequential complexities." The Journal of Machine Learning Research 16.1 (2015): 155-186.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJgZc5nt97" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>On i.i.d. data assumption and contributions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Skf-oo0qt7&amp;noteId=SJgZc5nt97"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper597 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Oct 2018</span><span class="item">ICLR 2019 Conference Paper597 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We appreciate your interest in our paper. It seems, however, that you are missing some key points in the paper.

1. Comment: 
For RNNs, the i.i.d. assumption (also for block-by-block i.i.d) is not the case.

1. Response: 
We consider i.i.d. input sequences (next to Figure 1 on page 2), not i.i.d. data. This allows data dependency within a sequence. Moreover, assuming i.i.d. sequences is reasonable in many seq2seq tasks. In image captioning, an image and its corresponding descriptions are naturally independent across pairs [1]. In health care, RNNs can be used to predict the physician diagnosis and medication order of the next visit [2]. Each patient displays independent symptoms across time, though, these symptoms are correlated for an individual.

From a theoretical perspective, if we further assume i.i.d. data input, the Rademacher complexity can be reduced to O(complexity/ \sqrt{mT}). Compared to the presented bound O(complexity/ \sqrt{m}) (Theorem 1, equation (1)), we lose the extra \sqrt{T} factor due to data dependency.

2. Comment: 
There are indeed some works that try to analyze the generalization property when learning with sequential data, such as sequential Rademacher Complexities [3].

2. Response:
To extend the analysis to full dependent data is quite challenging and still largely open. [3] derives complexity bounds for neural networks in Proposition 15. The result, however, is nearly negative. Since even with spectral normalization, the complexity bound is still exponential in the depth. Moreover, sequential Rademacher Complexity relies on the binary tree characterization of the labels, which restricts its application to binary classification problems. [4] also derives generalization bounds for dependent data, but under strong assumptions. Specifically, the sample sequence is \beta-mixing and they assume block independence after a sub-sample selection trick (Section 2 in [4]). With these difficulties, to extend our theory to fully dependent sequences is beyond the scope of the paper, and we leave it for future investigation. 

3. Comment: 
Furthermore, perhaps more important, the analysis of this paper is quite incremental, by adapting the work in [5] from DNNs to RNNs. The technical contributions are really not enough to our machine learning theory community.

3. Response: 
Our work is based on the PAC-learning framework, which is widely adapted by most of the learning theory papers, including [5-8] for neural networks. The key difference among these papers is how to characterize the model class complexities. 

We propose a simple and effective way to evaluate RNNs, which is quite different from [5]. As discussed in the introduction, our generalization bound (Theorem 2) is tighter than [8], which follows the idea of [5] and utilizes the technique in [7]. Our analysis, however, decouples the spectral norms of weight matrices and the number of parameters by exploiting the parametric form of RNNs (above Figure 1 on page 2). In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2). This key step allows us to construct a covering on RNNs from simple coverings of weight matrices. Note that, [5] adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument. Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \beta &gt; 1 and is still better for \beta \leq 1, only except when the weight matrices have low stable rank, and the input sequences are short.

To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3):
1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds. We also present refined bounds as a complementarity when additional norm constraints are available and compare among them.
2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5).

References
[1] Karpathy, Andrej, et al. "Deep visual-semantic alignments for generating image descriptions." 2015.
[2] Choi, Edward, et al. "Doctor ai: Predicting clinical events via recurrent neural networks." 2016.
[3] Rakhlin, Alexander, et al. "Online learning via sequential complexities." 2015.
[4] Kuznetsov, Vitaly, et al. "Generalization bounds for non-stationary mixing processes." 2017.
[5] Bartlett, Peter L., et al. "Spectrally-normalized margin bounds for neural networks." 2017.
[6] Golowich, Noah, et al. "Size-Independent Sample Complexity of Neural Networks." 2017.
[7] Neyshabur, Behnam, et al. "A pac-bayesian approach to spectrally-normalized margin bounds for neural networks." 2017.
[8] Zhang, Jiong, et al. "Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization." 2018.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>