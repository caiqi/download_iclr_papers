<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title> Large-Scale Visual Speech Recognition | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content=" Large-Scale Visual Speech Recognition" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HJxpDiC5tX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content=" Large-Scale Visual Speech Recognition" />
      <meta name="og:description" content="This work presents a scalable solution to open-vocabulary visual speech recognition. To achieve this, we constructed the largest existing visual speech recognition dataset, consisting of pairs of..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HJxpDiC5tX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a> Large-Scale Visual Speech Recognition</a> <a class="note_content_pdf" href="/pdf?id=HJxpDiC5tX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019,    &#10;title={ Large-Scale Visual Speech Recognition},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HJxpDiC5tX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">This work presents a scalable solution to open-vocabulary visual speech recognition. To achieve this, we constructed the largest existing visual speech recognition dataset, consisting of pairs of text and video clips of faces speaking (3,886 hours of video). In tandem, we designed and trained an integrated lipreading system, consisting of a video processing pipeline that maps raw video to stable videos of lips and sequences of phonemes, a scalable deep neural network that maps the lip videos to sequences of phoneme distributions, and a production-level speech decoder that outputs sequences of words. The proposed system achieves a word error rate (WER) of 40.9% as measured on a held-out set. In comparison, professional lipreaders achieve either 86.4% or 92.9% WER on the same dataset when having access to additional types of contextual information. Our approach significantly improves on other lipreading approaches, including variants of LipNet and of Watch, Attend, and Spell (WAS), which are only capable of 89.8% and 76.8% WER respectively.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">15 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HkeVIhvVa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Gap between size of train and test sets</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxpDiC5tX&amp;noteId=HkeVIhvVa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper304 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The authors mention that the model is trained on "3,886 hours of video" but only evaluated "on a held-out test set roughly 37 minutes long". Is there a reason for this gap? What would happen if a less radical train-test split is used? Would the results still be as good? Otherwise it is hard to tell whether the improvement comes from the model or the data, as AnnonReviewer2 pointed out.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJx0PnNoa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxpDiC5tX&amp;noteId=BJx0PnNoa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper304 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper304 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">This is an excellent observation. The current split was a consequence of how the data was generated and filtered. We fully agree that the test set could be larger and are working on this. However, we anticipated this and for this reason we also tested on the LRS3-TED test set, and report the results in the paper. In this out-of-sample test, the proposed approach outperformed the state-of-the-art method trained on LRS3-TED, thus providing us with good evidence of its generalization capabilities.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1lIVJ5Z6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Side-view faces and pipeline contribution</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxpDiC5tX&amp;noteId=B1lIVJ5Z6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper304 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Interesting engineering work. Could you please elaborate on the following questions? 

The samples in Fig. 10 are all looking (almost) towards the camera. What happens to the method for side-view of the face? Several lines of work in the vision community focus on that, e.g. face recognition. Is this a limitation of the dataset? 

The authors claim in a previous comment that this pipeline is a significant contribution, however significant part of this pipeline has been used in previous works (e.g. <a href="https://arxiv.org/pdf/1705.02966.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1705.02966.pdf</a> or its more recent extensions).

A major point of the paper is the engineering pipeline. Can the authors explain what other methods have they tried? A similar work submitted in this conference describes in details such efforts: 
https://openreview.net/pdf?id=B1xsqj09Fm (appendix C and E). 
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJeECW-Mp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxpDiC5tX&amp;noteId=SJeECW-Mp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper304 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper304 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt; Interesting engineering work. Could you please elaborate on the following questions? 

Thank you. We will try to do our best to answer your questions.

&gt; The samples in Fig. 10 are all looking (almost) towards the camera. What happens to the method for side-view of the face? Several lines of work in the vision community focus on that, e.g. face recognition. Is this a limitation of the dataset? 

The choice of +/- 30 degree face views was driven by our desire to focus on apps with good social impact, where the user has an incentive to look at the camera. Our intention is to purposely design tools to help people with speech impairments; 100,000s could benefit. We also recommend you examine the last paragraph of Section 5.1 and Table 2. There, our experiments show that even though our method was trained on faces at angles of +/- 30 degrees it outperforms the state of the art at angles of +/- 90 degrees on a different dataset that allows for this greater variation. Table 2 has the precise numbers.

&gt; The authors claim in a previous comment that this pipeline is a significant contribution, however significant part of this pipeline has been used in previous works (e.g. <a href="https://arxiv.org/pdf/1705.02966.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1705.02966.pdf</a> or its more recent extensions).

While this important line of work influenced our research, there are large and  important differences in our preprocessing pipelines, e.g. landmark smoothing, blurriness filtering, and extracting phoneme alignments, which differ from previous work on visual ASR and which we found crucial for obtaining a clean dataset from such a noisy source as YouTube. We will make sure to highlight these differences.

We should also distinguish between the “pre-processing pipeline” used to generate the dataset and the neural network architecture used for visual ASR. In this work we also provide comparisons with the (very different) neural network architecture proposed by the team you cite (seq2seq).

&gt; A major point of the paper is the engineering pipeline. Can the authors explain what other methods have they tried? A similar work submitted in this conference describes in details such efforts: 

&gt; https://openreview.net/pdf?id=B1xsqj09Fm (appendix C and E). 

With regards to the neural network architecture, we report a comprehensive set of the most important ablations, in Table 1, although we should note that each ablation takes about a month of computing on 64 GPUs.  

Thanks for drawing the connection to this other ICLR submission (Large Scale GAN) on scaling up representations and datasets. Indeed scaling up neural network representations is a very important topic in deep learning and at ICLR, as we pointed out to AnonReviewer1. 

Appendix C of the GAN paper touches on the difficulties associated with batch norm at scale. In this work the authors employ cross-replica Batch Norm, but in preliminary experiments we found this level of communication to be too expensive computationally. At sampling time the GAN work does make use of a GAN-specific approach which does not apply in our setting. Similar approaches could be considered in our setting, however they often trade off computational time for memory, and in our setting we are at the very limit of available memory in modern distributed GPU architectures due to the fact that video data has the extra time dimension (e.g. 360 frames for 12 seconds). As a result we found group-norm to be a very effective solution and it is one of the greatest contributors to our results (as it can be seen in Table 1). We will add further details discussing these tradeoffs to the paper.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByewPrEEpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Additional clarifications for engineering pipeline and dataset</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxpDiC5tX&amp;noteId=ByewPrEEpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper304 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the quick replies. However, I feel several significant details are somehow left out of the paper (or at least the authors' replies are not clear in the paper).  

The authors mention that significant difference from previous pipelines are used. Could the authors mention those clearly in the paper? Because, several lines of work use almost the same pipeline. It cannot be as a contribution in all the works. 

In addition about reproducibility: If it is so hard to train a model, are the authors planning on releasing the code along with the trained models? 

Since the authors replied that one of their driving motivations is large scale models, shouldn't they at least share some detailed insights? The models in Table 1 are a handful. If the main novelty is an large-scale pipeline, shouldn't a thorough experimentation take place?  Obviously the authors seem to have the resources, so why not perform the proper analysis? 

The dataset seems to be the core contribution to the community from this work. Have the authors made any other implicit assumptions that a user should be aware of (apart from the degrees of face rotation)? What about ethnicities included?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1xa2n4j6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' Response 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxpDiC5tX&amp;noteId=S1xa2n4j6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper304 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper304 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt; Thanks for the quick replies. However, I feel several significant details are somehow left out of the paper (or at least the authors' replies are not clear in the paper).  

Thank you for your feedback, we will do our best to try to clarify your questions next.

&gt; The authors mention that significant difference from previous pipelines are used. Could the authors mention those clearly in the paper? Because, several lines of work use almost the same pipeline. It cannot be as a contribution in all the works. 

We intend to incorporate the clarifications, mentioned in our previous reply and expanded below, in the revised version of the paper. As mentioned before, while we agree that the important line of work of Chung et. al., 2016 (<a href="https://arxiv.org/pdf/1611.05358.pdf)" target="_blank" rel="nofollow">https://arxiv.org/pdf/1611.05358.pdf)</a> has influenced our research, there are some important differences between our preprocessing pipeline and previous literature. In particular, our pipeline has the following key differences:

a) Landmark smoothing. The outputs of the resulting landmark positions from the face tracker module are smoothed using a temporal Gaussian kernel. Intuitively, this simplifies learning filters for the 3D convolution layers by reducing spatiotemporal noise. Empirically, our preliminary studies showed smoothing was crucial for achieving optimal performance. Chung et. al. don't perform this smoothing step.

b) Quality filtering. While we limit the minimum distance between the eyes to 80px, which allows us to keep only high resolution samples, we have additionally found that using the variance of Laplacian of each frame to be a very effective filter for blurriness detection in videos where the resolution is still high. This is something novel in the visual ASR literature. Chung et al. do not perform this type of filtering as their videos are limited to a standard format originating from the same source and our dataset is much more varied.

c) Low computational cost speaking filter. Processing ~16 years of video can be computationally intensive, this module had a crucial role in the performance of our preprocessing pipeline. Using the extracted and smoothed landmarks, minor lip movements and non speaking faces are discarded using a threshold filter on the standard deviation of the mouth openness. This classifier has very low computational cost, but high recall, e.g. voice-overs are not handled. This component is distinct from the speaking filter of Chung et al. but follows the same intuition. Arguably, in both cases the classifiers are noisy, and as noted above have high recall. The noise in this classifier may be slightly ameliorated by our use of the landmark smoothing, however this is a point that deserves further study.

d) Speaking classifier. V2P-Sync is our proposed architecture for an audio-video synchronisation classifier. We have found that V2P-Sync performs better compared to earlier work by Chung &amp; Zisserman (2016b) and Torfi et al. (2017). The key difference are the following: 1) compared to Chung &amp; Zisserman our work uses a 3D convolutions as from our V2P evaluation we've found they are much more applicable to video, 2) Torfi et al. do use 3D convolutions and a softmax classifier but doesn't use landmark smoothing, view canonicalisation and the inputs are lower resolution (100x60 vs 128x128). In practice we found that training a max margin classifier was easier. More details on the architecture of V2P-sync can be found in the Appendix section of our paper.

e) Phonemes. The aligned phoneme sequences are obtained from the transcripts via a standard forced alignment approach using a lexicon with multiple pronunciations. While the use of phonemes is a crucial part of our model and architecture, this is also something that must be extracted by the pre-processing pipeline, and to the best of our knowledge this is the first work to make use of a phoneme-level end-to-end trained model for visual ASR.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SygJnn4oaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' Response 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxpDiC5tX&amp;noteId=SygJnn4oaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper304 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper304 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt; In addition about reproducibility: If it is so hard to train a model, are the authors planning on releasing the code along with the trained models? 

While training the models benchmarked in the paper is computationally intensive, it is not necessarily difficult to implement our model given our description in the paper. If you feel there are architectural details of our model (V2P) missing from our paper please let us know, and we will do our best to assist.

&gt; Since the authors replied that one of their driving motivations is large scale models, shouldn't they at least share some detailed insights? The models in Table 1 are a handful. If the main novelty is an large-scale pipeline, shouldn't a thorough experimentation take place?  Obviously the authors seem to have the resources, so why not perform the proper analysis? 
&gt; The dataset seems to be the core contribution to the community from this work. 

The models benchmarked against in Table 1 represent the state-of-the-art over the last two years. The comparisons also include important ablations, which illustrate the value of what we think are the important things to know about when training models at this large scale. For example, it becomes essential to consider alternatives to batch normalization, as also pointed out in another effort being reviewed at this conference focusing on scale ( <a href="https://openreview.net/pdf?id=B1xsqj09Fm" target="_blank" rel="nofollow">https://openreview.net/pdf?id=B1xsqj09Fm</a> ). We hope this shared knowledge will be useful to other practitioners in our community.

For clarity, the novelty claims of the paper extend beyond the data processing pipeline or dataset, and are detailed in the introduction. These claims are empirically validated in the evaluation section. 

We present as many ablations as possible over months of experimentation, and again emphasize that we report what we think will be most useful to other researchers interested in large-scale experiments. We would be delighted to answer any questions in relation to this.

&gt; Have the authors made any other implicit assumptions that a user should be aware of (apart from the degrees of face rotation)? What about ethnicities included?

The dataset originates from YouTube and captures the wide diversity of people in that medium. Figure 10 illustrates random samples from CC videos of our dataset. As stated previously, having shared this knowledge with the research community, our current focus is on developing accessibility apps for speech and hearing impaired people. As such, we obviously need a method that works for everyone. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_B1xeN85yam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>nice data collection but no technical contribution</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxpDiC5tX&amp;noteId=B1xeN85yam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper304 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper304 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper presents a non-trivial data processing pipeline, a large data set, and a system based on CTC and FSTs for automatic lipreading from videos.

The review of the previous work is comprehensive. The authors are also awared of the state of the art in speech recognition, a highly related task.

The collection of the data set is definitely a contribution, but other than that, the technical novelty is scarce, since all of the techniques have been proposed either in lipreading from video or in speech recognition.

The numbers in Table 1 are impressive, but it is hard to tell where the improvement is coming from. It is worth running a few more experiments a) with the label set fixed while changing the network architecture b) with the network architecture fixed while changing the label set c) with the network and the label set fixed while changing dropout or group normalization. seq2seq is an odd child in this case, because you cannot really compare it to other settings.

The result in Table 2 is also impressive, but it would be nice to have the proposed system trained on LRS3-TED and compare against TM-seq2seq.

It is generally a consensus that a large model paired with a large amount of data gives you improvement, and this type of improvement is not considered a contribution. It is then the authors' responsibility to have a comprehensive experiments showing that the improvement is not just due to having a larger model and more data.

Here are some minor details:

p.6.

note that there must be a blank between the 'e' characters to avoid collapsing ...
--&gt; this is actually not true, at least not in the original CTC formulation, where removing the duplicates and blanks have to be done in that order.

To explain why modeling characters with CTC is problematic, ...
--&gt; this argument is not theoretically sound, so the question is does this happen in practice? the loss only measures at the independence level, but this doesn't prohibit the network to learn dependencies before the loss.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJltpiAgTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' Response 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxpDiC5tX&amp;noteId=HJltpiAgTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper304 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper304 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt; The paper presents a non-trivial data processing pipeline, a large data set, and a system based on CTC and FSTs for automatic lipreading from videos.
&gt; The review of the previous work is comprehensive. The authors are also awared of the state of the art in speech recognition, a highly related task.
&gt; The collection of the data set is definitely a contribution, but other than that, the technical novelty is scarce, since all of the techniques have been proposed either in lipreading from video or in speech recognition.

While the individual components exist in the literature, their combination to achieve state-of-the-art visual speech recognition is unique. Further, due to the large scale and distributed nature of our training, model and dataset, the choice of components demands additional thought. For example, at this scale we could do group-norm, but not batch-norm. Many alternative design decisions which would have been sensible at small scale fail at this large scale. In this sense, we believe we are innovating and pushing an important frontier.

&gt; The numbers in Table 1 are impressive, but it is hard to tell where the improvement is coming from. It is worth running a few more experiments a) with the label set fixed while changing the network architecture b) with the network architecture fixed while changing the label set c) with the network and the label set fixed while changing dropout or group normalization. seq2seq is an odd child in this case, because you cannot really compare it to other settings.

Training our model with 64 GPUs takes about 1 month. 

We conducted as many ablations as possible for over a year. In TABLE 1, we report what we thought were the most important ablations, which we would like to clarify as we think these already address many of your questions. First, starting with LipNet and switching from character labels to phoneme labels results in a modest gain (~3% gain in WER), and importantly we provide a section in the paper titled rationale for phonemes which discusses this label choice. 

Second, we increased the size of LipNet to match the size of our V2P model and this results in a decrease of  ~20% in WER for the same labels and dataset. Yet, while size matters, clever design plays the biggest role. By focusing on architectural changes, such as group norm instead of dropout and LSTM cores instead of GRUs, we are able to reduce the WER from 72.7% to 40.9% (an improvement of about 32% WER). So these architectural changes are the most important factors to achieve our final WER, and are crucial for optimising the neural net architectures to take full advantage of multi-GPU distributed systems --- as we are pushing the limits of computation, communication and storage.

Third, we provide ablations to measure the contribution of the language model and alternative architectures (fully convolutional). Finally, we compare against the seq2seq architecture because it was the previous state-of-the-art in visual speech recognition. 

Altogether we strongly feel this is a comprehensive and thorough set of ablations that clearly tests the value of different design choices.  </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Bkx42sCgpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' Response 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxpDiC5tX&amp;noteId=Bkx42sCgpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper304 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper304 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt; The result in Table 2 is also impressive, but it would be nice to have the proposed system trained on LRS3-TED and compare against TM-seq2seq.
&gt; It is generally a consensus that a large model paired with a large amount of data gives you improvement, and this type of improvement is not considered a contribution. It is then the authors' responsibility to have a comprehensive experiments showing that the improvement is not just due to having a larger model and more data.

Thank you. As our model takes about a month to train even with 64 GPUs, and the LRS3-TED dataset was released less than a month before the paper submission deadline, this was deemed infeasible. We think our ablations (Table 1) make it very clear that a large model is not enough --- we explicitly controlled for this, and found that one needs a well designed large model. The point of testing on the smaller LRS3-TED dataset was solely to illustrate that our model beats the state-of-the-art there even without being trained there --- that is, the objective of the experiment was to show that our model generalizes well to other data. 

&gt; note that there must be a blank between the 'e' characters to avoid collapsing ...
--&gt; this is actually not true, at least not in the original CTC formulation, where removing the duplicates and blanks have to be done in that order.

In the original CTC formulation, indeed removal of duplicates happens before the removal of blanks. So in our example and the original CTC paper's 'B' function, B(be#ee) = bee, where # denotes blank. We will reword that sentence for clarity in the next update.

&gt; To explain why modeling characters with CTC is problematic, ...
--&gt; this argument is not theoretically sound, so the question is does this happen in practice? the loss only measures at the independence level, but this doesn't prohibit the network to learn dependencies before the loss.

This argument is indeed backed up by our empirical findings. While the network could learn dependencies before the loss, this would hamper its ability to represent the uncertainty that is inherent in visual speech recognition due to phonemes that appear visually similar (visemes). In the case of audio ASR on clean audio, this argument is indeed less applicable.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rylyGAF53m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Engineering Marvel</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxpDiC5tX&amp;noteId=rylyGAF53m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper304 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper304 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper presents a large-scale lipreading system - no surprises there. This is good work and probably the strongest general purpose lip-reading system out there at this time, but i don't see both the work and the paper as a good fit for ICLR.

The authors take a large corpus of YouTube videos (on which Google has already trained direct acoustics-to-word speech recognizers, and which is manually transcribed), filter it, and extract regions that can be used for lipreading. They then describe a scalable preprocessing, and train a phone-based acoustic model using CTC. They seem to be using the (Miao et al., 2015) and Google WFST based decoding framework, and achieve a word error rate of ca 40%. That is impressive, but I don't see any novelty here, and the paper is full of contradictions, and leaves some important open questions:

- the authors argue for "phonemes and ctc", and no speech person would disagree with them; in fact (Miao et al., 2015) and many other papers show that the WERs with a good phoneme based dictionary in English are lower than with a character based model. it's just easier if one does not need a dictionary.
- why are the authors not using a viseme dictionary, or map their phoneme dictionary to a viseme dictionary. In visual space, their own "homonym" argument applies, too, and "mop" (or "mom") and "pop" should be mapped to the same "viseme" sequence - and the resulting uncertainty should be handled by the decoder, and not the classifier.
- how did the authors generate the one million word phoneme vocabulary? even google used around 100,000k words in their whole-word experiments, if i remember correctly? what happens if the authros reduce the vocabulary? could you provide some error analysis or at least deletions/ insertions/ substitutiosn, and compare them against an audio system?
- LipNet and the proposed architecture seem to be very similar - maybe you could provide some insight into which changes made the biggest difference?
- is the data going to be available?
- what is a "production-level speech decoder"? how come your model "is the first to combine a deep learning-based phoneme recognition model with production-grade word-level decoding techniques" if Google does essentially the same ("in production")?
- in Section 1, you say that "by design, the trained model only performs well when videos are shot at specific angles when a subject is facing the camera, [...] It does not perform well in other contexts". in Section 5, you demonstrate the "generalization power of our V2P approach"and find that it "is able to generalize well" - please clarify
- "speech impaired patients" often have non-canonical articulation, the proposed system may not work well for them
- it would be interesting to also know the absolute levels of insertions/ deletions/ substitutions for words and/ or phonemes, and for the audio only and visual systems, to be able to diagnose what the problems are. 
- finally, Figure 10 is really hard to view - i'd be happy to be shown fewer faces, the main message is that the quality of the face detection is really good?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1lEYoOyT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' Response 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxpDiC5tX&amp;noteId=B1lEYoOyT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper304 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper304 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt; The paper presents a large-scale lipreading system - no surprises there. This is good work and probably the strongest general purpose lip-reading system out there at this time, but i don't see both the work and the paper as a good fit for ICLR.

Thank you. We feel it is a good fit for various reasons. First, this work is about an important application of deep neural networks, just like audio speech recognition, image classification and translation, where results and execution have mattered and partly led to the success of this conference. Second, our work  demonstrates scaling to massive video datasets, a frontier of great interest. Third, it demonstrates some reasons for considering dynamic programming approaches (CTC here) over seq2seq attention models, which are more popular in this community. Finally, it provides ablations revealing the importance of different modules in constructing large-scale neural network architectures.  

&gt; The authors argue for "phonemes and ctc", and no speech person would disagree with them; in fact (Miao et al., 2015) and many other papers show that the WERs with a good phoneme based dictionary in English are lower than with a character based model. it's just easier if one does not need a dictionary.

While this might have been known for audio ASR, it certainly was not the case in visual ASR; the topic of this paper. In fact, the previous state-of-the-art in visual speech recognition was a seq2seq character-based model [1]. For this reason, we compared approaches and showed that using "phonemes and ctc" works better than characters and seq2seq in this domain. 

&gt; why are the authors not using a viseme dictionary, or map their phoneme dictionary to a viseme dictionary. In visual space, their own "homonym" argument applies, too, and "mop" (or "mom") and "pop" should be mapped to the same "viseme" sequence - and the resulting uncertainty should be handled by the decoder, and not the classifier.

Recent literature shows that there is no optimal viseme mapping that can generalize to all individuals [2]; that work compares 15 different mappings and our preliminary work in the same direction shows similar trends. For example /m/ and /p/ in "mop" and "pop" don't belong to the same viseme group according to Fisher et al. [3] or Hazen et al. [4], but they do in several other mappings. In this setting, using the phonemes, we sidestep the need to choose a single viseme mapping, and instead allow the uncertainty between inherently similar/identical phonemes to be encoded in the predictive distribution of the neural network directly. Further, the language model is subsequently able to manage this resulting uncertainty by mapping sequences of phoneme distributions to sequences of words, and we are able to leverage the fact that we can build very good language models.

&gt; how did the authors generate the one million word phoneme vocabulary? even google used around 100,000k words in their whole-word experiments, if i remember correctly? what happens if the authros reduce the vocabulary?

We use almost completely raw text from YT transcripts, which results in a much wider variety of words, acronyms, numbers, currency amounts, etc. than one would normally expect from cleaned text. Given the words in the training set, pronunciations are generated by a dictionary and falling back to a grapheme-to-phoneme model. We haven't tried experiments on reducing the vocabulary, but we expect it to have little effect on WER as long as it captures the probability mass. As expected WER would drop in a limited domain with a restricted vocabulary.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1gTwoOJ67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' Response 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxpDiC5tX&amp;noteId=r1gTwoOJ67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper304 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper304 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt; could you provide some error analysis or at least deletions/ insertions/ substitutiosn, and compare them against an audio system?

We provide heatmaps with deletions / insertions / substitutions in Figures 4 and 7 for V2P, and we could easily provide similar results for our audio baseline. Thanks for the suggestion.

&gt; LipNet and the proposed architecture seem to be very similar - maybe you could provide some insight into which changes made the biggest difference?

Overall we found that 1) introducing stabilization in the processing pipeline, changing the 2) network size, 3) depth, 4) replacing dropout with group normalization, and 5) working in phoneme level with a decoder pipeline were the key components of our performance, an ablation for each is shown in Table 1. We will ensure the differences are better emphasized in the main text.

&gt; is the data going to be available?

We are very interested in publishing our dataset / video timestamps and we are investigating possible ways to do so. We will only do so provided there are no privacy or security concerns. The massive scale and importance of this dataset demands responsible use.

&gt; what is a "production-level speech decoder"? how come your model "is the first to combine a deep learning-based phoneme recognition model with production-grade word-level decoding techniques" if Google does essentially the same ("in production")?

We mean specifically in the context of visual speech recognition. That is, the phoneme recognition model takes videos as input, not audio. Thank you, we will clarify it in our next update.

&gt; in Section 1, you say that "by design, the trained model only performs well when videos are shot at specific angles when a subject is facing the camera, [...] It does not perform well in other contexts". in Section 5, you demonstrate the "generalization power of our V2P approach"and find that it "is able to generalize well" - please clarify

We apologize for the confusion. We will be more precise in the next version. The precise facts are as reported: In the TED experiments we examined the performance of speaker angles outside our training set and, as shown in Table 2, outside this range the performance dropped by 8 WER, but indeed our model still outperforms TM-seq2seq. Thus, there is a drop in performance and this performance will eventually drop to zero if the lips stop appearing, but our approach is better at generalizing than other approaches.

&gt; "speech impaired patients" often have non-canonical articulation, the proposed system may not work well for them

Correct, hyperarticulation is a case where our proposed system would not work well. We are working closely with specialists for identifying the cases we can help. Our preliminary investigation can be found in Appendix A. We expect that patients who spoke normally for their whole lives but only recently lost the ability to produce sound will retain mostly normal articulation. We have conducted some tests and the results are positive, and as soon as we obtain proper approval, we will release these.

&gt; it would be interesting to also know the absolute levels of insertions/ deletions/ substitutions for words and/ or phonemes, and for the audio only and visual systems, to be able to diagnose what the problems are. 

A table with absolute values wouldn't easily fit in the PDF, but we can include these as additional supplementary material.

&gt; finally, Figure 10 is really hard to view - i'd be happy to be shown fewer faces, the main message is that the quality of the face detection is really good?

The point of this picture was to show the diversity of the LSVSR dataset. Thanks, we will add a note to this effect.


References:
[1] Chung, Joon Son, et al. "Lip Reading Sentences in the Wild." CVPR. 2017.

[2] Bear, Helen L., and Richard Harvey. "Phoneme-to-viseme mappings: the good, the bad, and the ugly." Speech Communication 95 (2017): 40-67.

[3] Fisher, Cletus G. "Confusions among visually perceived consonants." Journal of Speech, Language, and Hearing Research 11.4 (1968): 796-804.

[4] Hazen, Timothy J., et al. "A segment-based audio-visual speech recognizer: Data collection, development, and initial experiments." Proceedings of the 6th international conference on Multimodal interfaces. ACM, 2004.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1eY8B3_3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>review of the paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxpDiC5tX&amp;noteId=B1eY8B3_3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper304 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper304 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=B1eY8B3_3X" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This is a good paper. First of all, it presents a large-scale corpus for visual speech recognition. Second, it demonstrates a visual speech recognition system based on open-vocabulary that gives the state-of-the-art recognition accuracy.  The paper is very well written and all the technical details are clearly laid out.  I, for one, would like to thank the authors for this meticulous work to the community.   This is by far the largest dataset and the most impressive performance for VSR I have even seen in the ASR/VSR community.  I enjoyed reading this paper.  

I extend this review based on the replies.  One of the arguments is that the work presented in this paper is a great success in engineering but it lacks technical novelty and therefore can not be accepted by the conference, which I think otherwise.  First of all, the authors put together a very detailed and carefully designed technical pipeline for creating a very large visual speech recognition dataset, which is a valuable contribution to be community.  (I assumed that the databset will become available to the community when reviewing the paper, which turned out not to be totally accurate. My apologies.  I do hope the dataset will be made public.  This is a major reason I gave a high score.)  Second,  the authors have built systems that give the state-of-the-art performance on visual speech recognition. Although the models and architectures are already out there,  the impressive performance itself is an impact to the field.  This is not simply achieved by piling in a large amount of data (although it does play a role). This is a system paper but its impact and its performance should at least get it in to the conference. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">9: Top 15% of accepted papers, strong accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryldzq_kTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Authors' Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxpDiC5tX&amp;noteId=ryldzq_kTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper304 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper304 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your encouraging feedback.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>