<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>SpaMHMM: Sparse Mixture of Hidden Markov Models for Graph Connected Entities | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="SpaMHMM: Sparse Mixture of Hidden Markov Models for Graph Connected Entities" />
        <meta name="citation_author" content="Diogo Pernes" />
        <meta name="citation_author" content="Jaime S. Cardoso" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HJM4SjR5KQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="SpaMHMM: Sparse Mixture of Hidden Markov Models for Graph Connected..." />
      <meta name="og:description" content="We propose a framework to model the distribution of sequential data coming from&#10;  a set of entities connected in a graph with a known topology. The method is&#10;  based on a mixture of shared hidden..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HJM4SjR5KQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>SpaMHMM: Sparse Mixture of Hidden Markov Models for Graph Connected Entities</a> <a class="note_content_pdf" href="/pdf?id=HJM4SjR5KQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=dpc%40inesctec.pt" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="dpc@inesctec.pt">Diogo Pernes</a>, <a href="/profile?email=jaime.cardoso%40inesctec.pt" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="jaime.cardoso@inesctec.pt">Jaime S. Cardoso</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We propose a framework to model the distribution of sequential data coming from
a set of entities connected in a graph with a known topology. The method is
based on a mixture of shared hidden Markov models (HMMs), which are trained
in order to exploit the knowledge of the graph structure and in such a way that the
obtained mixtures tend to be sparse. Experiments in different application domains
demonstrate the effectiveness and versatility of the method.</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A method to model the generative distribution of sequences coming from graph connected entities.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">multi-entity sequential data, hidden markov models</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rJxczGfo6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJM4SjR5KQ&amp;noteId=rJxczGfo6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper81 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper81 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJlbNeoWTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>summary</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJM4SjR5KQ&amp;noteId=BJlbNeoWTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper81 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper81 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper is not very well written. I cannot find what is the main contribution of this paper. For example, equation (1) seems out of the scope of the main discussion of this paper. It does not look a big deal employing EM algorithm for estimating the HMM parameters, but it seems the main technical contribution of the paper.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1gtoRWmp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>RE: summary</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJM4SjR5KQ&amp;noteId=r1gtoRWmp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper81 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper81 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for your review. However, we think that you misunderstood central parts of our work, so please let us try to do some clarifications.

«I cannot find what is the main contribution of this paper.»

As pointed out in the abstract, the paper presents a principled way to model the distribution of sequences coming from a set of entities which are connected in a graph with known topology. Our main contributions are, then, twofold.
First, we propose to model the distributions of the sequences coming from different graph nodes as different mixtures over the same pool of HMMs. This differs from the two naive solutions (which we use as baselines in our experiments): 1) – training one HMM per graph node; and 2 - training a single HMM to model the sequences from all graph nodes.
Second, we propose a regularizer term (see equation (4)) for the usual log-likelihood objective which exploits the knowledge of the graph structure and promotes sparse mixtures. The rationale behind is that, in each node, the observations are well modeled by a mixture of only a few HMMs (components); similar nodes should have mixtures sharing some of these HMMs.

 «For example, equation (1) seems out of the scope of the main discussion of this paper.»

It is not. Note that equation (2), which (partially) defines our model, has exactly the same form as equation (1). There, the distribution $p(X|y)$ corresponds to the function $f$, the mixture coefficients $p(z | y)$ correspond to the coefficients $s_i$ and, finally, $p(X|z)$ corresponds to $\phi_i$. Basically, our model is a particular realization of equation (1), where the function $f$ represents a probability density function.

«It does not look a big deal employing EM algorithm for estimating the HMM parameters, but it seems the main technical contribution of the paper.»

Estimating the HMM parameters via EM is a very well-established procedure and is known in the literature as the Baum-Welch algorithm. Thus, this could not be the main technical contribution of our paper. We present however two realizations of the EM algorithm.
The first allows the estimation of parameters for our mixture model when no regularization is applied. As written in the paper, this follows directly from the application of EM to our model, under the usual log-likelihood objective. However, this derivation is necessary, since the optimization of the parameters for our model cannot be done by the Baum-Welch algorithm. This is because our model is not a single HMM and not even a standard mixture of HMMs, due to the conditional independence between the sequences $X$ and the nodes $y$ given the latent variable $z$.
The second requires a slight modification in the standard EM algorithm to accommodate the regularizer term, since the standard EM algorithm is designed to maximize the log-likelihood only. Therefore, we derive the algorithm with care to prove its correctness.
In any case, neither of these derivations aims to be the main contribution of the paper, otherwise they would not be entirely developed in the Appendix. However, presenting them is necessary to ensure the correctness of the optimization algorithms associated with the proposed models.
 
In summary, the main technical contribution is the complete methodology: the model itself; a new loss function to better capture the prior and contextual information; the required optimization procedure to learn the model from data; and finally, making inferences with the learned model.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rke132Ns3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A method to model multiple sequential data sources, with interconnections among them obeying a weighted undirected graph. The paper is not well organized and the model description is not convincing. Potentially important empirical exploration is missing.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJM4SjR5KQ&amp;noteId=rke132Ns3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper81 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper81 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose a method to model sequential data from multiple interconnected sources using a mixture of common pool of HMM's. The composition of the mixture for each data source is both sparse and similar to that of other data sources that are close to it, with the closeness determined by the weights of the edges of an undirected graph.

The introduction section is unfocused and sloppy. HMM's are well understood models in machine learning but the paper falls short in explaining the particular distributed scenario of interest. The narrative jumps from sports, to neuroscience to wireless communications to fMRI,  without mentioning the common denominator. The proposed model section lacks also some focus, jumping from distributed sparse representations to multitask learning. The key concept here seems to be the poorly defined concept of 'a sparse HMM mixture over a large dictionary of HMM's'. The formalization of this rather complicated object is not well justified and leaves a lot of guesswork to the reader. 

Instead of maximizing the likelihood, an alternative objective function (4) is proposed as maximizing the inner products of posterior probability vectors at each node. The authors probably try to say something sensible but the sloppy notation is not very helpful here.   

The way authors introduced the graph structure into their cost function creates potentially a flexibility. However, the authors could have spent more energy on explaining why sparseness of the mixtures should be a desirable property for the problems they hope to solve with the model. The graph structure could potentially be used without necessitating sparsity, so opting for sparsity needs to be justified.

One also suspects that the authors could have written a clearer generative model instead of modifying the maximum likelihood criterion for learning. This would have enabled the readers to appreciate the generative model better.

Moreover, the parameter $\lambda$ controls how much effect the graphical interrelations is to have on the final learned parameters of the model. The authors however do not present a detailed examination of empirical results of varying $\lambda$, and only suffice to determine it with cross-validation. A more interpretive stance towards lambda would confer increased understanding of how sparsity functions in this model. The cluster analysis at the end of the experiments indeed provide a more tangible demonstration of how sparsity affects the results obtained and why it might be desirable.

Overall, the paper is not of sufficient quality to be presented at ICLR.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJeANkM7p7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>RE: A method to model multiple sequential data sources...</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJM4SjR5KQ&amp;noteId=SJeANkM7p7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper81 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper81 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for your review and for your insightful comments. We, however, believe that there are some points that were misunderstood, probably because our explanation was not as clear as it should be. We would like to clarify those:

«The narrative jumps from sports, to neuroscience to wireless communications to fMRI, without mentioning the common denominator. »

As argued in the first sentences of that paragraph, in all those scenarios one has multiple data streams that come from a set of distinct entities that are somehow interdependent and interconnected, forming a network – this is the common denominator. What we propose in the paper is a principled way to exploit the known network topology in order to build a generative model for those streams.

«The key concept here seems to be the poorly defined concept of 'a sparse HMM mixture over a large dictionary of HMM's'. The formalization of this rather complicated object is not well justified and leaves a lot of guesswork to the reader.»

The model is described formally in section 2.2.1 and it actually corresponds to “a sparse HMM mixture over a ‘large’ dictionary of HMMs”. As clearly defined in equations 2 and 3, this is a mixture model, where each mixture component is an HMM. Moreover, the “’large’  dictionary of HMMs” consists of all $M$ HMMs of the mixture. Because the mixtures for each node $y$ tend to be sparse, the distributions $p(X|y)$ will then consist of the aforementioned “sparse HMM mixture over a ‘large’ dictionary of HMMs”.

«Instead of maximizing the likelihood, an alternative objective function (4) is proposed as maximizing the inner products of posterior probability vectors at each node. The authors probably try to say something sensible but the sloppy notation is not very helpful here.»

We are sorry that you did not understand the proposed regularizer term, since this was one of the central contributions of the paper. However, we do not agree that the notation is sloppy and we actually think that it is quite precise. The probabilities $p(z | y=k, \theta)$ do not denote any posteriors, but rather the vector $\alpha_k$  of mixture coefficients for the node $k$ or, equivalently, the prior distribution of the latent variable $z$ for the node $y = k$. Therefore, as pointed out in the proof of Proposition 1., the considered expectations correspond to the inner products of the vectors $\alpha_j$ and $\alpha_k$, which are the mixture coefficients for the nodes $j$ and $k$. Thus, if the weight $G_{j,k}$ associated with the edge connecting $j$ and $k$ is positive, the regularizer promotes that the mixtures for these two nodes are similar. Otherwise, if $G_{j,k}$ is negative, the regularizer promotes that the mixtures for these two nodes do not share any components.

«The graph structure could potentially be used without necessitating sparsity, so opting for sparsity needs to be justified.»

This is entirely correct. However, note that the rationale behind our model is that, in each node, the observations are well modeled by a mixture of only a few HMMs. By promoting sparsity in the mixtures, we may accomplish this goal and still have a large (overcomplete) pool of components (HMMs). Therefore, some components in the pool will specialize in describing the behavior of a few nodes, while others will specialize in different nodes. Moreover,  sparsity yields faster inference, less overfitting and more interpretable models, which are all desirable properties.
We agree that we should improve the explanation in the paper. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Syx4Kde6oX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Use of mixtures questionable</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJM4SjR5KQ&amp;noteId=Syx4Kde6oX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper81 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">24 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper81 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper is fairly easily understandable in content. The question addressed in the work is very significant as is shown by multiple applications on real datasets in the paper. The reviewer is not aware of any prior work in connection to the question.
This work appears to be a first of its kind. However, there are some issues which the reviewer is not clear on.
1. The penalization criteria enforced leads to sharing of one active component among all connected components of positive weights, irrespective of the magnitude of the weightage, thereby reducing it to a single component mixture distribution for  every connected subgraph of positive weights, which eliminates the need to use a mixture distribution in the first place. It is unclear why the use of mixtures could be useful therein.
2. Although pairwise maximization for distributions does achieve the results in Proposition 1, however, simultaneous maximization does not guarantee sparsity, unlike as pointed out by the authors. It has not been clearly explained why the results would continue to hold true for pairwise maximization setting.
3. A regularizer based on the value of the weights rather than only on the sign could prove more effective.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1eVZefQaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>RE: Use of mixtures questionable</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJM4SjR5KQ&amp;noteId=B1eVZefQaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper81 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper81 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for your review and for your insightful comments. We, however, believe that there are some points that were misunderstood, probably because our explanation was not as clear as it should be. We would like to clarify those:

«Use of mixtures questionable.»

The usage of mixtures and, in particular, the usage of “a sparse HMM mixture over a large dictionary of HMMs” allows for similar nodes to be described by similar components, while distinct nodes are described by distinct components. Note that we learn a single pool of HMMs but each node can “choose” which HMMs will describe its behavior, by learning different mixture weights for each node. Therefore, our model exploits the inter-correlations between the nodes, while being flexible enough to let each node be described by its own unique model (since each node will have its respective mixture coefficients).

«1. The penalization criteria enforced leads to sharing of one active component among all connected components of positive weights, irrespective of the magnitude of the weightage, thereby reducing it to a single component mixture distribution for every connected subgraph of positive weights»

If the graph is fully connected and all weights are positive, optimizing the penalization criteria only actually leads to a single component mixture distribution. This is natural, since if that is the case, then our prior knowledge (i.e. the graph structure) indicates that all nodes have a similar behavior and, in such situation, this behavior can be described by the same HMM. We see this as a positive aspect: our general model includes as a special case the single HMM model. Note, however, that the log-likelihood term may (and, in general, will) force the model to learn more than one active component for each node.

«2. Although pairwise maximization for distributions does achieve the results in Proposition 1, however, simultaneous maximization does not guarantee sparsity, unlike as pointed out by the authors. It has not been clearly explained why the results would continue to hold true for pairwise maximization setting.»

It is true that the result in Proposition 1. stands for pairwise maximization only, but we think it gives a good intuition about the behavior when one has multiple nodes being optimized simultaneously. Besides, we discuss two limiting cases (when either all graph weights are positive or negative and $\lambda$ is very large) in which we show that the obtained solutions shall be extremely sparse for any arbitrary number of nodes. Experimental results also suggest that the proposed methodology promotes sparsity.

«3. A regularizer based on the value of the weights rather than only on the sign could prove more effective.»

The regularizer is indeed based on the value of the weights, not only on their sign. Note that, according to equation 5 in the paper, each term is weighted by the respective graph weight $G_{j,k}$, which can be any real number and not only {-1, 0, 1}. In one of the experiments (wi-fi anomaly detection) we actually use weights that are not in {-1, 0, 1}, since there the weights correspond to the inverse of the distances between each pair of access points.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>