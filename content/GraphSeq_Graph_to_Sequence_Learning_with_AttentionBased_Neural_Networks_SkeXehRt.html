<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Graph2Seq: Graph to Sequence Learning with Attention-Based Neural Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Graph2Seq: Graph to Sequence Learning with Attention-Based Neural Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SkeXehR9t7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Graph2Seq: Graph to Sequence Learning with Attention-Based Neural..." />
      <meta name="og:description" content="The celebrated \emph{Sequence to Sequence learning (Seq2Seq)} technique and its numerous variants achieve excellent performance on many tasks. However, many machine learning tasks have inputs..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SkeXehR9t7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Graph2Seq: Graph to Sequence Learning with Attention-Based Neural Networks</a> <a class="note_content_pdf" href="/pdf?id=SkeXehR9t7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019graph2seq:,    &#10;title={Graph2Seq: Graph to Sequence Learning with Attention-Based Neural Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SkeXehR9t7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">The celebrated \emph{Sequence to Sequence learning (Seq2Seq)} technique and its numerous variants achieve excellent performance on many tasks. However, many machine learning tasks have inputs naturally represented as graphs; existing Seq2Seq models face a significant challenge in achieving accurate conversion from graph form to the appropriate sequence. 
To address this challenge, we introduce a general end-to-end graph-to-sequence neural encoder-decoder architecture that maps an input graph to a sequence of vectors and uses an attention-based LSTM method to decode the target sequence from these vectors. 
Our method first generates the node and graph embeddings using an improved graph-based neural network with a novel aggregation strategy to incorporate edge direction information in the node embeddings. 
We further introduce a novel attention mechanism that aligns node embeddings and the decoding sequence to better cope with large graphs.
Experimental results on bAbI, Shortest Path, and Natural Language Generation tasks demonstrate that our model achieves state-of-the-art performance and significantly outperforms existing Seq2Seq and Tree2Seq models; using the proposed aggregation strategy, the model can converge rapidly to the optimal performance.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Graph Encoder, Graph Decoder, Graph2Seq, Graph Attention</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Graph to Sequence Learning with Attention-Based Neural Networks</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BylkEj0gTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting work but lacking some organization</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkeXehR9t7&amp;noteId=BylkEj0gTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1059 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1059 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This work proposes an end-to-end graph encoder to sequence decoder model with an attention mechanism in between.
Pros (+) :
+ Overall, the paper provides a good first step towards flexible end-to-end graph-to-seq models.
+ Experiments show promising results for the model to be tested in further domains.
Cons (-) :
- The paper would benefit more motivation and organization.

Further details below (+ for pros / ~ for suggestions / - for cons):

The paper could benefit a little more motivation:
- Mentioning a few tasks in the introduction may not be enough. Explaining why these tasks are important may help. What is the greater problem the authors are trying to solve?
- Same thing in the experiments, not well motivated, why these three? What characteristics are the authors trying to analyze with each of these tasks?

Rephrase the novelty argument:
- The authors argue to present a “novel attention mechanism” but the attention mechanism used is not new (Bahdanau 2014 a &amp; b). The fact that it is applied between a sequence decoder and graph node embeddings makes the paper interesting but maybe not novel.
~ The novelty added by this paper is the “bi-edge-direction“ aggregation technique with the exploration of various pooling techniques. This could be emphasized more.

Previous work:
~ The Related Work section could mention Graph Attention Networks (<a href="https://arxiv.org/abs/1710.10903)" target="_blank" rel="nofollow">https://arxiv.org/abs/1710.10903)</a> as an alternative to the node aggregation strategy.

Aggregation variations:
+ The exploration between the three aggregator architectures is well presented and well reported in experiments.
~ The two Graph Embedding methods are also well presented, however, I didn’t see them in experiments. Actually, it isn’t clear at all if these are even used since the decoder is attending over node embeddings, not graph embedding… Could benefit a little more explanation

Experiments:
+ Experiments show some improvement on the proposed tasks compared to a few baselines.
- The change of baselines between table 1 for the first two tasks and table 2 for the third task is not explained and thus confusing.
~ There are multiple references to the advantage of using “bi-directional” node embeddings, but it is not clear from the description of each task where the edge direction comes from. A better explanation of each task could help.

Results:
- Page 9, the “Impact of Attention Mechanism” is discussed but no experimental result is shown to support these claims.


Some editing notes:
(1) Page 1, in the intro, when saying “seq2seq are excellent for NMT, NLG, Speech Reco, and drug discovery”: this last example breaks the logical structure of the sentence because it has nothing to do with NLP.
(2) Page 1, in the intro, when saying that “&lt;...&gt; a network can only be applied to sequential inputs”: replace network by seq2seq models to be exact.
(3) Typo on page 3, in paragraph “Neural Networks on Graphs”, on 8th line “usig” -&gt; “using”
(4) Page 3, in paragraph “Neural Networks on Graphs”, the following sentence: “An extension of GCN can be shown to be mathematically related to one variant of our graph encoder on undirected graphs.” is missing some information, like a reference, or a proof in Appendix, or something else…
(5) Page 9, the last section of the “Impact of Hop Size” paragraph talks about the impact of the attention strategy. This should be moved to the next paragraph which discusses attention.
(6) Some references are duplicates:
|_ Hamilton 2017 a &amp; c
|_ Bahdanau 2014 a &amp; b
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1x_Q__qhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkeXehR9t7&amp;noteId=B1x_Q__qhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1059 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1059 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a graph to sequence transducer consisting of a graph encoder and a RNN with attention decoder. 

Strengths:
- Novel architecture for graph to sequence learning.
- Improved performance on synthetic transduction tasks and graph to text generation. 
Weaknesses:
- Experiments could provide more insight into model architecture design and the strengths and weaknesses of the model on non-synthetic data. 

Transduction with structured inputs such as graphs is still an under-explored area, so this paper makes a valuable contribution in that direction. Previous work has mostly focused on learning graph embeddings producing outputs. This paper extends the encoder proposed by Hamilton et al (2017a) by modelling edge direction through learning “forward” and “backward” representations of nodes. Node embeddings are pooled to a form a graph embedding to initialize the decoder, which is a standard RNN with attention over the node embeddings. 

The model is relatively similar to the architecture proposed by Bastings et al (2017) that uses a graph convolutional encoder, although the details of the graph node embedding computation differs. Although this model is presented in a more general framework, that model also accounted for edge directionality (as well as edge labels, which this model do not support). 

This paper does compare the proposed model with graph convolutional networks (GCNs) as encoder experimentally, finding that the proposed approach performs better on shortest directed path tasks. However the paper could make difference between these architectures clearer, and provide more insight into whether different graph encoder architectures might be more suited to graphs with different structural properties. 

The model obtains strong performance on the somewhat artificial bAbI and Shortest path tasks, while the strongest result is probably that of strong improvement over the baselines in SQL to text generation. However, very little insight is provided into this result. It would be interesting to apply this model to established NLG tasks such as AMR to text generation.  

Overall, this is an interesting paper, and I’d be fine with it being accepted. However, the modelling contribution is relatively limited and it feels like for this to be a really strong contribution more insight into the graph encoder design, or more applications to real tasks and insight into the model’s performance on these tasks is required. 

Editing notes:
Hamilton et al 2017a and 2017c is the same paper. 
In some cases the citation format is used incorrectly: when the citation form part of the sentence, the citation should be inline. E.g. (p3) introduced by (Bruna et al., 2013)  -&gt; introduced by Bruna et al. (2013). 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJKBboE3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Weak increment on graph to sequence tasks</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkeXehR9t7&amp;noteId=HJKBboE3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1059 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1059 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The submission discusses a graph2seq architecture that combines a graph encoder that mixes GGNN and GCN components with an attentional sequence encoder. The resulting model is evaluated on three very simple tasks, showing small improvements over baselines.

I'm not entirely sure what the contribution of this paper is supposed to be. The technical novelty seems to be limited to new notation for existing work:
- (Sect. 3.1) The separation of forward/backward edges was already present in the (repeatedly cited) Li et al 2015 paper on GGNN (and in Schlichtkrull et al 2017 for GCN). The state update mechanism (a FC layer of the concatenation of old state / incoming messages) seems to be somewhere between a gated unit (as in GGNN) and the "add self-loops to all nodes" trick used in GCN; but no comparison is provided with these existing baselines.
- (Sect 3.2) The discussed graph aggregation mechanism are those proposed in Li et al and Gilmer et al; no comparison to these baselines is provided.
- (Sect. 3.3) This is a standard attention-based decoder; the fact that the memories come from a graph doesn't change anything fundamental.

The experiments are not very informative, as simple baselines already reach &gt;95% accuracy on the chosen tasks. The most notable difference between GGS-NNs and this work seems to be the attention-based decoder, but that is not evaluated explicitly. For the rebuttal phase, I would like to ask the authors to provide the following:
- Experimental results for either GGS-NN with an attentional decoder, or their model without an attentional decoder, to check if the reported gains come from that. The final paragraph in Sect. 4 seems to indicate that the attention mechanism is the core enabler of the (small) experimental gains on the baselines.
- The results of the GCN/GG-NN models (i.e., just as an encoder) with their decoder on the NLG task.
- More precise definition of what they feel the contribution of this paper is, taking into account my comments from above.

Overall, I do not think that the paper in its current state merits publication at ICLR.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>