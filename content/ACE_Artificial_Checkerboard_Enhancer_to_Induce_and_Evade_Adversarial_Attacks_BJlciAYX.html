<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>ACE: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="ACE: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJlc6iA5YX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="ACE: Artificial Checkerboard Enhancer to Induce and Evade..." />
      <meta name="og:description" content="The checkerboard phenomenon is one of the well-known visual artifacts in the computer vision field. The origins and solutions of checkerboard artifacts in the pixel space have been studied for a..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJlc6iA5YX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>ACE: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks</a> <a class="note_content_pdf" href="/pdf?id=BJlc6iA5YX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019ace:,    &#10;title={ACE: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BJlc6iA5YX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">The checkerboard phenomenon is one of the well-known visual artifacts in the computer vision field. The origins and solutions of checkerboard artifacts in the pixel space have been studied for a long time, but their effects on the gradient space have rarely been investigated. In this paper, we revisit the checkerboard artifacts in the gradient space which turn out to be the weak point of a network architecture. We explore image-agnostic property of gradient checkerboard artifacts and propose a simple yet effective defense method by utilizing the artifacts. We introduce our defense module, dubbed Artificial Checkerboard Enhancer (ACE), which induces adversarial attacks on designated pixels. This enables the model to deflect attacks by shifting only a single pixel in the image with a remarkable defense rate. We provide extensive experiments to support the effectiveness of our work for various attack scenarios using state-of-the-art attack methods. Furthermore, we show that ACE is even applicable to large-scale datasets including ImageNet dataset and can be easily transferred to various pretrained networks.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Adversarial Examples, Neural Network Security, Deep Neural Network, Checkerboard Artifact</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a novel aritificial checkerboard enhancer (ACE) module which guides attacks to a pre-specified pixel space and successfully defends it with a simple padding operation.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BkgTgd4Tam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Needs further clarifications</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlc6iA5YX&amp;noteId=BkgTgd4Tam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper827 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper827 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">I have to emphasize first that this is not my area of expertise so I am going to review it as an outsider. 

The authors argue that the checkerboard phenomenon can be exploited to make neural networks robust against adversarial attacks. They propose to enhance the checkerboard pattern by first adding a layer, called Artificial Checkerboard Enhancer (ACE), and then evading the attacks by zero-padding the image. The authors’ argument is that enhancing the checkerboard phenomenon will make attacks more targeted towards certain pixels, which can be evaded by shifting the image. 

Overall, I think the paper is difficult to read and is not suitable for publication. In terms of clarity, the authors do not use precise terminology that would allow the reader to reproduce their work. They allude to vague statements. For example, they introduce two KEY terminologies that are repeatedly used throughout the paper but are not properly defined (see for instance the “definition” of “Gradient Overlap” in Appendix C). 

In addition, in terms of the experiements, it certainly does not help to say that they were “reproduced by [the authors themselves]”. What does this mean?
 
In terms of originality, I agree with the first reviewer that the defense strategy seems to be easily breakable. The authors propose that they enhance the checkerboard phenomenon so that adversarial attacks become easier to implement by targeting individual pixels (the pixels in the checkerboard artifacts). Then, they pad the image with zero pixels to shift it to the right. I don’t understand how shifting the pixels would make it harder to attack (especially when the adversary knows the system). 

It would be really appreciated if the authors elaborate on the following points to help me understand their contribution: 
- The entire discussion about ACE in Section 4.1 is ad-hoc and not well-motivated. Why would ACE enhance the checkerboard patterm? Can you please explain why it works? This is not mentioned anywhere in the paper. The experiment in Section 4.2 helps a bit but it does not answer this question. 

- What wouldn't an adversary remove the padded pixels before generating the attack? In defense strategies, it is often assumed that the adversary knows the system. Can you please explain why that is not possible in this setting? 

- 

In Figure 4, the axes are \bar i and \bar j in the main body, but they are x and y in the figure. Please use the same notation. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkeyuETr3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>I worry that this defense will be easy to break</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlc6iA5YX&amp;noteId=rkeyuETr3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper827 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper827 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose that the "checkerboard phenomenon", whereby the gradients exhibit a repeating pattern over the pixel space, is a source of vulnerability to adversarial examples. They propose to first enhance this vulnerability for pre-trained models with a pre-conditioning layer, and then to evade it by zero padding the image to offset the pattern.

Clarity: I found the work difficult to follow in places, and I felt that some material crucial to the paper was relegated to appendices.

Originality: To my knowledge, the idea is original.

Quality and significance:
I feel the significance of this work is likely to be low. While the authors report positive "defense" results, I strongly suspect this is simply because the attacks considered did not uncover the defense strategy. I expect that this defense would be broken relatively quickly if the paper is accepted. The authors did not present evidence to suggest that their method reduces the probability that a misclassified example lies close to the training or test examples. As such, the defense seems to rely on the attacker being "tricked".

Specific comments:

1) Throughout the paper, I was unclear what specific kinds of attacks the method was intended to defend against.
2) In section 2.2, key definitions are relegated to appendix C.
3) Section 3.2-&gt; p = 0.3 is still 30% of the pixels. Could the authors provide a baseline with a random 30% of pixels?
4) Adaptive attack scenario: I would recommend that the authors also included a measure of the attack-ability under random noise in the epsilon ball. This would demonstrate whether the defense actually removes adversarial examples or just "attacks the attackers".</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJl5PX0-T7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our response to reviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlc6iA5YX&amp;noteId=HJl5PX0-T7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper827 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018 (modified: 09 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper827 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the detailed review and valuable comments. First of all, we believe that our defense will not be easy to break even if the adversary knows in advance about our method. We hope that this response can resolve all of your concerns. Plus, we will revise the overall paper organization for better clarity.

Before responding to the comments (from C1 to C6 below), we first point out the relationship between building a “robust model” and creating a “successful defense method”. The Euclidean distance between the fooled images and the originals (in C2 below), and the attack-ability inside an epsilon ball (in C6 below) are good measures for model robustness in metric spaces with the l_2-norm and its equivalent. The attack methods that are formulated within the l_2-equivalent norms, such as Carlini&amp;Wagner and PGD, can be defended if a model is robust concerning the measures above. However, for the attacks including OnePixel and JSMA, which are not formulated within the l_2-equivalent, the model can be fooled even when the model is robust concerning those measures. This is due to the fact that the constraint these attacks utilize cannot be bounded by the l_2-norm. Therefore, a “robust model” is robust to l_2-equivalent attacks and can be considered as a subset of “successful defense methods”. This can be demonstrated by evaluating the classification accuracy of the models adversarially-trained with PGD, when attacking with an l_0-norm-based attack.


C1) The attacks considered did not uncover the defense strategy.
-&gt; At the beginning of Section 5, we propose three types of threat models for attack algorithms. Among those attack scenarios, we considered the “white-box scenario” where the attacks uncover our defense strategy. To show that our method is not vulnerable in the scenario, we conducted the experiment against the PGD attack method, which is the strongest attack, combined with adversarial training. As shown in Appendix H, our method can successfully defend the attacks, thus we can guarantee our method can successfully defend in the white-box scenario as well.

C2) Evidence to suggest that the method can reduce the probability that a misclassified example lies close to the training or test examples was not presented.
-&gt; Using the suggested measure concerning the probability change could be suitable to identify the l_2-equivalent-robustness of a model. However, as mentioned above, since the probability cannot reflect the robustness against attacks that are not formulated within the l_2-equivalent such as OnePixel and JSMA attacks, our method did not aim to reduce this probability.

C3) The specific kinds of attacks the method was intended to defend against were unclear.
-&gt; Our goal is to propose a method that can be utilized in both black-box and white-box attack scenarios as explicitly mentioned in Section 5. Specifically, we targeted three scenarios: 1) the vanilla attack scenario, the adversary can access the target model but not our proposed defense method, 2) the transfer attack scenario, the adversary generates adversarial perturbations from a source model, which is different from the target model, and finally 3) the adaptive attack scenario (white-box attack scenario), the adversary knows every aspect of the model and the defense method so that it can directly exploit our defense.
 
C4)  In section 2.2, key definitions are relegated to Appendix C
-&gt; We will revise our paper for better readability as you suggested.

C5) Could the authors provide a baseline with a random 30% of pixels?
-&gt; Yes, we have been conducting an experiment based on the suggested setting. We will post the result as soon as possible. 

C6) Recommendation about including a measure of the attack-ability under random noise in the epsilon ball. 
-&gt; Thank you for the suggestion. It seems that we need to verify the robustness of our method against the attacks that do not utilize the gradients. Our method would be robust against such attacks because of the following three reasons. First, for a random noise in the epsilon ball, the probability that the noise can directly affect the labels is very low (according to Section 4 in [1]). Furthermore, the probability that it matches the “random” direction of shift (i.e., in the adaptive scenario) as mentioned in Appendix H is low as well. Finally, the random noise itself can be reduced through the autoencoder structure of the ACE module (according to Section 5 in [2]). We will conduct experiments by following your suggestion to verify these claims and strengthen our proposed defense method. 


[1] Xiaoyu Chao and Neil Zhenqiang Gong. “Mitigating Evasion Attacks to Deep Neural Networks via Region-based Classification”. ACSAC 2017.

[2] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. “Extracting and Composing Robust Features with Denoising Autoencoders”. ICML 2008.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1gYmGQAsX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Figure 8 question</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlc6iA5YX&amp;noteId=S1gYmGQAsX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018</span><span class="item">ICLR 2019 Conference Paper827 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Can you explain Figure 8? How are the X and Y axis selected?

It is confusing that traveling +/- 100 along the X axis does not change the class label, but traveling +/- along the Y axis quickly does.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1ec3QQz3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Explanation on Figure 8</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlc6iA5YX&amp;noteId=r1ec3QQz3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper827 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Oct 2018 (modified: 03 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper827 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your interest in our work. How Figure 8 was generated has been explained in Section 4.2, but we would like to elaborate more on this. We will explain about the axes in the figure first, and then give some detailed explanation on why the classified label map has such shape.

For a classified label map (Figure 4 and 8), we have an input image x without any perturbation at (0, 0), where X-axis is the gradient direction vector of checkerboard artifacts (C), and Y-axis is the gradient direction vector of non-checkerboard artifacts (X\C). Formally, this is expressed as \hat{e}_C and \hat{e}_{X\C} in Section 4.2. Note that C is the checkerboard pixels with high absolute gradients designed by our ACE module with 1 x1 conv and stride 2 (Figure 3.(b) shows the pixels in C that absolute gradients turn out to be in general greater than those in X\C). Then, each point in the classified label map is generated by perturbing the original image to the direction of (x, y) coordinates from -100 to 100 respectively. We have the classified label map after pixel perturbations using an example image of soup bowl as shown in Figure 4.

In Figure 4, we have empirically demonstrated the effect of this imbalance on gradients by creating a classified label map. This is an empirical backup of showing that our ACE module induces the vulnerable domain to the checkerboard. As \lambda increases, our model becomes more vulnerable on the perturbation on C, while the opposite behavior is observed on the perturbation on X\C. Therefore, the labels easily change on y-axis of classified label map with large lambda and the opposite on x-axis. You can think of this as an extension of Section 3 where we have shown that one-pixel attack success rates have checkerboard shape (Figure 2) due to the difference in the number of associated parameters on each pixel. We designed C to be the vulnerable domain which induces attacks on our known C. Thus, we successfully defended attacks with one-pixel padding.

We hope this explanation is clear enough.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_r1gqDA65jX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>I am not right person to review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJlc6iA5YX&amp;noteId=r1gqDA65jX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper827 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">22 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper827 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">I am a researcher in NLP and know little about vision, so I cannot review this paper. I have contacted general chair about this situation. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">1: The reviewer's evaluation is an educated guess</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>