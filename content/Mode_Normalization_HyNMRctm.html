<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Mode Normalization | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Mode Normalization" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HyN-M2Rctm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Mode Normalization" />
      <meta name="og:description" content="Normalization methods are a central building block in the deep learning toolbox. They accelerate and stabilize training, while decreasing the dependence on manually tuned learning rate schedules...." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HyN-M2Rctm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Mode Normalization</a> <a class="note_content_pdf" href="/pdf?id=HyN-M2Rctm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019mode,    &#10;title={Mode Normalization},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HyN-M2Rctm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=HyN-M2Rctm" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Normalization methods are a central building block in the deep learning toolbox. They accelerate and stabilize training, while decreasing the dependence on manually tuned learning rate schedules. When learning from multi-modal distributions, the effectiveness of batch normalization (BN), arguably the most prominent normalization method, is reduced. As a remedy, we propose a more flexible approach: by extending the normalization to more than a single mean and variance, we detect modes of data on-the-fly, jointly normalizing samples that share common features. We demonstrate that our method outperforms BN and other widely used normalization techniques in several experiments, including single and multi-task datasets.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Deep Learning, Expert Models, Normalization, Computer Vision</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We present a novel normalization method for deep neural networks that is robust to multi-modalities in intermediate feature distributions.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">12 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_B1x5phA1T7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Normalization method that assumes multi-modal distributions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyN-M2Rctm&amp;noteId=B1x5phA1T7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1240 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1240 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors proposed a normalization method that learns multi-modal distribution in the feature space. The number of modes $K$ is set as a hyper-parameter. Each sample $x_{n}$ is distributed (softly assigned) to modes by using a gating network. Each mode keeps its own running statistics. 

1) In section 3.2, it is mentioned that the MN didn't need and use any regularizer to encourage sparsity in the gating network. Is MN motivated to assign each sample to multiple modes evenly or to a distinct single mode? It would be better to provide how the gating network outputs sparse assignment along with the qualitative analysis.

2) The footnote 3 showed that individual affine parameters doesn't improve the overall performance. How can this be interpreted? If the MN is assuming multi-modal distribution, it seems more reasonable to have individual affine parameters.

3) The overall results show that increasing the number of modes $K$ doesn't help that much. The multi-task experiments used 4 different datasets to encourage diversity, but K=2 showed the best results. Did you try to use K=1 where the gating network has a sigmoid activation?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryghy-BgRX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: Normalization method that assumes multi-modal distributions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyN-M2Rctm&amp;noteId=ryghy-BgRX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1240 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1240 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Many thanks for the review. Regarding 1): we consider MN to be a generalization of BN, and – see paragraph 4 on p. 5 – wanted to make sure the normalization unit can assume the standard form of BN, whenever that is optimal and yields the best performance. The obvious benefit of not regularizing this behavior is that MN becomes seamlessly insertable into any deep network. Regarding sparseness: note that (even at test time) assignments are usually quite pronounced, at roughly 0.95-0.99 on average.

2): Allowing individual affine parameters only improves test performance minimally (differences are in the regime of 0.05-0.2%). In all likelihood this is because normalizing features with multiple means and standard deviations already standardizes them sufficiently.

3): As shown in paragraph 2, p. 5, when K=1, MN reduces to standard BN. We also went ahead and implemented your suggestion to activate with a sigmoid. Unfortunately, the resulting performance was worse than that of vanilla BN.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SkeHwZ-q2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Solid paper proposing a generalisation of Batch Normalisation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyN-M2Rctm&amp;noteId=SkeHwZ-q2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1240 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1240 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a generalisation of Batch Normalisation (BN) under the assumption that the statistics of the unit activations over the batches and over the spatial dimensions (in case of convolutional networks) is not unimodal. The main idea is to represent the unit activation statistics as a mixture of modes and to re-parametrise by using mode specific means and variances. The "posterior" mixture weights for a specific unit are estimated by gating functions with additional affine parameters (followed by softmax). A second, similar variant applies to Group Normalisation, where the statistics is taken over channel groups and spatial dimensions (but not over batches). 

To demonstrate the approach experimentally, the authors first consider an "artificial" task by joining data from MNIST, Fashion MNIST, CIFAR10 and SVHN and training a classifier (LeNet) for the resulting 40 classes. The achieved error rate improvement is 26.9% -&gt; 23.1%, when comparing with standard BN. In a second experiment the authors apply their method to "single" classification tasks like CIFAR10, CIFAR100 and ILSVRC12 and use large networks as e.g. VGG13 and ResNet20. The achieved improvements when comparing with standard BN are one average 1% or smaller.

The paper is well written and technically correct.

Further comments and questions to the authors:

- The relevance of the assumption and the resulting normalisation approach would need further justification. The proposed experiments seem to indicate that the node statistics in the single task case are "less multi-modal" as compared to the multi-task. Otherwise we would expect the comparable improvements by mode normalisation in both cases? On the other hand, it should be easy to verify the assumption of multi-modality experimentally, by collecting node statistics in the learned network (or at some specific epoch during learning ). It should be also possible to give some quantitative measure for it.

- Please explain the parametrisation of the gating units more precisely (paragraph after formula (3)). Is the affine mapping X -&gt; R^k a general one? Assuming that X has dimension CxHxW, this would require a considerable amount of additional parameters and  thus increase the VC dimension of the network (even if its primary architecture is not changed). Would this require more training data then? I miss a discussion of this aspect.

- When comparing different numbers of modes (sec. 4.1, table 1), the size of the batch size was kept constant(?). The authors explain the reduction of effectiveness of higher mode numbers as a consequence of finite estimation (decreasing number of samples per mode). Would it not be reasonable to increase the batch size proportionally, such that the amount of samples per mode is kept constant?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hygq9xlc27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Might have already been published and pushes BN towards small mini-batches</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyN-M2Rctm&amp;noteId=Hygq9xlc27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1240 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1240 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
Batch Normalization (BN) suffers from 2 flaws: 1) It performs poorly when the batch size is small and 2) computing only one mean and one variance per feature might be a poor approximation for multi-modal features. To alleviate 2), this paper introduces Mode Normalization (MN) a new normalization technique based on BN. It uses a gating mechanism, similar to an attention mechanism, to project the examples in the mini-batch onto K different modes and then perform normalization on each of these modes.

Clarity:
The paper is clearly written, and the proposed normalization is well explained.

Novelty: 
The proposed normalization is somewhat novel. I also found a similar paper on arXiv (submitted for review to IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018): M. M. Kalayeh, M. Shah, Training Faster by Separating Modes of Variation in Batch-normalized Models, arXiv 2018. I didn’t took the time to read this paper in details, but the mixture normalization they propose seems quite close to MN. Could the authors comment on this?

Pros and Cons:
+ Clearly written and motivated
+ Try to address BN’s weakness, which is an important direction in deep learning
- I found similar papier in the literature
- The proposed method aims to make BN perform better, but pushes it toward small batch settings, which is where BN performs poorly.
- Misses comparisons with other techniques (see detailed comments).

Detailed Comments:
1. Multi-modality:
It is not clear if the features are multimodal when performing classification tasks. Some histograms of a few features in the network would have help motivate the proposed normalization. However, it seems indeed to be an issue when training GANs: to make BN work when placed in the discriminator, the real and fake examples must be normalized separately, otherwise the network doesn't train properly. Moreover, when dealing with multimodal datasets (such as the one you created by aggregating different datasets), one can use the FiLM framework (V. Dumoulin et al., Feature-wise transformations, Distill 2018), and compute different means and variances for each datasets. How would the proposed method perform against such method?
2. Larger scale:
It would be nice to see how MN performs on bigger networks (such as the ResNet50, or a DenseNet), and maybe a more interesting fully-connected benchmark, such as the deep autoencoder.
3. Small batch regime:
It seems that the proposed method essentially pushes BN towards a regime of smaller mini-batch size, where it is known to performs poorly. For instance, the gain in performances on the ImageNet experiments drops quite a lot already, since the training is divided on several GPUs (and thus the effective mini-batch is already reduced quite a lot). This effect gets worse as the size of the network increases, since the effective mini-batch size gets smaller. This problem also appears when working on big segmentation tasks or videos: the mini-batch size is typically very small for those problems. So I fear that MN will scale poorly on bigger setups. I also think that this is the reason why you need to use extremely small K.
4. Validation set:
What validation sets are you using in your experiments? In section 4.1, the different dataset and their train / test splits are presented, but what about validation?

Conclusion:
Given the similarity with another paper already in the literature, I reject the paper. Also, it seems to me that the technique actually pushed BN towards a small batch regime, where it is known to perform poorly. Finally, it misses comparison with other techniques.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJgbLwQjam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: Might have already been published and pushes BN towards small mini-batches</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyN-M2Rctm&amp;noteId=BJgbLwQjam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1240 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1240 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Three main concerns were raised: (a.) a similar publication exists, giving grounds for a clear rejection of this paper. We thank the reviewer for bringing the interesting paper by Kalayeh &amp; Shah to our attention, but show below that this claim is unjustified. (b.) MN suffers from weaknesses that BN also suffers from in the small batch size regime, and (c.) the paper should discuss some additional related methods.

Regarding (a.): we are thankful for having this paper pointed out to us and will include it in our revision. That being said, we strongly rebut the claim that their paper is equivalent to ours, as their approach is very different. After reading their preprint in detail, we summarize below.

The crucial difference is that in MN we employ a Mixture of Experts (MoE) approach and parametrize each expert with a simple attention-like mechanism on the image’s features. MN can effortlessly be added to any modern deep convolutional network, can be optimized with standard SGD, has a very small computational overhead, and introduces only a single hyperparameter (number of modes K). On the other hand, Kalayeh &amp; Shah propose using a GMM to fit the feature distribution within the normalization unit (from hereon, we thus abbreviate MN-GMM). As it happens, we experimented with a GMM-based approach before designing MN, so we are well familiar with the several technical difficulties and impracticalities that using GMMs imposes:

*  Due to the complexity of fitting GMMs, in their experiments Kalayeh &amp; Shah never swap out all BN layers with MN-GMM layers, see p. 7 (right). So their resulting network is a mixture of BN and (very few, usually 1) MN-GMM normalizations. We designed MN to be lightweight and easy to deploy, and in our experiments show that MN can replace the entirety of BN layers, even in a deep network.
* As Kalayeh &amp; Shah explain on p. 6 (right column) they fit the GMM via EM, in a completely separate optimization step, outside the training loop of the network. In designing our method, it was important to us to sidestep this restriction, and MN can be trained end-to-end alongside the other parameters of the network.
* Further complicating MN-GMM is that it requires careful, manual decisions in its tuning. From our own experiments, we are well aware of the considerations one needs to ponder over in MN-GMM. A few examples: (i.) how many EM iterations are needed? (ii.) Which BN units should be replaced, which should remain intact? (iii.) How should the GMM parameters be initialized? (iv.) How many components should be assumed? In MN, the practitioner needs to make a single choice (in that K needs to be set). Once that choice has been made, MN can be used off-the-shelf, making it straightforward to use in an applied setting.

In MN-GNN Kalayeh &amp; Shah (2018) propose an interesting modification to BN, however it should be clear from the above points that the similarities to our method are extremely limited. R2 states that “I didn’t took the time to read this paper in details”, only to continue “given the similarity with another paper already in the literature, I reject the paper”. We were very surprised by the rejection based on a “quick read”, and – for a top-tier conference like ICLR – would have found it appropriate to read the mentioned paper and to compare it to ours in a more careful manner. Once more, we firmly reject the implication that our proposed method has been covered in their publication, or that we, in any way, copied from their work.

(b.): splitting up batches does introduce errors from finite estimation, which is an issue that we raise ourselves on p. 6, third paragraph. As we argue in our paper, many applications exist where the batch size restriction isn’t a major issue, and a larger error results from the underlying modality of the task. MN is aimed at alleviating issues in these particular tasks, we never designed it to solve the small batch size issues of BN, and at no point claim that it does.

That being said, even though MN splits minibatches into multiple modes by construction (thereby collecting statistics from less samples than BN), in practice MN still performs better than BN, even for small batch sizes. This is shown in Table 2, where MN clearly is more robust to smaller batch sizes than BN.

(c.): FiLM learns to adaptively influence the output of a neural network by applying transformations to intermediate features conditioned on some input. FiLM’ed networks still use BN, and thus FiLM does not address any shortcomings of BN, so MN can simply be used alongside FiLM. There is a weak connection to our paper in that MN can also be seen as a conditional layer, however with the completely different focus of adapting feature normalizations. We thank the reviewer for pointing out this work, and have included it in our revision.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByeV2TieRX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to your rebuttal</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyN-M2Rctm&amp;noteId=ByeV2TieRX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1240 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1240 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">(a.) Frist, I do apologize for letting you think I was accusing you of plagiarism. This is a serious offense, and by no means I implied such a thing. While reviewing your paper and looking up the recent literature about Batch Normalization, I quickly came across the paper by Kalayeh &amp; Shah, and I was surprised you didn’t mentioned it in your paper. I simply thought you had been scooped. I also apologize for not having taken a closer look (which I did now) at this paper.

That said, I thank you for your detailed comment on the difference between both paper. As you mentioned, such comparison should figure in your literature review, since both methods are designed to provide multi-modality to BN. The key difference is indeed how it is implemented: They use an outside-of-the-loop GMM, while you use an attention mechanism. Your method is certainly easier to implement and use in modern deep learning frameworks than the GMM approach. A comparison with the GMM approach would still have been nice, or some histogram plots showing the means and variances of different modes.

(b.) My point was that MN suffers even more than BN from the small size regime (note that this could also be a positive effect, as it could introduce stronger regularization). In Table 2, we can see that BN drops 3% error rate when going from 16 to 4 examples per mini-batch, where MN drops 4%. Also, this experiment is heavily multimodal in the first place (and thus one can expect BN to perform poorly, and this is the reason why I proposed (c.) for a more fair comparison). The gap in performances between MN and BN on CIFAR and ImageNet gets smaller and smaller, as the effective mini-batch size get smaller.

Also by my comment that your paper "try to address BN’s weakness, which is an important direction in deep learning", I meant that your paper is going beyond uni-modal normalization, not that it is designed to solve the small size issue of BN.

(c.) Sorry if I didn’t expressed myself clearly enough here. I was suggesting to use the information from which dataset D (MNIST, CIFAR, ...) one example comes from, and normalize it using the examples in the mini-batch that also come from dataset D. You would then obtain different statistics for different datasets. This would help to see how well your method compares against explicit separated normalization.

(d.) I'm still interested to know if 1. you ran experiments on deeper networks (like the ResNet50) and 2. what is the validation sets you used through your experiments.

I hope I let you enough time to answer again if you want to, and I will certainly increase the score of my review now that the difference between the two papers has been clearly established.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_HJx9Mg3gh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>About the Gating Network and Algorithm 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyN-M2Rctm&amp;noteId=HJx9Mg3gh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Kun_Yuan1" class="profile-link">Kun Yuan</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">26 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1240 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1. Since features in different layers represent differently, is there necessary to add a gating network alongside each normalization module? And what is the structure of your gating network?
2. Can you provide more details about Algorithm 1? Especially $y_{nk}$ and $x_n-\mu_k$，since different shape between (n,c,h,w) and (k,c) can not do subtraction directly.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyeC8uLEnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: About the Gating Network and Algorithm 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyN-M2Rctm&amp;noteId=SyeC8uLEnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1240 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1240 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, thanks for your interest and your questions. We parametrize the gating functions with an affine transformation followed by a softmax, see second paragraph on p. 5. Using an alternative in any subset of layers is certainly possible, this would need to be decided on a case-by-case basis though, as it depends on e.g. choice of architecture, or the task at hand.

Regarding your second question, we apply the normalization to the full image, while estimators are computed after pooling over height and width, so we follow the exact same protocol as in batch norm.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1gx9Ady5X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Details of experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyN-M2Rctm&amp;noteId=B1gx9Ady5X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1240 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">From table 1, it looks that increasing the number of K in MN also increases error rate. What value of K shall we use in practice?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJeTPa9lqX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: Details of experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyN-M2Rctm&amp;noteId=SJeTPa9lqX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1240 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1240 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Many thanks for your interest in our paper and your comment. Indeed, increasing the number of modes does not always increase performance, see also our third paragraph on p. 6.

Intuitively, one would expect larger choices of K to always improve performance (at the expense of some computational cost). The fact that this isn’t the case connects to the same issue that also makes BN vulnerable to small batch sizes: for fixed N, increasing K results in less and less samples being assigned to a joint mode. Estimators are then computed from smaller partitions, in turn making them less accurate. Besides this, a second dynamic arguably comes into play in the hierarchicality of deep architectures. If the original network has L normalizations, then – compared to BN – we introduce L(K-1) additional normalizations in MN. So even in its simplest configuration, MN comes with L additional normalizations, which could be more than the network needs to account for the relevant modes in the distribution.

In practice choosing K=2 gave us a significant performance boost in all our experiments (and therefore we recommend this value), going beyond that only resulted in benefits if the batch size was chosen to be sufficiently large, see the Appendix.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryxCU3AxcQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: Details of experiments </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyN-M2Rctm&amp;noteId=ryxCU3AxcQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1240 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for reply. I still have a question. Are the examples normalized by the same mode in MN from the same category?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1eOYkPQ9m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: Re: Details of experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyN-M2Rctm&amp;noteId=H1eOYkPQ9m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1240 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1240 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your continued interest. MN does not use any explicit label information, and (given the complexity of the datasets that we study here) is unable to uncover the underlying cluster structure, see penultimate paragraph on p. 5. Nonetheless, in our experiments we observe that MN does allocate samples into joint modes that have similar qualities, such as color or object size, c.f. Fig 2.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>