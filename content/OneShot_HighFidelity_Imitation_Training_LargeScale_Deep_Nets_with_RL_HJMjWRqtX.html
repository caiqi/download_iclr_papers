<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HJMjW3RqtX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets..." />
      <meta name="og:description" content="Humans are experts at high-fidelity imitation -- closely mimicking a demonstration, often in one attempt. Humans use this ability to quickly solve a  task instance, and to bootstrap learning of new..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HJMjW3RqtX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL</a> <a class="note_content_pdf" href="/pdf?id=HJMjW3RqtX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019one-shot,    &#10;title={One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HJMjW3RqtX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Humans are experts at high-fidelity imitation -- closely mimicking a demonstration, often in one attempt. Humans use this ability to quickly solve a  task instance, and to bootstrap learning of new tasks. Achieving these abilities in autonomous agents is an open problem. In this paper, we introduce an off-policy RL algorithm (MetaMimic) to narrow this gap. MetaMimic can learn both (i) policies for high-fidelity one-shot imitation of diverse novel skills, and (ii) policies that enable the agent to solve tasks more efficiently than the demonstrators. MetaMimic relies on the principle of storing all experiences in a memory and replaying these to learn massive deep neural network policies by off-policy RL. This paper introduces, to the best of our knowledge, the largest existing neural networks for deep RL and shows that larger networks with normalization are needed to achieve one-shot high-fidelity imitation on a challenging manipulation task.
The results also show that both types of policy can be learned from vision, in spite of the task rewards being sparse, and without access to demonstrator actions. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Imitation Learning, Deep Learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We present MetaMimic, an algorithm that takes as input a demonstration dataset and outputs (i) a one-shot high-fidelity imitation policy (ii) an unconditional task policy.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BylDJkZ_Tm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea to extend DDPGfD to use only state trajectories, but needs a further experimental validation. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJMjW3RqtX&amp;noteId=BylDJkZ_Tm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1205 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1205 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">**Summary**

The paper looks at the problem of one-shot imitation with high accuracy of imitation. The main contributions: 
1. learning technique for high fidelity one-shot imitation at test time. 
2. Policies to improve the expert performance through RL.  

The main improvements of this method is that demo action and rewards are not needed only state trajectories are sufficient. 


** Comments **
- The novelty of algorithm block
The main method is very similar to D4PG-fd. The off-policy method samples from a replay buffer which comprises of both the demos and the agent experience from the previous learner iterates. 

1. From a technical perspective, what is the advantage of training an imitation learner from a memory buffer of the total experience? 
If the task reward is not accessed, then when the imitation learner is training, then the data should not be used for training the task policy learner. On the other hand if task reward is indeed available then what is the advantage of not using it. 

2. A comparison with a BC policy to generate more experience data for the task policy agent/learning might also be useful. 

* Improved Comparisons
- Compare with One-Shot Performance
Since this is one of the main contributions, explicit comparison with other one-shot imitation papers needs to be quantified with a clearly defined metric for generalization. 

This comparison should be both for short-term tasks such as block pick and place (Finn et al, Pathak et al, Sermanet et al.) and also for long-term tasks as shown in (Duan et al. 2017 and also in Neural Task Programming/Neural Task Graph line of work from 2018)

- Compare High-Fidelity Performance
It is used as a differentiator of this method but without experimental evidence.
The results showing imitation reward are insufficient. The metric should be independent of the method. An evaluation might compare trajectory tracking error: for objects, end-effector, and joint positions. This is available as privileged information since the setup is in a simulation.

Furthermore, a comparison with a model-based trajectory tracking with a learned or fitted model of dynamics is also very useful.

- Compare Policy Learning Performance
In addition to D4PG variants, performance comparison with GAIL will ascertain that unconditional imitation is better than SoTA. 


* Tracking a reference (from either sim or demos) is a good idea that has been explored in sim2real literature[2,3] and imitation learning [4]. It is not by itself novel. The authors fail to acknowledge any work in this line as well as provide insight why is this good and when is this valid. For instance, with highly stochastic dynamics this may not work!


- "Diverse Novel Skills" 
The experiments are limited to a rather singular pick and place task with a 3-step structured reward model. It is unfair to characterize this domain as very diverse or complex from a robotics perspective. More experiments on continuous control would help.

- Bigger networks
"In fig. 3 we demonstrate that indeed a large ResNet34-style network (He et al., 2016) clearly outperforms" -- but Fig 3 is a network architecture diagram. It is probably fig 6!

- The authors are commended for presenting a broad overview of imitation based methods in table 2

** Questions **

1.  How different if the imitation learner (trained with imitation reward) from a Behaviour Cloning Policy. 

2. How is the local context considered in action generation in sec 2.1. 
The authors reset the simulation environment to o_1 = d_1. 
Then actions are generated with  \pi_{theta} (o_t, d_{t+1}). 
a. Is the environment reset every time step?
b. If not how is the deviation of the trajectory handled over time? 
c. how is the time horizon for this open loop roll out chosen. 

3. How is this different for a using a tracking based MPC with the same horizon? The cost can be set the same the similarity metric between states. 

4. The architecture uses a deep but simplistic model. When the major attribution of the model success is to state similarity -- especially image similarity -- why did the authors not use image comparators something like the Siamese model?

Suggestion:
The whole set of experiments are in a simulation. 
The authors go above and beyond in using Mitsuba for rendering images. But the images used are Mujoco rendered default. It would nice if the authors were more forthcoming about this. All image captions should clearly state -- Simulated robot results, show images used for agent training. The Mitsuba renders are only used for images but nowhere in the algorithm. So why do this at all, and if it has to be used please do it with a disclaimer. Right now this detail is rather buried in the text. 

References:
1. Neural Task Programming, Xu et al. 2018 (<a href="https://arxiv.org/abs/1710.01813)" target="_blank" rel="nofollow">https://arxiv.org/abs/1710.01813)</a>
2. Preparing for the Unknown: Learning a Universal Policy with Online System Identification (https://arxiv.org/abs/1702.02453)
3. Adapt: zero-shot adaptive policy transfer for stochastic dynamical systems (https://arxiv.org/abs/1707.04674)
4. A survey of robot learning from demonstration, Argall et al. 2009
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1xY8ktPT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Insufficient evidence/experimental validation for the main claims of the paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJMjW3RqtX&amp;noteId=H1xY8ktPT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1205 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1205 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary

This work porposes a approach for one-shot imitation with high accuracy, called "high fidelity imitation learning" by the authors. Furthermore, the work addresses the common problem of exploration in imitation learning, which would help to rescue from off-policy states.

Review

In my opinion, the main claims of this paper are not validated sufficiently in the experiments. I would expect the experiments to be designed specifically to support the claims made, but little evidence is provided:

- The authors claim that the method allows one-shot generalization to an unknown trajectory. To test this hypothesis the authors only provide experiments of generalization towards trajectories of a different demonstrator on the same task of stacking cubes. I would expect experiments with truly different trajectories on a different task than stacking cubes to test the hypothesis of one-shot imitation.
Until then I see no evidence for a "one-shot" imitation capability of the proposed method.

- That storing the trajectories of early training can act as replacement for exploration as rescue from off-policy states: This is never experimentally validated. This hypothesis could easiliy be validated with an ablation study, were the results of early would not be added to the replay buffer.

- High fidelity imitation: In the caption of Figure 7 the authors note that the unconditional task policy is able to outperform the demonstration videos. Thus the trajectories of the unconditional task policy allow a higher reward then the demonstrations.
Could the authors please comment on how the method still achieves high fidelity imitation even when the results of the unconditional task policy are added to the replay buffer? In prinicipal these trajectories allow a higher reward than the demonstration trajectories that should be imitated.

Mainly due to the missing experimental validation of the claims made I recommend to reject the paper.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SyeAUmWHTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>learning from video demonstration; exposition is confusing / misleading.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJMjW3RqtX&amp;noteId=SyeAUmWHTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1205 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1205 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents an RL method for learning from video demonstration without access to expert actions. The agent first learn to imitate the expert demonstration (observed image sequence and proprioceptive information) by producing a sequence of actions that will lead to the similar observations (require a renderer that takes actions and outputs images). The imitation loss is a similarity metric. Next, the agent explores the environment with both the imitation policy and task policy being learned; an off-policy RL algorithm D4PG is used for policy learning. Experiments are conducted on a simulated robot block stacking task.

The paper is really clearly written, but presenting the approach as "high-fidelity", "one-shot" learning is a bit confusing. First, it's not clear what's the motivation for high-fidelity. To me this is an artifact due to having to imitate the visual observation instead of the actions, which is a legitimate constraint, but not the original goal. Second, the one-shot learning setting consists of training on a set of stochastic demonstrations and testing on another set collected from a different person; both for the same task. Usually one-shot learning tests on slightly different tasks or environments, whereas here the goal is to generalize to novel demonstrations. It's not clear why do we care imitation per se in addition to the task reward.

What I find interesting is the proposed approach for learning for video demonstration without action labels. Currently this requires an executor to render the actions to images, what if we don't have such an executor or only have a noisy / approximate renderer? In the real world it's probably hard to find a good renderer, it would be interesting to see how this constraint can be relaxed.

Questions:
- While the authors have shown the average rewards of the two sets are different, I wonder what's the variance of each person's demonstration. 
- In Fig 5, on the validation set, in terms of imitation loss there aren't that much difference between the policies, but in terms of task reward, the 'red' policy goes to zero while others policies' rewards are still similar. Any intuition for why seemingly okay imitation doesn't translate to task reward?

Overall, I enjoyed reading the paper and the experiments are comprehensive. The current presentation angle seems a bit off though.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1lU9VybTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Well presented, but not suitable for ICLR</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJMjW3RqtX&amp;noteId=r1lU9VybTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1205 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1205 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
Summary:
This paper proposes MetaMimic, an algorithm that does the following:
(i) Learn to imitate with high-fidelity with one-shot. The setting is that we have access to several demonstrations (only states, no actions) of the same task. During training, we have pixel observations plus proprioceptive measurements). At test time, the learned policy can imitate a single new demonstration (consisting of only pixel observations) of the same task.
(ii) When given access to rewards, the policy can exceed the human demonstrator by augmenting its experience replay buffer with the experience gained while learning (i). Therefore, even in a setting with sparse rewards and no access to expert actions (only states), the policy can learn to solve the task.

Overall Evaluation:
This is a good paper. In my opinion however, it does not pass the bar for ICLR.

Pros:
- The paper is well written. The contributions are clearly listed, the methods section is easy to follow and the authors explain the choices they make. The illustrations are clear and intuitive.
- The overview of hyperparameter choice and tuning / importance factor in the Appendix is useful.
- Interesting pipeline of learning policies that can use demonstrations without actions.
- The results on the simulated robot arm (block stacking task with two blocks) are good.

Cons:
- The abstracts oversells the contribution a bit when saying that MetaMimic can learn "policies for high-fidelity one-shot imitation of diverse novel skills". The setting that's considered in the paper is that of a single task, but different demonstrations (different humans from different starting points). This seems restrictive, and could have been motivated better.
- Experimental results are shown only for one task; block stacking with a robot arm in simulation.
- Might not be a good topical fit for ICLR, but more suited for a conference like CoRL or a workshop. The paper is very specific to imitation learning for a manipulation / control tasks, where we can (1) reset the environment to the exact starting position of the demonstrations, (2) the eucledian distance between states in the demonstration and visited by the policy is meaningful (3) we have access to both pixel observations and proprioceptive measurements. The proposed method is an elegant way to solve this, but it's unclear how well it would perform on different types of control problems, or when we want to transfer policies between different (but related) tasks.

Questions:
- Where does the "task stochasticity" come from? Only from the starting state, and from having different demonstrations? Or is the transition function also stochastic?
- The learned policy is able to do one-shot imitation, i.e., given a new demonstration (of the same task) the policy can follow this demonstration. Do I understand correct that this mean that there is *no* additional learning required at test time?
- It is not immediately clear to me why the setting of a single task but new demonstrations is interesting. Could the authors comment on this? One setting I could imagine is that the policy is trained in simulation, but then executed in the real-world, given a new demonstration. (If that's the main motivation though, then the experiments might have to support that this is possible - if no real-world robot is available, maybe the same simulator with a slightly different camera angle / light conditons or so.)
- The x-axis in the figures says "time (hours)" - is that computation time or simulated time?

Other Comments:
- In 3.2, I would be interested in seeing the following baseline comparison: Learn the test task from scratch using the one available demonstration, with the RL procedure (Equation 2, but possibly without the second term to make it fair). In Figure 5, we can see that the performance on the training tasks is much better when training on only 10 tasks, compared to 500. Then why not overfit to a single task, if that's what we're interested in? 
- An interesting baseline for 3.3 might be an RL algorithm with shaped rewards: using an additional reward term that is the eucledian distance to the *closest* datapoint from the demonstration. Compared to the baselines shown in the results section, this would be a fairer comparison because (1) unlike D4PG we also have access to information from the demonstrations and (2) no additional information is needed like the action information in D4PGfD and (3) we don't have the need for a curriculum.

Nitpick (no influence on score):
[1. Introduction]
- I find the first sentence, "One-shot imitation is a powerful way to show agents how to solve a task" a bit confusing. I'd say one-shot imitation is a method, not a way to show how to solve a task. Maybe an introductory sentence like "Expert demonstrations are a powerful way to show agents how to solve a task." works better?
- Second sentence, the chosen example is "manufacturing" tasks - do you mean manipulation? When reading this, I had to think of car manufacturing - a task I could certainly not imitate with just a few demonstrations.
- Add note that with "unconditional policy" you mean not conditioned on a demonstration.
[2. MetaMimic]
- [2.1] Third paragraph: write "Figure 2, Algorithm 1" or split the algorithm and figure up so you can refer to them separately.
- [2.1] Last paragraph, second line: remove second "to"</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>