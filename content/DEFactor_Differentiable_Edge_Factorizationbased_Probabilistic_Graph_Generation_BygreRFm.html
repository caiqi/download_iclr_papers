<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>DEFactor: Differentiable Edge Factorization-based Probabilistic Graph Generation | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="DEFactor: Differentiable Edge Factorization-based Probabilistic Graph Generation" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Bygre3R9Fm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="DEFactor: Differentiable Edge Factorization-based Probabilistic..." />
      <meta name="og:description" content="Generating novel molecules with optimal properties is a crucial step in many industries such as drug discovery. Recently, deep generative models have shown a promising way of performing de-novo..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Bygre3R9Fm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>DEFactor: Differentiable Edge Factorization-based Probabilistic Graph Generation</a> <a class="note_content_pdf" href="/pdf?id=Bygre3R9Fm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019defactor:,    &#10;title={DEFactor: Differentiable Edge Factorization-based Probabilistic Graph Generation},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Bygre3R9Fm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=Bygre3R9Fm" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Generating novel molecules with optimal properties is a crucial step in many industries such as drug discovery. Recently, deep generative models have shown a promising way of performing de-novo molecular design.  Although graph generative models are currently available they are either computationally expensive, limiting their use to only small graphs or are formulated as a sequence of discrete actions needed to construct a graph, making the output graph non-differentiable w.r.t the model parameters, therefore preventing them to be used in scenarios such as conditional graph generation. In this work we propose a model for conditional graph generation that is computationally cheap, scalable, directly optimises properties of the graph, and generates a probabilistic graph, making the process differentiable, thus enabling end-to-end training with stochastic gradient descent. We demonstrate favourable performance of our model on prototype-based molecular graph conditional generation tasks.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">molecular graphs, conditional autoencoder, graph autoencoder</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">New scalable graph decoding scheme that allows to perform direct molecular graph conditional generation.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_B1glS5H76X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The paper is very poor--- not ready for publication</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bygre3R9Fm&amp;noteId=B1glS5H76X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1072 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1072 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a conditional graph generation that directly optimizes the properties of the graph. The paper is very weak.
1. I think almost all probabilistic graph generative models are differentiable. If the  objective is differentiable function of real   
    variables, it is usually differentiable.

2.  The authors claim that existing works Simonovsky and Komodakis (2018) and Cao &amp; Kipf (2018) are restricted to use small graphs with predefined maximum size. This work does not overcome the limitation of small graphs issue too.

3. The authors do not show any measure on validity, novelty or uniqueness which are now standard in literature.
   Also I do not find any comparison with molGAN paper which tackles a similar objective.

4. Could the authors show if the decoding process is permutation invariant? I am not really sure of that. I was trying to prove that thing formally, but I failed.


 </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BklYpixApm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Answer to Reviewer 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bygre3R9Fm&amp;noteId=BklYpixApm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1072 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1072 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for his comments and will answer to these one by one. 

1.
We also state in the paper that probabilistic graph generative models are differentiable. In the abstract “ In  this  work  we  propose  a  model  for  conditional  graph  generation  that directly  optimises  properties  of  the  graph,  and  generates  a  probabilistic graph,  making  the  decoding  process  differentiable”


2.  
They are limited to very small graph because of their parametriazations : the number of parameters depends on the predefined maximum graph size they have set. If their last hidden layer is of size d, the number of edges r, and the maximum graph size of size n then the weight matrix mapping the last layer to the edge tensor  will be of size n*n*r*d which is very limiting. 

Our factorization model does not have this limitation (the number of parameters only depends on the size of the embeddings we choose), and keeping in  memory of the full probabilistic graph is not an issue when working with molecules -&gt; we are talking here of graphs with a maximum number of heavy atoms around 100. So we overcome this limitation by having a model whose number of parameters does not depend on the maximum size of the graph. We will change the manuscript to be more precise regarding that point.


3. 
Those measures are standard for purely generative models (where the task is to generate molecules without other objective, and the molecules are sampled form the prior). Let us cite JT-VAE’s description of the reconstruction task to that extent :  “ 	We test the VAE models on the task of reconstructing input molecules from their latent representations, and decoding valid molecules when sampling from prior distribution. “

Our model is a conditional autoencoder, which is a new setting (we do not put any prior on the latent code). 

MolGAN does not tackle the constrained optimization scenario at all and its formulation is not easily transferable to that setting : MolGAN is an implicit generative model. One way to constrain the generation process could be to add a reward signal computing the similarity between the generator output and the query molecule (the prototype) but : 
This would mean retraining/fine-tuning the model for each query molecule
The constrained scenario would be explicit : which is not the case of other models (JT-VAE, GCPN and ours) in which the similarity constraint is not directly specified. JT-VAE finetunes the encoded representation, GCPN uses the prototype as a starting point, and we used a conditional formulation without retraining needed so such a comparison would be difficult and unfair anyway.

We will add in the appendix a comparative table of previous models to that extent and to justify the comparison effectively made in our manuscript. To the best of our knowledge, only JT-VAE and GCPNN are comparable models in the implicitly constrained optimization scenario.

4. 
We never claimed that the decoding process  was permutation invariant. We only made sure that we can make the encoder robust to permutations by training it on different permutations of the embeddings it encodes. However the decoder is trained to match the domain canonical order (heavy atoms are ordered as they appear in their SMILES canonical representation).

We thank the reviewer again and  hope our comments shed a clearer light on our manuscript.

The authors
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SklCA18chQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>review on "DEFactor: Differentiable Edge Factorization-based Probabilistic Graph Generation"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bygre3R9Fm&amp;noteId=SklCA18chQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1072 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1072 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposed a variant of the graph variational autoencoder [1] to do generative modeling of graphs. The author introduced an additional conditional variable (e.g., property value) into the decoder. By backpropagating through the discriminator, the model is able to find the graph with desired property value. 

Overall the paper reads well and is easy to follow. The conditional generation of graphs seems also helpful regarding the empirical performance. However, there are several concerns regarding the paper:

1) The edge factorization-based modeling is not new. In fact [1] already uses the node embeddings to factorize the adjacency matrix. This paper models extra information including node tags and edge types, but these are not fundamental differences compared to [1].

2) The paper claims the method is ‘cheaper’ and ‘scalable’. Since essentially the computation cost is similar to [1] which requires at least O(n^2) to generate a graph with n nodes, I’m not super confident about the author’s claim. Though this can be parallelized, but the memory cost is still in this order of magnitude, which might be too much for a sparse graph. Also there’s no large graph generative modeling experiments available.

3) Continue with 2), the adjacency matrix of a large graph (e.g., graph with more than 1k nodes) doesn’t have to be low rank. So modeling with factorization (with typically ~256 embedding size) may not be suitable in this case. 

Some minor comments:
4) Regarding Eq (2), why the lstm is used, instead of some simple order invariant aggregation?

5) the paper needs more refinement. E.g., in the middle of page 2 there is a missing citation. 

[1]  Kipf &amp; Welling, Variational Graph Auto-Encoders, <a href="https://arxiv.org/pdf/1611.07308.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1611.07308.pdf</a>
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HylN_T0kaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We think the novelty of the model has been misunderstood. We clarify that point</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bygre3R9Fm&amp;noteId=HylN_T0kaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1072 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1072 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the detailed and useful comments. We will proceed to the rebuttal as follows : 

    - Specific answers/clarifications on issues raised by the reviewer sequentially
    - Summary of clarifications made in the answer 
    - Summary of changes in the manuscript implied by the review 

 ----- SEQUENTIAL: Clarifications/Answers -----

Our Answer  on 1) Novelty of the model  :  
As referenced in  step 4 in section 3.1, we utilise the edge-factorization described in [1] (VGAE).  The goal here is to generate graphs of varying size given some input condition.  In practice this means being to generate both the nodes and edges of the graph, conditional on some latent  code.

In contrast the VGAE has been designed in the context of relational inference (eg. link prediction in citation network), where the number of nodes is fixed, and task is to learn a suitable representation for the nodes, such that we’re able to reconstruct/predict missing links.

 Because of the assumptions of this setting the VGAE only solves half of the problem we are trying to address: given a set of node embeddings [1] reconstructs the adjacency tensor. In contrast we want to be able to generate both the node embeddings whose number is unknown a priori and their adjacency tensor given some latent code.

In practice this is achieved by adding a new component (see step 3. Sec 3.1) to model how to go from a latent code z to an actual set of node embeddings.That specific node embeddings generator (that we parametrize with an LSTM)  is  the major contribution of our model. 

To clarify the differences between [1] and our DEFactor  we added a short paragraph in the related work section on edge-factorization.


Our Answer  on 2) and 3)  Clarification on "large" "cheap" and  "scalable": 
On the use of “scalable”  and “cheap” In the manuscript we state that scalable ``[..] means that the number of parameters of the decoder should not depend on a fixed pre-defined maximum graph size.`` and the number of parameters in DEFactor is independent of the (max) size of the graph unlike  [2] and [3]. We fixed the misleading use in the manuscript

On the large graph modeling concern : the model’s focus is on molecular graphs (which we think is an important problem on its own) thus “large” and “small” do not have the same signification here when compared to general graphs/ networks (that is why we put large in italic style in the first version of the manuscript in the bullet points of the related work section  but we updated it into “large molecular graphs”) . Small = less than 10 heavy atoms (like in [2] and [3], they specify small in their title) Large = around 60 heavy atoms which is large enough in the optimization tasks we are interested in the drug discovery pipeline.  



                   4) Minor comments , Reviewer :  “Regarding Eq (2), why the lstm is used, instead of some simple order invariant aggregation?the paper needs more refinement. E.g., in the middle of page 2 there is a missing citation.”

We actually tried the simpler order invariant aggregation function (avg and max)  but the convergence was bad so we directly went for a more complex/richer feature extractor such as LSTM.  We agree that concerns can be raised concerning the matter of the order when we use such sequential aggregation functions however (as specified in section 3.1 (step 1 and 2) ) we trained the LSTM with a randomly permuted order of the embeddings it has to encode and did not notice any change in the performance of the model. 

Broken link fixed, thanks :) 


-----  SUMMARY : What we think  we clarified ------

- The  novelty of the proposed decoder ( = autoregressive generation of nodes embeddings for graphs of varying size) which we think has been misunderstood by the reviewer.
- The misleading use of words “scalable” and “cheap” : we meant only meant that the number of parameters should not depend on the graph size (when compared to [2] and [3]).
- The meaning of “large” graphs in the context of molecular graphs (which we find is a relevant and important problem on its own).

---- ACTION POINTS : What we modified in the manuscript ----

- Corrected the misleading use of “scalable” and “cheap” in the manuscript
- Replace large graphs by large molecular graphs to specify the scale of graphs we are referring to 
- Added a paragraph in the related work section on the edge-factorization to further emphasize the true novelty of our decoder.
- Fixed the broken references

We hope that our answers clarified our contribution and thank the reviewer again, 

The Authors

--- REFERENCES ---

[1] Kipf &amp; Welling, Variational Graph Auto-Encoders , <a href="https://arxiv.org/pdf/1611.07308.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1611.07308.pdf</a>
[2] De Cao &amp; Kipf,  MolGAN : An implicit generative model for small molecular graphs, https://arxiv.org/abs/1805.11973
[3] Simonovsky &amp; Komodakis, GraphVAE : Towards Generation of Small Graphs Using Variational Autoencoders, https://arxiv.org/abs/1802.03480</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rylQT5-c27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Experiments and Writing Need Improvement</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bygre3R9Fm&amp;noteId=rylQT5-c27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1072 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1072 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this paper, authors propose a deep generative model and a variant for graph generation and conditional graph generation respectively. It exploits an encoder which is built based on GCN and GraphSAGE, a autoregressive LSTM decoder which generates the graph embedding, and a factorized edge based probabilistic model for generating edge and node type. For conditional generation, authors also propose a discriminating training scheme based on maximizing the mutual information. Experiments on ZINC dataset show that the proposed method is promising.

Strength:

1, The problem this paper tries to tackle is very challenging and of great significance. Especially, the conditional graph generation direction under the deep learning context is novel. 

2, The overall model is interesting although it is a bit complicated as it combines quite a few modules.

Weakness:

1, In the reconstruction experiment, comparisons with several recent competitive methods are missing. For example, the methods which have been already discussed in the related work, Li et al. (2018a), You et al. (2018a) and You et al. (2018b). Moreover, it is not explained whether the comparison setting is the same as Jin et al. (2018) and what the size of the latent code of their method is. It seems less convincing by just taking results from their paper and do the comparison.

2, Authors motive their work by saying in the abstract that “other graph generative models are either computationally expensive, limiting their use to only small graphs or are formulated as a sequence of discrete actions needed to construct a graph, making the output graph non-differentiable w.r.t the model parameters”. However, if I understood correctly, in Eq. (7), authors compute the soft adjacency tensor which is a dense tensor and of size #node by #node by #edge types. Therefore, I did not see why this method can scale to large graphs.

3, The overall model exploits a lot of design choices without doing any ablation study to justify. For example, how does the pre-trained discriminator affect the performance of the conditional graph generation? Why not fine-tune it along with the generator? The overall model has quite a few loss functions and associated weights of which the values are not explained at all.

4, Conditional generation part is not written clearly. Especially, the description of variational mutual information phase is so brief that I do not understand the motivation of designing such an objective function. What is the architecture of the discriminator?

5, How do authors get real attributes from the conditionally generated molecules? It is not explained in the paper.

Typos:

1, There are a few references missing (question mark) in the first and second paragraphs of section 2.

2, Methods in the experiment section are given without explicit reference, like GCPN.

3, Since edge type is introduced, I suggest authors explicitly mention the generated graphs are multi-graph in the beginning of model section. 

Overall, I do not think this paper is ready for publishing and it could be improved significantly.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkeqNivN5m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Table 1 reconstruction accuracy comparison is totally unfair</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bygre3R9Fm&amp;noteId=rkeqNivN5m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Oct 2018 (modified: 06 Oct 2018)</span><span class="item">ICLR 2019 Conference Paper1072 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear authors,

I believe the reconstruction accuracy comparison in Table 1 is totally unfair. First, all the baseline models (CVAE, GVAE, SD-VAE and JT-VAE) are variational autoencoders, and they computed the reconstruction accuracy by encoding the input molecule with stochastic noises. That is,  the latent encoding of x is sampled from the approximate posterior Q(z|x) (which is a Gaussian). It is a stochastic encoding rather than deterministic. 
However, the proposed model in this paper is an autoencoder, and the authors computed the reconstruction accuracy using the deterministic encoding of x. This is the main reason why the proposed model has better performance. In fact, all the baseline models followed Kusner et al.'s evaluation method -- sampling multiple z from Q(z|x) and average the reconstruction accuracy over all stochastic samples.

Second, does the proposed model decodes the stereochemistry (e.g. chirality)? If not, then the comparison is again not under the same scenario. I am asking this because I didn't see any stereochemistry presented in Figure 5 in the appendix. All the baseline models reconstruct both 2D and 3D information of an input molecule. It is important because there is no way to correctly reconstruct a molecule if its stereochemistry is not reconstructed correctly.

I think the authors should remove this problematic comparison, or recompute the accuracy of the proposed model so that all models are compared under the same setting. Table 1 is really misleading, especially to reviewers who are outside of this domain.

</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJeEJFpIs7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>On reconstruction accuracy table.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bygre3R9Fm&amp;noteId=HJeEJFpIs7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1072 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1072 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your interest in our article, and sorry for our delayed answer. 

--- On your  stereochemistry concern ---
The way we understood the graph-related prior work (ie. JT-VAE) is that it does NOT reconstruct the 3D structure. However they do evaluate on 3d structures (doing a post-ranking scheme). They actually reconstruct a 2D molecular graph, then list all the possible stereoisomers, rank them and take the most ‘likely’ by computing the cosine similarity score between the encoded molecule and the embedding of all the stereoisomers (see Appendix B  of the JT-VAE article). For that very reason ( ie. the all the stereoisomers are listable given a 2d structure) we believe that working on 2d structure is not only easier but also enough.


--- On your stochastic vs deterministic reconstruction concern ---
For the stochastic VAE vs. deterministic AE we totally agree with you and will add a note to specify this unbalance. However one might argue that it is rather that computing an exact reconstruction score may not be suited to evaluate a VAE model whereas it is a good indicator in our deterministic AE. However we did try to train the JT-VAE model in its AE version on 2d structures and we got a lower reconstruction score than the VAE version reported in the article, which we found weird and did not report it.


--- Aim of table 1 ---
All in all, the major aim of our table 1 is to give a sense of the representative power of our proposed decoder and the comparisons are just here as an indication as, again, the evaluation context is not the same. We will add a comment to clarify those discrepancies between our model and the prior ones.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJx7vcuDjX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>All methods (especially SMILES based methods) do reconstruct the stereochemistry</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bygre3R9Fm&amp;noteId=SJx7vcuDjX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1072 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi,

Thanks for your reply. Regarding stereochemistry:
1) SMILES based methods (e.g. SD-VAE) do reconstruct 3D structure (with one step). Therefore, Table 1 is a unfair comparison. As long as you compute the reconstruction accuracy based on "2D structure exact match", Table 1 cannot be right.
2) JT-VAE does multiple steps of generation. However, they still computed the reconstruction accuracy based on "exact match on 3D structures". 
3) Is stereochemistry that hard to reconstruct for your model? It's just additional edge labels right?

I totally agree that reconstruction accuracy is not a good metric for comparing VAEs. But regardlessly prior work did that comparison, and I am not happy with sloppy experiments. We should really be rigid in experiments and comparisons. After all, I am just trying to help you improve your manuscript. That's it.

Thanks again for your reply</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJl1HiKwsX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Will update the stereochemistry issue</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bygre3R9Fm&amp;noteId=SJl1HiKwsX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1072 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1072 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, 
Thanks for reply.

1 and 3)  Again we believe that adding the stereochemistry is an unnecessary burden but as it is simple to add to the model we will re run the reconstruction task taking into account the 3D structure (like you said it is just additional edge labels to add).
2) Our main concern with JT-VAE is that we tried to train in its deterministic AE and computed the 'exact match on the 2D structures' but found a lower score than on the 3d stochastic version, which is why we did not report it.

Thanks again for your comments and clarifications, we will update the manuscript regarding this point.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>