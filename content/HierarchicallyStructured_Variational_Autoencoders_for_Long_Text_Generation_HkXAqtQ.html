<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Hierarchically-Structured Variational Autoencoders for Long Text Generation | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Hierarchically-Structured Variational Autoencoders for Long Text Generation" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Hk41X2AqtQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Hierarchically-Structured Variational Autoencoders for Long Text..." />
      <meta name="og:description" content="Variational autoencoders (VAEs) have received much attention recently as an end-to-end architecture for text generation. Existing methods primarily focus on synthesizing relatively short sentences..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Hk41X2AqtQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Hierarchically-Structured Variational Autoencoders for Long Text Generation</a> <a class="note_content_pdf" href="/pdf?id=Hk41X2AqtQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 18 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019hierarchically-structured,    &#10;title={Hierarchically-Structured Variational Autoencoders for Long Text Generation},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Hk41X2AqtQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=Hk41X2AqtQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Variational autoencoders (VAEs) have received much attention recently as an end-to-end architecture for text generation. Existing methods primarily focus on synthesizing relatively short sentences (with less than twenty words). In this paper, we propose a novel framework, hierarchically-structured variational autoencoder (hier-VAE), for generating long and coherent units of text. To enhance the model’s plan-ahead ability, intermediate sentence representations are introduced into the generative networks to guide the word-level predictions. To alleviate the typical optimization challenges associated with textual VAEs, we further employ a hierarchy of stochastic layers between the encoder and decoder networks. Extensive experiments are conducted to evaluate the proposed method, where hier-VAE is shown to make effective use of the latent codes and achieve lower perplexity relative to language models. Moreover, the generated samples from hier-VAE also exhibit superior quality according to both automatic and human evaluations. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Natural Language Processing, Text Generation, Variational Autoencoders</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Propose a hierarchically-structured variational autoencoder for generating long and coherent units of text</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HJlvgTYkpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice experiments, but limited novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hk41X2AqtQ&amp;noteId=HJlvgTYkpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1322 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1322 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposed a hierarchical generative model for generating long text. This is an important problem and the authors performed extensive experiments.

My major concern is about the novelty of this paper.
Hierarchical LSTM for generating long txt has been widely studied.
So is hierarchical latent variables in VAE.

The notations of this paper is confusing, which hinders its readbility.
For example, in equation 5, the distribution is parameterized by theta.
In equation 6, p(x|z) is also parametrized by theta.

In the experiments, I'd like to see a comparison with hierarchical VAE.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1lZieB2n7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice samples but lack of comparison to existing hierarchical VAEs</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hk41X2AqtQ&amp;noteId=H1lZieB2n7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1322 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1322 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes using a hierarchical VAE to text generation to solve the two problems of long text generation and mode collapse where diversity in generated text is lost.

The paper does this by decoding the latent variable into sentence level latent codes that are then decoded into each sentence. The paper shows convincing results on perplexity, N-gram based and human qualitative evaluation.

The paper is well written though some parts are confusing. For example, equation 4 refers to q as the prior distribution but this seems like it's the posterior distribution as it is described just below equation 5. p(z_1|z_2) is also not well defined. It would be clearer to specify the full algorithm in the paper.

The work also mentions that words are generated for each sentence until the _END token is generated. Is this token always generated? What happens to a sentence if that token is not generated?

The novelty of this paper is questionable given the significant amount of existing work in hierarchical VAEs. It's also unclear why a more direct comparison can't be made with Serban et. al in terms of language generation quality and perplexity. If a downstream model is only able to make use of one latent variable, can't multiple variables simply be averaged?

It's also unclear how this work is novel with regards to the works below.

Hierarchical Variational Autoencoders for Music
Roberts, et. al
NIPS 2017 creativity workshop
This seems to have a similar hierarchical structure where there is an initial 16 step decoder that decodes the latent code for the lower level note level LSTMs to use during generation.

Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data
Hsu, et. al
NIPS 2017
This proposes a factorized hierarchical variational autoencoder which also has a double latent variable hierarchical structure, one that is conditional on the other.

Minor comments
- Typo in page 3 under Hierarchical structures in NLP: characters "from" a word
- Typo above section 4.3: hierarhical
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJxiPwaapm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for the valuable, critical feedback</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hk41X2AqtQ&amp;noteId=SJxiPwaapm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1322 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1322 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank the reviewer for the detailed comments and suggestions for the manuscript.

&gt; The paper is well written though some parts are confusing. For example, equation 4 refers to q as the prior distribution but this seems like it's the posterior distribution as it is described just below equation 5. p(z_1|z_2) is also not well defined. It would be clearer to specify the full algorithm in the paper. 

Thanks for pointing out this confusing typo. In the revised version, we have corrected ‘prior’ to ‘posterior’ while describing equation 4. As to the specific configuration of p(z_1|z_2), it is actually described in the supplementary materials (due to the space limit). To avoid any confusions here, we added a `Model specification’ section in the revised version, at the beginning of ``Experiments’’ part, to illustrate these details and hyperparameter choices.

&gt; The work also mentions that words are generated for each sentence until the _END token is generated. Is this token always generated? What happens to a sentence if that token is not generated?

The reviewer brings up a good point about the end_token generation. In our training data, all sentences have an _END token (at the end of each sentence), and the sentence-level LSTM is able to learn that there will be an _END token after each sentence and generates that token.  We should also note that, we constrained the model to generate a maximum number of words for each sentence. We use maximumly 20 words for the Yelp reviews dataset and 25 for the arXiv dataset. We have made this part more clear in the revised version. 

&gt; It's also unclear how this work is novel with regards to the works below.

We agree with the reviewer that there are similarities between our model and the VHRED model proposed by Serban et. al., however our model is quite different in terms of the architecture and motivation. Firstly, the VHRED model infers a latent variable for each context/utterance, and the context is provided to the decoder while generating each response. In contrast, our model leverages a global latent variable to model the entire paragraph without any additional inputs (no contexts are provided). As a result, our model is designed to potentially abstract globally meaningful features from the entire paragraph. 

Secondly, we have leveraged a hierarchy of latent variables to mitigate the ‘posterior collapse’ issue, which has given rise to promising empirical results. Specifically, the generative network has made better use of the latent variable information, indicated by a larger KL term (please refer to our response to Reviewer 2 for additional information). This strategy has not been employed and discussed in the ‘Hierarchical Variational Autoencoders for Music’ paper either.

To further prove that our model can extract globally informative features, we conducted an additional experiment, in the revised paper, to visualize the learned latent variable. Specifically, from the arXiv dataset, we select the most frequent four classes/topics and re-train our hier-VAE-D model on the corresponding abstracts. We plot the bottom-level latent variable for our hier-VAE-D with t-SNE (which is supposed to contain the global features). The result is shown in Figure 2 of the revised manuscript. It can be observed that the latent codes of paragraphs from the same topic are indeed grouped together in the embedding space, indicating that the latent variables has encoded high-level features from the input paragraph. 

As to the factorized hierarchical VAE paper, although they utilize the hierarchical nature of sequential data, their VAE model only takes a sub-sequence (segment) of the entire sequence as the input (as illustrated in Figure 3 of the factorized hierarchical VAE paper). Therefore, their model cannot be used to generate/sample an entire sequence in a hierarchical manner (their main motivation is to learn disentangled representations).

&gt; Minor comments

Thanks for pointing the typos out. We have corrected them accordingly in the revised paper.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HyehKP0uhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hk41X2AqtQ&amp;noteId=HyehKP0uhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1322 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1322 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a hierarchical variational autoencoder for modeling paragraphs. The model creates two levels of latent variables; one level of sentence-level latent variables and another single global latent. This avoids posterior collapse issues and the authors show convincing results on a few different applications to two datasets.

Overall, it is an impressive result to be able to convincingly model paragraphs with a useful global latent variable. Apart from some issues with confusing/incomplete notation (see below), my main criticism is that the authors fail to compare their approach to "A Hierarchical Latent Structure for Variational Conversation Modeling" by Park et al. As far as I can tell, the approaches are extremely similar, except that Park et al. may not learn the prior parameters and also use a hierarchical RNN encoder rather than a CNN (which may be irrelevant). They also are primarily interested in dialog generation, so the lower-level of their hierarchy models utterances in a conversation rather than sentences in general, but I don't see this as a major difference. I'd encourage the authors to compare to this and potentially use it as a baseline. More generally, it would have been nice to see more ablation experiments (e.g. convolutional vs. LSTM encoder). Finally, I know that space is tight, but other papers on global-latent-variable models tend to include more demonstrations that teh global variable is capturing meaningful information, e.g. with attribute vector arithmetic. The authors could include results of manipulating review sentiment via attribute vector arithmetic, for example.


Specific comments:

- "The Kullback-Leibler (KL) divergence term ... which can be written in closed-form (Kingma &amp; Welling, 2013), encourages the approximate posterior distribution qφ(z|x) to be close to the multivariate Gaussian prior p(z)." The prior is not always taken to be a multivariate Gaussian. You should add a sentence stating that the VAE prior is often taken to be a diagonal-covariance Gaussian for convenience.
- 3.2 has a few things which are unclear. In the second paragraph, you define z as the sampled latent code which is fed through an MLP "to obtain the starting state of the sentence-level LSTM decoder". But then LSTM^{sent} appears to be fed z at every timestep. LSTM^{sent} is also not defined - am I to assume that its arguments are the previous state and current input, so that z is the input at every timestep? Also, you write "where h^s_0 is a vector of zeros" which makes it sound like the starting state of the sentence-level LSTM decoder is a vector of zeros, not the output of the MLP which takes z as input. In contrast, LSTM^{word} takes three arguments as input. Which are the "state" and which are the "input" to the LSTM?
- I don't see any description of your CNN encoder (only the LSTM decoder in section 3.2, 3.3 only covers the hierarchy of latent variables, not the CNN architecture). What is its structure? Figure 1 shows a CNN encoder generating lower-level sentence embeddings and a high-level global embedding. How are those computed? It is briefly mentioned in 4.1 under "Datasets" but this seems insufficient.
- p_\theta(x | z) is defined as the generating distribution, but also as a joint distribution of z_1 and z_2. Unless I am missing something I think you are overloading the notation for p_\theta.
- I don't think enough information is given about the AAE and ARAE baselines. Are they the same as the flat-VAE, except with the KL term replaced by the an adversarial divergence between the prior and approximate posterior?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SklM0La66m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for your kind and helpful comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hk41X2AqtQ&amp;noteId=SklM0La66m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1322 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1322 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for taking the time to provide such thorough comments and detailed feedback.

&gt; my main criticism is that the authors fail to compare their approach to "A Hierarchical Latent Structure for Variational Conversation Modeling" by Park et al.

Thanks for referring to this interesting paper. The VHCR model (as denoted in their paper) indeed bears close resemblance to our proposed method. However, there are two key differences that make our work unique: 

1) To allow the model to make more use of the latent variable, this reference proposes to employ an ‘utterance drop’ strategy. Their goal is to weaken the autoregressive power of hierarchical RNNs by dropping the utterance encoder vector with a certain probability. Although the KL divergence term tends to get larger with this modification, the modeling capacity of the LSTM decoder may be sacrificed. Instead, we try to resolve the same issue from a different perspective (without weakening the decoder during training). Specifically, we propose to improve the flexibility and expressiveness of the prior distribution, by leveraging a hierarchy of latent variables. This setup endows the encoder/inference networks with stronger ability to extract high-level (global) features of a paragraph. Notably, the model proposed in [1] shares the same high-level idea of making the inference networks more expressive to resolve the ‘’posterior collapse’’ problem.

To compare the effectiveness of the two different strategies (to mitigate the `posterior collapse’ issue), we experiment the ‘utterance drop’ (u.d) method based upon our hier-VAE-S model on the Yelp dataset (note that to allow fair comparison, we use hier-VAE-S as the baseline to evaluate the two strategies). The corresponding language modeling results are shown as below:

			                    NLL        KL        PPL 
hier-VAE-S: 		           160.8      3.6       46.6
hier-VAE-S (with u.d):     161.3  	  5.6       47.1
hier-VAE-D: 		           160.2      6.8       45.8

As shown above, their u.d strategy allows better usage of the latent variable (indicated by a larger KL divergence value). However, the NLL of the language model becomes even worse with the u.d method, possibly due to the weakening of the decoder during training (similar observations have also been shown in Table 2 of the VHCR paper). In contrast, our ‘hierarchical prior’ strategy yields larger KL terms as well as lower NNL value, indicating the advantage of our strategy to mitigate the ‘posterior collapse’ issue.

2) The previous VHCR model considers a multi-turn dialogue generation scenario, where the dialog context is provided at each time step (in terms of the higher-level LSTM) and the model seeks to generate the corresponding response conditioned on the context. What is different in our hier-VAE model is that, we are interested in generating long and coherent units merely conditioned on the latent variable (no additional context information is provided to the decoder). As a result, the underlying data distribution of the entire paragraph is captured in the bottom-level latent variable. While in their setup, the responses are modeled/generated conditioned on both the latent variables and the contexts. In this sense, the problem we are trying to tackle is relatively more challenging. 

&gt; More generally, it would have been nice to see more ablation experiments (e.g. convolutional vs. LSTM encoder)

At the initial stage of this project, we found that the hierarchical CNN encoder employed here is important for the VAE model to work well (relative to a flat CNN encoder). Based on reviewer’s suggestion, we re-ran the language modeling experiments with a flat CNN encoder and a hierarchical LSTM encoder (on the Yelp dataset). The results are shown below:

					                          NLL        KL        PPL 
Flat CNN encoder:			         164.6      2.3       50.2
Hierarchical LSTM encoder:		 161.3	 5.7       46.9
Hierarchical CNN encoder:                 160.2      6.8       45.8

It can be observed that the model with a flat CNN encoder yields worst (largest) perplexity, suggesting that it is beneficial to make the encoder hierarchical. Additionally, hierarchical CNN encoder exhibits slightly better results than hierarchical LSTM encoder according to our experiments.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJgPXD66am" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Continue due to the character limit</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hk41X2AqtQ&amp;noteId=BJgPXD66am"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1322 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1322 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt; Finally, I know that space is tight, but other papers on global-latent-variable models tend to include more demonstrations that the global variable is capturing meaningful information, e.g. with attribute vector arithmetic. 

We share the same intuition with the reviewer that exploring what information has been meaningfully abstracted in the (global) latent variable is valuable to understand the strengths of our proposed model. We would like to mention that in our original submission, we included a section in the experiments where we reported results to measure the continuity of the learned latent space. This measure is commonly used in latent-variable models to evaluate how smooth the learned VAE latent space is. Our results demonstrated that the generated samples are syntactically and semantically reasonable. We even found that the generated sentences, along a linear trajectory, gradually transit from positive to negative sentiment (as shown in Table 4). Having said that, we followed reviewer's suggestion and are currently running additional experiments to manipulate the review sentiment via attribute vector arithmetic. We will update with the corresponding results shortly.

Furthermore, we’ve just reported the results of our new experiment to visualize the learned global latent variable by plotting the corresponding t-SNE embeddings on the arXiv dataset (please refer to our response to Reviewer 1 for additional information).

Below, please also find our responses to the specific comments of the reviewer:

[VAE prior] Thanks for pointing this out. We have revised this sentence according to your suggestion.

[Feeding z to LSTM] We apologize for the confusion. As to feeding the latent codes z to LSTM, we follow [2] and use z to infer the initial state of LSTM and feed it to every step of LSTM as well. That said, z is employed as the input to the sentence-level LSTM (higher level decoder) at every time step. For the word-level LSTM, the plan vectors are utilized by the decoder in a similar manner. Different from the higher-level LSTM, the input to the word-level LSTM at each step is the concatenation of z and the word embedding of the previous token. We have revised the description of this part in Section 3.2 to avoid any confusion.

[CNN encoder structure] Due to space limit, the specific structure of CNN encoder is described in the supplementary materials. To make it more clear, we added a ''Model specification‘’ section in the revised version, in the beginning of ''Experiments‘’ part, detailing the encoder structure..

[Notation] \theta is defined as the parameters of the whole generative network. The individual network that parametrize p(x|z_1) and p(z_1|z_2) are both part of the generative networks. Therefore, we use \theta to denote the parameters of both distributions.

[AAE and ARAE baselines] Yes, the setups of the two baseline methods are the same as flat-VAE, except that adversarial divergence, instead of the KL divergence, is employed to match the prior and posterior distributions. 
 
[1] Kim, Y., Wiseman, S., Miller, A.C., Sontag, D.A., &amp; Rush, A.M. (2018). Semi-Amortized Variational Autoencoders. ICML.
[2] Bowman, S.R., Vilnis, L., Vinyals, O., Dai, A.M., Józefowicz, R., &amp; Bengio, S. (2016). Generating Sentences from a Continuous Space. CoNLL.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJxL95kZAQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hk41X2AqtQ&amp;noteId=BJxL95kZAQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1322 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1322 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your detailed consideration of my comments. While I agree that a big difference between your work and that of Park et al. is their use of "utterance drop", I'm not sure I agree that the hierarchical structure of the two models is significantly different. You wrote

&gt; Specifically, we propose to improve the flexibility and expressiveness of the prior distribution, by leveraging a hierarchy of latent variables. This setup endows the encoder/inference networks with stronger ability to extract high-level (global) features of a paragraph. Notably, the model proposed in [1] shares the same high-level idea of making the inference networks more expressive to resolve the ‘’posterior collapse’’ problem.

It appears they also do this - see equations (24) and (25) in Park et al., where the utterance variables are treated as stochastic and are dependent on the global conversation variable. If there is indeed an important different in the structure of your latent variables, you need to make that absolutely clear in your updated draft. Otherwise, it seems that the only difference is their use of "utterance drop" and the difference in applications, neither of which are terribly significant. Looking at your updated draft, it appears you have only mentioned Park et al. in pointing out their use of utterance drop. I think you need to flesh out this comparison.

With the revisions you made, experiments with "attribute vector arithmetic", and some additional words comparing your work to Park et al.,  I would be happy to raise my score.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>