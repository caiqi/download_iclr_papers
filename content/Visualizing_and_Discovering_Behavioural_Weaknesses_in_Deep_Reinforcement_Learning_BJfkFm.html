<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Visualizing and Discovering Behavioural Weaknesses in Deep Reinforcement Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Visualizing and Discovering Behavioural Weaknesses in Deep Reinforcement Learning" />
        <meta name="citation_author" content="Christian Rupprecht" />
        <meta name="citation_author" content="Cyril Ibrahim" />
        <meta name="citation_author" content="Chris Pal" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJf9k305Fm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Visualizing and Discovering Behavioural Weaknesses in Deep..." />
      <meta name="og:description" content="As deep reinforcement learning is being applied to more and more tasks, there is a growing need to better understand and probe the learned agents. Visualizing and understanding the decision making..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJf9k305Fm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Visualizing and Discovering Behavioural Weaknesses in Deep Reinforcement Learning</a> <a class="note_content_pdf" href="/pdf?id=BJf9k305Fm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=christian.rupprecht%40in.tum.de" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="christian.rupprecht@in.tum.de">Christian Rupprecht</a>, <a href="/profile?email=cyril.ibrahim%40elementai.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="cyril.ibrahim@elementai.com">Cyril Ibrahim</a>, <a href="/profile?email=christopher.pal%40polymtl.ca" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="christopher.pal@polymtl.ca">Chris Pal</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">As deep reinforcement learning is being applied to more and more tasks, there is a growing need to better understand and probe the learned agents. Visualizing and understanding the decision making process can be very valuable to comprehend and identify problems in the learned behavior. However, this topic has been relatively under-explored in the reinforcement learning community. In this work we present a method for synthesizing states of interest for a trained agent. Such states could be situations (e.g. crashing or damaging a car) in which specific actions are necessary. Further, critical states in which a very high or a very low reward can be achieved (e.g. risky states) are often interesting to understand the situational awareness of the system. To this end, we learn a generative model over the state space of the environment and use its latent space to optimize a target function for the state of interest. In our experiments we show that this method can generate insightful visualizations for a variety of environments and reinforcement learning methods. We explore these issues in the standard Atari benchmark games as well as in an autonomous driving simulator. Based on the efficiency with which we have been able to identify significant decision scenarios with this technique, we believe this general approach could serve as an important tool for AI safety applications.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Visualization, Deep Reinforcement Learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We present a method to synthesize states of interest for reinforcement learning agents in order to analyze their behavior. </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HklBSScnaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJf9k305Fm&amp;noteId=HklBSScnaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1009 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1009 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJg44S5haQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response from the authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJf9k305Fm&amp;noteId=rJg44S5haQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1009 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1009 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewers for their comments and feedback on our work. The reviewers find the paper to be novel (R1), interesting (R2, R3) and well-written (R1). However, there are numerous elements of the work which we believe the reviewers have misunderstood. We believe these issues could be addressed through some clarifications in the manuscript and provide a response below; however, we have decided to withdraw the paper.

R1
 - Even though guided backpropagation is unreliable as an explanation method, we only use it as a saliency to weigh the reconstruction loss. As shown in Table 1, this is necessary to recover the agent’s behavior on the reconstructed frames, because it puts emphasis on details (salient regions) when training the generative model. The actual insights come from the generative model and do not depend on gradient-based feature visualization. 
 - We agree with the reviewer that experiments like the ones in Sec. 4.3 and 4.6 on a larger scale would further showcase the usefulness of the visualizations in identifying agent flaws. However, we do not see a way to perform these studies in practice, as they fully depend on a) the performance of the agent and b) the rules of the environments, which cannot be easily controlled to artificially introduce and validate flaws. We think that the demonstrated cases together with the visualizations and quantitative evaluations show that this method is capable to deliver insights in real-world applications.
 - We will incorporate further analysis of the replay buffer baseline; however, we would like to point out that one of the motivations behind our pixel difference experiments in Section 4.7 is to compare our visualizations with what could be found in the replay buffers.
Minor comment 1) is correct, we will revise the claim. 


R2
 - We do indeed explore seven different target functions which we term T^+, T^-, T^+-, S^+, S^-, S^+-, action maximization.
 - The analysis of identifying critical states in the replay buffer has been discussed in Sec. 4.6 and 4.7.
 - Thank you for the reference. We will include the very recent work of Huang in the discussion.


R3
 - Using our method the user does not need to come up with adversarial examples, the user can probe any model learned in an arbitrary simulation. Our Atari game experiments operate in precisely this setting. Our driving simulator experiments highlight the sort of weaknesses that can arise in the agent. Since we do not have access to complex, industrial-grade car simulation environments, we focus on the ability of our method to identify weaknesses in settings where we hypothesized there is likely to exist a weakness in a learned agent (for example, running over pedestrians).
 - We show many qualitative examples in the Appendix. These are randomly selected for a variety of target functions.  
 - We propose seven different target functions with different semantic meanings that the user can choose from.
The proposed method is used to synthesize states of interest that lead the agent to specific actions. A human user can validate the agent’s behavior on the visualized states and identify flaws. Of course in our experiments with the driving simulator we do indeed check if the identified flaw actually exists in the behavior of the agent. 
 - We will update the notation in Section 3 to be more precise.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Syem_icn6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response from AnonReviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJf9k305Fm&amp;noteId=Syem_icn6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1009 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1009 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the response. If it's helpful, I've provided responses to your comments below:

- Re guided backpropagation as part of the reconstruction loss, I think the question I was trying to hone in on is this: if guided backprop is unreliable a a method for discerning what the agent cares about in the input, wouldn't it mislead the visualizations by putting emphasis on reconstructing portions of the input which are not necessarily important for the agent? And because it appears to primarily emphasize regions which are useful for the task in an agent-independent way, isn't it possible that it would result in visualizations which make it seem like an agent cares about some input component,  even when the agent doesn't? 

- Re experiments like those in Sec 4.3, one potential strategy might be to modify the reward function during training runs to induce agents to take in particular ways, and test quantitatively whether this changes the visualizations you produce (for quantification, in Sec 4.3, one could quantify the oxygen meter in the visualizations in a blinded fashion, and then analyze the actions conditional on this). 

As mentioned in my review, there are definitely interesting and worthwhile threads in this work, and I look forward to seeing a new and improved version in the future. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJeLsNXo2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJf9k305Fm&amp;noteId=HJeLsNXo2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1009 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1009 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value"># Summary
This paper proposes a generative model of visual observations in RL that is capable of generating observations of interests. The idea is to first train a VAE which reconstructs a given observation with a regularizer that encourages the generated images from which the policy can reconstruct its output. This regularizer makes the VAE focus on reconstructing parts of images that are important for the given policy. After training the VAE, this paper proposes to optimize the latent variable of the VAE with respect to a target function (e.g., high/low values, specific actions) to generate states of interest. The experimental result shows that the proposed model can generate realistic images in Atari games that are more interpretable to the agent compared to a vanilla VAE. It is also shown that it is possible to generate states of interest such as high/ow-value states by plugging in different target functions, which allows users to analyze RL agents easily. 

[Pros]
- An interesting attempt to learn a controllable generative model of states to analyze deep RL agents. 

[Cons]
- (Major) The experimental results are not comprehensive.
- (Minor) The usefulness of the proposed generative model is not clear.
- (Minor) Inconsistent equations/notations

# Novelty and Significance
- The proposed regularizer for VAE and gradient descent approach for generating particular types of states are novel and interesting.
- Although this type of generative model can be useful for analyzing learned RL agents more efficiently, it is unclear how this could be useful for improving AI safety as claimed by this paper. In Section 4.6, for example, this paper shows "distracted pedestrians" example and claims that their generative model is useful to verify whether the policy can handle such an adversarial example or not. However, users still need to come up with such a scenario, manually construct such an adversarial example, and test it using the policy. I do not see any role of the proposed generative model here. A more convincing example or experiment would be necessary to support the main contribution of this work. 

# Quality and Experiments
- This paper shows a few generated samples for qualitative evaluation. However, it is unclear whether such samples are cherry-picked or randomly-picked. It would be important to present many "random" samples from the generative model to evaluate the quality of samples. 
- Related to the above comment, it would be also important to show interpolation / extrapolation in the latent space to show that the model has learned a reasonable manifold instead of overfitting to the training data.
- Instead of presenting Table 2 to show generalization performance, it would be more informative to show examples of generative samples + nearest neighbors from the training data, which is a common practice to verify overfitting in generative models.
- Figure 5 (+ discussion on why activation maximization does not work) looks relatively less important, which could be condensed or moved to the appendix.
- It would be interesting to see how the generated samples change as the number of gradient-descent steps for the target function increases.

# Clarity and Presentation
- This paper does not fully describe how the energy function is optimized (initial latent variable, the number of gradient-descent steps).
- There are some inconsistent and undefined notations, which need to be fixed.
1) KL divergence should be defined over two probability distributions. However, KL-term in this paper is defined over the output of the network (Equation 1).
2) What is the form of the generator g? It is introduced as "g(\mu, \sigma, z)" but described as g(x, z) in Equation 4.
3) In VAE, we normally define the latent variable z as random variable from N(\mu, \sigma^2). But, it seems like this paper uses z as unit Gaussian, which is a bit confusing.
4) I_n in Equation 1 is not defined. 
5) In page 3, A(s)_a is not a well-defined notation?
6) Is \pi(s) a vector or scalar?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1g2MFg537" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A reasonable approach for visualizing states of interest.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJf9k305Fm&amp;noteId=H1g2MFg537"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1009 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1009 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary.
The paper proposes an approach that involves two stages: (1) a variational autoencoder that learns to reconstruct the state space (i.e., input images in Atari games) and (2) an optimization step that finds a set of conditioning parameters to generate a synthetic image. The effectiveness of the proposed method is qualitatively evaluated on Atari games and a driving simulator. Though the approach looks interesting, clarification on claims/evaluations is required. 

Strengths.
- An interesting problem in the current CV/RL community.
- Well-surveyed related work.

Generative models vs. Query-based models.
Though the authors mention several different target functions (T) can be chosen, the paper only explores only one simple function that could easily be achieved in other ways. For example, given a trained agent, it could be feasible to collect observations where the agent outputs high or low Q-values for all possible actions. Also, the following existing paper successfully finds a few critical states in which a certain action is important to be taken. This was done by computing entropy over the policy’s output and by computing the baselined maximum Q-value. 

[1] Huang et al., “Establishing Appropriate Trust via Critical States”, IROS 2018.

Providing other possible target functions will be helpful to convince readers about its usefulness.

Synthesizing unseen scenarios.
I fully agree that synthesizing unseen scenarios is an important and interesting problem (especially for self-driving vehicle controls). However, I am worried about the ability of VAE in generating novel images, which is basically trained to reconstruct the manifold of a domain of training data. Counting the number of different pixels would not be sufficient evaluation metric especially in vehicle controls where small visual cues (i.e., pedestrians, vehicle, etc)  plays an important role. Can authors provide more detailed analysis of the ability to synthesize unseen scenarios?

Clarification in the Section Methods.
Notations are used without providing a careful definition, which makes hard to understand the methodology. For example, ‘z’ and ‘d’ in the definition (2). Also, can authors clarify the term pi(s) - which is defined as a scalar value from the agent network’s output - with the del operator? 

Minor concerns.
Typos</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Byg6j0gt2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review of "Visualizing and Discovering Behavioural Weaknesses in Deep Reinforcement Learning"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJf9k305Fm&amp;noteId=Byg6j0gt2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1009 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1009 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper introduces a new method for visualizing states which are likely to lead to various outcomes (e.g., high/low reward or a particular agent action). This method is based on a VAE modified to produce samples which not only allow reconstruction of the input, but also contain salient input features (as measured by guided backpropagation) and which will result in the same agent output as the original input. The paper then goes on to use this method to generate inputs for Atari games which result in particular rewards (high/low), actions (e.g., left), or uncertainty (states in which different actions can either lead to very good or bad states). 

The paper is clear and well-written, and a large number of generated samples are provided (especially in the Appendix, which contains examples for all games in the ALE for a variety of outcomes). The method for regularizing the samples is novel to my knowledge. However, while the method for generating samples is interesting, this paper contains several critical issues (detailed below). I therefore think that this paper needs more work and is not yet ready for publication in a venue such as ICLR. 

Major comments: 

1) While the visualizations are indeed interesting, it’s unclear whether these visualizations provide information about particular agents or about the task itself. This is a critical distinction, especially in light of recent results (cited by the authors) which suggest that many current visualization methods produce the same outputs regardless of the model studied [1, 2, 3]. Moreover, this issue is especially important given the use of guided backpropagation in the sample generation method itself.

2) Assuming the visualizations are model-specific, it’s unclear what (if any) falsifiable statements can be generated from these visualizations. Section 4.3 takes a stab at this, stating that the visualizations revealed that a particular agent didn’t understand the oxygen mechanic in Sequest, which was confirmed by rolling out the agent (though no data is presented along with this anecdote). This paper would be dramatically strengthened by more experiments like this section (with provided data, of course). Without such a link to the agent’s ultimate behavior (and by extension, ground truth), it’s unclear whether these visualizations actually lead to any understanding at all.

3) Further experiments are needed to demonstrate the benefit of the proposed method over simply analyzing a replay buffer. The experiment included in the manuscript based on pixel differences is flawed as pixel differences are a poor measure of mode collapse since they are sensitive to simple image transformations such as translation. This issue has been investigated in the context of GANs, in which pixel differences were found to work only for centered images such as face datasets [4]. In order to evaluate whether the proposed method is actually advantageous over analyzing the replay buffer, a perceptual metric would need to be used.

Minor comments:

1) The claim in the third paragraph of Section 4.5 that unused conv1 weights lead to more easily “distracted” models is unsubstantiated.


[1] Adebayo J, Gilmer J, Muelly M, Goodfellow I, Hardt M, Kim B. Sanity checks for saliency maps. arXiv preprint arXiv:1810.03292. 2018 Oct 8.

[2] Hooker S, Erhan D, Kindermans PJ, Kim B. Evaluating feature importance estimates. arXiv preprint arXiv:1806.10758. 2018 Jun 28.

[3] Kindermans PJ, Hooker S, Adebayo J, Alber M, Schütt KT, Dähne S, Erhan D, Kim B. The (Un) reliability of saliency methods. arXiv preprint arXiv:1711.00867. 2017 Nov 2.

[4] Arora S, Zhang Y. Do GANs actually learn the distribution? an empirical study. arXiv preprint arXiv:1706.08224. 2017 Jun 26.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>