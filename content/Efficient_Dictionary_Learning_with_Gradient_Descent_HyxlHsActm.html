<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Efficient Dictionary Learning with Gradient Descent | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Efficient Dictionary Learning with Gradient Descent" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HyxlHsActm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Efficient Dictionary Learning with Gradient Descent" />
      <meta name="og:description" content="Randomly initialized first-order optimization algorithms are the method of choice for solving many high-dimensional nonconvex problems in machine learning, yet general theoretical guarantees cannot..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HyxlHsActm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Efficient Dictionary Learning with Gradient Descent</a> <a class="note_content_pdf" href="/pdf?id=HyxlHsActm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019efficient,    &#10;title={Efficient Dictionary Learning with Gradient Descent},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HyxlHsActm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Randomly initialized first-order optimization algorithms are the method of choice for solving many high-dimensional nonconvex problems in machine learning, yet general theoretical guarantees cannot rule out convergence to critical points of poor objective value. For some highly structured nonconvex problems however, the success of gradient descent can be understood by studying the geometry of the objective. We study one such problem -- complete orthogonal dictionary learning, and provide converge guarantees for randomly initialized gradient descent to the neighborhood of a global optimum. The resulting rates scale as low order polynomials in the dimension even though the objective possesses an exponential number of saddle points. This efficient convergence can be viewed as a consequence of negative curvature normal to the stable manifolds associated with saddle points, and we provide evidence that this feature is shared by other nonconvex problems of importance as well. </span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We provide an efficient convergence rate for gradient descent on the complete orthogonal dictionary learning objective based on a geometric analysis.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">dictionary learning, nonconvex optimization</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SklG6dwsnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Needs some improvement.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxlHsActm&amp;noteId=SklG6dwsnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper59 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper59 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper presents a convergence analysis for manifold gradient descent in complete dictionary learning. I have three major concerns:

(1) The optimization problem for complete orthogonal dictionary learning in this paper is very different from overcomplete dictionary learning in practice. It is actually more similar to tensor decomposition-type problems, especially after smoothing. From this point of view, it is not as interesting as the optimization problem.

Arora et al. Simple, Efficient, and Neural Algorithms for Sparse Coding, 2015

(2) Some recent works focus on analyzing gradient descent for phase retrieval and matrix sensing. These obtained results are significantly improved and near-optimal. However, the convergence rate in this paper is very loose. Besides, the paper even does not look into the last phase of gradient descent, when there exists restricted strong convexity. Thus, only sublinear convergence rate is presented.

Chen et al. Gradient Descent with Random Initialization: Fast Global Convergence for Nonconvex Phase Retrieval, 2018

The quality of this paper could be improved, if the author could sharpen their analysis.

(3) The analysis for the manifold gradient methods is something new, but not very significant. There have already been some works on manifold gradient methods. For example, the following paper has established convergence rates to second order optimal solutions for general nonconvex function over manifold.

Boumal et al. Global rates of convergence for nonconvex optimization on manifolds. 2016.

The following paper has established the asymptotic convergence to second order optimal solutions for general nonconvex function over manifold.

Lee et al. First-order Methods Almost Always Avoid Saddle Points, 2017.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SygOCLP527" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Iteration complexity analysis of Riemannian gradient descent for orthogonal dictionary learning with sparse factors.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxlHsActm&amp;noteId=SygOCLP527"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper59 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper59 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors analyze the convergence performance of Riemannian gradient descent based algorithm for the dictionary learning problem with orthogonal dictionary and sparse factors. They demonstrate a polynomial time convergence from a random initialization for a smooth surrogate objective for the original non-smooth one. The problem and the analysis are of interest, but I have several questions regarding the paper as follows.

My first concern is that the analysis is on a smooth surrogate of the non-smooth sparse minimization for solving the dictionary learning problem, so it is not clear what is relationship between the global minimizer of the smooth problem to the underlying true dictionary. More specifically, how far is the global minimizer of problem (1) or (2) to the true dictionary parameter, and whether they share (approximately) the space or components regarding the sparse factors. Without clarifying this, it is not well motivated why we are interested in studying the problem considered in this paper at the beginning. Intuitively, since the recovered factors are not sparse anymore, it will impact the dictionary accordingly due to the linear mapping, which may lead to a very different set of dictionary components. Thus, explicit explanation is necessary here to avoid such degenerate case.

My second concern is the eligibility of assuming the dictionary A to be an identity matrix and extending it to the general orthogonal matrix case. The analysis uses the fact that rows of A are canonical basis, i.e., each row only has one non-zero entry. I do not see a straightforward extension by replacing A to be an orthogonal matrix as the authors claimed on page 3, since then the inner product of one row of A and one column of X is not just the corresponding entry of the column of X. It will be helpful if the authors can explain this explicitly or adjust the analysis accordingly to make this valid.

Another issue is the clarity of the paper. Some statements in the paper are not very clear. Form example, on page 3, third paragraph of Section 3, what does row(Y) = row(X_0) mean? Also, in eqn (1), y_k means k-th column of Y, and in eqn (2), q_i means i-th entry of q? Since both are bold lower case letters, clear distinction will help. Moreover, the reference use (. ) instead [ .], which can be confusing sometimes. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1emP_AJ3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>I believe I miss some thing important in this paper. This paper seems not to be self contained. I do not understand the paper very well. Therefore, I have reservations about the paper.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxlHsActm&amp;noteId=H1emP_AJ3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper59 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">26 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper59 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper analyzes the surface of the complete orthogonal dictionary learning problem, and provides converge guarantees for randomly initialized gradient descent to the neighborhood of a global optimizer. The analysis relies on the negative curvature in the directions normal to the stable manifolds of all critical points that are not global minimizer.

Exploring the surface of a function and analyzing the structure of the negative curvature normal to the stable manifolds is an interesting idea. However, I believe I miss some thing important in this paper. This paper seems not to be self contained. I do not understand the paper very well. See details below. Therefore, I have reservations about the paper.

*) The terminology "stable manifolds" is used from the first page, while its formal definition is given on page 4.
*) P3, the dictionary learning problem is not formally given. It is stated in the paper that the task is to find A and X, given Y. However, what optimization problem does the author consider? Is it \min_{A, X} \|Y - A X\|_F^2? assuming both dictionary A and sparse code X are unknown or \min_{A} \|Y - A X\|_F^2 assuming only dictionary is unknown?
*) P3, second paragraph in Section 3: what is the variable q? It is not defined before.
*) P3, third paragraph in Section 3: What is the function row()? Why does row(Y) equal row(X)?
*) P3: How does the dictionary learning problem reformulate into the problem in the third paragraph of Section 3? If I understand correctly, the task is to find A, X such that A^* Y = X since A is orthogonal. Consider the first column in A and denote it by q. Then the first column of X is approximated by q^* Y. Since X is sparse, the task is to find q so that q^* Y as sparse as possible. But how about the other columns in matrix $A$? 
*) The Riemannian gradient algorithm is not stated in this paper.


</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>