<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Neural Program Repair by Jointly Learning to Localize and Repair | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Neural Program Repair by Jointly Learning to Localize and Repair" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ByloJ20qtm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Neural Program Repair by Jointly Learning to Localize and Repair" />
      <meta name="og:description" content="Due to its potential to improve programmer productivity and software quality, automated program repair has been an active topic of research. Newer techniques harness neural networks to learn..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ByloJ20qtm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Neural Program Repair by Jointly Learning to Localize and Repair</a> <a class="note_content_pdf" href="/pdf?id=ByloJ20qtm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019neural,    &#10;title={Neural Program Repair by Jointly Learning to Localize and Repair},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=ByloJ20qtm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=ByloJ20qtm" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Due to its potential to improve programmer productivity and software quality, automated program repair has been an active topic of research. Newer techniques harness neural networks to learn directly from examples of buggy programs and their fixes. In this work, we consider a recently identified class of bugs called variable-misuse bugs. The state-of-the-art solution for variable misuse enumerates potential fixes for all possible bug locations in a program, before selecting the best prediction. We show that it is beneficial to train a model that jointly and directly localizes and repairs variable-misuse bugs. We present multi-headed pointer networks for this purpose, with one head each for localization and repair. The experimental results show that the joint model significantly outperforms an enumerative solution that uses a pointer based model for repair alone.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">neural program repair, neural program embeddings, pointer networks</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Multi-headed Pointer Networks for jointly learning to localize and repair Variable Misuse bugs</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">11 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_r1eodmKxRQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Feedback Incorporated In New Paper Version</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByloJ20qtm&amp;noteId=r1eodmKxRQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1014 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1014 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks to all the reviewers for their helpful and constructive feedback. We have uploaded a new paper revision to address the comments and feedback:

1. Added a new section 4.4 on evaluation of the model in practice on realistic bugs.
2. Added a discussion about key differences with the previous work: DeepBugs and Sk_p (Section 2).
3. Added a discussion on performance comparison with enumerative approaches (Section 4.1).
4. Changed the problem definition to clarify the definition of V_def set (Section 3.1).
5. Added a discussion on the relationship between previous automated program repair (APR) work based on tests/specifications and neural program repair approaches (Section 2).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkxYwwjqTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting Model and Incremental Improvement on Synthetic Datasets and Problematic Problem Definition</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByloJ20qtm&amp;noteId=rkxYwwjqTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1014 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1014 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents an LSTM-based model for bug detection and repair of a particular type of bug called VarMisuse, which occurs at a point in a program where the wrong identifier is used. This problem is introduced in the Allamanis et al. paper. The authors of the paper under review demonstrate significant improvements compared to the Allamanis et al. approach on several datasets.

I have concerns with respect to the evaluation, the relation of the paper compared to the state-of-the-art in automatic program repair (APR), and the problem definition with respect to live-variable analysis.

My largest concern about both this paper and the Allamanis et al. paper is how it compares to the state-of-the-art in APR in general. There is a large and growing amount of work in APR as shown in the following papers:
[1] L. Gazzola, D. Micucci, and L. Mariani, “Automatic Software Repair: A Survey,” IEEE Transactions on Software Engineering, pp. 1–1, 2017.
[2] M. Monperrus, “Automatic Software Repair: A Bibliography,” ACM Comput. Surv., vol. 51, no. 1, pp. 17:1–17:24, Jan. 2018.
[3] M. Motwani, S. Sankaranarayanan, R. Just, and Y. Brun, “Do automated program repair techniques repair hard and important bugs?,” Empir Software Eng, pp. 1–47, Nov. 2017.

Although the proposed LSTM-based approach for VarMisuse is interesting, it seems to be quite a small delta compared to the larger APR research space. Furthermore, the above papers on APR are not referenced.

The paper under review mostly uses synthetic bugs. However, they do have a dataset from an anonymous industrial setting that they claim is realistic. In such a setting, I would simply have to trust the blinded reviewers. However, the one industrial software project tells me little about the proposed approach’s effectiveness when applied to a significant number of widely-used software programs like the ones residing in state-of-the-art benchmarks for APR, of which there are at least the following two datasets:
[4] C. L. Goues et al., “The ManyBugs and IntroClass Benchmarks for Automated Repair of C Programs,” IEEE Transactions on Software Engineering, vol. 41, no. 12, pp. 1236–1256, Dec. 2015.
[5] R. Just, D. Jalali, and M. D. Ernst, “Defects4J: A Database of Existing Faults to Enable Controlled Testing Studies for Java Programs,” in Proceedings of the 2014 International Symposium on Software Testing and Analysis, New York, NY, USA, 2014, pp. 437–440.

The above datasets are not used or referenced by the paper under review.

My final concern about the paper is the formulation of live variables. A variable is live at certain program points (e.g., program statements, lines, or tokens as called in this paper). For example, from Figure 1 in the paper under review, at line 5 in (a) and (b), object_name and subject_name are live, not just sources.  In the problem definition, the authors say that "V_def^f \subseteq V denotes the set of all live variables", which does not account for the fact that different variables are alive (or dead) at different points of a program. The authors then say that, for the example in Figure 1, "V_def^f contains all locations in the program where the tokens in V appear (i.e., tokens in the Blue boxes), as well as token sources from line 1”. The explanation of the problem definition when applied to the example does not account for the fact that different variables are alive at different program points. I’m not sure to what extent this error negatively affects the implementation of the proposed model. However, the error could be potentially quite problematic.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJx4h1YgCX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer4</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByloJ20qtm&amp;noteId=BJx4h1YgCX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1014 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1014 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the thoughtful review and constructive feedback.

We will include a discussion about the differences between our work and the automated program repair (APR) techniques in the literature, as outlined below. The traditional APR approaches differ from our work in the following ways: 1) They require a form of specification of correctness to repair a buggy program, usually as a logical formula/assertion, a set of tests or a reference implementation. 2) They depend on hand-designed search techniques for localization and repair. 3) The techniques are applied to programs which violate the specifications (e.g., a program which fails some tests), that is, to programs which are already known to contain bugs. In contrast, a recent line of research in APR is based on end-to-end learning, of which ours is an instance. Our solution (like some other learning based repair solutions) has the following contrasting features: 1) Our solution does not require any specification of correctness. Instead it learns to fix a common class of errors directly from source code examples. 2) Our solution does not perform enumerative search for localization or repair. We train a neural network to perform localization and repair directly. 3) Our solution is capable of first classifying whether a program has the specific type of bug or not, and subsequently localizing and repairing it.

ManyBugs, IntroClass, and Defects4J are benchmarks designed for test-based program repair techniques. The bugs relate to the expected specification of individual programs (captured through test cases of the program) and the nature of bugs vary from program to program. These benchmarks are therefore suitable to evaluate repair techniques guided by test executions. Learning based solutions like ours focus on common error types so that it is possible for a model to generalize across programs, and work directly on embeddings of source code.

Thank you for your comment about the variable liveness. We misused the term of live variables, and we will update the paper accordingly. The V_def^f set contains all variables defined in a function f, including the function arguments; in this way constructing a set of all variables that can be used within the scope. We construct one V_def^f set per function, representing a set of candidate variables for fixing bugs in that function. In this way, the V_def^f set is a (safe) over-approximation of the in-scope variables at each program location. The over-approximation can lead to predicting an undefined variable as a repair, however, this is not an error and model over time learns not to predict undefined variables. The V_def^f set is not constrained to only live variables; as there are cases when solution to a bug is using a variable that is defined in the scope but not live (not used elsewhere), e.g., subject_name variable in Figure 1a. Regarding your comment about Figure 1, in the blue boxes we show variable usages, not V_def^f set.

We want to clarify that the examples in our industrial dataset (Section 4.4) are not from a single industrial project. The examples do come from multiple software projects.

Please let us know if this helped clarify the confusion regarding the problem definition of candidate variable set and the relationship with previous APR work and the more recent neural program repair approaches (ours, Allamanis et. al and others). </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Byg7TN9daQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Simple model and really good results, but uninteresting contributions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByloJ20qtm&amp;noteId=Byg7TN9daQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1014 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1014 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper considers the problem of VarMisuse, a kind of software bug where a variable has been misused. Existing approaches to the problem create a complex model, followed by enumerating all possible variable replacements at all possible positions, in order to identify where the bug may exist. This can be problematic for training which is performed using synthetic replacements; enumeration on non-buggy positions does not reflect the test case. Also, at test time, enumerating is expensive, and does not accurately capture the various dependencies of the task. This paper instead proposes a LSTM based model with pointers to break the problem down into multiple steps: (1) is the program buggy, (2) where is the bug, and (3) what is the repair. They evaluate on two datasets, and achieve substantial gains over previous approaches, showing that the idea of localizing and repairing and effective.

I am quite conflicted about this paper. Overall, the paper has been strengths:
- It is quite well-written, and clear. They do a good job of describing the problems with earlier approaches, and how their approach can address it.
- The proposed model is straightforward, and addresses the problem quite directly. There is elegance in its simplicity.
- The evaluation is quite thorough, and the resulting gains are quite impressive.

However, I have some significant reservations about the novelty and the technical content. The proposed model doesn't quite bring anything new to the table. It is a straightforward combination of LSTMs with pointers, and it's likely the benefits are coming from the reformulation of the problem, not from the actual proposed model. This, along with the fact that VarMisuse is a small subset of the kinds of bugs that can appear in software, makes me feel the ideas in this paper may not lead to significant impact on the research community.

As a minor aside, this paper addresses some specific aspects of VarMisuse task and the Allamanis et al 2018 model, and introduces a model just for it. I consider the Allamanis model a much more general representation of programs, and much more applicable to other kinds of debugging tasks (but yes, since they didn't demonstrate this either, I'm not penalizing this paper for it).
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1ecT0_gR7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByloJ20qtm&amp;noteId=H1ecT0_gR7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1014 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1014 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the thoughtful review and constructive feedback. 

Our paper proposes a joint model for localization and repair using pointers, which is novel and the main technical contribution of the paper. Even though it is applied specifically to the variable misuse problem, the idea of using pointers is fundamental and portable to other program repair problems. In particular, all program repair techniques require the bug localization step and pointers seem like an ideal mechanism for this as they can pinpoint a buggy location precisely at the token-level. Other previous works in the program repair literature either use enumerative search for localization, or perform localization at the granularity of lines or depend on external tools for localization (such as compiler error messages for syntactic error localization). In contrast, our proposal to use pointers enables an end-to-end learning based solution. We use the pointer mechanism on top of sequence based encoding of programs, but pointers can be combined naturally with other representations of programs; e.g., trees or graphs.

As we demonstrate in the paper, the previous enumerative approaches assume independence among different predictions that is problematic and leads to poor results. The end-to-end joint localization and repair is an essential step to overcome this issue, and we believe this idea of joint prediction is going to generalize to many other program repair tasks and even program completion tasks.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HkgAKQg93Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Weak baseline and lack trade-offs discussion makes it hard to say if idea is good.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByloJ20qtm&amp;noteId=HkgAKQg93Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1014 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1014 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Several recent works propose to discover bugs in code by creating dataset of presumably correct code and then to augment the data by introducing a bug and creating a classifier that would discriminate between the buggy and the correct version. Then, this classifier would be used to predict at each location in a program if a bug is present.

This paper hypothetizes that when running on buggy code (to discover the bug) would lead to such classifier misbehave and report spurious bugs at many other locations besides the correct one and would fail at precisely localizing the bug. Then, they propose a solution that essentially create a different classifier that is trained to localize the bug.

Unfortunatley this leads to a number of weaknesses:
 - The implementation and evaluation are only on a quite syntactic system with low precision and that needs to sift through a huge amount of weak and irrelevant signals to make predictions.
 - The gap here is huge: the proposed system is only based on program syntax and gets 62.3% accuracy, but state-of-the-art has 85.5% (there is actually another recent technique [1] also with accuracy in the &gt;80% range)
 - It is not clear that the entire discussed problem is orthogonal to the selection of such weak baselines to build the improvements on.
 - Trade-offs are not clear: is the proposed architecture slower to train and query than the baselines?

Strengths of the paper are:
 - Well-written and easy to follow and understand.
 - Evaluation on several datasets.
 - Interesting architecture for bug-localization if the idea really works.

[1] Michael Pradel, Koushik Sen. DeepBugs: a learning approach to name-based bug detection</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyeyTXFyTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByloJ20qtm&amp;noteId=SyeyTXFyTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1014 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1014 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the review and constructive feedback. We believe there are a few major misunderstandings in the review and we would like to take this opportunity to clarify them. We will be happy to discuss them in more detail if more clarifications might be needed or there are more questions.

We would first like to point out that ours is the first model that jointly learns to perform both localization and repair of the variable misuse bugs. It exploits the property of this particular class of variable misuse bugs -- both the location and repair corresponds to variable use locations in the program. Unlike Allamanis et al. 2018 that uses an enumerative approach to make a number of predictions for a program that is linear in number of variable uses, our model makes a single prediction using a two pointer based mechanism. 

Thanks for the pointer to the DeepBugs paper. Note that there are several differences of our work with the DeepBugs paper, which we explain below. We will add this to our revision as well.
1. DeepBugs learns a classifier over single expressions. It takes a single program expression as input (e.g. “2% i == 0”) and classifies it as positive or negative. On the other hand, in addition to classifying programs, our model learns to localize and also repair the bug using a two-headed pointer network.
2. Our model uses the full program (up to 250 number of tokens) for learning the vector representation. DeepBugs only looks at a single expression at a time.
3. Finally, the 80% accuracy number for DeepBugs is only for expression classification. It has no direct comparison with our model’s accuracy since it is a different problem (classifying a single expression as correct compared to analyzing a full program to identify bug location and the corresponding repair). Moreover, our pointer models also get to 82.4% classification accuracy for full programs (Table 1).

Allamanis et al. only report the accuracy of repair-only model, where the model predicts a single variable at a time for each slot location in a program. Translating their 85.5% repair accuracy number to a number that corresponds to repairing the full program would lead to a very different result. In Table 1, we try to replicate a similar experiment and show that jointly learning the model leads to significant improvements without sacrificing true positive and classification accuracy. Moreover, Allamanis et al. 2018 perform a significant amount of program preprocessing including type inference, control flow, and data flow analysis to add different types of graph edges. Without such pre-processing, they achieve an accuracy of 55.3% on repair-only tasks (Section 4.3). In our work, we want our distributed representations to automatically learn good representations of programs without any manual feature engineering.

Performance trade-off: In fact, our proposed architecture is significantly more scalable and easier to train. Since we are using sequence models to compute pointer attentions that are easier to batch over multiple examples, it is much more scalable to train compared to graph models that are difficult to batch because of different graph sizes. Our own graph implementation was significantly slower to train. 

In addition to that, it is also significantly faster at inference time, as it does not need to perform an O(n) number of model predictions, where n is the number of variable use locations in the program under test. For our model, it performs a single prediction, which is much faster.

Please let us know if this helped clarify the questions and comments.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1lIQfpZ6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Paper can be updated</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByloJ20qtm&amp;noteId=S1lIQfpZ6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1014 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1014 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the response. Certainly, many of the issues discussed can be incorporated in the paper, not in comments. The task the discussed papers introduce is used in practice to do anomaly detection and then to report bugs. The bug localization of Allamanis et al has very high positive rate (they did not share it in their paper, but text implies it is a few bugs per hundreds of reports). Pradel and Sen however share accuracy on a non-synthetic task and it is around 50%. It does not yet look like that the result of this submission will translate to any better bug-finding technique in practice, but I am looking forward to see if the proposed technique is a good idea on a more realistic scenario.

Also, this is not the first paper to propose both localization and fixing. The following work does it and their accuracy is lower, but on a more practical task:
Pu, Yewen, Karthik Narasimhan, Armando Solar-Lezama, and Regina Barzilay. “Sk_p: a Neural Program Corrector for MOOCs.”, OOPSLA 2016

One possibility to improve the submission is to try the neural approach on their dataset and report state-of-the-art results.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJgADrtPT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Paper Updated</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByloJ20qtm&amp;noteId=HJgADrtPT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1014 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1014 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for more comments and helpful suggestions.

Revised version of the paper is added. We incorporated some of the above discussion on comparison with DeepBugs (in section 2 - related work) and a discussion on the performance comparison (in section 4.1). We also added a discussion on differences with the Sk_p paper (in section 2), and the results of our model in realistic scenarios (section 4.4), which we also explain below.

Thanks for the suggestion on evaluating the model on realistic scenarios. We have been collecting such a dataset for evaluating the model. In particular, we examined development histories in a software company (name elided for anonymity) to extract pairs of consecutive snapshots of code (on a level of functions) which differ by a single variable occurrence. These are indicative of variable misuses; several of which were explicitly pointed out as bugs by code reviewers during the manual code review process. For each snapshot pair (x,y), where x is a function before change and y is the same function after change, we collected all functions from the same file in which function x was present. We expect our model to classify all functions other than x as bug-free. For the function x, we want the model to classify it as buggy, and moreover, localize and repair the bug where the repair is the difference between y and x. In all, we collected 4592 such snapshot pairs. From these, we generated a test dataset of 41672 non-buggy examples and 4592 buggy examples. We trained the pointer model on a training dataset from which we exclude the 4592 files containing the buggy snapshots. When applied on the test dataset, the model achieved true positive rate of 67.3%, classification accuracy of 66.7%, localization accuracy of 21.9% and localization+repair accuracy of 15.8%. In all, on this dataset, our model could localize and repair 727 variable misuse instances. These are promising results on data collected from real developer histories. We have also added a subsection 4.4 to describe the evaluation and the results.

Relationship with Sk_p and applying the model on their dataset:
We would like to point out that Sk_p does not perform direct localization, instead it performs an enumerative search (for potential bug locations) which we eschew. Given a program, it considers each statement individually. For each program statement s_i, it considers the previous program statement s_{i-1} and the following statement s_{i+1} as inputs to an encoder while the decoder generates the full statement s’_i that should be present in between the two statements. It performs this prediction for each statement individually and then reports discrepancies as repairs. Note that it can only produce full statements as repairs unlike our approach for predicting a single variable usage repair. Moreover, similar to the DeepBugs approach, it would be difficult for the model to predict repairs that include variables present at two or more lines above/below the buggy variable location. 

Their dataset is unfortunately not useful for our evaluation for the following reasons:
1. Our model is designed for the variable misuse problem, which typically occurs when programmers copy-paste code fragments and forget to change certain variables. The sk_p dataset is coming from small student programs (5-10 LOC) submitted for MOOC exercises, which will likely not have many variable misuse bugs.
2. The Sk_p model (similar to the SynFix paper) exploits the fact that many students are solving the same programming problem and likely will write similar solutions which can then be used to train the models. In our case, we are generalizing from programs written by developers for different tasks.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1gAbwMlAX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Baselines</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByloJ20qtm&amp;noteId=H1gAbwMlAX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1014 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1014 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">It seems that the real dataset has a different distributions than the synthetic bugs dataset based on Py150. Did you observe similar improvements over the baselines on it?

In general, all current bug-finding research suffers from having no sense of "recall" of whether it discovers only very few bugs or it discovers most of the bugs from a certain class. Since the paper aims to look into such issues, this would be good to say what is happening on real data and also why  10% of the samples were chosen as being buggy, while the frequency of the bug is likely much lower. And localization accuracy is in the 21.9% range - still low, do the anomaly-based baseline techniques get to similar numbers?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJg8Qn6gAQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByloJ20qtm&amp;noteId=SJg8Qn6gAQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1014 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1014 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the suggestion to compare the results of the joint model on the real dataset (Section 4.4) with the enumerative baseline. The repair-only model (underlying the enumerative baseline) from Section 4.1 is trained on the Py150 dataset. For a fair comparison, we have started training the repair-only model on the dataset used for training the joint model in Section 4.4. We will report the results (analogous to Table 1) as soon as the model training is finished in a few days. However, note that the issues with enumerative approaches are fundamental and independent of the choice of datasets. 

In this paper, our aim is to develop new models that can better capture certain family of program repair tasks (such as VarMisuse) and improve upon previous enumerative approaches on datasets previously used in the literature.

Further, in order to evaluate how well the model performs on realistic scenarios, we created the dataset used in Section 4.4. Note that the dataset was created by capturing pairs of consecutive snapshots of functions from development histories that differ by a single variable occurrence. We then also include other functions in those files that weren’t changed as sources of correct functions. We did not intentionally set the percentage of buggy samples to 10%, but it was an artifact of the way we created our dataset by including non-buggy functions from the files that differed by one variable change in consecutive snapshots. The rationale behind this procedure was that it estimates when given files with VarMisuse bugs, in how many cases can the model learn to classify the faulty functions (from among all functions in the file) as faulty and further localize and repair the bugs in those faulty functions.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>