<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Recurrent Experience Replay in Distributed Reinforcement Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Recurrent Experience Replay in Distributed Reinforcement Learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=r1lyTjAqYX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Recurrent Experience Replay in Distributed Reinforcement Learning" />
      <meta name="og:description" content="Building on the recent successes of distributed training of RL agents, in this paper we investigate the training of RNN-based RL agents from experience replay. We investigate the effects of..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_r1lyTjAqYX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Recurrent Experience Replay in Distributed Reinforcement Learning</a> <a class="note_content_pdf" href="/pdf?id=r1lyTjAqYX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019recurrent,    &#10;title={Recurrent Experience Replay in Distributed Reinforcement Learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=r1lyTjAqYX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Building on the recent successes of distributed training of RL agents, in this paper we investigate the training of RNN-based RL agents from experience replay. We investigate the effects of parameter lag resulting in representational drift and recurrent state staleness and empirically derive an improved training strategy. Using a single network architecture and fixed set of hyper-parameters, the resulting agent, Recurrent Replay Distributed DQN, triples the previous state of the art on Atari-57, and surpasses the state of the art on DMLab-30. R2D2 is the first agent to exceed human-level performance in 52 of the 57 Atari games.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">RNN, LSTM, experience replay, distributed training, reinforcement learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Investigation on combining recurrent neural networks and experience replay leading to state-of-the-art agent on both Atari-57 and DMLab-30 using single set of hyper-parameters.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">10 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Ske9lOYp27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Rrecurrent NNs in distributted RL settings as a clear improvement of the feed-forward NN variations in partially observed environments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lyTjAqYX&amp;noteId=Ske9lOYp27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper765 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper765 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper investigates the use of recurrent NNs in distributted RL settings as a clear improvement of the feed-forward NN variations in partially observed environments. The authors present "R2DR" algorithm as a A+B approach from previous works (actually, R2D2 is an Ape-X-like agent using LTSM), as well as an empirical study of a number of ways for training RNN from replay in terms of the effects of parameter lag (and potential alleviating actions) and sample-afficiency. The results presented show impressive performance in Atari-57 and DMLab-30 benchmarks.

In summary, this is a very nice paper in which the authors attack a challenging task and empirically confirm that RNN agents generalise far better when scaling up through parallelisation and distributed training allows them to benefit from huge experience. The results obtained in ALE and DMLab improves significantly upon the SOTA works, showing that the trend-line in those benchmarks seem to have been broken. 

Furthermore, the paper presents their approach/analyses in a well-structured manner and sufficient clarity to retrace the essential contribution. The background and results are well-contextualised with relevant related work. 

My only major comments are that I’m a bit skeptical about the lack of a more thorough (theoretical) analysis supporting their empirical findings (what gives me food for thought is that LSTM helps that much on even fully observable games such as Ms. Pacman); and the usual caveats regarding evaluation: evaluation conditions aren't well standardized so the different systems (Ape-X, IMPALA, Reactor, Rainbow, AC3 Gorilla, C51, etc.) aren't all comparable. These sort of papers would benefit from a more formal/comprehensive evaluation by means of an explicit enumeration of all the dimensions relevant for their analysis: the data, the knowledge, the software, the hardware, manipulation, computation and, of course, performance, etc. However only some of then are (partially) provided.   

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1lgURl52X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The proposed RL agent leads to interesting results but the technical details need to be clarified</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lyTjAqYX&amp;noteId=r1lgURl52X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper765 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper765 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: 
Leveraging on recent advances on distributed training of RL agents, the paper proposes the analysis of RNN-based RL agents with experience replay (i.e., integrating the time dependencies through RNN). Precisely, the authors empirically compare a state-of-the-art training strategy (called zero start state) with three proposed training strategies (namely; zero-state with burn-in, stored-state and stored-state with burn-in). By comparing these different strategies through a proposed metric (Q-value discrepancy), the authors conclude on the effectiveness of the stored-state with burn-in strategy which they consider for the training of their proposed Recurrent Replay Distributed DQN (R2D2) agent. 

The proposed analysis is well-motivated and has lead to significant results w.r.t. the state-of-the-art performances of RL agents.

Major concerns: My major concerns are three-fold:
- The authors do not provide enough details about some "informal" experiments which are sometimes important to convince the reader about the relevance of the suggested insights (e.g., line 3 page 5). Beyond this point, the paper is generally hard to follow and reorganizing some sections (e.g., sec. 2.3 should appear after sec. 3 as it contains a lot of technical details) would certainly make the reading of the paper easier.
- Hausknecht &amp; Stone (2015) have proposed two training strategies (zero-state and Replaying whole episode trajectories see sec. 3 page 3). The authors should clarify why they did not considered the other states in their study.
- The authors present results (mainly, fig. 2 and fig. 3) suggesting that the proposed R2D2 agent outperform the agents Ape-X and IMPALA, where R2D2 is trained using the aforementioned stored-state with burn-in strategy. It is not clear which are the considered training strategies adopted for the (compared to) state-of-the-art agents (Ape-X and IMPALA). The authors should clarify more precisely this point.

Minor concerns: 
- The authors compare the different strategies only in terms of their proposed Q-value discrepancy metric. It could be interesting to consider other metrics in order to evaluate the ability of the methods on common aspects.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkep84aYhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A thorough investigation of using recurrent networks with experience replay, with impressive results on Atari</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lyTjAqYX&amp;noteId=rkep84aYhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper765 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper765 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this submission, the authors investigate using recurrent networks in distributed DQN with prioritized experience replay on the Atari and DMLab benchmarks. They experiment with several strategies to initialize the recurrent state when processing a sub-sequence sampled from the replay buffer: the best one consists in re-using the initial state computed when the sequence was originally played (even if it may now be outdated) but not doing any network update during the first k steps of the sequence (“burn-in” period). Using this scheme with LSTM units on top of traditional convolutional layers, along with a discount factor gamma = 0.997, leads to a significant improvement on Atari over the previous state-of-the-art, and competitive performance on DMLab.

The proposed technique (dubbed R2D2) is not particularly original (it is essentially “just” using RNNs in Ape-X), but experiments are thorough, investigating several important aspects related to recurrence and memory to validate the approach. These findings are definitely quite relevant to anyone using recurrent networks on RL tasks. The results on Atari are particularly impressive and should be of high interest to researchers working on this benchmark. The fact that the same network architecture and hyper-parameters also work pretty well on DMLab is encouraging w.r.t. the generality of the method.

I do have a couple of important concerns though. The first one is that a few potentially important changes were made to the “traditional” settings typically used on Atari, which makes it difficult to perform a fair comparison to previously published results. Using gamma = 0.997 could by itself provide a significant boost, as hinted by results from “Meta-Gradient Reinforcement Learning” (where increasing gamma improved results significantly compared to the usual 0.99). Other potentially impactful changes are the absence of reward clipping (replaced with a rescaling scheme) and episodes not ending with life loss: I am not sure whether these make the task easier or harder, but they certainly change it to some extent (the “despite this” above 5.1 suggests it would be harder, but this is not shown empirically). Fortunately, this concern is partially alleviated by Section 6 that shows feedforward networks do not perform as well as recurrent ones, but this is only verified on 5 games: a full benchmark comparison would have been more reassuring (as well as running R2D2 with more “standard” Atari settings, even if it would mean using different hyper-parameters on DMLab).

The second important issue I see is that the authors do not seem to plan to share their code to reproduce their results. Given how time consuming and costly it is to run such experiments, and all potentially tricky implementation details (especially when dealing with recurrent networks), making this code available would be tremendously helpful to the research community (particularly since this paper claims a new SOTA on Atari). I am not giving too much weight to this issue in my review score since (unfortunately) the ICLR reviewer guidelines do not explicitly mention code sharing as a criterion, but I strongly hope the authors will consider it.

Besides the above, I have a few additional small questions:
1. “We also found no benefit from using the importance weighting that has been typically applied with prioritized replay”: this is potentially surprising since this could be “wrong”, mathematically speaking. Do you think this is because of the lack of stochasticity in the environments? (I know Atari is deterministic, but I am not sure about DMLab)
2. Fig. 3 (left) shows R2D2 struggling on some DMLab tasks. Do you have any idea why? The caption of Table 3 in the Appendix suggests the absence of specific reward clipping may be an issue for some tasks, but have you tried adding it back? I also wonder if maybe training a unique network per task may make DMLab harder, since IMPALA has shown some transfer learning occurring between DMLab tasks? (although the comparison might be to the “deep experts” version of IMPALA — this is not clear in Fig. 3 — in which case this last question would be irrelevant)
3. In Table 1, where do the IMPALA (PBT) numbers on DMLab come from? Looking at the current arxiv version of their paper, their Fig. 4 shows it goes above 70% in mean capped score, while your Table 1 reports only 61.5%. I also can’t find a median score being reported on DMLab in their paper, did you try to compute it from their Fig. 9? And why don’t you report their results on Atari?
4. Table 4’s caption mentions “30 no-op starts” but you actually used the standard “random starts” setting, right? (not a fixed number of 30 no-ops)

And finally a few minor comments / suggestions:
- In the equation at bottom of p. 2, it seems like theta and theta- (the target network) have been accidentally swapped (at least compared to the traditional double DQN formula)
- At top of p. 3 I guess \bar{delta}_i is the mean of the delta_i’s, but then the index i should be removed
- In Fig. 1 (left) please clarify which training phase these stats are computed on (whole training? beginning / middle / end?)
- p. 4, “the true stored recurrent states at each step”: “true” is a bit misleading here as it can be interpreted as “the states one would obtain by re-processing the whole episode from scratch with the current network” =&gt; I would suggest to remove it, or to change it (e.g. “previously”). By the way, I think it would have been interesting to also compare to these states recomputed “from scratch”, since they are the actual ground truth.
- I think you should mention in Table 1’s caption that the PBT IMPALA is a single network trained to solve all tasks
- Typo at bottom of p. 7, “Indeed, Table 1 that even...”</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJgGyvfxqm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Parameter notation mixup</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lyTjAqYX&amp;noteId=HJgGyvfxqm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Oct 2018</span><span class="item">ICLR 2019 Conference Paper765 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I think you have a mixup in the target/online parameter notation at the bottom of page 2. The argmax should be computed using the online parameters and the target value computed using the target parameters.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rylmsYeZ9m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: Parameter notation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lyTjAqYX&amp;noteId=rylmsYeZ9m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper765 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Oct 2018</span><span class="item">ICLR 2019 Conference Paper765 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your interest in our paper, and thank you for catching this bug, we will fix this in a future revision. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SJeCT3qk5Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Minor contribution</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lyTjAqYX&amp;noteId=SJeCT3qk5Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Oct 2018</span><span class="item">ICLR 2019 Conference Paper765 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The paper proposes to replace the feed-forward NN with recurrent version in Distributed RL setting, solving few technical issues (such as how to properly initialize the RNN), and shows improved performance in Atari-57 and DMLab-30 domain.

While the experimental results are impressive, the contribution of the paper is rather straightforward application of the RNN. I'd consider it only a minor improvement over the state-of-the art and would like to see either
a) more thorough analysis why it improves performance in almost-observable MDPs, or
b) more fundamental additional contribution
to consider it for publication.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hkx94FeZ9X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our contributions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lyTjAqYX&amp;noteId=Hkx94FeZ9X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper765 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Oct 2018</span><span class="item">ICLR 2019 Conference Paper765 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your interest in our paper! 

In fact, our work started out as an investigation of a novel algorithmic technique.  
We developed an Ape-X-like agent enhanced with an LSTM as a platform for this research. 

The empirical improvement over previous agents was sufficiently surprising that we considered it worth further study, isolated from confounding factors from unrelated techniques. Towards the aim of understanding R2D2's empirical performance, we contributed a study of the effects of parameter lag on representation drift and recurrent state staleness, potential mitigation strategies, and the memory-dependence in a learned recurrent policy.

This paper focuses on presenting and analyzing effective ways of training a recurrent memory from replay, resulting in a more universal agent achieving SOTA results across multiple environment suites with a single set of parameters. While more analysis remains to be done to fully understand the role of memory in RL, we believe the above contributions provide valuable insights to the RL community.  </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ryxcW6xRYm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Questions on evaluation conditions and sample efficiency</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lyTjAqYX&amp;noteId=ryxcW6xRYm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Sep 2018</span><span class="item">ICLR 2019 Conference Paper765 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Congratulations on these very impressive results! I have a few comments and questions on evaluation conditions and sample efficiency:

1.  Regarding no-op vs. human starts: Table 1 seems to use median scores from Ape-X in the human start condition (358.1%, vs. the 358% listed in Table 1 of the Ape-X paper). But only no-op scores are listed in the Appendix of this paper, and it also seems plausible that you are referring to the 20 hr no-op score of Ape-X.  Could you clarify what evaluation condition you're using in this paper, and which condition you are comparing against in Ape-X?

2.Could you clarify how many frames are used per game by R2D2? A naive conversion from Ape-X to R2D2, using Table 1 from the Ape-X paper, accounting for the reduced wall time (3 vs 5 days) and number of actors (360 vs. 256) seems to suggest roughly 10 billion frames per game, but I'm not sure if I'm missing something that would affect these calculations. Also, it would be interesting to know just how severe the remaining sample efficiency gap mentioned in the conclusion is - median scores for Rainbow vs. R2D2 with comparable frames might be illustrative, for example.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJeijL3J5m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: Questions on evaluation conditions and sample efficiency</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lyTjAqYX&amp;noteId=BJeijL3J5m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper765 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Oct 2018</span><span class="item">ICLR 2019 Conference Paper765 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1. Thank you for catching this, we inadvertently included the human-start values for Ape-X instead of the no-op starts evaluation results. All mean/median values throughout the paper are referring to 30 no-op evaluation conditions. The correct values for Ape-X should be mean=434.1%,  median=1695.6%. The other values in the table correctly refer to 30 no-op results for the other agents.

2. On Atari, the R2D2 agent consumes approximately 5.7B environment frames per day (with 256 actors, each running at approximately 260 fps), i.e. approximately 17B for the reported 3-day version. On DMLab, the frame rate is around half of the Atari frame rate, ~2.65B env frames per day, i.e. ~10.6B frames for the entire 4-day run whose results we report. Regarding a comparison with Rainbow in terms of environment frames, we agree this is informative and will include such a comparison when revisions open up.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJenv9e497" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for clarifying</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1lyTjAqYX&amp;noteId=SJenv9e497"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Oct 2018</span><span class="item">ICLR 2019 Conference Paper765 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the clarification on frames + no-op/human starts! I look forward to the Rainbow comparison, as well. P.S. I assume you meant median=434.1, mean=1659.6 :) </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>