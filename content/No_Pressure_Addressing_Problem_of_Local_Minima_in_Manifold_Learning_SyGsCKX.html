<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>No Pressure! Addressing Problem of Local Minima in Manifold Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="No Pressure! Addressing Problem of Local Minima in Manifold Learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Sy4G8sC9KX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="No Pressure! Addressing Problem of Local Minima in Manifold Learning" />
      <meta name="og:description" content="Nonlinear embedding manifold learning methods provide invaluable visual in- sights into a structure of high-dimensional data. However, due to a complicated nonlinear objective function, these..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Sy4G8sC9KX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>No Pressure! Addressing Problem of Local Minima in Manifold Learning</a> <a class="note_content_pdf" href="/pdf?id=Sy4G8sC9KX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019no,    &#10;title={No Pressure! Addressing Problem of Local Minima in Manifold Learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Sy4G8sC9KX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Nonlinear embedding manifold learning methods provide invaluable visual in- sights into a structure of high-dimensional data. However, due to a complicated nonlinear objective function, these methods can easily get stuck in local minima and their embedding quality can be poor. We propose a natural extension to several manifold learning methods aimed at identifying pressured points, i.e. points that stuck in the poor local minima and have poor embedding quality. We show that the pressure can be decreased by temporarily allowing these points to make use of an extra dimension in the embedding space. In the evaluation we show that our method is able to improve the objective function value of existing methods even after they get stuck in a poor local minimum.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">manifold learning, nonlinear optimization, local minima</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BylJ1QPGT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting paper about a useful optimization shortcut, but too brief experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Sy4G8sC9KX&amp;noteId=BylJ1QPGT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper157 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper157 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">(I have reviewed an earlier version of this paper before for another venue.)

Description:

This paper proposes a method to identify, from a manifold embedding solution, data points that may be stuck in a bad local minimum. The idea is to temporarily allow such points an extra embedding dimension, and evaluate how much the point would like to move away from its current position along that dimension. Examples are shown for Stochastic Neighbor Embedding (SNE), t-distributed SNE, and Elastic Embedding. Moreover, the paper proposes to temporarily provide such points the extra dimension during optimization, so that regularization makes the extra dimensions vanish at the end.

The identification of pressured points is demonstrated for COIL-20 and MNIST data; for the same data sets it is demonstrated that the optimization with temporary extra dimensions for pressured points speeds up convergence for the methods.

Evaluation:

Better optimization and avoiding local minima are important topics in dimensionality reduction, hence the paper addresses an important issue. The paper is well written, and methodologically it seems sound, although it seems the analyses are somewhat specific to the particular dimensionality reduction methods considered, and was is not clear enough if the same can be done for any method (or if not for any method, for which types of cost functions?).

The concept of pressure seems to me to be related to computing the contributions of individual data items in decomposable (or approximately decomposable) objective functions. Decomposition of neighbor embedding cost functions to individual point contributions is used in evaluation frameworks like CheckViz of Lespinats and Aupetit (2011) to identify local areas in dimensionality reduction that contain distortion: such decomposition simply takes advantage of the fact that neighbor embedding cost functions are typically sums over pairwise terms, although it may not take into account roles of the data items in normalization terms of distributions.
As a difference to evaluation frameworks like CheckViz of Lespinats and Aupetit (2011), authors state that some cost functions are not easily decomposed, and that large objective function alone does not indicate being stuck in a local minimum. However,
- The examples of SNE/t-SNE objective functions seem a bit weak, though - aren't they composed of a sum of pairwise terms, where each pairwise term is complicated but is still mostly dominated by corresponding pairwise distance?
- It is unclear how often, in neighbor embedding applications, the optimized solution would involve a pairwise term that has a high cost but is *not* be stuck in a local minimum. In methods such as SNE, a local objective function value directly denotes difference from gold standard neighborhoods, which are achievable in a high enough dimensional space assuming the original neighborhoods arise from high-dim. coordinates; thus any point having a nonzero local cost should have "pressure" to improve.
- Authors mention that a point could be "on its way" to a local minimum: ok, but this should primarily happen during optimization, not at the end of it? (if so, clarify).
- No experimental comparisons to other methods (like CheckViz) are done.


For the demonstration of identifying pressured points for embedding quality analysis, it is somewhat unclear how strong conclusions can be drawn, since there is no gold standard and no comparison to any other way of identifying badly optimized points. Authors do describe the findings, but it is unclear whether this method yields better identification than e.g. CheckViz or similar approaches.

For the optimization experiments, two data sets is a too small amount, and differences in the final output quality are not visually very strong; however, the speed advantage does seem notable and useful.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJg3oQvc2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The authors propose a method to improve stability/performance of manifold learning algorithms like SNE and t-SNE. However, the method could do with more extensive empirical validation.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Sy4G8sC9KX&amp;noteId=SJg3oQvc2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper157 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper157 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary of contributions:
The authors carefully study the objective function used in popular nonlinear embedding (or manifold learning) algorithms, and identify a specific failure mode that they call "pressure points". This arises due to conflicting terms in the objective function, and a simple derivative check reveals which data points are "pressured" and which are not. This also motivates two modifications of the original algorithms to check for data points that might have sub-optimal embeddings, and appropriately adjust them by using an extra auxiliary dimension that helps gradient descent over the objective bypass minima.

Evaluation Criteria:
The problem is significant, and as far as I know the approach is novel. However, the authors do not provide enough evidence that the method improves upon existing approaches.

Comments:

The "pressure points visualization" results in Section 4 are nice but it is unclear how useful this technique is here. One can detect poorly clustered points presumably by other techniques too (e.g. do a k-means clustering and detect points far away from cluster centers).

The proposed optimization method (Alg 1) works for only 2 out of the 3 studied algorithms (and not for t-SNE, which is arguably the most popular of the three). So it is unclear whether the idea generalizes at all.

The weakest part of the paper, in my opinion, are the empirical evaluations. Small, not-very-challenging datasets (Coil-20 and MNIST); no real quantitative benefits; unclear how exactly the new approach improves upon previous visualization methods. Figures 4 and 5 show very little improvement in objective function value over standard EE and standard SNE. (For SNE, the improvement is in the second decimal point).</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1eFaMoF27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A new optimizaiton method for NLE, but unclear and not convincing so far</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Sy4G8sC9KX&amp;noteId=S1eFaMoF27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper157 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper157 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a new optimization method for minimizing nonlinear embedding costs such as Elastic Embedding and Stochastic Neighbor Embedding. The proposed method is based on identifying and removing particular points in the embedding space called pressured points. The presented method is tested on two image data sets COIL-20 and MNIST and is claimed to be able to improve the objective function value.

There are several things are unclear or not convincing.

1) There is no definition of pressured points. The word "pressure" is used before its definition. Since it is not so obvious to direct borrow from physics or everyday English, a generic definition in the NLE context is needed but lacking here. Now it is defined only in individual NLE methods. Even with the pressure definition in EE and SNE, I still don't understand how to threshold the pressure to identify pressured points.

2) The results are not convincing. 
 2.1) The authors should demonstrate the benefits in using such a new optimization method. For example, is it faster or giving lower NLE costs?
 2.2) Objective vs. iterations are not convincing, because in PP each iteration is more expensive. Objective vs. time would be better. In Figures 4 and 5, it is hard to see PP achieves lower costs than SD.
 2.3) There are only two data sets,, where both are images. Moreover, for the large data set MNIST, only EE results are shown. For MNIST visualizations, it is hard to say the PP result is better than SD if the colors are removed.
 2.4) There exist many other optimization methods. Just defeating SD is insufficient. So far it could be interesting only to readers who are familiar with SD.

3) The authors seem not to be aware of saddle points. In optimization, an algorithm can get stuck in two kinds of stationary points: local optima and saddle points. There is no checking of local optima in the work. Therefore many claims which do not consider saddle points are wrong or unsuitable.

4) There is no proof that Algorithm 1 will converge. Moreover, the last clause "until convergence" is unclear: what is the convergence criterion?

5) The title is confusing. The word "pressure" is different our common understanding and is not widely used in machine learning or ICLR literature. Moreover, at the end the PP embedding still has pressure (maybe just no pressured points). So the authors should consider a more precise and pertinent title.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>