<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SyVuRiC5K7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR..." />
      <meta name="og:description" content="The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SyVuRiC5K7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING</a> <a class="note_content_pdf" href="/pdf?id=SyVuRiC5K7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019learning,    &#10;title={LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SyVuRiC5K7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">few-shot learning, meta-learning, label propagation, manifold learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">13 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BJgohCDj6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reproducibility issues</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyVuRiC5K7&amp;noteId=BJgohCDj6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~anon_ml_reviewer1" class="profile-link">anon ml reviewer</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper911 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Since the paper has not provided a reproducible code, based on my implementation in PyTorch 1.0.0.dev20181105, unfortunately I could not reproduce their results on Mini-Imagenet dataset for 5-way, 1-shot and 5-shot scenarios. Using the exact mentioned hyper-parameters, the model didn't learn much in the end-to-end manner and the test accuracy for 5-way, 1-shot was around 25% trained in 50,000 episodes and learning-rate is halved every 10,000 episodes. Instead I pretrained the emebedding in the train set and decreased alpha (label propagation) to 0.9 then it started learning better. 

The best accuracy I could get for the 5-way, 1-shot case (trained with 5-way, 1-shot so no higher-shot) is with alpha=0.6 and it is 47.93% (+/- 1.14% as the 95% confidence interval) which is much lower than the claimed 53.75%. More precisely, I trained with batch-size=15 as described in RelationNet, k in knn=20, Xavier initialization of Conv layers, BatchNorm unit weight initialization and zero bias, zero-mean normal initialization with std 0.01 for Linear layers and unit bias, and then tested with 15 query examples where results were averaged over 600 randomly generated episodes from the test set.

The paper did not mention any pre-processing step, so I only resized images to 84 by 84 and normalized the mini-imagenet data using imagenet mean and std.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJxOdw3sTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Implementation details. Code will be released soon.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyVuRiC5K7&amp;noteId=rJxOdw3sTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper911 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper911 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the comment and interest about our paper. 
According to the blind review policy, we can not release the code at this moment. We will release our code and the trained model as soon as the review process ends. Meanwhile, we have sent an email to the program chairs to check if it is allowed to release the code anonymously.  We will share the code upon approval.

We are sure about the reproducibility of the results shown in our paper. And in order to ensure the reproducibility, we ran the test procedure 10 times (each with 600 randomly generated episodes) and reported the average results to avoid accidentally high results. We are not sure if you have reproduced the result as outlined in [1]. If not, we sincerely hope you first try to reproduce the baseline method [1], so you may be closer to the right implementation. It took us quite a while to reproduce [1] even the code has been released. 
Nevertheless, we would like to provide more details below which could be useful for you to reproduce the results of our paper.

(1) Our implementation is based on Tensorflow 1.3+, and we also tested on Pytorch 0.4.0. There is only a slight accuracy difference.
(2) The reason why your results only achieved 25% could be caused by value issues such as divided by zero. Sincerely hope you could double check your code and please make sure you have added an epsilon whenever you call a divide operation. 
(3) Our model is learned end-to-end from scratch, and no pretrain is needed. We did not see your code, but we reckon you did not use the validation set to decide the early stopping iteration, which is commonly used in few-shot learning, such as Prototypical networks. Please use this practice if it is the case.
(4) The detailed hyperparameters are: alpha=0.99, k=20, query=15, lr=0.001 and halved every 10,000 episodes for at most 100,000 episodes. 
(5) Network architecture details: feature extraction module is exactly the same as Prototypical networks [1], graph construction module is described in Figure 4 of Appendix A. Note that BatchNorm is applied only in Conv layers. In Figure 4, there is no Relu activation after FC layer2. More training details: we use Tensorflow default initialization, BatchNorm with default parameters: decay=0.999 and epsilon=0.001. 
(6) As to preprocessing, for miniImagenet, we follow Prototypical networks [1] while for tieredImagnet we follow Ren et al. [2].

We have endeavored our best to 'guess' what mistakes you may have made, but there could be other issues that we are unable to enumerate. 
We highly suggest that a basic starting point is to reproduce the results of Prototypical networks.  Below we provide a few good implementation codes of some related papers. 
Prototypical networks: 
	<a href="https://github.com/jakesnell/prototypical-networks" target="_blank" rel="nofollow">https://github.com/jakesnell/prototypical-networks</a>
	https://github.com/cyvius96/prototypical-network-pytorch
tieredImagenet: 
	https://github.com/renmengye/few-shot-ssl-public


[1] Snell, Jake, Kevin Swersky, and Richard Zemel. "Prototypical networks for few-shot learning." NIPS. 2017.
[2] Ren, Mengye, et al. "Meta-learning for semi-supervised few-shot classification." ICLR. 2018.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkxfh9RTTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Problem persisted</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyVuRiC5K7&amp;noteId=rkxfh9RTTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~anon_ml_reviewer1" class="profile-link">anon ml reviewer</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper911 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the clarifications! I have already used Snell's code for the embedding and admit baseline implementation is tricky. I have not used Snell's pre-processing as the github repository contains the pre-processing for omniglot only, instead used the RelationNet data pre-processing and loading for mini-imagenet for few corrections such as the normalization mean, std from imagenet <a href="https://github.com/floodsung/LearningToCompare_FSL" target="_blank" rel="nofollow">https://github.com/floodsung/LearningToCompare_FSL</a>

I have double checked the hyper-parameters and accessed the values in debug mode extensively and wherever division happens, I added an epsilon of 1e-6 or 1e-8 including the element-wise division of f_phi(x_i) / (sigma_i + epsilon) and in the computation of D^{-1/2} where I previously used torch.rsqrt() and replaced it with 1.0 / (w.sum(1) + epsilon).sqrt(), (where w is the graph knn matrix with applied masked k_max=20 for each row and zero everywhere else). However, the issue persisted with alpha=0.99 and model does not learn. As said previously, changing alpha to 0.9 or even lower 0.6 helped learning a lot but the final accuracy for 5-way, 1-shot case remained around the previous result of 47.9%. 

I hope ICLR authorities take anonymous code release into the considerations as this is a major barrier for assessing the reproducibility.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_Skx7vDii3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting empirically</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyVuRiC5K7&amp;noteId=Skx7vDii3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper911 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper911 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes to address few-shot learning in a transductive way by learning a label propagation model in an end-to-end manner.  Semi-supervised few-shot learning is important considering the limitation of the very few labeled instances. This is an interesting work. 

The merits of this paper lie in the following aspects: (1) It is the first to learn label propagation for transductive few-shot learning. (2) The proposed approach produced effective empirical results.

The drawbacks  of the work include the following: (1) There is not much technical contribution. It merely just puts the CNN representation learning and the label propagation together to perform end-to-end learning. Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.  (2) Empirically, it seems TPN achieved very small improvements over the very baseline label propagation.  Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature. For example,  on miniImageNet, TADAM(Oreshkin et al, 2018) reported 58.5 (1-shot) and 76.7(5-shot), which are way better than the results reported in this work. This is a major concern.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1x4ca-chQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Novel idea, but important details and deeper analysis are missing</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyVuRiC5K7&amp;noteId=S1x4ca-chQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper911 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper911 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary
This paper proposes a meta-learning framework that leverages unlabeled data by learning the graph-based label propogation in an end-to-end manner.  The proposed approaches are evaluated on two few-shot datasets and achieves the state-of-the-art results. 

Pros. 
-This paper is well-motivated. Studying label propagation in the meta-learning setting is interesting and novel. Intuitively, transductive label propagation should improve supervised learning when the number of labeled instances is low. 
-The empirical results show improvement over the baselines, which are expected. 

Cons.
-Some technical details  are missing. In Section 3.2.2, the authors only explain how they learn example-based \sigma, but details on how to make graph construction end-to-end trainable are missing. Constructing the full weight matrix requires the whole dataset as input and selecting k-nearest neighbor is a non-differentiable operation. Can you give more explanations?
-Does episode training help label propagation? How about the results of label propagation without the episode training? 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJx6UQbfhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Transductive few-shot by meta-learning to propagate labels for . Solid work.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyVuRiC5K7&amp;noteId=HJx6UQbfhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper911 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">27 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper911 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper studies few-host learning in a transductive setting: using meta learning to learn to propagate labels from training samples to test samples. 

There is nothing strikingly novel in this work, using unlabeled test samples in a transductive way seem to help slightly. However, the paper does cover a setup that I am not aware that was studied before. The paper is written clearly, and the experiments seem solid. 

Comments: 
-- What can be said about how computationally demanding the procedure is? running label propagation within meta learning might be too costly. 
-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)
-- solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization 




</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SklAweC0qX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Experiments and Methods</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyVuRiC5K7&amp;noteId=SklAweC0qX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Oct 2018</span><span class="item">ICLR 2019 Conference Paper911 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">This paper tried to introduce transductive networks for few-shot learning. 
I want to know about the experiments here especially about the transductive process that was done for query set. I hope that you can make it clear what was specifically performed in the batch of query set to help you gain the performance?
Do you also have any results if you increase the number of query set will affect your performance too? Because I believe this is the contribution that you can have as well from your work. 

One more thing:
I have a question in your results for semisupervised few-shot learning. 
I read the experiments in semisupervised few-shot learning protocol[1] that there are distractor classes in which I did not see this thing in your paper.
I intuitively think that this method might be appropriate for the unlabeled data without many outliers/distractors.
Do you also have the experiments about this before? It is  fine if you also show the drawback of this method, so the improvements can be proposed in the future to tackle that problem.




[1] Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum, Hugo
Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classification. International Conference on Learning Representations, 2018.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1l4b54ijm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to experiments and methods</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyVuRiC5K7&amp;noteId=H1l4b54ijm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper911 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">22 Oct 2018</span><span class="item">ICLR 2019 Conference Paper911 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the comments.

For each episode, we first utilize both the support set and query set to construct the graph structure. Then, label propagation is performed according to the graph information to get all query set labels. The performance gain comes from the fact that we share information among all query examples and learn to propagate labels. In contrast, inductive methods predict query examples one by one, which does not enjoy this benefit.

As to query set number experiments, please refer to Appendix B.2 for detailed information.

For distractor classes, this is not the main focus of our paper. However, in order to explore the extent of our method, we performed experiments in the presence of distractor classes (same setting as [1]). The results are shown below:
Model                                  mini-5way1shot     mini-5way5shot    tiered-5way1shot    tiered-5way5shot
Soft k-Mean [1]                     48.70+/-0.32            63.55+/-0.28           49.88+/-0.52            68.32+/-0.22
Soft k-Mean+Cluster [1]      48.86+/-0.32            61.27+/-0.24           51.36+/-0.31            67.56+/-0.10
Masked Soft k-Means [1]    49.04+/-0.31            62.96+/-0.14           51.38+/-0.38            69.08+/-0.25
TPN-semi (Ours)                   50.43+/-0.84            64.95+/-0.73           53.45+/-0.93            69.93+/-0.80

It can be seen that our TPN-semi algorithm outperforms [1] in all cases, although our method is not specifically designed for the distractor-classes problem.
We believe with care design, the performance of our method will continue to increase. This will be the future work.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ryekRHiecQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>About the experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyVuRiC5K7&amp;noteId=ryekRHiecQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Oct 2018</span><span class="item">ICLR 2019 Conference Paper911 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">It is interesting that this paper use a label propagation way to solve the low-data testing problem. However, the state-of-art few-shot(zero-shot) methods: Relation Net and Prototypical Net used both minImageNet, Omniglot for few-shot Testing and CUB-200 for zero-shot. So what's your results on Omniglot since you follow the idea of Prototypical Net. In addition, is it possible that your proposed TPN can deal with zero-shot problems since a general few-shot framework can Easily extend to cope with zero-shot problems?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkli_tYd5Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to the experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyVuRiC5K7&amp;noteId=Bkli_tYd5Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper911 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Oct 2018</span><span class="item">ICLR 2019 Conference Paper911 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the comments.
The state-of-the-art performance on Omniglot is quite high (&gt;99% except for 20way-1shot setting), which means this problem is nearly solved. Also, there is a tendency that recent high-quality papers do not report results on Omniglot, such as TADAM [1] (NIPS2018), Delta-encoder [2] (NIPS2018), LEO [3] (ICLR19 submission). For the 20way-1shot setting, we compare our TPN results with Relation Net and Prototypical Net as follows:
					20way-1shot
Prototypical Net           96.00
Relation Net                  97.60
TPN				        98.03

Although zero-shot learning is not our focus, TPN can be easily adapted to zero-shot setting. The modification is similar to Prototypical network or Relation Network. First, a function g can be used to map class-level semantic feature into the same space of visual feature. Then, we can construct graph structure using both features and perform label propagation as in few-shot setting. 

[1] Oreshkin, Boris N., Alexandre Lacoste, and Pau Rodriguez. "TADAM: Task dependent adaptive metric for improved few-shot learning." NIPS2018
[2] Schwartz, Eli, et al. "Delta-encoder: an effective sample synthesis method for few-shot object recognition." NIPS2018
[3] Anonymous, "Meta-Learning with Latent Embedding Optimization." ICLR2019 submission. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1xbcLqkcQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Pointer to a Related Work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyVuRiC5K7&amp;noteId=H1xbcLqkcQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Oct 2018</span><span class="item">ICLR 2019 Conference Paper911 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value"> The paper looks very interesting as transductive approaches are powerful for metric learning and semi-supervised learning. And, learning to transductive learn is an interesting direction. I would like to point a related work which authors probably missed which performs metric learning/transfer learning using transduction: <a href="https://papers.nips.cc/paper/6360-learning-transferrable-representations-for-unsupervised-domain-adaptation" target="_blank" rel="nofollow">https://papers.nips.cc/paper/6360-learning-transferrable-representations-for-unsupervised-domain-adaptation</a> </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryeo4LvecQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for pointing out related work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyVuRiC5K7&amp;noteId=ryeo4LvecQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper911 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Oct 2018</span><span class="item">ICLR 2019 Conference Paper911 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for pointing out the related work. We would like to include this reference to our manuscript in the next version. 

Our paper and the mentioned paper share the same idea of using metric learning and transduction. However, the target tasks are different. We focus on few-shot learning and meta-learning while the mentioned paper deals with unsupervised domain adaptation. This distinction leads to different algorithm designs: we learn to propagate labels while the mentioned paper proposes the transduction and adaptation steps.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rygAusX19m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reasonable and efficacy idea</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SyVuRiC5K7&amp;noteId=rygAusX19m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Oct 2018</span><span class="item">ICLR 2019 Conference Paper911 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">This paper proposes a novel meta-learning framework, which aims to propagate labels from labeled instances to unlabeled test instances. This framework learns a graph construction module, exploiting the manifold structure in the data. The idea is reasonable and efficacy, and the experiments are comprehensive.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>