<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Soft Q-Learning with Mutual-Information Regularization | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Soft Q-Learning with Mutual-Information Regularization" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HyEtjoCqFX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Soft Q-Learning with Mutual-Information Regularization" />
      <meta name="og:description" content="We propose a reinforcement learning (RL) algorithm that uses mutual-information regularization to optimize the prior action distribution for better performance and exploration. Entropy-based..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HyEtjoCqFX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Soft Q-Learning with Mutual-Information Regularization</a> <a class="note_content_pdf" href="/pdf?id=HyEtjoCqFX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019soft,    &#10;title={Soft Q-Learning with Mutual-Information Regularization},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HyEtjoCqFX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We propose a reinforcement learning (RL) algorithm that uses mutual-information regularization to optimize the prior action distribution for better performance and exploration. Entropy-based regularization has previously been shown to improve both exploration and robustness in challenging sequential decision-making tasks. It does so by encouraging policies to put probability mass on all actions. However, entropy regularization might be undesirable when actions have significantly different importance. In this paper, we propose a theoretically motivated framework that dynamically weights the importance of actions by using the mutual-information. In particular, we express the RL problem as an inference problem where the prior probability distribution over actions is subject to optimization. We show that the prior optimization introduces a mutual-information regularizer in the RL objective. This regularizer encourages the policy to be close to a non-uniform distribution that assigns higher probability mass to more important actions. We empirically demonstrate that our method significantly improves over entropy regularization methods, attaining state-of-the-art performance. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">reinforcement learning, regularization, entropy, mutual information</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_ryl__Ymjh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea, more experimental results needed</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyEtjoCqFX&amp;noteId=ryl__Ymjh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper643 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper643 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">** Summary: **

The authors use the reformulation of RL as inference and propose to learn the prior policy. The novelty lies in learning a state-independent prior (instead of a state-dependent one) that can help exploration in the presence of universally unnecessary actions. They derive an equivalence to regularizing the mutual information between states and actions.

** Quality: **
The paper is mathematically detailed and correct.

** Clarity: **
The paper is sufficiently easy to follow and explains all the necessary background.

** Originality &amp; Significance: **
The paper proposes a novel idea: Using a learned state-independent prior as opposed to using a learned state-dependent prior. While not a big change in terms of mathematical theory, this could lead to positive and interesting results empirically for exploration. Indeed they show promising results on Atari games: It is easy to see how Atari games could benefit as they have up to 18 different actions, many of which are redundant. 

My two main points where I think the paper could improve are:
- More experimental results, in particular, how strong are the negative effects of MIRL if we have actions that are important, but have a lower probability in the stationary action distribution?
- A related work section comparing their approach to the many recent similar papers in Maximum Entropy RL</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1lIeTW537" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Simple approach that appears to work well</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyEtjoCqFX&amp;noteId=B1lIeTW537"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper643 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper643 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This work introduces SoftQ with a learned, state-independent prior. One derivation of this objective follows standard approaches from an RL as inference to derive the ELBO objective.

A more novel view derived here connects this objective with the rate-distortion problem to view the objective as an RL objective subject to a constraint on the mutual information between the state and action distribution.

They also outline a practical off-policy algorithm for optimizing this objective and compare it with Soft Q Learning (essentially, the same method but with a flat-prior) and DQN. They find that this results in small gains across most Atari games, with big gains for a few games.

This work is well-explained except in one-aspect. The rate-distortion view of the objective is not well-justified. In particular, why is it desirable in the context of RL to constrain this mutual information?

Empirical Deep RL performance is notoriously difficult to test (e.g. Henderson et al., 2017). The hyper-parameters are simply stated here, but no justification is given for how they are chosen / whether the baselines perform better under different choices. Given the gains compared with SoftQ are not that large, this information is important for understanding how much weight to place on the empirical result.

The fact that the prior does not converge in some environments (e.g. Seaquest) is noted, but it seems this bears further discussion.

Overall it appears this work provides:
- An algorithm for Soft Q learning with a learned independent prior
- Moderate evidence for gains compared with a flat prior on Atari.
- A connection with this approach and regularization by constraining the mutual information between state and action distributions.

It could be made a stronger piece of work by showing improvements in domains others than Atari, justifying the choice of regularization more. It would also benefit from positioning this work more clearly in relation to related approaches such as MPO (non-parametric state-dependent prior) and DistRL (state-dependent prior but shared across all games).</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Byx_Yunv37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting, but motivation and experiments need improvements</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyEtjoCqFX&amp;noteId=Byx_Yunv37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper643 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper643 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors take the control-as-inference viewpoint and learn a state-independent prior (which is typically held fixed). They claim that this leads to better exploration when actions have different importance. They relate this objective to a mutual information constrained RL objective in a limiting case. They then propose a practical algorithm, MIRL and compare their algorithm against DQN and Soft Q-learning (SQL) on 19 Atari games and demonstrate improvements over both.

Generally I found the idea interesting and at a high level the deficiency of entropy regularization makes sense. However, I had great trouble understanding the reasoning behind their method and did not find the connection to mutual information helpful. Furthermore, I had a number of questions about the experiments. If the authors can clarify their motivation and reasoning and strengthen the experiments, I'd be happy to raise my score.

In Sec 3.1, why is it sensible to optimize the prior? Can the authors give intuition for maximizing \log p(R = 1) wrt to the prior? This is critical for justifying their approach. Currently, the authors provide a connection to MI, but don't explain why this matters. Does it justify the method? What insight are we supposed to take away from that? 

The experiments could be strengthened by addressing the following:
* What was epsilon during training? Why was epsilon = 0.05 in evaluation? This is quite high compared to previous work, and it makes sense that this would degrade MIRLs performance less than DQN and SQL.
* What is the performance of SQL if we use \rho as the action selector in \epsilon-greedy. This would help understand if the performance gains are due to the impact on the policy or due to the changes in the behavior policy.
* Plotting beta over time
* Comparing the action distributions for SQL and MIRL to understand the impact of the penalty. In general, a deeper analysis of the impact on the policy is important. 
* Are their environments we would expect MIRL to outperform SQL based on your theoretical understanding? Does it?
* How many seeds were run per game?
* How and why were the 19 games selected from the full set?

Comments:

The abstract claims state-of-the-art performance, however, what is actually shown is that MIRL outperforms DQN and SQL.

With a fixed prior, the action prior can be absorbed into the reward (e.g., Levine 2018), so it is of no loss of generality to assume a uniform prior.

Could state that the stationary distribution is assumed to exist and be unique.

In Sec 3.1, why is the prior state independent?

In Sec 3.1, p(R = 1|\tau) is defined to be proportional to exp(\beta \sum_t r_t). Is this well-specified? How would we compute the normalizing constant since p(R = 0 | \tau) is not defined?

Throughout, I suggest that the authors not use the phrases "closed form" and "analytic" for expressions that are in terms of intractable quantities. 

It should be noted that Sec 3.2 Optimal policy for a fixed prior \rho follows from Levine 2018 and others by transforming the fixed prior into a reward bonus.

In Sec 3.2, the last statement does not appear to be necessary for the next subsection. Remove or clarify?

I believe that the connection to MI can be simplified. Plugging in the optimal \rho into Eq 3, we can see that Eq 3 simplifies to \max_\pi E_q[ \sum_t \gamma^t r_t] - (1 - gamma)/\beta MI_p(s, a) where p(s, a) = d^\pi(s) * \pi(a | s) and d^\pi is the discounted state visitation distribution. Thus Eq 3 can be thought of as a lower bound on the MI regularized objective.

In Sec 4, the authors state the main difference between their soft operator and the typical soft operator. What other differences are there? Is that the only one?

Sec 5 references the wrong Haarnoja reference in the first paragraph.

In Sec 5, alpha_beta = 3 * 10^5. Is that correct?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJxlECDns7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Connection to prior work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyEtjoCqFX&amp;noteId=HJxlECDns7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">23 Oct 2018</span><span class="item">ICLR 2019 Conference Paper643 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hello,

Thanks for the paper. I would like to point out a paper from  ICLR2018 that shares similarities in both 

1- The derivations of RL objective from Inference perspective 
2- The resulting objective function for learning the prior 

please see,

Maximum a-Posteriori Policy Optimisaiton
<a href="https://arxiv.org/pdf/1806.06920.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1806.06920.pdf</a>

In the paper above, the mutual information (Or expected KL ) regularized objective is derived in E-step (see equation 7). And the optimal solution is given in (8) when a non parametric variational distribution is used. 

It would be useful if authors discuss the connections and differences.

Thank you,
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkxW2SgCo7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Differences between MPO and our approach</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyEtjoCqFX&amp;noteId=rkxW2SgCo7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper643 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">24 Oct 2018</span><span class="item">ICLR 2019 Conference Paper643 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comment. 

Framing RL as an inference problem has been addressed before in the literature [1,2] and can be done in different ways.  The difference between the variational inference formulation in MPO and our variational inference formulation is the following:
- The policy of the generative model in our case is state-independent (similar to [1]) with the optimal solution being the marginal distribution over actions ([1] does not consider an optimal marginal distribution though). In contrast, in MPO the generative policy is state-dependent and given by the previous-round behavioural policy. 

Importantly, our specific choice of state-dependent variational policy and state-independent generative policy directly leads to a mutual information regularizer. Note that the mutual information is not any expected KL, but a specific expected KL under the assumption of an optimal marginal policy (which is exactly what we model). MPO does not have the notion of an optimal marginal policy (in the sense of a state-independent marginal policy) and therefore the expected KL in MPO is not a mutual information.

In our experimental section we empirically validate that our mutual information regularized objective leads to improvements over soft-q learning (see [1]) where the generative policy is also state-independent but not subject to optimization (but instead given by a uniform distribution). 

We will clarify this point in a revised version of the manuscript.

[1] Levine, S. Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review. arXiv 2018.
[2] Neumann, G. Variational Inference for Policy Search in changing Situations. ICML 2011.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJx6sNdCom" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Question</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyEtjoCqFX&amp;noteId=rJx6sNdCom"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018</span><span class="item">ICLR 2019 Conference Paper643 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for the reply.

Then if MPO use a state-independent generative policy, it will reduce to the proposed algorithm?
I understand that a learned state-independent generative policy is better than a uniform one. My question is that, why state-independent generative policy should be better than state-dependent generative policy as used by MPO?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Skgno2oEhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyEtjoCqFX&amp;noteId=Skgno2oEhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper643 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018</span><span class="item">ICLR 2019 Conference Paper643 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Both our algorithm and MPO can be seen as optimizing the same  evidence lower bound (ELBO).  MPO proposes a general coordinate ascent type optimization in which the ELBO is updated in alternating steps, either with respect to the variational policy or the prior policy (while the other policy is kept fixed). Different design choices for the policies and optimization procedures give rise to different, but related algorithms. This approach is also common in variational inference based policy search and describes a large family of related policy search algorithms (see Deisendroth et al, 2013 for an overview.)

Our algorithm follows recent soft Q-learning algorithms (e.g. Fox et al, 2016, Haarnoja et al. 2017). These algorithms consider the same ELBO, but omit the optimization with respect to the prior policy and only optimize the variational policy pi.  This can be seen as an entropy-regularized version of standard Q-learning algorithms.  When the prior is fixed to be a constant uninformative policy, this procedure reduces to max-entropy policy learning. The algorithm replaces the classic Bellman operator with a soft Bellman-operator to prevent deviations from a state-independent fixed prior policy. Several papers (e.g.Haarnoja et al 2017, Schulman et al 2017 ) have shown that these “softened” algorithms offer advantages over their unsoftened counterparts, in terms of exploration, generalization and composability. Our approach further improves on soft Q-learning (as shown in our Atari experiments) by allowing for optimizing the prior (while still being state-independent). As shown in the paper, this results in a mutual information constraint (rather than a max entropy constraint) on the resulting policy.

So while we follow the same general scheme as soft Q-learning, we do update our prior policy as in the MPO algorithm. However, contrary to MPO, we do not consider the alternating, coordinate descent style optimization. Rather than executing a separate prior maximization step, we solve the ELBO for the optimal prior in the special case of state-independent priors.  We then directly estimate this optimal prior in our algorithm, instead of performing a gradient style update on the ELBO. While it is possible to consider the same class of state-independent priors with MPO, the way in which both algorithms optimize the ELBO will still be different. 

A modified MPO that uses a state-independent generative policy would converge to a solution that is penalized by an optimal marginal policy. However, since the parameter epsilon (that determines the deviation between the variational and the generative policy) is fixed and not scheduled in the course of training, the final solution is still constrained by the marginal policy which is sub-optimal because it is state-independent. This constraint would essentially limit the asymptotic performance of such a modified MPO. Of course, this could be alleviated by setting epsilon to a large value but this would correspond to an ordinary actor critic  approach without any regularization in the policy.

If the prior policy in our algorithm is replaced by a state-dependent prior, the optimal solution for such a prior is the variational policy (i.e. pi) itself. This essentially would eliminate the KL-constraint and reduce our algorithm to standard Q-learning. Q-learning is known to suffer from sample-inefficiency caused by the hard max-operator in the target (this leads to overestimated q-values). This is exactly the problem that was been addressed by soft Q-learning with entropy regularization. 

Deisenroth, M. P., Neumann, G., &amp; Peters, J. (2013). A survey on policy search for robotics. Foundations and Trends® in Robotics, 2(1–2), 1-142.

Schulman, J., Chen, X., &amp; Abbeel, P. (2017). Equivalence between policy gradients and soft q-learning. arXiv preprint arXiv:1704.06440.

Haarnoja, T., Tang, H., Abbeel, P., &amp; Levine, S. (2017). Reinforcement learning with deep energy-based policies. arXiv preprint arXiv:1702.08165.

Fox, Roy, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft updates. UAI (2016).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>