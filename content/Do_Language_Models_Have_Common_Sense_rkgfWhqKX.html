<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Do Language Models Have Common Sense? | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Do Language Models Have Common Sense?" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rkgfWh0qKX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Do Language Models Have Common Sense?" />
      <meta name="og:description" content="It has been argued that current machine learning models do not have commonsense, and therefore must be hard-coded with prior knowledge (Marcus, 2018). Here we show surprising evidence that language..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rkgfWh0qKX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Do Language Models Have Common Sense?</a> <a class="note_content_pdf" href="/pdf?id=rkgfWh0qKX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019do,    &#10;title={Do Language Models Have Common Sense?},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rkgfWh0qKX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">It has been argued that current machine learning models do not have commonsense, and therefore must be hard-coded with prior knowledge (Marcus, 2018). Here we show surprising evidence that language models can already learn to capture certain common sense knowledge. Our key observation is that a language model can compute the probability of any statement, and this probability can be used to evaluate the truthfulness of that statement.  On the Winograd Schema Challenge (Levesque et al., 2011), language models are 11% higher in accuracy than previous state-of-the-art supervised methods. Language models can also be fine-tuned for the task of Mining Commonsense Knowledge on ConceptNet to achieve an F1 score of 0.912 and 0.824, outperforming previous best results (Jastrzebskiet al., 2018).  Further analysis demonstrates that language models can discover unique features of Winograd Schema contexts that decide the correct answers without explicit supervision.</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We present evidence that LMs do capture common sense with state-of-the-art results on both Winograd Schema Challenge and Commonsense Knowledge Mining.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">10 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_H1e8tLW9hm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>some interesting results, but could use more rigor and empirical exploration</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgfWh0qKX&amp;noteId=H1e8tLW9hm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1151 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1151 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper evaluates language models for tasks that involve "commonsense knowledge" such as the Winograd Schema Challenge (WSC), Pronoun Disambiguation Problems (PDP), and commonsense knowledge base completion (KBC). 

Pros:

The approach is relatively simple in that it boils down to just applying language models. 

The results outperform prior work, in some cases by pretty large margins. 

The language models are quite large and it appears that this is the first time that large-scale language models have been applied seriously to the Winograd Schema Challenge (rather than, say, to the NLI version of it in GLUE, to which it is hard to compare these results). 

Some of the additional and ablation experiments are interesting. 


Cons:

While this paper has some nice results, there are some aspects of it that concern me, specifically related to hyperparameter tuning and experimental rigor:

There are three methods given for using an LM to make a prediction: full, full-normalized, and partial. For PDP, full (or perhaps full-normalized?) works best, while for WSC, partial works best. The differences among methods, at least for WSC, are quite large: from 2% to 10% based on Figure 3. I don't see a numerical comparison for PDP, so I'm not sure how these methods compare on it. Since the datasets are so small, there is no train/dev/test split, so how were these decisions made? They seem to be oracle decisions. This is concerning to me, as there is not much explanation given for why one method is better than another method. 

My guess is that the reason why partial works better than full for WSC is because the WSC sentences were constructed such that the words up to and including the ambiguous pronoun were written such that it would be difficult to identify the antecedent of the pronoun. The rest of the sentence would be needed to identify the antecedent. I'll assume for this discussion that the sentence can be divided into three parts x, y, and z, where x is the part before the pronoun, y is the phrase that replaces the pronoun, and z is the part after the pronoun. Then p(z|xy), which is partial scoring, corresponds to p(xyz)/p(xy), which can be viewed as "discounting" or "normalizing for" the probability of putting y in place of the pronoun given the context x. For WSC, I think one of the goals in writing the instances is to make the "true" p(xy) approximately equal for both values of y. The language model will not naturally have this be the case (i.e., that p(xy) is the same for both antecedents), so dividing by p(xy) causes the resulting partial score to account for the natural differences in p(xy) for different antecedents. This could be explored empirically. For example, the authors could compute p(xy) for both alternatives for all PDP and WSC instances and see if the difference (|p(xy_1) - p(xy_2)|, where y_1 and y_2 are the two alternatives) is systematically different between WSC and PDP. Or one could see if p(xy) is greater for the antecedent that is closer to the pronoun position or if it is triggered by some other effects. It could be the case that the PDP instances are not as carefully controlled as the WSC instances and therefore some of the PDP instances may exhibit the situation where the prediction can be made partially based on p(xy). The paper does not give an explanation for why full scoring works better for PDP and chalks it up to noise from the small size of PDP, but I wonder if there could be a good reason for the difference.

The results on KBC are positive, but not super convincing. The method involves fine-tuning pretrained LMs on the KBC training data, the same training data used by prior work. The new result is better than prior work (compared to the "Factorized", the finetuned LM is 2.1% better on the full test set, and 0.3% better on the novelty-based test set), but also uses a lot more unlabeled data than the prior work (if I understand the prior work correctly). It would be more impressive if the LM could use far fewer than the 100K examples for fine-tuning. Also, when discussing that task, the paper says: "During evaluation, a threshold is used to classify low-perplexity and high-perlexity instances as fact and non-fact." How was this threshold chosen?

I also have a concern about the framing of the overall significance of the results. While the results show roughly a 9% absolute improvement on WSC, the accuracies are still far from human performance on the WSC task. The accuracy for the best pretrained ensemble of LMs in this paper is 61.5%, and when training on WSC-oriented training data, it goes up to nearly 64%. But humans get at least 92% on this task. This doesn't mean that the results shouldn't be taken seriously, but it does suggest that we still have a long way to go and that language models may only be learning a fraction of what is needed to solve this task. This, along with my concerns about the experimental rigor expressed above, limits the potential impact of the paper.


Minor issues/questions:

In Sec. 3.1: Why refer to the full scoring strategy as "naive"? Is there some non-empirical reason to choose partial over full?

The use of SQuAD for language modeling data was surprising to me. Why SQuAD? It's only 536 articles from Wikipedia. Why not use all of Wikipedia? Or, if you're concerned about some of the overly-specific language in more domain-specific Wikipedia articles, then you could restrict the dataset to be the 100K most frequently-visited Wikipedia articles or something like that. 

I think it would be helpful to give an example from PDP-60.

Sec. 5.1: How is F_1(n) defined?  I also don't see how a perfect score is 1.0, but maybe it's because I don't understand how F_1(n) is defined.

Sec. 6.1: Why would t range from 1 to n for full scoring? Positions before k are unchanged, right? So q_1 through q_{k-1} would be the same for both, right?

In the final example in Figure 2, I don't understand why "yelled at" is the keyword, rather than "upset". Who determined the special keywords?

I was confused about the keyword detection/retrieval evaluation. How are multi-word keywords handled, like the final example in Figure 2? The caption of Table 5 mentions "retrieving top-2 tokens". But after getting the top 2 tokens, how is the evaluation done?

Sec. 6.3 says: "This normalization indeed fixes full scoring in 9 out of 10 tested LMs on PDP-60." Are those results reported somewhere in the paper? Was that normalization used for the results in Table 2?

Sec. 6.3 says: "On WSC-273, the observation is again confirmed as partial scoring, which ignores c [the candidate] altogether, strongly outperforms the other two scorings in all cases" -- What is meant by "which ignores c altogether"?  c is still being conditioned on and it must not be ignored or else partial scoring would be meaningless (because c is the only part that differs between the two options). 


Typos and minor issues:

Be consistent about "common sense" vs. "commonsense".

Be consistent about "Deepnet" vs. "DeepNet" (Tables 2-3).

Sec. 1:
"even best" --&gt; "even the best"
"such as Winograd" --&gt; "such as the Winograd"
"a few hundreds" --&gt; "a few hundred"
"this type of questions" --&gt; "this type of question"
"does not present" --&gt; "is not present"
"non-facts tuples" --&gt; "non-fact tuples"

Sec. 2:
"solving Winograd" --&gt; "solving the Winograd"
"Store Cloze" --&gt; "Story Cloze"
"constructed by human" --&gt; "constructed by humans"

Sec. 4:
What is "LM-1-Billion"?
Why SQuAD?
"Another test set in included" --&gt; "Another test set is included"

Sec. 5.2:
Check margin in loss_new

"high-perlexity" --&gt; "high-perplexity"

Sec. 6:
Figure 2 caption: "keyword appear" --&gt; "keyword appears"

Sec. 6.2:
"for correct answer" --&gt; "for the correct answer"

Appendix A:
"acitvation" --&gt; "activation"
Appendix B:
Figure 4 caption: "is of" --&gt; "is"
The right part of Figure 4 has some odd spacing and hyphenation.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkx4ukiFnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Two somewhat disconnected small contributions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgfWh0qKX&amp;noteId=rkx4ukiFnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1151 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1151 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper uses a language model for scoring of question answer candidates in the Winograd schema dataset, as well as introduces a heuristic for scoring common-sense knowledge triples.

Quality:
Pros: The paper shows improvements over previous papers for two tasks related to common-sense knowledge. They both mainly utilise simple language models, which is impressive. The second one uses an additional supervised collaborative filtering-style model. The authors further perform a detailed error analysis and ablation study.
Cons: The paper isn't very well-written. It contains quite a few spelling mistakes and is unclear in places. The Winograd Scheme Challenge isn't a very interesting dataset and isn't widely used. In fact, this is evidenced by the fact that most cited papers on that datasets are preprints and technical reports.

Clarity:
The paper is confusing in places. It should really be introduced in the abstract what is meant by "common sense". Details of the language model are missing. It is only clear towards the end of the introduction that the paper explores two loosely-related tasks using language models.

Originality:
Pros: The suggested model outperforms others on two datasets.
Cons: The suggested models are novel in themselves. As the authors also acknowledge, using language models for scoring candidates is a simple baseline in multiple-choice QA and merely hasn't been tested for the Winograd schema dataset.

Significance:
Other researchers within the common-sense reasoning community might cite this paper. The significance of this paper to a larger representation learning audience is rather small.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BklPaakus7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Studying whether LM encode common-sense information. Novelty, clarity and methodology concerns</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgfWh0qKX&amp;noteId=BklPaakus7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1151 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1151 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper experiments with pre-trained language models for common sense tasks such as Winograd Schema Challenge and ConceptNet KB completion. While the authors get high numbers on some of the tasks, the paper is not particularly novel, and suffers from methodology and clarity problems. These prevent me from recommending its acceptance.

This paper shows that pre-trained language models (LMs) can be used to get strong improvements on several datasets. While some of the results obtained by the authors are impressive, this result is not particularly surprising in 2018. In the last year or so, methods based on pre-trained LMs have been shown extremely useful for a very wide number of NLP tasks (e.g., Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018). Moreover, as noticed to by the authors, Schwartz et al. (2017) demonstrated that LM perplexity can be useful for predicting common-sense information for the ROC story cloze task. As a result, the technical novelty in this paper is somewhat limited. 

The paper also suffers from methodological problems:
-- The main results observed by the author, the large improvement on the (hard!) Winograd schema challenge, is questionable: The GLUE paper (Wang et al., 2018) reports that the majority baseline for this dataset is about 65%. It is unclear whether the authors here used the same version of the dataset (the link they put does not unambiguously decide one way or another). If so, then the best results published in the current paper is below the majority baseline, and thus uninteresting. If this is not the same dataset, the authors should report the majority baseline and preferably also run their model on the (hard) version used in GLUE. 
-- The authors claim that their method on ConceptNet is unsupervised, yet they tune their LM on triplets from the training set, which makes it strongly rely on task supervision.

Finally, the paper suffers clarity issues. 
-- Some sections are disorganized. For instance, the experimental setup mentions experiments that are introduced later (the ConceptNet experiments). 
-- The authors mention two types of language models (word and character level), and also 4 text datasets to train the LMs on, but do not provide results for all combinations. In fact, it is unclear in table 2 what is the single model and what are the ensemble (ensemble of the same model trained on the same dataset with different seeds? or the same model with different datasets?).
-- The authors do not address hyper-parameter tuning. 
-- What is the gold standard for the "special word retrieved" data? how is it computed?


Other comments: 
-- Page 2: "In contrast, we make use of LSTMs, which are shown to be qualitatively different (Tang et al., 2018) and obtain significant improvements without fine-tuning.": 1. Tang et al. (2018) do not discuss fine-tuning. 2. Levy et al. (ACL 2018) actually show interesting connections between LSTMs and self-attention.
-- Schwartz et al. (2017) showed that when using a pre-trained LM, normalizing the conditional probability of p(ending | story) by p(ending) leads to much better results than  p(ending | story). The authors might also benefit from a similar normalization. 
-- Page 5: how is F1 defined?

Minor comments: 
-- Page 2: " ... despite the small training data size (100K instances).": 100K is typically not considered a small training set (for most tasks at least)
-- Page 5: "... most of the constituent documents ...": was this validated in any way? how?
-- The word "extremely" is used throughout the paper without justification in most cases.


Typos and such:
page 1: "... a relevant knowledge to the above Winograd Schema example, **does** not present ... ": should be "is"
page 5: "In the previous sections, we ***show*** ...": showed
page 7: "For example, with the ***test*** ...": "test instance"
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJeWt75Qsm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reproducibility</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgfWh0qKX&amp;noteId=rJeWt75Qsm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1151 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi,

Thanks for your work ! Using your code available on Github, I tried to reproduce the results on the Winograd Schema Challenge. Regarding the ensemble of 10LMs and the ensemble of 14LMs, I get a similar accuracy (61.5% and 63.7% accuracy). However, regarding the performance of the single LM, I don't get the same accuracy. I have the following results:

Model |  LM1  |  LM2  |  LM3  |  LM4  |  LM5  |  LM6  |  LM7  |  LM8  |  LM9  |  LM10  |  LM11  |  LM12  |  LM13  | LM14 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Acc.     |54.6% |50.2% |54.2%  |55.0% |54.2% |55.0% | 55.3% | 56.8%|57.9% | 57.5%   | 55.7%  | 58.2%   | 60.8% | 56.0%

The results clearly show that the performance of the single LM is not random and that they capture patterns that are useful for the task. However, I don't understand what is the accuracy reported in table 3, 56.4% for a single LM and the accuracy reported at the end of paragraph 5.1 'with one word level LM achiving 62.6% accuracy'. Could you comment on that ?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJlmKFV4nX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: Reproducibility</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgfWh0qKX&amp;noteId=SJlmKFV4nX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1151 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1151 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi! Thank you for using our code and report your results here. It seems some numbers from the table are different than what we had and the latest release of Tensorflow indeed produces those number. We are checking if there is a mismatch in terms of software or the language model version. Either way, we will make updates so reported results match with the open-source release. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJxXFkLtqm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Model Selection</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgfWh0qKX&amp;noteId=BJxXFkLtqm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1151 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the work. 

Concerning the table 7 and table 8 in the appendix, it seems to me that you have 8 LM variations for each corpus which represents 40 possible single LM models. When you ensemble the choice is not only an arbitrary subset of 14 of them, but involves combinations of these LM variations that are not at all consistent (for example, why do you use LM-2 on SQUAD and not LM-1?). Did you use any auxiliary task to do the model selection? If yes, I think it should be added to the paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryxyxGDt5Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Models are chosen based on validation perplexity</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgfWh0qKX&amp;noteId=ryxyxGDt5Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1151 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1151 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, thanks for the question. Below we include all details throughout our experiments to answer your question as well as any other potential inquiries about the training process.

TLDR: First Heuristic =  training corpus diversity (see section 6.3 and Figure 3-right for relevant analysis), Secondary heuristic = validation perplexity on corresponding held-out data.

Ensemble choice is made to first include as many corpora as possible (Section 6.3 and Figure 3-Right show relevant analysis):

* For single models on PDP-60 we chose Gutenberg as this is also the training corpus used in the previous SOTA [1]. Single model Char-LM result is not included to avoid complicating the tables, but its performance is also better than USSM (53.5%). 
* For ensemble of 5, we simply add all 3 of the remaining datasets (hence 1 LM each). 
* For ensemble of 10, we repeat the previous corpora choice twice. 
* For ensemble of 14, we add 4 LMs from Stories.

Once the training corpus profile is decided, we train and chose LMs based on perplexity on a held-out set. One such held-out set is constructed for each training data, as opposed to a single joint held-out set for all training corpora, since later on we want to demonstrate the effect of training corpus choice on commonsense reasoning test performance.

Note that we did not construct and train Word-LM-4 and Char-LM-4 until Section 5.2 (evaluation on Winograd Schema Challenge). There is no particular reason besides we want to push ensemble performance for better results by adding more LMs (even though single models are already better than previous results, see Table 3). 

Not all of our LMs converged to a good perplexity (below 40 points) on the corresponding validation sets, some other LMs diverged (perplexity &gt; 100), we discarded those models. We initially choose learning rate 0.2 following [2] and randomly try some other learning rates for wordLM2, charLM3 and charLM4 since some of them diverged on LM1B (see footnote 8). Those learning rate are finally fixed and used on all subsequent datasets (CommonCrawl, SQuAD, Gutenberg, and Stories), which is why not all LM-corpus pairs work out in the end. 

Other than trying out some learning rate values above, we did not perform any tuning since it takes time (on average, an LM took at least 1 million steps for its held-out perplexity to stop improving, which amounts to approximately 01 month of training on a Tesla P-100 GPU), and we already obtain good results.

[1] Quan Liu, Hui Jiang, Zhen-Hua Ling, Xiaodan Zhu, Si Wei, and Yu Hu. Combing context and
commonsense knowledge through neural networks for solving winograd schema problems. CoRR,
abs/1611.04146, 2016.

[2] Rafal JÃ³zefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the
limits of language modeling. CoRR, abs/1602.02410, 2016.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJeCbxuFqX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Ensembling</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgfWh0qKX&amp;noteId=SJeCbxuFqX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1151 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, Thanks for your answer. My question was not about the choice of parameters for the single LM, which we agree seems to outperform USSM and random baseline.

Instead, my question was about model selection when you ensemble. Consider that I have 40 random classifiers for the WSC; if I choose 14 of them based on the accuracy on the the WSC (test set), it's really likely that I get good results.   

You have 5 corpora (LM1b, SQUAD, CommCrawl, Gutenberg and Stories) and 8 different LM settings (ranging between hyperparameters that differ from the base settings as well as choice of word-level vs character level). This amounts to 40 possible LMs, of which you choose 14. This number (14) is a hyperparameter in itself. How did you come up with it? In addition, when you do model selection to find an ensemble of 10 models (in the case of not using Stories) you use 4 different LMs trained on Gutenberg (2  Word-LM1â€™s with different random seeds as well as a one Char-LM4 and one Char-LM 1), but only two on CommonCrawl (Char-LM 4, Char-LM 3). Obviously, the choice seems inconsistent and it does not seem to be based on validation perplexity. Otherwise, why would you use the same model two times?

Could you be a bit more clear on how you selected the models?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJedsCKt5m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>I see your point, validation perplexity is what we used.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgfWh0qKX&amp;noteId=HJedsCKt5m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1151 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1151 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt; This amounts to 40 possible LMs, of which you choose 14. This number (14) is a hyperparameter in itself.

I see what you mean, which we have already addressed in the previous answer. We do not train a very large number of LMs and then tune the ensemble size or selection. We gradually train and add new LMs to the ensemble up to 10 LMs originally (Section 5.1) and observe diminishing returns, so we push further using another direction (Section 5.2) of customizing data.

Why stop at 14? We have very large ensembles that achieve only slightly better results (64.4\%), which is not meaningful as 64% accuracy is still very far from human level accuracy. Besides, single model on Stories has already achieved 62.6\% accuracy.

The point of our paper is proving that LMs can perform better than previous methods, and we demonstrated two ways to improve upon single-LM (ensembling and customizing training data). If one tune the LMs more we believe 70\% is achievable, but 80\% or above will need something entirely different, but that is entirely speculative.

&gt; you use 4 different LMs trained on Gutenberg ... but only two on CommonCrawl.

As noted above, we do not decide the total number of LMs before hand, but add new LMs as experiments go:

The original 2 LMs on Gutenberg are Word-LM1 and Char-LM1. Gutenberg is used as it is used to trained USSM. With 66.7\% of USSM + knowledge bases + supervised deep net, single LM are now far behind (60\%) and we started to explore ensembling with other training corpora.

The ensemble of 5 is just adding SQuAD, LM1T, LM1B. The ensemble of 10 is just doubling the choice from the previous ensemble of 5, leading to 4 on Gutenberg and 2 on all other datasets. Doubling from 5 to 10 is a simple and obvious choice to us, albeit somewhat arbitrary. A different choice could result in better or worse, but is likely to improve upon ensemble of 5, which will also support our method of ensembling.

&gt; Obviously, the choice seems inconsistent and it does not seem to be based on validation perplexity. Otherwise, why would you use the same model two times?

It is clearly not the same model twice, since they started from different initialization. Our training of LMs is full of models that failed to converge, implementation debugging, transferring pretrained parameters (footnote 8), so it might not be as clean as one wish to see from the first glance. We tried our best to summarize necessary details in Appendix A and the above comment. Thanks for going through them in details.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HyetCdcY5X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re-wording OP's question.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgfWh0qKX&amp;noteId=HyetCdcY5X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1151 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi there, 

I believe the original poster has raised an important question about your paper, and I agree that you are not directly answering his question. Repeating results like 64.4%, 62.6, potential for 70% has nothing at all to do with the important question of *model selection*. You say you "gradually train and add new LMs to the ensemble up to 10 LMs originally". However, the question is, what made you choose *certain* LM's over others at each step. For example, you would choose to add an LM-2 which would vary from the base setting in terms of some hyperparameter, and then suddenly jump (seemingly arbitrarily) to another LM choice, say, char LM-4. What drove these arbitrary choices? Was it a greedy process on the Winograd Schema Challenge accuracy? I understand that you say that *each* LM had a validation perplexity to it; is this what you used to choose certain LM's over others? And if that is the case, I'm surprised that none of these details were included in the paper (as well as even mentioning that the validation perplexity was used). Ultimately, when you consider that you "gradually" ensembled on WSC, which is a *test set*, and observed accuracies en route, this is precisely antithetical with the purpose of a test set.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>