<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Domain Adaptive Transfer Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Domain Adaptive Transfer Learning" />
        <meta name="citation_author" content="Jiquan Ngiam" />
        <meta name="citation_author" content="Daiyi Peng" />
        <meta name="citation_author" content="Vijay Vasudevan" />
        <meta name="citation_author" content="Simon Kornblith" />
        <meta name="citation_author" content="Quoc Le" />
        <meta name="citation_author" content="Ruoming Pang" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HyxH2o05FQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Domain Adaptive Transfer Learning" />
      <meta name="og:description" content="Transfer learning is a widely used method to build high performing computer vision models. In this paper, we study the efficacy of transfer learning by examining how the choice of data impacts..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HyxH2o05FQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Domain Adaptive Transfer Learning</a> <a class="note_content_pdf" href="/pdf?id=HyxH2o05FQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=jngiam%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="jngiam@google.com">Jiquan Ngiam</a>, <a href="/profile?email=daiyip%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="daiyip@google.com">Daiyi Peng</a>, <a href="/profile?email=vrv%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="vrv@google.com">Vijay Vasudevan</a>, <a href="/profile?email=skornblith%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="skornblith@google.com">Simon Kornblith</a>, <a href="/profile?email=qvl%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="qvl@google.com">Quoc Le</a>, <a href="/profile?email=rpang%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="rpang@google.com">Ruoming Pang</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Transfer learning is a widely used method to build high performing computer vision models. In this paper, we study the efficacy of transfer learning by examining how the choice of data impacts performance. We find that more pre-training data does not always help, and transfer performance depends on a judicious choice of pre-training data. These findings are important given the continued increase in dataset sizes. We further propose domain adaptive transfer learning, a simple and effective pre-training method using importance weights computed based on the target dataset. Our methods achieve state-of-the-art results on multiple fine-grained classification datasets and are well-suited for use in practice.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_H1ePE3c2aX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxH2o05FQ&amp;noteId=H1ePE3c2aX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper708 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper708 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1gn3Tch2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>limited novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxH2o05FQ&amp;noteId=B1gn3Tch2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper708 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper708 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper provides an analysis on transfer learning by fine-tuning. It focuses on the importance of the relation between the content of the source dataset used for pre-training and the following target dataset on which the model will be applied. Several experiments show that this aspect is more important than the cardinality of the source data and this becomes particularly clear when dealing with fine-grained classification tasks in target.

The main technical contribution of this work is in the application of an importance weighting approach to select the data when training the source model.  The theory in section 3.3 is inherited from previous works and the only variant is in the way in which it is applied when the source and target class set is different: the authors propose a heuristic approximation of P_t(y) based on target self-labeling.

- I find the technical novelty of this work very limited. Indeed here we just see a pre-existing sample-selection based unsupervised domain adaptation method applied on several datasets. 
- more details about the self-labeling procedure should be provided: of course the prediction of the source model on the target varies during model training, being very poor in the beginning and possibly improving in the following epochs. Is he sample selection active since the very first epoch?
- The comparison in table 4 with state of the art should be better explained. Are the other baseline methods using the same AmoebaNet architecture? If not it is not the comparison might be unfair.


</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1xrnMuwpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxH2o05FQ&amp;noteId=r1xrnMuwpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper708 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper708 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the comments and questions.

We recognize that the importance weighting method is not novel and derived from previous work. However, we also emphasize that this paper is the first to apply these ideas to the pre-training and fine-tuning method of transfer learning. This novel application, despite its simplicity, is highly effective and outperforms recent work (Cui et. al, 2018) and (Ge &amp; Yu, 2017) which use much more involved methods. As such we think that it is worth highlighting this approach as a simple, effective and practical one.

The sample selection method is done before model training, using a pre-trained model on the entire ANON source dataset, and evaluated on the on the target dataset. This is fixed in our method and does not vary during the target model training.

In Table 4, we provide baselines with AmoebaNet that train on the entire ImageNet and ANON datasets. We show that the method works with large models as well. In comparing to the existing best published results, we note that they all use a mix of different model architectures and hence make fair comparisons harder: our goal in this comparison is to show that our methods are competitive with best published methods while remaining simple and effective.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_r1ebJEnq2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A paper with limited novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxH2o05FQ&amp;noteId=r1ebJEnq2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper708 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper708 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper is of limited technical contribution. The proposed method in Section 3.3 is just too close to covariate shift where importance weighting has been widely used. I don’t know whether authors are aware of these works.

The findings listed in Section 1.1 are obvious and intuitive. There is no interesting finding.

It is better to move the introduction of the datasets to the experimental section.

A typo: hand-curated -&gt; handcrafted</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJgWzm_P6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxH2o05FQ&amp;noteId=rJgWzm_P6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper708 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper708 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the comments and questions.

We recognize that our method is close to covariate shift, and in particular, prior probability shift (closely related to covariate shift) which often employs importance weighting. We have cited the relevant past work on this topic. However, we also would emphasize that this paper is the first to apply these ideas to the pre-training and fine-tuning method of transfer learning. This novel application, despite its simplicity, is highly effective and outperforms recent work (Cui et. al, 2018) and (Ge &amp; Yu, 2017) which use much more involved methods. As such we think that it is worth highlighting this approach as a simple, effective and practical one.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ryxWg7d53m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>review for "Domain Adaptive Transfer Learning"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxH2o05FQ&amp;noteId=ryxWg7d53m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper708 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper708 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper tackles the problem of transfer learning. The approach
proposed is simple but effective. It identifies the source training
examples that are most relevant for the target task and then over-samples 
these examples when pre-training the classification network. The
 over-sampling is based on importance weights measuring the ratio 
of the prior probability for each source label in the target and source datasets. As not all source
labels will necessarily be present in the target dataset, the target
prior probability for a source label is estimated by learning a
generic classifier on the source dataset, applying it to each example
in the target dataset, computing the average output probability for
each source label an then using this average probability as the source
label's prior probability.  After pre-training, fine-tuning is applied
with the labelled target data.

Extensive experiments are performed -  pre-training on a very large source
dataset and using large classification networks (Inception-v3 and
AmoebaNet-B) and transferring each pre-trained network to 6 standard
and relatively large target datasets. 

The results show that pre-training which focuses on the subsets of the
source data that are the most similar to the target data is more
effective, in general, than pre-training on all the source data which
treats each example equally. This finding is increasingly relevant the
more dissimilar the target and source datasets are and/or the more
"irrelevant" examples for the target task the source dataset contains. 



Pros:

+ The experimental results of this paper are its main
  strength. Results are presented on pre-training on a very large and
  diverse dataset called "ANON" and applying the important sample
  pre-training approach to both the Inception-v3 and AmoebaNet-B
  networks.

+ It adds more solid evidence that learning generic classification
networks from diverse datasets do not outperform more specialised
networks learnt from more relevant training data for specific tasks.

+ The importance sampling approach to pre-training is compared to
pre-training on different subsets of the source dataset corresponding
to images with certain high-level labels. Each subset is (potentially)
relevant to at least one particular target task. The importance
sampling approach does not always outperform pre-training
exclusively with the most relevant subset approach has a consistently high
performance across the board.

+/- A new very large image dataset (which would seem to be a
compliment to ImageNet) is introduced though it is unclear
whether this dataset will be made available to the research
community at a later date.


Cons:

- Details are lacking about the "ANON" dataset introduced in this
  paper (where do the photos come from and the labels, visualization of a few examples...)

- There are not many technical issues discussed in the paper and that
  is fine as the main idea is relatively simple and its
  effectiveness is mainly demonstrated empirically, but I
  feel the paper is missing a discussion about the importance of the initial
  classifier trained to estimate the target prior probabilities for
  the source labels and whether it is crucial that it has a certain
  level of accuracy etc.

- The approach in the paper implies a practitioner should have 
  access to a very large target dataset and the computational and time
  resources to appropriately pre-train a complex network for each new
  target task encountered. This is probably not feasible if many
  target tasks are considered. Unfortunately the paper does not
  give insights into how pre-training from scratch for each new target
  could be avoided. 


- The references in the paper, especially the "Exploring the limits of
  weakly supervised pre-training", demonstrate that it is already
  known that you do not increase the accuracy for the target task by
  pre-training with many source examples that are not very relevant to the
  target task. So one could argue that the findings in the paper are
  not particularly novel.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Skl5r7dP6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxH2o05FQ&amp;noteId=Skl5r7dP6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper708 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper708 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the detailed comments and suggestions, they will help us make this paper better. We appreciate the suggestions to discuss the importance of the pre-trained classifier and how to pre-train networks.

While the paper on “Exploring the limits of weakly supervised pre-training” describe that pre-training with irrelevant examples does not improve performance, they do not suggest a general solution for this and instead use heuristics based on the distribution of images per hashtag. Our paper considers a general approach instead that is also applicable to their setting.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>