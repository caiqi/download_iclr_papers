<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>DyRep: Learning Representations over Dynamic Graphs | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="DyRep: Learning Representations over Dynamic Graphs" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HyePrhR5KX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="DyRep: Learning Representations over Dynamic Graphs" />
      <meta name="og:description" content="Representation Learning over graph structured data has received significant attention recently due to its ubiquitous applicability. However, most advancements have been made in static graph..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HyePrhR5KX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>DyRep: Learning Representations over Dynamic Graphs</a> <a class="note_content_pdf" href="/pdf?id=HyePrhR5KX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 14 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019dyrep:,    &#10;title={DyReP: Learning Representations over Dynamic Graphs},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HyePrhR5KX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=HyePrhR5KX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Representation Learning over graph structured data has received significant attention recently due to its ubiquitous applicability. However, most advancements have been made in static graph settings while efforts for jointly learning dynamic of the graph and dynamic on the graph are still in an infant stage. Two fundamental questions arise in learning over dynamic graphs: (i) How to elegantly model dynamical processes over graphs? (ii) How to leverage such a model to effectively encode evolving graph information into low-dimensional representations? We present DyRep - a novel modeling framework for dynamic graphs that posits representation learning as a latent mediation process bridging two observed processes namely -- dynamics of the network (realized as topological evolution) and dynamics on the network (realized as activities between nodes). Concretely, we propose a two-time scale deep temporal point process model that captures the interleaved dynamics of the observed processes. This model is further parameterized by a temporal-attentive representation network that encodes temporally evolving structural information into node representations which in turn drives the nonlinear evolution of the observed graph dynamics. Our unified framework has inductive capability to generalize over unseen nodes and we design an efficient unsupervised procedure for end-to-end training. We demonstrate that DyRep outperforms state-of-art baselines in quantitative analysis using dynamic link prediction and time prediction tasks. We further present extensive qualitative insights into our framework to discern indispensable role of various components of our framework.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Dynamic Graphs, Representation Learning, Dynamic Processes, Temporal Point Process, Attention, Latent Representation</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Models Representation Learning over dynamic graphs as latent hidden process bridging two observed processes of Topological Evolution of and Interactions on dynamic graphs.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">11 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rkl_EsCKpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting idea which could use clearer theoretical justification and larger scale experimental validation.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyePrhR5KX&amp;noteId=rkl_EsCKpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1553 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1553 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Overall the paper suffers from a lack of clarity in the presentation, especially in algorithm 1, and does not communicate well why the assumption of different dynamical processes should be important in practice. Experiments show some improvement compared to (Trivedi et al. 2017) but are limited to two datasets and it is unclear to what extend end the proposed method would help for a larger variety of datasets. 

Not allowing for deletion of node, and especially edges, is a potential draw-back of the proposed method, but more importantly, in many graph datasets the type of nodes and edges is very important (e.g. a knowledge base graph without edges loses most relevant information) so not considering different types is a big limitation. 

Comments on the method (sections 2-4).

About equation (1):
 \bar{t} is not defined and its meaning is not obvious. The rate of event occurrence does not seem to depend on l (links status) whereas is seems to be dependent of l in algorithm 1. 

I don’t see how the timings of association and communication processes are related, both \lambda_k seem defined independently. Should we expect some temporal dependence between different types of events here? The authors mention that both point processes are “related through the mediation process and in the embedding space”, a more rigorous definition would be helpful here. 

The authors claim to learn functions to compute node representations, however the representations z^u seem to be direct embeddings of the nodes. If the representations are computed as functions it should be clear what is the input and which functional form is assumed.

I find algorithm 1 unclear and do not understand how it is formally derived, its justification seems rather fuzzy. It is also unclear how algorithm 1 relates to the loss optimisation presented in section 4. 

What is the mechanism for addition of new nodes to the graph? I don’t see in algorithm 1 a step where nodes can be added but this might be handled in a different part of the training. 

Comments on the experiments section.

Since the proposed method is a variation on (Trivedi et al. 2017), a strong baseline would include experiments performed on the same datasets (or at least one dataset) from that paper. 

It is not clear which events are actually observed. I can see how a structural change in the network can be observed but what exactly constitutes a communication event for the datasets presented?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJeonJJo3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Marked Point Process extension of (Trivedi et al., 2017)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyePrhR5KX&amp;noteId=BJeonJJo3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1553 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1553 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=BJeonJJo3X" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Overall, the contribution of the paper is somewhat limited [but a little more than my initial assessment, thanks to the rebuttal]. It is essentially an extension of (Trivedi et al. 2017), adding attention to provide self-exciting rates, applied to two types of edges (communication edges and “friendship” edges). Conditioned on past edges, future edges are assumed independent, which makes the math trivial. The work would be better described as modeling a Marked Point Process with marks k \in {0,1}.
Other comments:
1.	[original comment was not understood, rephrased (it is not a criticism)] DyRep-No-SP is as good as the proposed approach, maybe because the graph is assumed undirected and the embedding of u can be described by its neighbors (author rebuttal describes as Localized Propagation), as the neighbors themselves use the embedding of u for their own embedding (which means that self-propagation is never "really off"). Highly active nodes have a disproportional effect in the embedding, resulting in the better separated embeddings of Figure 4. [after rebuttal: what is the effect of node activity on the embeddings?]
2.	[unresolved] The Exogenous Drive W_t(t_p – t_{p−1}) should be more personalized. Some nodes are intrinsically more active than others. [after rebuttal: answer "$W_t(t_p - t_{p-1})$ is personalized as $t_p$ is node specific", I meant personalized as in Exogenous Drive of people like Alice or Bob]
3.	[unresolved] Fig 4 embeddings should be compared against (Trivedi et al. 2017) [after rebuttal: author revision does not make qualitative comparison against Trivedi et al. (2017)]

Besides the limited innovation, the writing needs work. 
4.	[resolved] Equation 1 defines $g_k(\bar{t})$ but does not define \bar{t}. Knowing (Trivedi et al. 2017), I immediately knew what it was, but this is not standard notation and should be defined. 
5.	[resolved] $g_k$ must be a function of u and v
6.	[still confusing] “$k$ represent the dynamic process” = &gt;  “$k$ represent the type of edge” . The way it is written $k$ would need to be a stochastic process (it is just a mark, k \in {0,1})
7.	[resolved] Algorithm 1 is impossibly confusing. I read it 8 times and I still cannot tell what it is supposed to do. It contains recursive definitions like $z_i = b + \lambda_k^{ji}(t)$, where $\lambda_k^{ji}(t)$ itself is a function of $z_i(t)$. Maybe the z_i(t) and z_i are different variables with the same name?
8.	[resolved] The only hint that the graph under consideration is undirected comes from Algorithm 1, A_{uv}(t) = A_{vu}(t) = 1. It is *very* important information for the reader.
Related work (to be added to literature):
Dynamic graph embedding: (Yuan et al., 2017) (Ghassen et al., 2017)
Dynamic sub-graph embedding: (Meng et al., 2018)

Minor:
state-of-arts =&gt; state-of-the-art methods
list enumeration “1.)” , “2.)” is strange. Decide either 1) , 2) or 1. , 2. . I have never seen both.
MAE =&gt; mean absolute error (MAE)

Yuan, Y., Liang, X., Wang, X., Yeung, D. Y., &amp; Gupta, A., Temporal Dynamic Graph LSTM for Action-Driven Video Object Detection. ICCV, 2017.
Jerfel,  , Mehmet E. Basbug, and Barbara E. Engelhardt. "Dynamic Collaborative Filtering with Compound Poisson Factorization." AISTATS 2017. 
Meng, C., Mouli, S.C., Ribeiro, B. and Neville, J., Subgraph Pattern Neural Networks for High-Order Graph Evolution Prediction. AAAI 2018.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rygMqV4YTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 1 - Part II</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyePrhR5KX&amp;noteId=rygMqV4YTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1553 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1553 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Responses to Other Comments:
========================

1) This is incorrect as self-propagation mainly captures the recurrent evolution of one’s own latent features independent of others. Self-propagation principle states: A node evolves in the embedded space with respect to its previous position (e.g. set of features) and not in a random fashion. Based on Localized Propagation principle described above, a node's embedding is described by information it receives from other node and not exclusively it's own neighbors. The good performance of DyRep-No-SP signifies that the Localized Propagation term in Eq 4. is able to account for the relative position of node with respect to its previous position more often than not. Further, both dynamic of network and dynamic on network contribute to updates to a node's embedding. The interplay of multi-scale temporal behavior of these processes and evolving features leads to better discriminative embeddings, not just the rate of activities - this is evident by other exploratory use cases we discuss.

2) $W_t(t_p - t_{p-1})$ is personalized as $t_p$ is node specific.

3,4) We add the suggested changes to the revised version.

5) The intention for the *qualitative* exploratory analysis was not to make a performance comparison, which is already available against dynamic baselines in our *quantitative* predictive analysis. The goal of Figure 4 and appendix experiments is to draw the comparison between how embeddings learned using state-of-the-art static methods would differ from our dynamic model in terms of capturing evolving properties over time. To our knowledge, such extensive analysis for dynamic embeddings is not available in previous works. Further, we believe that visualizing embeddings from another dynamic method against our model may not provide informative insights.

6) This is incorrect - please check our main response above

7) “z" in Algorithm 1 is a temporary variable whose scope is limited to the algorithm. Please note that $\lambda$ is an input to the algorithm and hence “z" within Algorithm 1 has no interaction with the node embedding z (which always has a superscript) used throughout the paper. Hence, there is no recurrence, however, to avoid any further confusion, we change the temporary variable to “y".

Details explaining Algorithm 1 in full are available on Page 7. Here we provide a simplified high-level explanation. As a starting point, we refer you to the point 2 in paragraph before Eq 4 page 5.  To capture the effect described there, we parameterize the attention module with element of matrix S corresponding to an existing edge that signifies information/effect propagated by that edge. Algorithm 1 computes/updates this S matrix. Please note that S is parameter for a structural temporal attention which means temporal attention is only applied on structural neighborhood of a node. Hence, the value of S are only updated/active in two scenarios: a) the current event is between nodes which already has structural edge (communication between associated nodes or l=1, k=1) and b) the current event is an association event (l=0, k=0). Now, given a neighborhood of node ‘u’, $b$ represents background (base) attention for each edge which is uniform attention based on neighborhood size. Whenever an event occurs between two nodes, this attention changes in following ways: For case (a), just change the attention value for corresponding S entry using the intensity of the event. For case (b), repeat same as (a) but also adjust the background attention for each node as the neighborhood size grows in this case.

8) Thank you for pointing this. It is true that we consider undirected graphs in proposed work. However, our model can be easily generalized to directed graphs. Specifically, the difference would appear in the update of matrix A used in Algorithm 1, which would subsequently lead to different neighborhood and attention flow for each node. We will add this clarification in the revised paper.

We have uploaded a revised version of the paper to add the above clarifications, address your points and discuss related work cited by you (thank you for the pointers). Please let us know if something is still not clear and we will be happy to further discuss and address your concerns.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Sygt67NF67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 1 - Part I</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyePrhR5KX&amp;noteId=Sygt67NF67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1553 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1553 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review! We appreciate your comments and suggestions.

As a preface to our response, we wish to mention that, unlike existing approaches, our  work expresses dynamic graphs at multiple time-scales as follows:
a)  Dynamic ”of” the Network:  This corresponds to the topological changes of the network – insertion or deletion of nodes and edges. We use "Association" to label the observed process corresponding to this dynamic.
b)  Dynamic ”on” the Network:  This corresponds to  activities on a *fixed* network topology – self evolution of node’s features,  change in node’s features due to exogenous drive (activities external to network),  information  propagation  within  network  and  interactions  between nodes which may or may not have direct edge between them. We use "Communication" to label the observed process of interaction between nodes (only the observed part of dynamic ”on” the network).

General Comment:
==============
Overall, the contribution of the paper is limited. It is essentially a minor extension of (Trivedi et al. 2017), adding attention, applied to two types of edges (communication edges and “friendship” edges). Edges are assumed independent, which makes the math trivial. The work would be better described as modeling a Marked Poisson Process with marks k \in {0,1}.

Response:
=========
We politely disagree with these comments as this is an incorrect characterization of our work. It seems that the misunderstanding arises from your assumption (including point 6) that ‘k’ is type of an edge, ‘k’ is a mark and ‘k’ has independence, none of which is true. ‘k’ truly distinguishes scale of event dynamics (not type of edge) in our two-time scale model. In fact, when k=1, it is an interaction event which is not considered as an edge between nodes in our model. The edge (which forms graph structure) only appears through an association event (k=0). Indeed, ‘k’ corresponds to stochastic processes at different time scales and hence $\psi_k$ is the rate (scale) parameter corresponding to each dynamic. Further, every time when k=0, an edge is created between different node pairs. As we clearly mention in the paper, we do not consider edge type in this work and hence ‘k’ is not a mark. However, edge type can be added to Eq 4 in case it is available. Finally, dynamic processes realized by k=0 and k=1 are not independent and are highly interleaved in a nonlinear fashion. For instance, formation of a structural edge (k=0) affects interactions (k=1) and vice versa. Algorithm 1 captures this intricate dependencies as we will describe below. Based on the above points, it follows that our model is not a marked Poisson process. In fact, it does not take any specific form of point process - rather learns the conditional intensity function through a function approximation.

In terms of contributions, we argue that our approach of modeling dynamic graphs at multiple scales and learning dynamic representations as latent mediation process bridging the two dynamic processes, is a significant innovation compared to any existing approaches. This is a non-trivial effort for a setting where the dynamic processes evolve in a complex and nonlinear fashion. Further, our temporal point process based structural-temporal self-attention mechanism to model attention based on event history of a node is very novel and has not been attempted before. Our attention model can: 1) take into account temporal dynamics of activities on edge and 2) capture effects from faraway nodes due to dependence on event history. This is a formal advancement to state-of-the-art models of non-uniform attention (such as Graph Attention networks). 

Further, the paper provides an in-depth comparison with (Trivedi et. al. 2017) (including Table 1). Here we reiterate the differences: (Trivedi et. al. 2017) model events at single time scale and do not distinguish between two dynamic processes. They only consider edge level information for learning the embeddings. Our model considers a higher order neighborhood structure to compute embeddings. More importantly, in their work, the embedding update  for a node ‘u’ considers the edge information for the same node ‘u’ at a previous time step. This is entirely different from our structural model based on ”Localized Embedding Propagation” principle which states: Two nodes involved in an event form a temporary (communication) or a permanent (association) pathway for the information to propagate from the neighborhood of one node to the other node. This means, during the update of embedding for node ‘u’, information is propagated from the neighborhood of node ‘v’ (and not node ‘u’, please check Eq. 4) to node ‘u’. Subsequently, (Trivedi et. al. 2017) does not have any attention mechanism as they don't consider structure.


</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1g5-1zxRm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>I also politely disagree, but mostly because some of my comments were misunderstood</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyePrhR5KX&amp;noteId=S1g5-1zxRm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1553 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1553 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your reply. I do realize now that the process is not Poisson as the definition of \lambda clearly depends on past marks (it is not an externally driven process like a non-homogeneous Poisson process). I will change my review accordingly. 

I also apologize but I fear we are talking past each other here (“We disagree with these comments as this is an incorrect characterization of our work” … ). I will strive to be more specific from now on. 

“It seems that the misunderstanding arises from your assumption (including point 6) that … ‘k’ is a mark” =&gt; By your own definition of O = \{(u, v, t, l, k)_p\}_{p=1}^P , which fits the Definition 2.1.2 of Jacobsen (2006) where T_p is your p-th event time and Y_p = (u, v, l, k) is an element of a Polish space E. When you say O is a not a Marked point process, what is the basis for the claim? Why would Y_p not be represented by a Polish space? 

Formally, any time-varying graph is a Marked point process where the edges are the marks. When I say “Graph process”, it is implicit that it has edge marks. Thus, my comment “Graph process” with edge marks k implies a measure (density) over the sigma algebra (sequence) given by O = \{(u, v, t, k)_p\}_{p=1}^P. The variable “l” is not properly a mark because it can be re-constructed from the process (l_p = 1 if there has been any event with k=0 in the past). Algorithm 1 uses this marks definition when it does “if k = 0 then Auv(t) = Avu(t) = 1“, i.e., k=0 is a mark of an observable edge (see description next). 

“It seems that the misunderstanding arises from your assumption (including point 6) that ‘k’ is type of an edge,
Possibly my general use of the ill-defined term “edge” was not clear. I am thinking of (u,v) as a tuple. If (u,v) is a physical edge or a virtual edge “interaction”, k \in \{0,1\} defines a mark (physical or virtual). 

“It seems that the misunderstanding arises from your assumption (including point 6) that ‘k’ has independence, none of which is true.” 
We seem be to talking about different things. Marks (u, v, t, k) are conditionally independent given the model and past marks, per your likelihood \mathcal{L}. This is the independence I was referring to. Adding these marks to Trivedi et al. (2017) is rather (mathematically) straightforward given the independent nature of the model. Mathematically straightforward does not mean it is easy to get it to work in practice and releasing the code would be important.

Jacobsen, Martin. Point process theory and applications: marked point and piecewise deterministic processes. Springer Science &amp; Business Media, 2006.

Minor: 
Page 3, λ(t)dt:= P[event .. ] missing brackets

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_SyeBsVc93m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper presents a dynamic graph embedding method, which considers two types of dynamics in evolving networks: association events with node and edge grows, and communication events with node-node interactions.     </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyePrhR5KX&amp;noteId=SyeBsVc93m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1553 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1553 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper is very well written. The proposed approach is appropriate on modeling the node representations when the two types of events happen in the dynamic networks. Authors also clearly discussed the relevance and difference to related work. Experimental results show that the presented method outperforms the other baselines.
Overall, it is a high-quality paper. 
There are only some minor comments for improving the paper:
ν	Page 6, there is a typo. “for node v by employing …”  should be “for node u”
ν	Page 6, “Both GAT and GaAN has”   should be  “Both GAT and GaAN have”
ν	In section 5.1, it will be great if authors can explain more what are the “association events” and “communication events” with more details in these two evaluation datasets.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJgv8BVFTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyePrhR5KX&amp;noteId=BJgv8BVFTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1553 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1553 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review! We appreciate your time and supportive feedback and we are glad that you find our work interesting. Details about the corresponding association and communication events in the two datasets are provided in Appendix E.1. We uploaded a revised version that contains your suggested changes.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1eidr-Eom" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>comment</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyePrhR5KX&amp;noteId=S1eidr-Eom"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1553 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The paper presents its content in the most complicated way. It defines new concepts of Association (refers to topological evolution) and Communication (refers to node interactions) for dynamic graphs and formulate the problem based on them. In reality, dynamic networks are represented by insertion and deletion of nodes and insertion or deletion of edges between existing nodes. The edges and nodes may have features or labels. The paper defines two new concepts of communication and association which I think are inherited from the edge concept with subtle differences. Association has global effects and communication has local effects on information exchange. I am really confused if we really need to define such new concepts and then propose a model for that, while in reality dynamic graphs usually do not contain these kinds of constraints. Assuming we have the realization of these concepts, can we formulate the problem using simpler models such as networks with typed edges or weighted edges? I am skeptical about how the authors use the datasets in the experiment. For example, in the Social Evolution Dataset, what is association and what is communication? How did you interpret the dataset to find these concepts? Do we really need to consider these concepts in the Social Evolution Dataset to do the link prediction? I think authors can elaborate on new concepts definitions and necessity for considering them in their method.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkx45Fl3jX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to comment</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyePrhR5KX&amp;noteId=rkx45Fl3jX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1553 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">23 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1553 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your interest in our work.

Inspired from [1], our  work expresses dynamic graphs at multiple scales as follows:
a.)  Dynamic ”of” the Network:  This corresponds to the topological changes in network – insertion or deletion of nodes and edges
b.)  Dynamic ”on” the Network:  This corresponds to various activities in the network – self evolution of node’s interests/features,  change in node’s features due to exogenous drive (activities external to net-work),  information  propagation  within  network  and  within-network interactions  between nodes which may or may not have direct edge between them. 

We  do not  define  "Association"  and  "Communication"  as  two  new  concepts  or constraints  on  dynamic  graphs neither do we claim that in the paper.   Instead,  we  use  those  two  words  to  label  the  well-known  and  naturally *observed* processes corresponding to the dynamics mentioned in (a) and (b) – Association events maps to observed insertion of nodes or edges and Communication events maps to observed interactions between nodes (which is observed part of dynamic ”on” the network).  Nevertheless, this dichotomy of dynamic network processes is well-known and has been subject of several studies [1,  2,  3,  4,  5] in segregated manner.  But none of the existing machine learning approaches has jointly modeled them for representation learning over dynamic graphs (our key objective) to the best of our knowledge. 

”In  reality,  dynamic  networks  are  represented  by  insertion  and  deletion  of  nodes  and  insertion  or  deletion of edges between existing nodes.”

This is a rather limited or constrained view of dynamic graphs as there are many dynamic processes (as listed in b above) occurring on such a graph which cannot be realized by just modeling growth or shrinkage of graph. Approaches based on such model of dynamic network cannot distinguish or model interleaved evolution of network processes which leads to multiple shortcomings:
– Such a model may capture structural evolution, but it lacks the ability to effectively and correctly capture dynamics ”on” the network.  Concretely, the dynamic process under which a node’s features evolve or node interactions happen within a network (thus leading to information propagation) has vastly different behavior from the dynamic  process  that  leads  to  growth  (shrinkage)  of  the  network  structure.   For  example,  social  network activities such as liking a post or posting on discussion or sharing a video happen at much accelerated rate compared to slow rate of making friends and thereby growing the network.  Hence it is important to express dynamic graphs at different time scales.  
– Edge types only serve as  feature information and they can be readily added in our model if available. Edge weights may or may not be available apriori and may need to be inferred. Both of them are insufficient to effectively model the evolutionary multi-time scale dynamics of structure and network activities and their influence on each other.  Further, neither of them express node specific dynamic properties.  This, in turn, will not help to learn the effect of evolving node representations on observed processes and vice versa.

Extended Details on use of both datasets is available in Appendix E. 

[1] Natural algorithms and influence systems.
[2] The dynamics of transmission and the dynamics of networks.
[3] Dynamics on networks:  competition of temporal and topological correlations.
[4] Dynamic pattern evolution on scale-free networks.
[5] Coevolve:  A Joint Point Process Model for Information Diffusion and Network Evolution.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SkeXykWAYm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>prior works on graph deep learning</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyePrhR5KX&amp;noteId=SkeXykWAYm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Michael_Bronstein1" class="profile-link">Michael Bronstein</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Sep 2018</span><span class="item">ICLR 2019 Conference Paper1553 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I would like to draw the authors' attention to multiple recent works on deep learning on graphs directly related to their work. Among spectral-domain methods, replacing the explicit computation of the Laplacian eigenbasis of the spectral CNNs Bruna et al. with polynomial [1] and rational [2] filter functions is a very popular approach (the method of Kipf&amp;Welling is a particular setting of [1]). On the other hand, there are several spatial-domain methods that generalize the notion of patches on graphs. These methods originate from works on deep learning on manifolds in computer graphics and recently applied to graphs, e.g. the Mixture Model Networks (MoNet) [3] (Note that Graph Attention Networks (GAT) of Veličković et al. are a particular setting of the MoNet [3]). MoNet architecture was generalized in [4] using more general learnable local operators and dynamic graph updates. Finally, the authors may refer to a review paper [5] on non-Euclidean deep learning methods. 


1. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering, arXiv:1606.09375

2. CayleyNets: Graph convolutional neural networks with complex rational spectral filters, arXiv:1705.07664,

3. Geometric deep learning on graphs and manifolds using mixture model CNNs, CVPR 2017. 

4. Dynamic Graph CNN for learning on point clouds, arXiv:1712.00268

5. Geometric deep learning: going beyond Euclidean data, IEEE Signal Processing Magazine, 34(4):18-42, 2017
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJg59orrqQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for interesting pointers</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyePrhR5KX&amp;noteId=HJg59orrqQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1553 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1553 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We view the work on geometric deep learning as a very interesting direction for representation learning over graphs. However, most current works including cited papers in geometric deep learning over graphs primarily deal with static graphs, while our work focuses on dynamic graphs to jointly model both - topological evolution (dynamic of the network) and node interactions (dynamic on the graph).  It would be interesting complimentary direction to extend cited spectral/spatial domain methods to derive local graph operators that can take into account both both temporal and spatial dynamics. We will add a related discussion section in the updated version of the paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>