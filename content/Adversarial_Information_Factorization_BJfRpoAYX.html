<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Adversarial Information Factorization | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Adversarial Information Factorization" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJfRpoA9YX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Adversarial Information Factorization" />
      <meta name="og:description" content="We propose a novel generative model architecture designed to learn representations for images that factor out a single attribute from the rest of the representation. A single object may have many..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJfRpoA9YX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Adversarial Information Factorization</a> <a class="note_content_pdf" href="/pdf?id=BJfRpoA9YX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 14 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019adversarial,    &#10;title={Adversarial Information Factorization},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BJfRpoA9YX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=BJfRpoA9YX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We propose a novel generative model architecture designed to learn representations for images that factor out a single attribute from the rest of the representation. A single object may have many attributes which when altered do not change the identity of the object itself. Consider the human face; the identity of a particular person is independent of whether or not they happen to be wearing glasses. The attribute of wearing glasses can be changed without changing the identity of the person. However, the ability to manipulate and alter image attributes without altering the object identity is not a trivial task. Here, we are interested in learning a representation of the image that separates the identity of an object (such as a human face) from an attribute (such as 'wearing glasses'). We demonstrate the success of our factorization approach by using the learned representation to synthesize the same face with and without a chosen attribute. We refer to this specific synthesis process as image attribute manipulation. We further demonstrate that our model achieves competitive scores, with state of the art, on a facial attribute classification task.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">disentangled representations, factored representations, generative adversarial networks, variational auto encoders, generative models</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Learn representations for images that factor out a single attribute.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">29 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_H1gUKn2d2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Method clarity can be improved, and lacks some key comparisons experimentally.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=H1gUKn2d2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 14 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper851 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=H1gUKn2d2Q" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this paper, the authors introduce a neural network architecture that has three components.
First a VAE is used to encode images in to two latent states \hat{y} and \hat{z}, with \hat{z}
intended to be class (e.g. face attribute) agnostic. The decoder reconstructs images from \hat{y}
and \hat{z} concatenated together. A GAN style discriminator attempts to distinguish the 
decoded image from the original input image as real or fake, allowing the decoder to produce 
higher quality decoded images. An auxiliary network A attempts to classify the face attribute y
from the class agnostic features \hat{z}, with the idea being that the encoder should try to produce 
\hat{z} vectors from which the class cannot be predicted. An additional classifier is trained
using a classification loss \hat{L}_{class} on the encoded reconstructed image, the use of which 
I don't understand.

I think additional work on section 2.5 through section 3 would be helpful to improve clarity.
As one example, "y" is unnecessarily overloaded: y denotes a specific attribute, \hat{y}
denotes a latent vector that is intended to not be class agnostic, \tilde{y} denotes the
prediction of an auxiliary network on an intended class-agnostic latent vector \hat{z} of
the presence of the original attribute y, and \hat{\hat{y}} denotes the non agnostic latent
vector achieved by passing the decoded image back through the encoder.

This notational complexity is compounded by the fact that a number of steps in the method are
not well motivated in the text, and left to the reader to understand their purpose. For example,
the authors state that "we incorporate a classification model into the encoder so that our model may
easily be used to perform classification tasks." What does this mean? In the diagram (Figure 1),
where is this classification model? Why in the GAN loss is there a term that compares the
fake loss with the result of classifying a decoded z vector? Is this z \hat{z}, or a latent vector
drawn from a distribution p(z)? If it is the former, how does this term differ from the second
term in the GAN loss. If it is the latter, then shouldn't it be concatenated with some y in order to
be used as input to the decoder D_{\theta}?

Why is it important to extract \hat{\hat{y}} from \hat{x}? In the paper you state that the loss
"provides a gradient containing label information to the decoder," but why can't we use the known label y
of the original input x to ensure that the encoder and decoder preserve this information if it is used as \hat{y}?
Later in the paper, you explicitly state that \hat{\mathcal{L}_{class}} "does not provide any clear benefit."
If that is the case, then you should ideally include it neither in the model nor in the paper. If it was
included primarily because previous models included it, then I would recommend you introduce its use
in a background section on Bao et al., 2017 rather than including it in your model description with an
explanation like "so that our model may easily be used to perform classification tasks."

Ultimately, this last point brings us to a good summary of my concerns with the model: the inclusion
of too many moving parts, some of which the authors explicitly say later on provide no benefit.

Moving on to experimental results, I think this is another area where I have a few concerns. First, in
Figure 2, the authors argue that your model is "better for 6 out of 10 attributes" and comparable results for most others. The authors include a gap of 0.1 in the "Gray_hair" category as "better" but label a gap of 0.5
in the Black hair category as "comparable." I think results in several of the categories are sufficiently close
that error bars would be necessary to draw actual conclusions. If "better" were to mean "better by 0.5" for example,
then the authors method is better on 4 tasks (smiling, blonde hair, heavy makeup, mustache) and worse on 3 (black hair, brown hair, wavy hair).

With respect to the actual attribute editing, my main concern here is a lack of comparison to models other than Bao et al., despite the fact that face attribute changing is an exhaustively studied task. A number of papers like Perarnau et al., 2016, Upchurch et al., 2017, Lample et al., 2017 and others study this task from machine learning perspectives, and in some cases can perform photorealistic image attribute editing without complicated machinery on megapixel face
images. At least the images in Figure 3 and 4 are substantially downsampled from the typical resolution found in the Celeba dataset, suggesting that there was some failure mode on full resolution images.

----

Edit: I've reviewed the authors' addressing my concerns in their paper and am happy to increase my rating as a result.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkePHaX_TX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>"There is no additional classifier in our model. The encoder itself acts as a classifier"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=BkePHaX_TX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Authors] 
We would like to sincerely thank the reviewer for reading our paper and for providing constructive feedback. The reviewer appears to have understood all the components of our model well, with the exception of the \hat{L}_{class} loss, which has been the focus of majority of the comments. We have addressed all of the reviewer's comments below, as well as improved our paper where necessary (please see the updated version).

[Reviewer]
An additional classifier is trained using a classification loss \hat{L}_{class} on the encoded reconstructed image, the use of which I don't understand.

[Authors]
There is no additional classifier in our model. The encoder itself acts as a classifier, predicting both a latent vector \hat{z}, and a label \hat{y}. The primary loss for ensuring that the encoder is an effective classifier is L_{class}, not \hat{L}_{class} as claimed by the reviewer. 

With regards to \hat{L}_{class}, the following quote from our paper (Section 3.1) explains its purpose:
“The classification loss, \hat{L}_{class}, provides a gradient containing label information to the decoder, which otherwise the decoder would not have \citep{chen2016infogan}.”

We found that this term did not play an important role in our model. This is stated in our paper as follows (Section 4.2):
“\hat{L}_{class} does not provide any clear benefit. We explored the effect of including this term since a similar approach had been proposed in the GAN literature \cite{chen2016infogan,odena2016conditional} for conditional image synthesis.”</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkxudqvuTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>My point is that \hat{L}_{class} should not be in the paper at all.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=SkxudqvuTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">With regards to the purpose of \hat{L}_{class}, the quote you provide is clearly not sufficient to adequately explain its purpose. Evidence for this is the fact that, as you later demonstrate with an ablation study, the loss has no purpose. 

In my opinion, when explaining your model, you should not include terms that "did not play an important role in [your] model" and "[do] not provide any clear benefit.", *regardless of whether those terms have been used in prior art.* If you feel mentioning this term is sufficient, a simple explanation of why it *wasn't* included would suffice.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJeg7Nhtpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We have removed \hat{L}_{class} from our model description -- simplifying our model.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=HJeg7Nhtpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have now revised our paper and we do not include \hat{L}_{class} in our model description. As per your helpful suggestion (below), we have added a single experiment in our ablation study to demonstrate that \hat{L}_{class} was not helpful.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bygh8K2Ypm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for addressing my concerns, I have increased my rating.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=Bygh8K2Ypm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi authors,

Thank you for addressing my concerns in the text of the paper.

I do feel that the paper is overall much improved, particularly the model explanation with one fewer moving component. I have increased my score to be in line with the other reviewers.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_rkes_n7OT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Explaining the `classification model'.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=rkes_n7OT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Reviewer]
I think additional work on section 2.5 through section 3 would be helpful to improve clarity. As one example, "y" is unnecessarily overloaded: y denotes a specific attribute, \hat{y} denotes a latent vector that is intended to not be class agnostic, \tilde{y} denotes the prediction of an auxiliary network on an intended class-agnostic latent vector \hat{z} of the presence of the original attribute y, and \hat{\hat{y}} denotes the non agnostic latent vector achieved by passing the decoded image back through the encoder.

[Authors]
We believe our notation is consistent, correct and necessary, and this reviewer (as well as the others) have understood our model. We provide a clear diagram as well as an algorithm and descriptions in the text.

[Reviewer]
This notational complexity is compounded by the fact that a number of steps in the method are not well motivated in the text, and left to the reader to understand their purpose. 

[Authors]
Our core contribution is the introduction of an auxiliary network for factorizing attribute information from the rest of the latent code. This is communicated clearly in Section 3.2 and the reviewer appears to understand the * motivation * for the auxiliary network well:

Reviewers own words: "An auxiliary network A attempts to classify the face attribute y from the class agnostic features \hat{z}, with the idea being that the encoder should try to produce \hat{z} vectors from which the class cannot be predicted."

We incorporate our network into a pre-existing model, the VAE-GAN which is also communicated clearly in Section 2. In Section 2 we introduce VAEs and GANs, and explain the benefits of combining the two in Section 2.3 'Best Of Both GAN And VAE'. Finally, we also explain conditional GANs. The VAE, GAN and VAE-GAN are previous related works, motivated by their respective authors.


[Reviewer]
For example, the authors state that "we incorporate a classification model into the encoder so that our model may easily be used to perform classification tasks." What does this mean? In the diagram (Figure 1), where is this classification model?

[Authors]
We would like the thank the reviewer for raising this concern. To avoid confusion, we have amended the paper in Section 3.1 to read as follows:
"Additionally, the encoder also acts as a classifier, outputting an attribute vector, \hat{y}, along side a latent vector, \hat{z}."

There is no separate classification network. The encoder, which takes an image, x, as input, is split in two, outputting a latent vector \hat{z} and an attribute vector \hat{y}. The model is trained such that the attribute vector may be used to classify an input image, x, therefore the encoder is also a classifier.

This is described in Section 2.4 (quote from our paper):
“the encoder outputs both a latent vector, \hat{z}, and an attribute vector, \hat{y}”

Our point here, is that rather than adding a separate classifier that takes reconstructed images as input, as is done in Bao et al. (illustrated in our paper in Figure 1(b)), we incorporate the classifier into the encoder, E_{y, \phi}, (illustrated in Figure 1(a)) which allows our model to be used both for image attribute editing and classification. This also avoids the need to train an entirely separate classifier network. We are essentially killing two birds with one stone in this proposed approach.

[Reviewer]
Why in the GAN loss is there a term that compares the fake loss with the result of classifying a decoded z vector?

[Authors]
There are three terms in the GAN loss. In the first, the discriminator, C_\chi, is applied to real samples. In the second and third term the discriminator is applied to fake images. There are two sources of fake images; (1) the reconstructed images, D_\theta(E_\phi(x)), and (2) synthetic images, D_\theta(E_{\phi,z}(x), y). The GAN loss is similar to the one used by Bao et al. (see Line 9 of Algorithm 1 of Bao et al.). 

[Reviewer]
Is this z \hat{z}, or a latent vector drawn from a distribution p(z)?

[Authors]
By definition z is drawn from the prior, p(z), this is described in Section 2.3 when describing VAE-GAN; “latent variable z, which is drawn from a specified random distribution, p(z)”. This is also illustrated in line 5 of Algorithm 1. We have made this clearer by adding the following after defining L_{gan}: “and z \sim p(z)”.

[Reviewer]
If it is the former, how does this term differ from the second term in the GAN loss. If it is the latter, then shouldn't it be concatenated with some y in order to be used as input to the decoder D_{\theta}?

[Authors]
We would like to thank the reviewer for catching this typo. Indeed, there is supposed to be a y input to the decoder, we have updated the paper to reflect this, the third term now reads: “L_{bce}(y_{fake}, C_\chi(D_\theta(z, y)))]”.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1e2knXOam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The encoder acts as a classifier, outputting an attribute vector, hat{y}, as well as a latent vector, z.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=S1e2knXOam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Reviewer]
Why is it important to extract \hat{\hat{y}} from \hat{x}? In the paper you state that the loss "provides a gradient containing label information to the decoder," but why can't we use the known label y of the original input x to ensure that the encoder and decoder preserve this information if it is used as \hat{y}?

[Authors]
L_{class} ensures that \hat{y} contains label information, but this loss is not dependant on the parameters of the decoder and, therefore, cannot be used to update the decoder. Note that there is a precendence for computing \hat{\hat{y}}: the Bao et al. model also use it to provide label information to the decoder. We could have placed an additional classifier at the output of the decoder, as is done by Bao et al., to compute \hat{\hat{y}}. However, rather than introducing and training another classifier, we made use of our encoder which is already able to predict labels, thus we pass the reconstructed image back through the encoder. 

[Reviewer]
Later in the paper, you explicitly state that \hat{\mathcal{L}_{class}} "does not provide any clear benefit." If that is the case, then you should ideally include it neither in the model nor in the paper. If it was included primarily because previous models included it, then I would recommend you introduce its use in a background section on Bao et al., 2017 rather than including it in your model description with an explanation like "so that our model may easily be used to perform classification tasks."

[Authors]
The explanation for using \hat{L}_{class} is not "so that our model may easily be used to perform classification tasks.", it is because  (Section 3.1) it "provides a gradient containing label information to the decoder".

We chose to investigate the need for \hat{L}_{class} because other works had proposed a similar idea - that is to train a classifier on reconstructed samples \cite{odena2016conditional, bao2017cvae}. We did not know a priori that this component would be redundant, and only discovered this following investigation. Indeed, it is a useful and relevant finding of our work for the representation learning community. Rather than simply leaving this component out of our model, we chose to perform an extensive ablation study to provide evidence for this.

We would like to make a final note concerning the quotation "so that our model may easily be used to perform classification tasks". This was simply referring to the fact that the encoder outputs an attribute label vector, \hat{y}, and hence may be used as a classifier. As mentioned above, we have amended this section of the text (Section 3.1) to read:

"Additionally, the encoder also acts as a classifier, outputting an attribute vector, \hat{y}, along side a latent vector, \hat{z}."

[Reviewer]
Ultimately, this last point brings us to a good summary of my concerns with the model: the inclusion of too many moving parts, some of which the authors explicitly say later on provide no benefit.

[Authors]
Since our model is no more complex than that of Bao et al., an accepted paper, we assert that this is not grounds for rejection. The original Bao et al. model consists of 6 components (see Equation 7 of Bao et al.). There is only one term in our loss function that following investigation, we considered to be redundant. Rather than just leaving this out, we performed an extensive ablation study to provide evidence for this.

The source of the confusion seems to be summed up by the question, "where is this classification model?". While the reviewer has understood the core contributions of our paper, this misunderstanding has lead to most of the questions above. Simply, the encoder acts as a classifier because it predicts both an attribute, \hat{y}, and a latent vector, \hat{z}. We have amended our paper to make this more clear, adding the following to Section 3.1:

"Additionally, the encoder also acts as a classifier, outputting an attribute vector, \hat{y}, along side a latent vector, \hat{z}."

Crucially, the encoder in our model may be used as a classifier, unlike in other attribute editing models and we demonstrate that our classifier achieves results that are competitive with state of the art classification results.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkemBfdO6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>More responses</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=BkemBfdO6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt; Since our model is no more complex than that of Bao et al., an accepted paper, we assert that this is not grounds for rejection.
&gt; The explanation for using \hat{L}_{class} ...

I covered \hat{L}_{class} in a comment above, but since you are addressing it again, I am happy to as well. 

First, I disagree with your assertion. In my opinion, presenting a model with moving parts that you claim *in the paper* serve no purpose adds needless complexity. 

In my opinion, components of your model that do nothing should not be included in the full description of your model. It  strictly adds unnecessary complexity, and the section should be a description of the full model you are proposing, not every component that was tried along the way. 

The ablation study is great, but it would suffice to mention in a single experimental result that following other literature you tried adding such a term, but as Table 1 demonstrates it didn't help and therefore was excluded from the model. 

The fundamental problem with including components like this just because other papers did is that useless components of models propagate this way. If you feel your paper is the one to finally discover that it is useless, then great! Your paper should be the first to not include it as part of the model.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryex243tpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank your helping us to simplify our model.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=ryex243tpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have now revised our paper and we do not include \hat{L}_{class} in our model description. As per your helpful suggestion, we now have add a single experiment in our ablation study to demonstrate that \hat{L}_{class} was not helpful.

Thank you for this suggestion and for helping us to improve our paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_SkeKbjXdpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Addressing concerns about experimental results.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=SkeKbjXdpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Reviewer]
Moving on to experimental results, I think this is another area where I have a few concerns. First, in Figure 2, the authors argue that your model is "better for 6 out of 10 attributes" and comparable results for most others. The authors include a gap of 0.1 in the "Gray_hair" category as "better" but label a gap of 0.5 in the Black hair category as "comparable." I think results in several of the categories are sufficiently close hat error bars would be necessary to draw actual conclusions. If "better" were to mean "better by 0.5" for example, then the authors method is better on 4 tasks (smiling, blonde hair, heavy makeup, mustache) and worse on 3 (black hair, brown hair, wavy hair).

[Authors]
The quotation "better for 6 out of 10 attributes" is no where to be found in our paper. The paper read "outperformed for 6 out of 10". However, we understand the reviewer's concerns and agree that the values are close. For this reason, throughout the paper we have stressed that the results are competitive. We have amended the text in the results section to reflect this also:

"Results in Figure \ref{fig:state_of_art} show that our model is highly competitive with a state of the art facial attribute classifier \cite{zhuang2018multi}. We outperformed by more than 1% on $2$ out of $10$ categories, underperformed by more than 1% on only $1$ category and remained competitive with all other attributes."

[Reviewer]
With respect to the actual attribute editing, my main concern here is a lack of comparison to models other than Bao et al., despite the fact that face attribute changing is an exhaustively studied task. A number of papers like Perarnau et al., 2016, Upchurch et al., 2017, Lample et al., 2017 and others study this task from machine learning perspectives, and in some cases can perform photorealistic image attribute editing without complicated machinery on megapixel face images. 

[Authors]
We would like to thank the reviewer for pointing out additional related work, this has helped us to improve the related work section of our paper.

The focus of our work has been to learn a representation that factors attribute information from the rest of the representation. We test this factorization process in two ways: (1) attribute editing and (2) attribute classification.

Perarnau et al., 2016 is similar to our model without the L_{aux}, \hat{L}_{class} or \hat{L}_KL. While Perarnau et al. does perform image attribute editing, they do not present classification results.

The method proposed by Upchurch et al., 2017 requires a reverse mapping procedure which is very computationally intensive, applying gradient descent in image space. Additionally, the focus of the work by Upchurch et al., 2017 is not on representation learning and may not be used for image classification. 

In our paper, the goal is to learn representations for images. For this goal, our encoder network needs to encode more than just the attribute-invariant information, but also the attribute information itself. The encoder of the Fader Network proposed by Lample et al., 2017 predicts only the attribute-invariant information, while our encoder network predicts both attribute-invariant information and the attribute. This makes our model not only suitable for attribute editing, but it also makes our model suitable for classification. Ultimately, developping models capabable of more than just one task are exciting and important steps forward for the field. 

We have updated our paper to include references to Perarnau et al., 2016, Upchurch et al., 2017 and Lample et al., 2017, as per the reviewer's suggestions, to strengthen the related work section of our paper.

[Reviewer]
At least the images in Figure 3 and 4 are substantially downsampled from the typical resolution found in the Celeba dataset, suggesting that there was some failure mode on full resolution images.

[Authors]
We use the standard image size, 64x64, and these were used directly without down-sampling. Unfortunately, some resolution was unintentionally lost in Figure 4, when annotated with \hat{y}=0 and \hat{y}=1. We have rectified this and have updated the image with a higher resolution version. However, images in Figure 3 were not affected and we are not aware of any failure modes.

We use images of size 64x64, rather than larger image sizes, (a) to make our ablation study more computationally feasible and (b) to make our results more reproducible by those with modest resources. We believe that this is generally good for the field.

We provide extensive quantitative results which show (a) that the attribute manipulation is reliable and (b) that we achieve a low reconstruction error. Additionally, our classification results are further evidence that our model does indeed factor attribute information from the rest of the latent vector, which is our objective.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJgjWSOOpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>More responses</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=SJgjWSOOpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt; The quotation "better for 6 out of 10 attributes" is no where to be found in our paper. The paper read "outperformed for 6 out of 10". 

Okay.

&gt; However, we understand the reviewer's concerns and agree that the values are close...

Thanks. I do feel that the updated text more honestly and scientifically represents the results of Figure 2.

&gt; The focus of our work has been to learn a representation that factors attribute information from the rest of the representation. We test this factorization process in two ways: (1) attribute editing and (2) attribute classification.

I agree that you present results on both attribute classification and attribute editing. My concern is whether it is clear that you perform either task significantly better than state of the art methods in either task. I focused on attribute editing papers simply because this is a case where I think there are very strong baselines that work on megapixel images. 

You argue that you use smaller images "(a) to make our ablation study more computationally feasible and (b) to make our results more reproducible by those with modest resources." I definitely agree it is useful to include the smaller results for this purpose. However, my concern is that a lack of any results on larger face images makes it difficult to compare your approach with methods that succeed at editing larger images.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rygdXunFTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our results are sufficient to confirm that our model achieves good factorization -- the objective of our paper.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=rygdXunFTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Reviewer]
I agree that you present results on both attribute classification and attribute editing. My concern is whether it is clear that you perform either task significantly better than state of the art methods in either task.

[Authors]

Our work presents a method for learning representations that factor the attribute information from the rest of the latent representation. To evaluate our factorisation method we perform facial attribute editing and classification. Our results are (more than) sufficient to confirm that our model achieves good factorization -- it is not necessary for our model to achieve state of the art results to confirm this.

We do indeed claim that our results are competitive with a state of the art classification, but we do not claim to present state of art classification results. Please note that if we were aiming to achieve state of the art classification we would, for example, have used much deeper networks,  Zhuang et al. 2018 use 13 layers while our encoder (which we use for classification) has only seven.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJlTePhKT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We have included comparison to IcGAN</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=rJlTePhKT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Reviewer]

You argue that you use smaller images "(a) to make our ablation study more computationally feasible and (b) to make our results more reproducible by those with modest resources." I definitely agree it is useful to include the smaller results for this purpose. However, my concern is that a lack of any results on larger face images makes it difficult to compare your approach with methods that succeed at editing larger images.

[Authors]

For work that focuses only on attribute editing, it may make sense to consider higher resolution images, however we focus on representation learning, where it is common to used images at resolution 64x64 (or less) [higgins2016beta, bao2017cvae, li2017alice, larsen2016autoencoding, burgess2018understanding, kumar2018variational]. Of the three papers you propose (Upchurch et al., Lample et al., Perarnau et al.) none of them are motivated by representation learning and only Perarnau et al. proposes an encoder that outputs a representation that is sufficient to describe an input image.

As per your suggestion, we have included a comparison with IcGAN (Perarnau et al.) in Tables 1 and 3, taking values from Lample et al. We compare to IcGAN since they also train on images that are 64x64. Our model without residual layers obtains that same reconstruction error as IcGAN, 0.028, while our model with residual layers achieves a much lower reconstruction error, 0.011. Lample et al. also suggests that the IcGAN only successfully edits attributes (Smiling --&gt; Not Smiling) 9.9% of the time, while our model (with residual layers) successfully edits them at least 98% of the time. Our model without residual layers edits successfully edits them 81% of the time.

[Please note that there is a typo in the Lample et al. paper, the authors write RMSE rather than MSE. We have contacted the authors and they confirm that this was a typo and they were in fact reporting MSE.]

Thank you for helping us to improve our paper with additional comparisons to related work.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_S1llhcmdTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our model is no more complex that related models and we compare to a state of art classification model.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=S1llhcmdTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The reviewer's main concerns appear to be complexity of the model and comparison to related work.

[1] Complexity:
The reviewer's main concern is that the model is too complex, however, our proposed model is no more complex than the accepted paper of Bao et al. Our cost has the same number of components and hyper parameters and our model has the same number of networks (our encoder network has two outputs). Most of our components are also less complex because losses are computed on network outputs rather than on features extracted from multiple intermediate layers. Additionally, we demonstrate that terms in our loss function, \hat{L}_{class}, may be excluded, making our model less complex.

Throughout our work we have been intentionally explicit and detailed about the costs we use. This may have resulted in the complexity of our approach being excessively emphasised, however, it is merely a thorough presentation of our idea, intended to make the work reproducible. Complexity appears to be the reviewer’s main concern, however, since our paper is no more complex than papers previously accepted, we assert that our paper, detailing a novel approach, should be accepted. 


[2] Comparison to related work:
The focus of our work has been to learn a representation that factors attribute information from the rest of the representation. We test this factorization process in two ways: (1) attribute editing and (2) attribute classification. 

The papers recommended by the reviewer focus only on attribute editing and not on representation learning and they (Upchurch et al., 2017 and Lample et al., 2017) may not be used for, or (Perarnau et al., 2016) have not been demonstrated for attribute classification. To the best of our knowledge, our work is the only approach to learn disentangled representations that may be applied to both image attribute manipulation and simultaneously achieves competitive results with state of the art models on image classification. This novel versatility of the model is certainly a strength of our paper and grounds for acceptance.

When comparing our model to previous work, we chose the most challenging benchmark for facial attribute classification, not just comparing to models intended for attribute editing. Our classification results are highly competitive with this benchmark.

We hope that following the revisions suggested by the reviewer and the inclusion of recommended citations, the reviewer will take our response into consideration and revise their assessment of our paper. Thank you.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Hyecgv9dhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>missing references to previous work </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=Hyecgv9dhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper851 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: 

This paper builds upon the work of Boa et al (2017 ) (Conditional VAE GAN) to allow attribute manipulation in the synthesis process. 

In order to disentangle the identity information from the attributes the paper proposes adversarial information factorization : let z be the latent code and y be the attribute the paper proposes to have p(y) =  p(y|z= E_phi(x)), i.e to have z independent of y.  This disentanglement is implemented through a GAN on the variable y  min _phi Distance (p(y), p(y|z)), the distance is defined via a discriminator on y.  

Experiments are presented on celeba dataset,  1) on attribute manipulation from smiling to non smiling for example, on 2) attribute classification results are presented , 3) ablation studies are given to study the effect of each component of the model highlighting the effect of the adversarial information factorization. 

Originality Novelty: 

There is a large body of work on disentanglement that the paper does not cite or compare to for instance, InfoGAN,  Beta- VAE <a href="https://openreview.net/pdf?id=Sy2fzU9gl" target="_blank" rel="nofollow">https://openreview.net/pdf?id=Sy2fzU9gl</a> and disentangled latent concepts https://arxiv.org/pdf/1711.00848.pdf

Note that for example that in beta- VAE it is a similar idea where but it is on z and z|x and the distance used is KL (since it is has closed form with gaussian) , min_phi Loss+ beta KL (p(z), p(z|x)), a discussion of the previous related work in the paper is necessary.  

The work is also related to MINE https://arxiv.org/pdf/1801.04062.pdf where one would like to minimize the mutual information I(z;y)  this mutual information is estimated through a min/max game.
 
Questions: 

-  why is RMSprop used for optimization, your model and the Bao et al baseline might benefit from the use of Adam?

- (Table 3 in appendix ) Have you tried higher values of alpha the weight of KL, with the model of Bao et al (it is recommended in beta VAE to have high value of what you call alpha)?

Overall assessment: 

The paper novelty is using min/max game to estimate the mutual information between y (attribute) and z (identity code). Disentanglement and use of min/max games for estimating mutual information has been explored before.  Further discussion and comparaison to previous work is needed. 



 

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyeFIpxXam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for helping us to improve our paper.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=SyeFIpxXam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to sincerely thank the reviewer for reading and understanding our paper and for providing constructive feedback. We have addressed all of the comments below and in one case we respectfully ask for some clarification, please. The feedback from the reviewer has been very helpful for improving our paper (please see the updated version).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ryx4STlmaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Improvement to related work.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=ryx4STlmaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Reviewer]
There is a large body of work on disentanglement that the paper does not cite or compare to for instance, InfoGAN,  Beta-VAE <a href="https://openreview.net/pdf?id=Sy2fzU9gl" target="_blank" rel="nofollow">https://openreview.net/pdf?id=Sy2fzU9gl</a> and disentangled latent concepts https://arxiv.org/pdf/1711.00848.pdf ([Authors] (DIP-VAE)).

[Authors]
We appreciate the additional references that the reviewer proposed, two of which we had already included (beta-VAE and InfoGAN). Based on these suggestions we have improved the related work section of our paper by adding the following:

"Finally, while we use labelled data to learn representations, we acknowledge that there an many other models that learn factored, or disentangled, representations from unlabelled data including several VAE variants \citep{higgins2016beta, kumar2018variational}."

We would also like to draw the reviewer's attention to the six instances where we had already cited InfoGAN in our paper and the one instance of beta-VAE.

Below is one quoted example where we investigated the inclusion of a component of our model that was inspired by a similar approach in InfoGAN (Section 4.1, page 6): 

"Using \hat{\mathcal{L}}_{class} does not provide any clear benefit. We explored the effect of including this term since a similar approach had been proposed in the GAN literature \cite{chen2016infogan,odena2016conditional} for conditional image synthesis (rather than attribute editing). To the best of our knowledge, this approach has not been used in the VAE literature. This term is intended to maximise I(x,y) by providing a gradient containing label information to the decoder, however, it does not contribute to the factorization of attribute information, y, from \hat{z}."

Though we have cited beta-VAE (Section 4.3, page 8), we did not make a direct comparison to beta-VAE since it is trained without labelled data and a classifier is trained post-hoc; this is similar to the DIP-VAE (an improvement on the beta-VAE). Instead, we chose to compare our results to a state of the art classification model, since it outperforms all other methods, including those whose objective is to learn disentangled representations. For the sake of completeness, below is the comparison of our classification results (which are competitive with state of the art) and those reported in the DIP-VAE paper:

	Label		|	DIP-VAE		|	ours	|
-———————————————————————
Black hair		|	80.6 		|	89.8 	|
Blonde hair		|	91.9  	        | 	97.2 	|
Heavy Makeup	|	81.5 		|	92.8 	|
Wavy hair		|	71.5. 		| 	84.5 	|
Lipstick. 		|	84.7 		|	94.4 	|

As is clear, our model strongly out-performs DIP-VAE, which is why we did not explicitly compare with these results in our paper, and rather chose to compare to a state of the art classification model. We felt this was a vastly more challenging benchmark.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1eFm6xmpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Improvements to related work and a request for clarification.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=B1eFm6xmpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Reviewer]
Note that for example that in beta-VAE it is a similar idea where but it is on z and z|x and the distance used is KL (since it is has closed form with gaussian), min_phi Loss+ beta KL (p(z), p(z|x)), a discussion of the previous related work in the paper is necessary.  

[Authors]
The authors whole heartedly appreciate the contributions beta-VAE has made to the field and specifically to representation learning. As per the reviewer's helpful suggestion, we have added the following to improve the related work section of our paper (adding an additional citation to both beta-VAE and Burgess et al.):

"The beta-VAE \cite{higgins2016beta} objective is similar to the information bottle neck \cite{burgess2018understanding}, minimizing mutual information, I(x;z), which forces the model to exploit regularities in the data and learn a disentangled representation. In our approach we perform a more direct, supervised, factorisation of the latent space, using a mini-max objective, which has the effect of approximately minimizing I(z;y)."

[Reviewer]
The work is also related to MINE <a href="https://arxiv.org/pdf/1801.04062.pdf" target="_blank" rel="nofollow">https://arxiv.org/pdf/1801.04062.pdf</a> where one would like to minimize the mutual information I(z;y) this mutual information is estimated through a min/max game.

[Authors]
Thank you very much for the pointers to additional literature.

We have one question about the connection to MINE (Belghazi et al): To the best of our knowledge, we cannot find the implied connection of MINE with performing an explicit mini-max game, which our paper proposes?

The method presented in the paper, Belghazi et al. 2018, learns a model, T_\theta, that takes set of two variables (e.g. a \in A, b \in B) as input and predicts the mutual information (e.g I(A;B)). According to Algorithm 1 of Belghazi et al., T_\theta is learned via gradient ascent only, there is no mini-max objective for estimating T_\theta. Depending on the application, the mutual information may be minimized or maximized.

Two applications that Belghazi et al. propose include:
(1) Using T_\theta as a regularizer in a GAN, maximizing the mutual information, I(x;z), between data, x and latent code, z.
   (a) The only minimax game here is between the generator and discriminator.
   (b) There is no mini-max objective for estimating T_\theta.
   (c) The purpose of using T_\theta as a regularizer is to prevent mode dropping.
(2) T_\theta is used to approximate the mutual information term, I(x;z), in the information bottleneck. In this example, I(x,z), is minimized.
   (a) There is no minimax game here.
   (b) There is no mini-max objective for estimating T_\theta.

The only mention of mini-max we found is in Equation 16, which only corresponds to a GAN setting but does not correspond to their method for approximating mutual information.

Additionally, we could not find any example of mutual information computed between a latent code z and a label y, which is our setting.

Could you let us know what you had in mind when making that connection?

Thank you in advance.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hkg9k6gmTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Addressing Questions Of Reviewer 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=Hkg9k6gmTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Reviewer]
Questions: 
-  why is RMSprop used for optimization, your model and the Bao et al baseline might benefit from the use of Adam?

[Authors]
We experimented with both RMSprop and Adam for both the Bao et al model and ours and generally found RMSprop to give better quality reconstructions.

[Reviewer]
- (Table 3 in appendix ) Have you tried higher values of alpha the weight of KL, with the model of Bao et al (it is recommended in beta VAE to have high value of what you call alpha)?

[Authors]
As discussed in the beta-VAE paper, higher beta values (in our case alpha) often lead to worse reconstruction, which in this case would mean worse preservation of identity. We refer to this in our paper, referencing beta-VAE (Section 4.3, page 8): 

"It is challenging to learn a representation that both preserves identity and allows factorisation \cite{higgins2016beta}"

We have indeed tried higher values of alpha and observed how these affect classification and reconstruction. For alpha=1.0, in our model that uses res-nets, the MSE rises to 0.041 (very poor reconstruction) and we generally see no improvement in classification. We point the reviewer to Section 4.3 where we discuss this: 

"We found that the naive cVAE-GAN (Bao et al. \cite{bao2017cvae}) failed to synthesise samples with the desired target attribute ‘Not Smiling’. This failure demonstrates the need for models that can deal with both reconstruction and attribute-editing. Note that we achieve good reconstruction by reducing weightings on the KL and GAN loss terms, using \alpha=0.005 and \delta=0.005 respectively."

In most VAE based models there is a trade off between reconstruction and factorization. In our model, factorization comes from the auxiliary loss so the KL term may be weighted less strongly, hence we are able to use a small alpha weighting on the KL term.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SkgHp2l76m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Improved Related Work.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=SkgHp2l76m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Reviewer]
Overall assessment: 
The paper novelty is using min/max game to estimate the mutual information between y (attribute) and z (identity code). Disentanglement and use of min/max games for estimating mutual information has been explored before.  Further discussion and comparison to previous work is needed. 

[Authors]
As mentioned above, to the best of our knowledge, we cannot find the implied connection of MINE with performing an explicit mini-max game, which our paper proposes. We would appreciate it if the reviewer could please let us know what they had in mind when making this connection?

We appreciate the additional references that the reviewer proposed, two of which we had already included (beta-VAE and InfoGAN). Based on these very helpful and constructive suggestions we have improved the the related work section of our paper, by adding the following:

"Finally, while we use labelled data to learn representations, we acknowledge that there are many other models that learn factored, or disentangled, representations from unlabelled data including several VAE variants \citep{higgins2016beta, kumar2018variational}. The beta-VAE \cite{higgins2016beta} objective is similar to the information bottleneck \cite{burgess2018understanding}, minimizing mutual information, I(x;z), which forces the model to exploit regularities in the data and learn a disentangled representation. In our approach we perform a more direct, supervised, factorisation of the latent space, using a mini-max objective, which has the effect of approximately minimizing I(z;y)."

We agree that disentanglement has indeed been studied before, however, when making comparisons we have focused on comparing to models that, like ours, make use of labelled data. When comparing our model to previous work, we chose the most challenging benchmark for facial attribute classification, not just those that use disentangled representations. Those that use disentangled representations perform worse than this benchmark. Our classification results are highly competitive with this benchmark.

To the best of our knowledge, our work is the * only * approach to learn disentangled representations which enable image attribute manipulation and simultaneously achieves competitive results with state of the art models on image classification. We believe the demonstrated versatility and novelty of this work are strong grounds for acceptance.

Again, we would like to sincerely thank the reviewer for helping us to improve our paper with their constructive suggestions.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Syglr36U2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea, Too complex model</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=Syglr36U2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper851 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposed a generative model to learn the representation which can separates the identity of an object from an attribute. Authors extended the autoencoder adversarial by adding an auxiliary network. 

Strength
The motivation of adding this auxiliary network, which is to distinguish the information between latent code z and attribute vector y, is clean and clear.
Experiments illustrate the advantage of using auxiliary network and demonstrating the role of classify. Experimental results also show the proposed model learning to factor attributes from identity on the face dataset.

Weakness 
The proposed model seem to be unnecessarily complex. For example, the loss of  in (6) actually includes 6 components (5 are from L_enc) and 4~5 tuning hyper-parameters. The L_gan also includes 3 parts. The reason of adding gan loss lacks either theoretical or empirical analysis. So as L_KL. In addition, the second term in L_gan is unnecessary since you already have a reconstruction loss. It also make it to be unclear what we obtain if the equilibrium of the GAN objective achieved.

The written of this paper can be improved to make it more clear. 
It looks \hat_y and \tilde_y are same thing. 
How do you get \hat_z? Do you assume the posterior distribution is Gaussian and use the reparameterization trick? What are \hat_y and \hat_\hat_y? Are they binary or a scalar between 0 and 1?  How do you generate \hat_x? When generating \hat_x, do you sample \hat_z and \hat_y? If so, how do treat the variance problem of \hat_y? 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyxqTW-va7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for acknowledging that our paper is `clear' and `interesting'</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=SyxqTW-va7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Reviewer]
This paper proposed a generative model to learn the representation which can separates the identity of an object from an attribute. Authors extended the autoencoder adversarial by adding an auxiliary network.

[Reviewer]
Strength
The motivation of adding this auxiliary network, which is to distinguish the information between latent code z and attribute vector y, is clean and clear.
Experiments illustrate the advantage of using auxiliary network and demonstrating the role of classify. Experimental results also show the proposed model learning to factor attributes from identity on the face dataset.

[Authors]
We thank the reviewer for acknowledging this paper is “clear”, “interesting” and that experiments presented in our paper do indeed support our proposed method. The reviewer appears to have a very good understanding of the contributions made in our paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rklJqbWPp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We have addressed comments from reviewer 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=rklJqbWPp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Reviewer]
Weakness 
The proposed model seem to be unnecessarily complex. For example, the loss of in (6) actually includes 6 components (5 are from L_enc) and 4~5 tuning hyper-parameters. 

[Authors]
We appreciate that our model has several components, however, the original Bao et al. model also consists of 6 components (see Equation 7 of Bao et al.) along with 4 hyper-parameters. Since our model is no more complex than that of Bao et al. — an accepted paper — we assert that this should not be seen as a weakness of our paper.

Despite having a few hyper-parameters, our model does not require extensive hyper-parameter tuning. However, to obtain high fidelity reconstructions it is necessary to select low values of delta (the weight on the L_gan term) and alpha (the weight on the KL term).

[Reviewer]
The L_gan also includes 3 parts. 

[Authors]
The L_gan term is similar to the one used by Bao et al, which also has three components. Note also that the GAN loss is intended purely to improve the visual quality of the samples. Our contribution is still valid without L_gan, since our main contribution is the introduction of an auxiliary network, A_\psi, and the loss, L_aux.

[Reviewer]
The reason of adding gan loss lacks either theoretical or empirical analysis. So as L_KL.

[Authors]
The use of the gan loss and the KL loss are motivated already by Bao et al. in the VAE-GAN which we introduce in Section 2.

According to our understanding of the GAN literature, it is generally accepted that GAN loss improves the visual quality of samples. Experimentally, we found this to be true for our model also.  Similarly, regularisation of the latent space helps with generalisation to test samples. We found that our model performed better with a small amount of KL regularisation than without any, and that models trained without regularisation overfit and had very poor reconstruction (MSE=0.0381). 

Interestingly, when our model is trained without a GAN loss or KL loss, it is still able to edit attributes with high accuracy, however, the visual quality of samples is poor. This shows that the attribute information is still factored from the rest of the latent representation, which is the main contribution of our work. 

[Reviewer]
In addition, the second term in L_gan is unnecessary since you already have a reconstruction loss. It also make it to be unclear what we obtain if the equilibrium of the GAN objective achieved.

[Authors]
We use a similar L_gan to that used by Bao et al. Please refer to Algorithm 1, line 9 of Bao et al. In the cVAE-GAN there are two sources of `fake' images, (a) reconstructed images, D_\theta(E_\phi(x)) and (b) sampled images, D_\theta(z), z ~ p(z), where p(z) is the prior. This is why there are three terms in the GAN loss. Reconstruction loss alone is often not enough to achieve high quality reconstruction.

We explicitly wrote out the GAN loss as three terms for clarity, but the GAN loss could still be written as two terms, where C_\chi is the discriminator:
E_p_real(x) log C_\chi(x) + E_p_fake(x) log (1 - C_\chi(x)), where sampling x_fake ~ p_fake(x), includes both reconstructed images and sampled images. This is the same as the original GAN loss.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Skg76lIDpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>thanks for clarifying</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=Skg76lIDpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1) While these components(e.g., L_gan and L_KL) are verified in Bao et al., it is still possible that they are not necessary in this model since you add some new staff. Since you have the experiments which demonstrates the importance of adding these staffs, can you add the results to clarify this or at least add several sentences to mention this?

2) In the original GAN, if the equilibrium of min-max objective is achieved, we will have p_data = p_model. Is there anything similar in your model? What will we obtain if the equilibrium of your min-max objective is achieved? This part seems to be not very clear and make your method to be not that "principle".</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1lH7aDuTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Added results for our model trained without L_gan or L_KL</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=B1lH7aDuTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Reviewer]
1) While these components(e.g., L_gan and L_KL) are verified in Bao et al., it is still possible that they are not necessary in this model since you add some new staff. Since you have the experiments which demonstrates the importance of adding these staffs, can you add the results to clarify this or at least add several sentences to mention this?

[Authors]
Thank you for the suggestion. We have updated Table 3 to include results for our model trained without L_gan and without L_KL and added additional text to the appendix. We have also added Figure 6 which demonstrates the blurred images obtained if the GAN loss is not used.


[Reviewer]
2) In the original GAN, if the equilibrium of min-max objective is achieved, we will have p_data = p_model. Is there anything similar in your model? What will we obtain if the equilibrium of your min-max objective is achieved? This part seems to be not very clear and make your method to be not that "principle".

[Authors]

The objective function as we have written it above, E_p_real(x) log C_\chi(x) + E_p_fake(x) log (1 - C_\chi(x)), is in the exact same form as the original objective, simply with different notation. In our case, we refer to p_data and p_model as p_real and p_fake respectively. C_\chi is the discriminator. Therefore, if the generator (in our case the decoder) and the discriminator are optimal, it follows that p_real = p_fake. Recall (from above) p_fake is the distribution of reconstructed and synthesised images (p_model) and p_real are samples from the (training) data (p_data). Therefore, when optimal the reconstructed and synthesised samples appear to come from the same distribution as the training data.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_Skg-zbZwTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Addressing questions of reviewer 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=Skg-zbZwTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Reviewer]
The written of this paper can be improved to make it more clear. 
It looks \hat_y and \tilde_y are same thing. 

[Authors]
Line 6 of Algorithm 1 (as well as Figure 1) show that \hat{y} is one of the outputs of the encoder. This is also described in Section 2.4: “the encoder outputs both a latent vector, \hat{z}, and an attribute vector, \hat{y}”

Line 8 of Algorithm 1 (as well as Figure 1) show that \tilde{y} is the output of the auxiliary classifier. This is also described in Section 3.1: "We introduce an auxiliary network, A_\psi : \hat{z} —&gt; \tilde{y},”

To re-iterate, \hat{y} is one of the outputs of the encoder. \tilde{y} is the output of the auxiliary classifier. They are not the same thing.

[Reviewer]
How do you get \hat_z? Do you assume the posterior distribution is Gaussian and use the reparameterization trick?

[Authors]
Yes, we assume a Gaussian prior and posterior and use the re-parametrization trick. On page 4 we say “We integrate this novel factorisation method into a VAE-GAN.” And we describe VAE-GANs in Section 2. Specifically, we describe the re-parametrization trick in Section 2.1:

"The encoder predicts, \mu_\phi(x) and \sigma_\phi(x) for a given input x and a latent sample, \hat{z}, is drawn from q_\phi(z|x) as follows: \epsilon \sim \mathcal{N}(\mathbf{0},I) then z = \mu_\phi(x) + \sigma_\phi(x) \odot \epsilon."

There was a typo here, the paper now reads: “then \hat{z} = \mu_\phi(x) + \sigma_\phi(x)”

[Reviewer]
What are \hat_y and \hat_\hat_y? Are they binary or a scalar between 0 and 1? 

[Authors]
Line 6 of Algorithm 1 (as well as Figure 1) show that \hat{y} is one of the outputs of the encoder.

Line 8 of Algorithm 1 (as well as Figure 1) show that \hat\hat{y} is the “predicted label of a reconstructed data sample”. 

\hat_y and \hat_\hat_y are continuous scalars between [0,1]. We have now added in Section 2.5:  “\hat{y} \in [0,1]” and in Section 3.1 we have added “\hat{\hat{y}} \in [0,1]” to make this more clear.

Thank you for helping us to improve our paper with this comment.

[Reviewer]
How do you generate \hat_x? When generating \hat_x, do you sample \hat_z and \hat_y? If so, how do treat the variance problem of \hat_y? 

[Authors]
To obtain \hat{x} we pass \hat{z} and \hat{y} through the decoder, D_\theta. During training \hat{z} and \hat{y} are the output of the encoder, which takes a sample from the data as input. During attribute manipulation, \hat{z} is still the output of the encoder, and we set \hat{y} to 0 or 1. This is detailed in the paper as follows:

During training:
Lines 4, 6, and 7 of Algorithm 1 show how \hat_x is synthesised. 
Line 4: An image x is sampled from the data
Line 6: x is passed through the encoder, which outputs \hat{z} and \hat{y}.
Line 7: \hat{z} and \hat{y} are concatenated and passed to the decoder.

During attribute manipulation: 
Quote taken from Section 3.3 of our paper:
we encode the image to obtain a \hat{z}, the identity representation, append it to our desired attribute label, \hat{y} &lt;— y, and pass this through the decoder. We use \hat{y}=0 and \hat{y}=1 to synthesize samples in each mode of the desired attribute e.g. `Smiling' and `Not Smiling'.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rklwCe-Pa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our model is no more complex than others.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=rklwCe-Pa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The reviewer's main concern is that the model is too complex, however, our proposed model is no more complex than the accepted paper of Bao et al. Our cost has the same number of components and hyper parameters and our model has the same number of networks (our encoder network has two outputs). Most of our components are also less complex because losses are computed on network outputs rather than on features extracted from multiple intermediate layers. Additionally, we demonstrate that terms in our loss function, \hat{L}_{class}, may be excluded, making our model less complex.

Throughout our work we have been intentionally explicit and detailed about the costs we use. This may have resulted in the complexity of our approach being excessively emphasised, however, it is merely a thorough presentation of our idea. If complexity is the reviewer’s main concern, our paper is no more complex than papers previously accepted. If this is the main criticism, our paper should be accepted.

We would again like to thank the reviewer for their constructive feedback, which has enabled us to improve our paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1ldvnemam" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=r1ldvnemam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1gtDi-g3m" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=B1gtDi-g3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper851 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ryxDOqG0qQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Why not cite FaderNetworks in NIPS2017</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=ryxDOqG0qQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Oct 2018</span><span class="item">ICLR 2019 Conference Paper851 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I just wonder why not cite "Fader Networks:Manipulating Images by Sliding Attributes" in NIPS2017.  In my opinion,  The adversarial factorisation in latent space in this paper is quite similar with the referred paper.  Both aim to disentangle the attribute-invariant representations and the attribute labels via an adversarial classifier. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJguthVAc7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We predict both the *attribute* and the attribute-invariant information, making our model suitable for classification.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJfRpoA9YX&amp;noteId=BJguthVAc7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper851 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Oct 2018</span><span class="item">ICLR 2019 Conference Paper851 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hello, 

Thank you for showing interest in our paper and for pointing us to this work. We agree that there are some similarities between our work, however there is also a key difference.

In our paper, the goal is to learn representations for images, for this our encoder network needs to encode more than just the attribute-invariant information, but also the attribute information itself. The encoder of the Fader Network predicts only the attribute-invariant information, while our encoder network predicts both attribute-invariant information and the attribute. This makes our model not only suitable for synthesising images, but it also makes our model suitable for classification. 

We present classification results, using our model, that are highly competitive with state of the art classification results of  Zhuang et al. (2108).

(Additional details)

As mentioned above, unlike in the Fader Networks, our encoder model predicts attribute information, \hat{y}, along side the attribute-invariant information, \hat{z}, which we refer to as identity. During training, our decoder network is fed with predicted attribute values, \hat{y}, rather than the ground truth values, y, as in the Fader Networks. Training the decoder to reconstruct images from predicted attributes, \hat{y}, combined with the adversarial factorization, forces the encoder network to put attribute information into \hat{y}, resulting in the encoder being an excellent classifier.

We will add Fader Networks to the related work section of our paper in the next revision.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>