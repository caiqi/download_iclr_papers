<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Cross-Entropy Loss Leads To Poor Margins | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Cross-Entropy Loss Leads To Poor Margins" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ByfbnsA9Km" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Cross-Entropy Loss Leads To Poor Margins" />
      <meta name="og:description" content="Neural networks could misclassify inputs that are slightly different from their training data, which indicates a small margin between their decision boundaries and the training dataset. In this..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ByfbnsA9Km" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Cross-Entropy Loss Leads To Poor Margins</a> <a class="note_content_pdf" href="/pdf?id=ByfbnsA9Km" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019cross-entropy,    &#10;title={Cross-Entropy Loss Leads To Poor Margins},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=ByfbnsA9Km},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=ByfbnsA9Km" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Neural networks could misclassify inputs that are slightly different from their training data, which indicates a small margin between their decision boundaries and the training dataset. In this work, we study the binary classification of linearly separable datasets and show that linear classifiers could also have decision boundaries that lie close to their training dataset if cross-entropy loss is used for training. In particular, we show that if the features of the training dataset lie in a low-dimensional affine subspace and the cross-entropy loss is minimized by using a gradient method, the margin between the training points and the decision boundary could be much smaller than the optimal value. This result is contrary to the conclusions of recent related works such as (Soudry et al., 2018), and we identify the reason for this contradiction. In order to improve the margin, we introduce differential training, which is a training paradigm that uses a loss function defined on pairs of points from each class. We show that the decision boundary of a linear classifier trained with differential training indeed achieves the maximum margin. The results reveal the use of cross-entropy loss as one of the hidden culprits of adversarial examples and introduces a new direction to make neural networks robust against them.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Cross-entropy loss, Binary classification, Low-rank features, Adversarial examples, Differential training</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We show that minimizing the cross-entropy loss by using a gradient method could lead to a very poor margin if the features of the dataset lie on a low-dimensional subspace.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_r1lrQuRxpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>weight decay baseline</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByfbnsA9Km&amp;noteId=r1lrQuRxpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Angus_Galloway1" class="profile-link">Angus Galloway</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper685 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Could the authors compare their new loss function to *SGD* with cross entropy loss and (L2) weight decay with large regularization penalties?</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ryx8tGiohX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A set of nice results that is insightful and clarifies some controversy </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByfbnsA9Km&amp;noteId=ryx8tGiohX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper685 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper685 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper challenges recent claims about cross-entropy loss attaining max margin when applied to linear classifier and linearly separable data. Along the road, it presents a couple of nice results that I find quite interesting and I believe they provide useful insights. Finally it presents a simple modification to the cross-entropy loss, which the authors refer to as differential training, that alleviates the problem for the case of linear model and linearly separable data.

CONS:
I find the paper useful and interesting mainly because of its insightful results rather than the final algorithm. The algorithm is evaluated in a very limited setting (linear model, synthetic data, binary classification); it is not clear if similar benefits would carry over to nonlinear models such as deep networks. In fact, I strongly encourage the authors to do a generalization comparison by comparing the **test accuracy** obtained by their modified cross-entropy against: 1. Vanilla cross-entropy as well as 2. A deep model large margin loss function (e.g. as in "Large Margin Deep Networks for Classification" by Elsayed). Of course on a realistic architecture and non-synthetic datasets (e.g. CIFAR-10).

PROS:
Putting the algorithm aside, I find the theorems interesting. In particular, Theorem 3 shows that some earlier claims about cross-entropy's ability to attain large margin (in the linearly separable case) is misleading (due to neglecting a bias term). This is important as it changes the faith of the community in cross-entropy and more importantly creates hope for constructing new loss functions with improved margin.
I also find the connection between the dimension of the subspace that contains the points and quality of margin obtained by cross-entropy insightful.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BylxpBRx6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByfbnsA9Km&amp;noteId=BylxpBRx6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper685 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper685 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Reviewer 1,

Thank you for reading our submission closely, and thanks for appreciating our results.

As you have also noticed, Section 3 of our paper, and Theorem 3 in particular, is the punch line of our work. The algorithm, differential training, was introduced in this paper only to open a door for further research and not to finish this paper with a negative result. That is, we wanted to show that there could be a solution for the problem we have identified. We agree that further study of differential training for neural networks is necessary and important, and that is our ongoing work.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJegWQkshX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting work, but slightly incremental</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByfbnsA9Km&amp;noteId=rJegWQkshX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper685 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper685 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies the cross-entropy loss for binary classification problems. The authors show that if the norms of samples in two linear separable classes are different, gradient descent based methods minimizing cross-entropy loss may give a linear classifier that gives small margin.

Pros

1. The paper is clearly written and very easy to follow. 

2. The authors show that for two point classification problems, if the norms of the points are very different then gradient descent will give a very small margin.

3. Further theoretical results are given explaining the relation between cross-entropy loss and SVM.

4. A new loss function called differential training is proposed, which is guaranteed to give SVM solution.

Cons

1. My biggest concern is that, the paper, especially the title, may be slightly misleading in my opinion. Although the authors keep claiming that cross-entropy loss can lead to poor margins in certain circumstances (which I agree), in fact Theorem 1 and Theorem 2 have already clearly shown the connection between the cross-entropy solution and the maximum margin direction. For example, Theorem 1 literally proves that when the two points have the same norm (normalized data?), cross-entropy loss leads to maximum margin. Theorem 2 also clearly states that cross-entropy loss and SVM are closely related. Based on these two theorems, perhaps â€˜cross-entropy loss is closely related to maximum marginâ€™ is a more convincing statement.

2. The theoretical results given in this paper is slightly incremental. As the authors mentioned, Theorem 1 and Theorem 2 are essentially already proved in previous works. The other results are not very significant either.

3. The authors do not clearly state the advantages of the differential training method compared to SVM. It seems that one can just use SVM if the goal is maximum margin classifier.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJgZ7PCga7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByfbnsA9Km&amp;noteId=HJgZ7PCga7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper685 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper685 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Reviewer 3,

Thanks for reviewing our paper.

1a) The goal of our submission is not to make further positive claims about the use of cross-entropy minimization; it is the opposite. As Reviewer 1 also stated, we wanted to challenge the faith of the community in the use of cross-entropy loss, and we wanted to show that minimizing this loss function on low-dimensional datasets such as images can lead to extremely poor margins. For this reason, the title of our submission is very accurate. We updated Figure 1 to highlight the drastic difference between the SVM solution and the solution obtained by the cross-entropy minimization.

1b) As we clearly stated in Remark 1, normalizing a dataset in the input space does not correspond to normalizing the features of the points if the feature mapping is nonlinear. In particular, we will not have normalized features if we use neural networks. If we want to get a right intuition about the effect of cross-entropy minimization on neural network training, we cannot simply assume the features of the training points will be normalized. This is why we strictly avoid the assumption of a normalized dataset, as explained in Remark 1.

2a) It is unfortunate, and somewhat curious, that our results in Section 3 (Theorem 3 and the remarks following it) were completely neglected. Section 3 clarifies why the conclusions of the works [1,2,3,4,5] are erroneous and shows that the reality is drastically different from their conclusions. Showing that there was a critical error in a line of previous works, which leads to a drastic change in the conclusion, is not an "incremental contribution". In fact, given [1] appeared in ICLR last year, it is essential that the ICLR community be given the correction this year.

2b) Theorem 3 and Remark 3 are the most critical results of our paper. Please make sure you have understood them. The last paragraph of Section 5 verifies that the assumptions of Theorem 3, the low-dimensionality of the features, indeed arises in practice. In other words, the assumptions of Theorem 3 are not an edge case, and the conclusion of Theorem 3 has critical implications for practice.

3) Our paper starts with the question "Is cross-entropy loss really the right cost function to use with gradient descent algorithm?". We use linear classifier and linearly separable dataset to answer this question on a simple setting. By doing so, our work gives intuition that the cross-entropy loss function has responsibility in the poor margin of the decision boundaries. We introduce differential training as a method to improve the margin **while still using gradient descent algorithm**. As we stated in the Discussion section, this allows the feature mapping to remain trainable while ensuring a large margin, and therefore, it provides an initial attempt to combine the benefits of neural networks and the SVM. And please note that when [1,2,3,4,5] claimed that cross-entropy loss finds the same solution with the SVM, they did not suggest that the ML community stop using cross-entropy minimization and replace it with SVM.

[1] Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable data. In International Conference on Learning Representations, 2018.
[2] D. Soudry, E. Hoffer, M. Shpigel Nacson, S. Gunasekar, and N. Srebro. The Implicit Bias of Gradient Descent on Separable Data. ArXiv e-prints, 2018.
[3] M. Shpigel Nacson, J. Lee, S. Gunasekar, P. H. P. Savarese, N. Srebro, and D. Soudry. Convergence of Gradient Descent on Separable Data. ArXiv e-prints, 2018a.
[4] M. Shpigel Nacson, N. Srebro, and D. Soudry. Stochastic Gradient Descent on Separable Data: Exact Convergence with a Fixed Learning Rate. ArXiv e-prints, 2018b.
[5] Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. CoRR, abs/1803.07300, 2018.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJeCZjJ52m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The technical results can be obtained by a simple combination of previous work.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByfbnsA9Km&amp;noteId=rJeCZjJ52m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper685 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper685 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: 
This paper investigates the properties of minimizing cross-entropy of linear functions over separable data (looks like logistic loss). The authors show a simple example where the minimizer of the cross-entropy loss leads to maximum margin hyperplane where the bias term is regarded as an extra dimension, which is different from the standard max. margin solution of  SVMs with bias not regarded as an extra dimension. The authors then propose a method to obtain the latter solution by minimizing the cross-entropy loss.


Comments:

There is a previously known result quite related to this paper: 

Ishibashi, Hatano and Takeda: Online Learning of Approximate Maximum p-Norm Margin Classifiers with Bias, COLT2008. 

Theorem 2 of Ishibashi et al. shows that the hard margin optimization with linear classifier with bias is equivalent to those without bias over pairs of positive and negative instances. 

Combined with Theorem 3 of (Soudry et al., 2018)), I am afraid that the main result Theorem 5 can be readily derived. 

For this reason, I am afraid that the main technical result is quite weak.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1l15DRl67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 2: Main result is not Theorem 5</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByfbnsA9Km&amp;noteId=S1l15DRl67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper685 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper685 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Reviewer 2,

Thank you for your review, and thanks for pointing out this reference. We were not aware of this past work, and it certainly deserves a reference.

Nevertheless, our main technical result is Theorem 3 and Remark 3 -- not Theorem 5. As the title of our submission reflects, and as the list of our contributions on page 2 describes, differential training is not the heart of our work. As we stated in our response to Reviewer 1, differential training was introduced in this paper only to open a door for further research and not to finish this paper with a negative result. 

Please note that Theorem 3 and Theorem 4, along with the related remarks, are original. We would appreciate if you have any suggestions to further highlight that Section 3 is the critical part of our work.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByxobfCsT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Paper has been updated</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByfbnsA9Km&amp;noteId=ByxobfCsT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper685 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper685 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1) We changed the titles of Section 2 and Section 3 to reflect their importance.

2) We added citations to (Ishibashi et al., 2008) and one of its references, (Keerthi et al., 2000), in the first paragraph of Section 4.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_H1gqEmdSim" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Improve implementation of conditional nature of mutual information</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByfbnsA9Km&amp;noteId=H1gqEmdSim"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Ignacio_Arroyo-Fern%C3%A1ndez1" class="profile-link">Ignacio Arroyo-FernÃ¡ndez</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Oct 2018</span><span class="item">ICLR 2019 Conference Paper685 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The proposed approach is very interesting as it revisits notions on margin-based and
discriminant classification, and brings those notions to model-based and Information-
Theoretic learning. However, there are two main concerns with the approach the authors 
shoud seriously consider. The Cross entropy is a form of Mutual information, which is
in turn computed from two entropies. As this suggests, you have two probability measures
interacting. The events drawn from from one measure can occur given (or jointly
with) the occurrence of events drawn from the other. If the authors do not
consider these basic aspects, the convexity of the Mutual information measure can
be violated. Thus the measure does not converge. This is a possible cause for parameter 
divergence. I suggets to consider these issues in order to improve the quality 
of this paper, which provides a very interesting approach.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>