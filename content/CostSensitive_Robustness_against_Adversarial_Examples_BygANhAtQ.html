<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Cost-Sensitive Robustness against Adversarial Examples | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Cost-Sensitive Robustness against Adversarial Examples" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BygANhA9tQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Cost-Sensitive Robustness against Adversarial Examples" />
      <meta name="og:description" content="Several recent works have developed methods for training classifiers that are certifiably robust against norm-bounded adversarial perturbations. However, these methods assume that all the..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BygANhA9tQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Cost-Sensitive Robustness against Adversarial Examples</a> <a class="note_content_pdf" href="/pdf?id=BygANhA9tQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 15 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019cost-sensitive,    &#10;title={Cost-Sensitive Robustness against Adversarial Examples},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BygANhA9tQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=BygANhA9tQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Several recent works have developed methods for training classifiers that are certifiably robust against norm-bounded adversarial perturbations. However, these methods assume that all the adversarial transformations provide equal value for adversaries, which is seldom the case in real-world applications. We advocate for cost-sensitive robustness as the criteria for measuring the classifier's performance for specific tasks. We encode the potential harm of different adversarial transformations in a cost matrix, and propose a general objective function to adapt the robust training method of Wong &amp; Kolter (2018) to optimize for cost-sensitive robustness. Our experiments on simple MNIST and CIFAR10 models and a variety of cost matrices show that the proposed approach can produce models with substantially reduced cost-sensitive robust error, while maintaining classification accuracy.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Cost-sensitive learning, Certified robustness, Adversarial examples</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A general method for training certified cost-sensitive robust classifier against adversarial perturbations</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HJg7nLR0n7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting initiative, ad-hoc model</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygANhA9tQ&amp;noteId=HJg7nLR0n7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1496 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1496 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors define the notion of cost-sensitive robustness, which measures the seriousness of adversarial attack with a cost matrix. The authors then plug the costs of adversarial attack into the objective of optimization to get a model that is (cost-sensitively) robust against adversarial attacks.

The initiative is novel and interesting. Considering the long history of cost-sensitive learning, the proposed model is rather ad-hoc for two reasons:

(1) It is not clear why the objective should take the form of (3.1). In particular, if using the logistic function as a surrogate for 0-1 loss, shouldn't the sum of cost be in front of "log"? If using the probability estimated from the network in a Meta-Cost guided sense, shouldn't the cost be multiplied by the probability estimate (like 1/(1+exp(...))) instead of the exp itself? The mysterious design of (3.1) makes no physical sense to me, or at least other designs used in previous cost-sensitive neural network models like

Chung et al., Cost-aware pre-training for multiclass cost-sensitive deep learning, IJCAI 2016
Zhou and Liu, Training cost-sensitive neural networks with methods addressing the class imbalance problem, TKDE 2006 (which is cited by the authors)

are not discussed nor compared.

(2) It is not clear why the perturbed example should take the cost-sensitive form, while the original examples shouldn't (as the original examples follow the original loss). Or alternatively, if we optimize the original examples by the cost-sensitive loss, would it naturally achieve some cost-sensitive robustness (as the model would naturally make it harder to make high-cost mistakes)? Those issues are yet to be studied.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1e-WxMi6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Additional experiments regarding cost-sensitive learning</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygANhA9tQ&amp;noteId=B1e-WxMi6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1496 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1496 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We’ve added an Appendix B.3 to the revised paper that addresses the question you raised about whether standard cost-sensitive loss trained on original examples would improve cost-sensitive robustness. The results from our experiments show that standard cost-sensitive loss does not result in a classifier with cost-sensitive robustness.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1llJV-z6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Objective justification</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygANhA9tQ&amp;noteId=H1llJV-z6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1496 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1496 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review. Your comments about the model being ad hoc stem from a few misunderstandings, which we hope to clarify:

1. Justification of training objective (3.1)
The design of (3.1) is not ad hoc, but follows from previous cost-sensitive learning work such as MetaCost, and is inspired by the cost-sensitive CE loss (see equation (10) of [1] for a detailed definition). To be specific, class probabilities for cost-sensitive CE loss are computed by multiplying the corresponding cost and then normalizing the result vector. As a result, transformations that induce larger cost will receive larger penalization by minimizing the cost-sensitive CE loss. We neglected to include this explanation in the paper, and will revise it to make this clear. 

For the first question, moving the sum of cost in front of “log” is unreasonable because the loss for each seed example will not be a negative log-likelihood term as in the case of cross-entropy. We can check the sanity of the objective by examining whether it reduces to standard CE loss if we set C = 1*1^\top-I. For the second question, we indeed multiply the probability estimates by the cost, but the result vector has to be normalized before plugging into the cross entropy loss. Thus, the sum of cost will appear in front of the “exp” term.

2. Comparison with other alternative designs
The cost-sensitive neural network models you mentioned are only demonstrated to be effective in the non-adversarial settings, whereas we show that our proposed classifier is effective in the adversarial setting. Thus, comparing our method with theirs is not appropriate, since it is unclear whether such alternative cost-sensitive models can be adapted and remain effective in the adversarial setting. Even if they can be adapted, it is still not the main focus of our paper, as our main goal is to show that our proposed classifier achieves significant improvements in cost-sensitive robustness in comparison with models trained for overall robustness.

3.  Why are the original examples are not in cost-sensitive form?
The training objective (3.1) is constructed for maximizing both cost-sensitive robustness and standard classification accuracy, and allows us to use the alpha hyperparameter to control the weighting between these goals. Thus, the first term in (3.1) doesn’t involve cost-sensitivity. We regard the standard classification accuracy as an important criteria for measuring classifier performance. Besides, the cost matrix for misclassification of original examples might be different from the cost matrix of adversarial transformations. For instance, misclassifying a benign program as malicious may still induce some cost in the non-adversarial setting, whereas the adversary may only benefit from transforming a malicious program into a benign one. In a scenario where the model is cost-sensitive regardless of adversaries, it could make sense to incorporate a cost-sensitive loss function as the first term also, but we have not explored this and are focused on the adversarial setting where cost-sensitivity is with respect to adversarial goals.

4. What if we only optimize the original examples by cost-sensitive loss
Given the vulnerability of deep learning classifiers against adversarial examples, we highly doubt that if we only optimize the original training by the cost-sensitive loss it would achieve significant cost-sensitive robustness (this expectation is based on how poorly models trained with the goal of overall accuracy do at achieving overall robustness). To be more convincing, we are running an experiment to test the robustness of a standard cost-sensitive classifier and will post the results soon.

Reference
[1]. Khan, et al., Cost-Sensitive Learning of Deep Feature Representations from Imbalanced Data. <a href="https://arxiv.org/abs/1508.03422" target="_blank" rel="nofollow">https://arxiv.org/abs/1508.03422</a>
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1xshyzjTX" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygANhA9tQ&amp;noteId=B1xshyzjTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper1496 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_B1lNPh9c3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An incremental paper that straightforwardly applies cost-sensitive loss to robust adversarial learning.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygANhA9tQ&amp;noteId=B1lNPh9c3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1496 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1496 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper introduces a new concept of certified cost-sensitive robustness against adversarial attacks. A cost-sensitive robust optimization formulation is then proposed for deep adversarial learning. Experimental results on two benchmark datasets (MNIST, CIFAR-10) are reported to show the superiority of the proposed method to overall robustness method, both with binary and real-value cost matrices. 

The idea of cost-sensitive adversarial deep learning is well motivated. The proposed method is clearly presented and the results are easy to access. My main concern is about the novelty of the approach which looks mostly incremental as a rather direct extension of the robust model (Wong &amp; Kolter 2018) to cost-sensitive setting. Particularly, the duality lower-bound based loss function and its related training procedure are almost identical to those from (Wong &amp; Kolter 2018), up to certain trivial modification to respect the pre-specified misclassification costs. The numerical results show some promise. However, as a practical paper, the current empirical study appears limited in data scale: I believe additional evaluation on more challenging data sets can be useful to better support the importance of approach. 

Pros: 

- The concept of certified cost-sensitive robustness is well motivated and clearly presented.

Cons:

-  The novelty of method is mostly incremental given the prior work of (Wong &amp; Kolter 2018).
- Numerical results show some promise of cost-sensitive adversarial learning in the considered settings, but still not supportive enough to the importance of approach.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SklzwA-oaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Novelty is cost-sensitive robustness</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygANhA9tQ&amp;noteId=SklzwA-oaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1496 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1496 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review. Please see our responses below.

1. Concern regarding the novelty
The review correctly notes that the method we use to achieve cost-sensitive robustness is a straightforward extension to the training procedure in Wong &amp; Kolter (2018). The novelty of our paper lies in the introduction of cost-sensitive robustness as a more appropriate criteria to measure classifier’s performance, and in showing experimentally that the cost-sensitive robust training procedure is effective. Previous robustness training methods were designed for overall robustness, which does not capture well the goals of adversaries in most realistic scenarios. We consider it an advantage that our method enables cost-sensitive robustness to be achieved with straightforward modifications to overall robustness training.

2. Limitation in data scale
We agree with the reviewer that certified robustness methods, including our work, are a long way from scaling to interesting models. All previous work on certified adversarial defenses has been limited to simple models on small or medium sized datasets (e.g., [1-3] below), but there is growing awareness that non-certified defenses are unlikely to resist adaptive adversaries and strong interest in scaling these methods. The method we propose and evaluate for incorporating cost-sensitivity in robustness training is generic enough that we expect it will also work with most improvements to certifiable robustness training. So, even though our implementation is not immediately practical today, we believe our results are of scientific interest, and the methods we propose are likely to become practical as rapid progress continues in scaling certifiable defenses. 


[1] Wong and Kolter, Provable defenses against adversarial examples via the convex outer adversarial polytope. <a href="https://arxiv.org/abs/1711.00851" target="_blank" rel="nofollow">https://arxiv.org/abs/1711.00851</a>
[2] Raghunathan, et al., Certified Defenses against Adversarial Examples. https://arxiv.org/abs/1801.09344
[3] Wong, et al., Scaling Provable Adversarial Defenses. https://arxiv.org/abs/1805.12514
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ryxEPIvKhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygANhA9tQ&amp;noteId=ryxEPIvKhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1496 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1496 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Strengths:

Well written and clear paper
Intuition is strong: not all source-target class pairs are as beneficial to find adversarial examples for 

Weaknesses:

Cost matrices choices feel a bit arbitrary in experiments
CIFAR experiments still use very small norm-balls

The submission builds on seminal work by Dalvi et al. (2004), which studied cost-sensitive adversaries in the context of spam detection. In particular, it extends the approach to certifiable robustness introduced by Wong and Kolter with a cost matrix that specifies for each pair of source-target classes whether the model should be robust to adversarial examples that are able to take an input from the source class to the target (or conversely whether these adversarial examples are of interest to an adversary).

While the presentation of the paper is overall of great quality, some elements from the certified robustness literature could be reminded in order to ensure that the paper is self-contained. For instance, it is unclear how the guaranteed lower bound is derived without reading prior work. Adding this information in the present submission would make it easier for the reader to follow not only Sections 3.1 and 3.2 but also the computations behind Figure 1.b. 

The experiments results are clearly presented but some of the details of the experimental setup are not always justified. If you are able to clarify the following choices in your rebuttal, this would help revise my review. First, the choice of cost matrices feels a bit arbitrary and somewhat cyclical. For instance, binary cost matrices for MNIST are chosen according to results found in Figure 1.b, but then later the same bounds are used to evaluate the performance of the approach. Yet, adversarial incentives may not be directly correlated with the “hardness” of a source-target class pair as measured in Figure 1.b. The real-valued cost matrices are better justified in that respect. Second, would you be able to provide additional justification or analysis of the choice of the epsilon parameter for CIFAR-10? For MNIST, you were able to improve the epsilon parameter from epsilon=0.1 to epsilon=0.2 but for CIFAR-10 the epsilon parameter is identical to Wong et al. Does that indicate that the results presented in this paper do not scale beyond simple datasets like MNIST?

Minor comments:


P2: The definition of adversarial examples given in Section 2.2 is a bit too restrictive, and in particular only applies to the vision domain. Adversarial examples are usually described as any test input manipulated by an adversary to force a model to mispredict.
P3: typo in “optimzation” 
P5: trade off -&gt; trade-off 
P8: the font used in Figure 2 is small and hard to read when printed.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HklCJkMs6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your positive and constructive comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygANhA9tQ&amp;noteId=HklCJkMs6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1496 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1496 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We hope the following explanations address your questions:

1. Regarding the choice of the cost matrices
Our goal in the experiments was to evaluate how well a variety of different types of cost matrices can be supported. MNIST and CIFAR-10 are toy datasets, thus defining cost matrices corresponding to meaningful security applications for these datasets is difficult. Instead, we selected representative tasks and designed cost matrices to capture them. Our experimental results show the promise of the cost-sensitive training method works across a variety of different types of cost matrices, so we believe it can be generalized to other cost matrix scenarios that would be found in realistic applications.

It is a good point that the cost matrices that were selected based on the robust error rates in Fig 1B are somewhat cyclical, but it does not invalidate our evaluation. We use the “hardness” of adversarial transformation between classes only for choosing representative cost matrices, and the robust error results on the overall-robustness trained model as a measure for transformation hardness. Further, the transformation hardness implied by the robust error heatmap is generally consistent with intuitions about the MNIST digit classes (e.g., “9” and “4” look similar so are harder to make robust to transformation), as well as  with the visualization results produced by dimensional reduction techniques, such as t-SNE [1]. 

2. Regarding the choice of epsilon for CIFAR-10
In our CIFAR-10 experiments, we set epsilon=2/255, the same experimental setup as in [2]. Our proposed cost-sensitive robust classifier can be applied to larger epsilon for CIFAR-10 dataset, and similar improvements have been observed for different epsilon settings. In particular, we have run experiments on CIFAR-10 with epsilon varying from {2/255, 4/255, 6/255} for the single seed task. The comparison results are reported in Figure 5(b), added to the revised PDF. These results support the generalizability of our method to larger epsilon settings.

[1] Maaten and Hinton, Visualizing Data using t-SNE. <a href="http://www.jmlr.org/papers/v9/vandermaaten08a.html" target="_blank" rel="nofollow">http://www.jmlr.org/papers/v9/vandermaaten08a.html</a>
[2] Wong, et al., Scaling Provable Adversarial Defenses. https://arxiv.org/abs/1805.12514
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SygPPR_l0X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>re: explanations</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BygANhA9tQ&amp;noteId=SygPPR_l0X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1496 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1496 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for taking the time to write a response to my review. 

Regarding 1., the explanation does provide some useful context for the choice of cost matrices. Do you have an intuition as to whether adversarial incentives will always correlate with transformation hardness? In other words, could there exist settings where the adversary would benefit more from a change in class that is relatively easy to make (and hard to defend against) compared to other class pairs? 

Thank you for providing additional experimental results regarding 2. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>