<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Adversarially Robust Training through Structured Gradient Regularization | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Adversarially Robust Training through Structured Gradient Regularization" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HyxBpoR5tm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Adversarially Robust Training through Structured Gradient..." />
      <meta name="og:description" content="We propose a novel data-dependent structured gradient regularizer to increase the robustness of neural networks vis-a-vis adversarial perturbations. Our regularizer can be derived as a controlled..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HyxBpoR5tm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Adversarially Robust Training through Structured Gradient Regularization</a> <a class="note_content_pdf" href="/pdf?id=HyxBpoR5tm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019adversarially,    &#10;title={Adversarially Robust Training through Structured Gradient Regularization},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HyxBpoR5tm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We propose a novel data-dependent structured gradient regularizer to increase the robustness of neural networks vis-a-vis adversarial perturbations. Our regularizer can be derived as a controlled approximation from first principles, leveraging the fundamental link between training with noise and regularization. It adds very little computational overhead during learning and is simple to implement generically in standard deep learning frameworks. Our experiments provide strong evidence that structured gradient regularization can act as an effective first line of defense against attacks based on long-range correlated signal corruptions.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Adversarial Training, Gradient Regularization, Deep Learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a novel data-dependent structured gradient regularizer to increase the robustness of neural networks against adversarial perturbations.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">13 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HklHzUwsn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Trying to address an important problem, but approach/results are not convincing</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxBpoR5tm&amp;noteId=HklHzUwsn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper802 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper802 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose a new defense against adversarial examples that relies on a data-dependent regularization (instead of adversarial training). They then benchmark the performance of this new defense against popular white-box and transfer attacks, as well as propose a new long range correlated adversarial attack.

Comments:
I find the premise of this paper interesting - developing regularization strategies to help with generalization to adversarial perturbations. For instance, it is well known that state-of-the-art defenses such as PGD have generalization gaps as large as 50% between robust train and test accuracies. It has also been previously hypothesized that this could be due to a data scarcity problem [Schmidt et al., 2018].  The authors here propose to tackle this problem using a new data-dependent regularization technique. 

My primary issue with this paper is that the authors do not clearly illustrate what the advantage of their method over standard methods is
- The problem this paper aims to solve is overfitting to a specific attack/virtual adversarial examples presented during adversarial training by using regularization instead. However, the authors do not actually illustrate that their technique reduces overfitting. For instance, the authors do not contrast the robust train-test accuracies using their method to other standard methods. Thus it is not clear that this paper met the objectives laid out in the introduction. 
- The claim in this paper is that SGR helps against attacks with long range dependencies. However, in their experiments (e.g., in Figure 3), the authors do not evaluate other standard defenses. It is thus unclear whether other standard methods are already robust to such attacks. In fact, based on the results of Table 1, it doesn’t seem like attacks from SGR  are able to reduce the robustness of PGD/FGSM trained models.

Because of these two points, along with the lower robustness to various attacks (in Table 1) as compared to approaches such as PGD, it is not really clear to me what the real merit of this new approach is. Ultimately, having a defense which is more robust to a particular attack is not very meaningful if there exists an alternative attack that reduces the robustness of the defense.

I am also surprised that the authors chose to use this regularization as an alternative to adversarial training instead of complementary to it. I would be interested to see if such regularization could actually help to bridge the generalization gap observed while using adversarial training.

The paper is at times is poorly written and confusing. For instance, the description of CovFun is hard to parse. The authors should make this explanation more clear. The authors also do not state what their attack model is - Linf vs L2 perturbations. They also choose to evaluate attacks differently, using an average accuracy over different epsilons rather than reporting individual accuracies. This does make the results harder to compare to other work. The authors should include a full table of individual accuracies (at least in the appendix) to make the numbers easier to parse and compare.

In the derivation in Section 3.1, the authors use the assumption that the robust classifier is almost equal to the Bayes optimal classifier to justify dropping terms corresponding to the Hessian(\phi_y). I am not sure how realistic this assumption is in the adversarial setting - one can construct simple distributions for which the Bayes optimal classifier is not the robust classifier.

With regards to Figure 3, the authors state -
“As the decay length goes to zero, the synthetic covariance matrix converges to the identity matrix and SGR performance approaches GN performance” 
Could the authors clarify why this is obvious? After all these two models are trained very differently.

The plot in Figure 3 and the results in Table 1 seems to illustrate that SGR is no better than GN as you can find an attack where they perform as well/badly. The authors say that this is due to the short-range nature of current attacks. I do not understand this rationale though - the goal of the defenses should be to be more robust to all attacks, both short range and long range. Thus arguing that there may be an attack under which their model performs better is not sufficient. I do agree that finding long range attacks that can break current SOTA robust models would be interesting, however the authors do not seem to achieve that in this work.

I find the observation on transfer attacks interesting - PGD attacks from SGR/GN models are better than PGD models. Do the authors have any insight as to why this is the case?

In general, my concern about gradient regularization based defenses is that they only give a very local picture of the landscape and thus can only protect against small eps attacks. This could probably explain why the SGR/GN models are less robust than PGD. As mentioned previously, it would be valuable to see accuracies against individual eps values (rather than averaged) to understand this better. If this is the case, this regularization would not provide any additional benefits when combined with adversarial training either.

References:
Schmidt, Ludwig, et al. "Adversarially Robust Generalization Requires More Data." arXiv preprint arXiv:1804.11285 (2018).</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HylEkYgY3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Some interesting ideas but unconvincing empirical evaluation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxBpoR5tm&amp;noteId=HylEkYgY3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper802 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper802 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Short paper summary: This work proposes a novel method of gradient regularization (SGR) which utilizes the covariance structure of adversarial examples generated during training. The authors propose simple techniques to reduce the computational overhead of SGR. Empirically, the authors compare their method to standard adversarial training and gradient norm regularization.

Brief review summary: There are some interesting ideas in this work but I feel that the some practical aspects lack formal justification and the comparison to existing work is inconclusive.

Detailed comments:

In addition to some minor comments, I have two concerns. First, with the SGR algorithm itself. And second with the empirical analysis. While I suspect that the first concern may be clarified with discussion I think that the second is more serious and is the primary factor behind my review score.

1) As the SGR algorithm is written I wonder whether the regularization term may be computed more efficiently using something like a Hutchinson trace estimation trick. I suspect that if the random vector used to estimate the trace was the xi from Algorithm 1 then the same Mahalanobis gradient norm would be recovered. This would hold only in the case beta=1, bringing me to my second point.

2) What is the purpose of the running average of the covariance? A relatively small beta value is used in practice but I do not see any strong justification for this. Is there a good reason why we do not want the covariance matrix to be a close approximation for the local gradient landscape? This seems like an important part of the algorithm, especially as it may shed light on my next note.

3) In practice, Algorithm 1 uses adversarial attack schemes to generate the perturbations. In simple cases like FGM, this would give the covariance of the input-output gradient which seems that it would have a direct interpretation as a form of classical gradient regularization. To this extent, I also wonder how the SGR algorithm could be related to interpretations of adversarial training as gradient smoothing (when using small perturbations).

I recognize that the above points are (so far as I could tell) not directly addressed in the work, and some may be fairly considered out of scope. However, due to the direct comparison to adversarial training later and the need to tie SGR to adversarial attacks I feel that it would be important to distinguish these cases.

Overall, I felt that the first three sections did well to introduce the motivation and techniques used and were was easy to follow. The derivation of the SGR algorithm was clear and concise but I believe that some of the practical details (covariance running average, computational efficiency [at first glance, it looks like the full Jacobian must be computed, but practically the sum over K reduces this to a single backprop call]) could have been elaborated on.

For the empirical evaluation the authors provided ample detail on the experimental set up and have performed a fairly thorough investigation in terms of existing defenses and attacks. I felt that the bulk of the study which is contained in Table 1 is fairly inconclusive or at the very least, difficult to interpret completely. Additional comments:

4) I felt that Figure 1 and 2 are a little difficult to interpret at first. It would help to clearly define what is meant by short- and long-range signal corruptions. However, they do suggest some interesting findings. As these covariance matrices depend directly on the model itself, I think it is worth investigate (or commenting on) how this structure may change when introducing things like SGR (or GN). The authors claim that unregularized classifiers give too much weight to short range correlations but they should show that SGN (or other methods) correct this.

5) My biggest concern with this work is with the results presented in Table 1. In terms of how they are presented: first I think that the fool column requires further explanation, or perhaps more simply the column could show accuracy instead of the average perturbation size. Second, I am not sure why the reported accuracies are averaged over attack strengths in a range. So far as I am aware, this is not standard and makes it difficult to interpret the performance of the models in this way. Figure 4 in the appendix does a better job of describing the behavior over a range of attack strengths.

6) From the table, it is not obvious to me that SGR provides any improvements to robustness over existing techniques. Indeed, the authors write that SGR achieves white-box accuracies which are between those of the clean and adversarially trained models and claim that SGR improves on the clean accuracy for CIFAR-10. But in the table the gap between FGSM and GN/SGR clean accuracies seem fairly small with FGSM providing better robustness (for most source attacks). Even more concerning, is the fact that GN seems to outperform SGR. I do not find these results substantial enough to motivate SGR as a robustness defense compared with adversarial training (or even GN), especially as SGR has the same computational limitations involved with expensive adversarial perturbations.


I felt that the study into the covariance structure of adversarial perturbations was interesting but as it stands was not complete enough to be informative in general. In the conclusion the authors write that they provide evidence that current adversarial attacks act by perturbing the short-range correlations of signals but this has only been confirmed for unregularized classifiers. Despite these issues, I thought that the paper was well written and hope that the empirical study can be improved and clarified.


Minor comments:

- Section 2.1, set of transformations only introduced briefly then forgotten. Leaving output invariant confused me, as this does not apply to adversarial examples.
- Section 2.3, second paragraph l3: In Maaten et al. should be citet.
- Section 3.1, should  make clear that derivative is with respect to the data.
- Section 3.1, define delta as the Hessian clearly (it is used for the simplex in the previous section). Though this is easy to figure out.
- Section 7.1, starts with (iii), is this intentional? Perhaps an introductory sentence could make this clearer.
- Section 7.3, for label leaking, I'm not convinced by this argument alone. Assuming the covariance structure is still computed from a particular adversarial example, I see no compelling reason that this would not occur.


Clarity: The paper is very clearly written and is easy to follow.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1eTf52unm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>simple and reasonable idea, somewhat unconvincing theoretical analysis, weak experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxBpoR5tm&amp;noteId=H1eTf52unm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper802 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper802 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary of the paper:
This paper proposes to use structured gradient regularization to increase adversarial robustness of neural network. Here, the gradient regularization is to regularize some norm of the gradients on neural network input. "structured" means that instead of just minimizing the L2 norm of the gradients, a "mahalanobis norm" is minimized. The covariance matrix is updated continuously to track the "structure" of gradients/perturbations. Whitebox attack and blackbox attack 

The paper is well written, both theory and experiments are well explained. The analysis of LRC attack on SGR trained models are interesting.

However, I believe the paper has major flaws in several aspects.

The whitebox robustness evaluation is weak. Whitebox PGD with 10 iterations is not enough for discovering true robustness of a neural network, which makes the experiments unconvincing. PGD with 100 iterations and 50 random starts would make the evaluation much convincing wrt to whitebox attack. <a href="https://github.com/MadryLab/mnist_challenge" target="_blank" rel="nofollow">https://github.com/MadryLab/mnist_challenge</a>
I noticed that in Table 1, the authors reported averaged results across different epsilons. Although I see the motivation to give equal weights to small and large perturbations, it makes it hard to compare with previous papers. I think the authors should a least report commonly used eps in the literature, including MNIST eps=0.1, 0.2, 0.3 and CIFAR10 eps=8/255. Currently, for MNIST eps=32/255=0.125 is much below the standard eps for benchmarking MNIST.

In my opinion, when evaluating robust optimization / gradient regularization methods, robustness under the strongest whitebox should be the major benchmark. Because "intrinsic" robustness is their goal. In contrast, black-box results are less important. This is because 1) evaluating black-box robustness on a few attacks hardly give any conclusive statements; 2) if we're pursuing black-box robustness, there're many randomization methods that boosts black-box robustness under various settings. How does a gradient regularization method help on top of those should be at least evaluated.
So if the paper wants to claim black-box robustness, it needs at least include experiments like 2), so it provides useful benchmarks to practitioners.

There're also a few problems in the motivation / analysis. 
"""A remedy to these problems is through the use of regularization. The basic idea is simple: instead of sampling virtual examples, one tries to calculate the corresponding integrals in closed form, at least under reasonable approximations."""
The adversarial robustness problem is not about integral over a neighborhood, it is about the maximum loss over a neighborhood. This is likely why previous attempts on gradient regularization and adversarial training on FGSM attack fails. And the success is of PGD training is largely due to that the loss minimize over the adversarial example that gives the maximum loss.

"""Thus, under the assumption that \phi \approx \phi^* and of small perturbations (such that we can ignore higher order terms."""
The Bayes optimal assumption seems to be arbitrary to me. If \phi is nearly Bayes-optimal, why would we worry about adversarial examples?



Other relatively minor problems

In the caption of Figure 1, """Covariance matrices of PGD, FGSM and DeepFool perturbations as well as CIFAR10 training set (for comparison). The short-range structure of the perturbations is clearly visible. It is also apparent that the first two attack methods yield perturbations with almost identical covariance structure."""
PGD and FGSM have very different attack power. If they are similar by any measure, wouldn't that mean the measure (covariance structure) is too coarse?

In Section 3.1, the paper talks about both centered and uncentered adversarial examples.
I assumed that the authors mean that the distribution of perturbations are centered?
First, I think this the authors should make this more explicit.
Second, I think this is not a realistic to assume the perturbations to be centered, because for image data, the epsilon-ball usually intersects with data domain boundary. So I'm wondering in the experiments, which version was used? centered or uncentered?

Figure 5 shows periodic patterns on covariance matrices. I didn't find explanation of the periodic patterns in the covariance matrices. It would nice if the authors can explain it or point me the relevant sections in the paper.

I don't fully get the idea of LRC attack. Is it purely sampling? are there optimization involved?

Figure 3, I suggest the authors show perturbations with different decay lengths on the same original images, which would make it easier to compare.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1xN-wnf5X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Table 1 shows the defense is worse than the baseline</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxBpoR5tm&amp;noteId=B1xN-wnf5X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Oct 2018</span><span class="item">ICLR 2019 Conference Paper802 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">As mentioned in another comment, the "fool" column is strange, and if we can trust it, then all the defenses are shown to be completely broken.

If we ignore the fool column and just look at the other columns that seem more believable, then how does this model look on CIFAR-10?
The strongest attack against it is PGD, which results in an accuracy AUC of 41.5.
This is worse than the baseline of just doing gradient regularization with no Mahalanobis distance, which has a worst-case accuracy AUC of 41.9.
It is also worse than either of the two defenses based on adversarial training (which get 55 and 62 AUC).

We also see more or less the same thing on MNIST. Here the strongest attack against the proposed SGR defense is T-PGD, which gets 96.5 AUC. Traditional gradient regularization actually ties it, also with 96.5 AUC, just with a different attack causing the worst case performance. Both of the defenses based on adversarial training perform strictly better.

</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1l41x189Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>SGR and GN trained models achieve statistically indistinguishable attack accuracies (within one standard deviation of each other).</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxBpoR5tm&amp;noteId=S1l41x189Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper802 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Oct 2018</span><span class="item">ICLR 2019 Conference Paper802 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">- The "fool" column is strange, and if we can trust it, then all the defenses are shown to be completely broken.

See [Moosavi Dezfooli et al., “Deepfool: a simple and accurate method to fool deep neural networks.” 2016] on how to interpret those numbers correctly.

- Worse than the baseline of just doing gradient regularization with no Mahalanobis distance?

The PGD white-box attack accuracies reported for SGR and GN trained models are within one standard deviation of each other, which is $\sigma = 0.5$ (computed over 10 runs). What we can conclude from this table is that SGR and GN trained models achieve statistically indistinguishable accuracies for these particular results.

As stated in Section 4.4, SGR/GN trained models achieve white-box attack accuracies that are intermediate between those of the clean model and adversarially trained models. Note, however, that we do not equate “robustness” with “white-box attack accuracy”. If we look at the transfer-attack accuracies (bold-face numbers), then SGR and GN trained models are statistically on par with adversarially trained models.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rylfcB3MqX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>What is the "fool" column of table 1?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxBpoR5tm&amp;noteId=rylfcB3MqX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Oct 2018</span><span class="item">ICLR 2019 Conference Paper802 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The right way to read adversarial vulnerability tables is to take the min accuracy across different attacks: it doesn't matter if your defense is good at beating a lot of attack algorithms; if there is one attack that performs well then an attacker will use that.

In table 1 it looks like the "fool" attack is able to completely break the proposed defense, resulting in &lt; 1% accuracy.

However, there are some other things that are weird. For example, the "fool" column also reports &lt; 1% accuracy for a PGD-trained model. DeepFool is not previously known to break PGD-trained models, so this either indicates an interesting research finding, or a bug in your accuracy calculations, or a bug in your PGD-trained model.

</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkgpUeJI5Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The Fool column reports the noise-to-signal ratio of the DeepFool attack. Those numbers are not accuracies.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxBpoR5tm&amp;noteId=SkgpUeJI5Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper802 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Oct 2018</span><span class="item">ICLR 2019 Conference Paper802 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">- The right way to read adversarial vulnerability tables is to take the min accuracy across different attacks

We totally agree with this statement. We report various different attacks so that the reader can draw his or her own conclusions.


- In table 1 it looks like the "fool" attack is able to completely break the proposed defense, resulting in &lt; 1% accuracy.


The numbers reported in the Fool column are not accuracies (which is why we did not use % sign but reported decimal numbers). If you read our paper carefully, the Fool column reports the noise-to-signal ratio of the DeepFool attack computed according to Eq.(2) in [Moosavi Dezfooli et al., “Deepfool: a simple and accurate method to fool deep neural networks.” 2016], as stated in the Experimental Setup Section 4.1.


- We have checked our implementations and run many sanity-checks and we do believe they are correct. We do not see any evidence pointing to the contrary in our results.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SkexZVhMcQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The motivation for introducing the long-range correlated noise attack seems backward</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxBpoR5tm&amp;noteId=SkexZVhMcQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Oct 2018</span><span class="item">ICLR 2019 Conference Paper802 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">If I understand section 4.3 correctly, you introduce the LRC attack because you expect that your proposed defense will be able to beat it. This is not the way that you should evaluate new defense papers. New defenses should perform well against pre-existing attacks. Papers on new defenses sometimes need to introduce new attacks, but these should be new attacks that are *hard* for the defense to beat, not attacks that are designed to be *easy* for the defense to beat. For example, a new defense based on non-differentiable operations might perform poorly against pre-existing gradient-based attacks, so to evaluate it properly it is necessary to introduce new gradient-free attacks.

</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1e-Zmy85X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The purpose of the LRC attack experiment is to establish whether there is a potential benefit in using a structured covariance matrix in the SGR regularizer.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxBpoR5tm&amp;noteId=H1e-Zmy85X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper802 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Oct 2018</span><span class="item">ICLR 2019 Conference Paper802 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for raising this question for which we believe we need to reiterate several points.

First of all, we did not design the LRC attack with the purpose to be easy to beat. If you read our submission carefully, you will notice that the purpose of the LRC attack experiment is to establish whether there is a potential benefit in using a structured covariance matrix in the SGR regularizer versus using an “unstructured” diagonal covariance (corresponding to gradient-norm regularization) in the presence of long-range correlated noise. In other words, this experiment simply tests whether the SGR regularizer extracts useful information about the long-range correlation structure of the perturbations, which it indeed does.

Secondly, we do not claim that LRC is stronger than other pre-existing attacks. In your criticism, you seem to imply that we claimed that structured gradient regularization defends against pre-existing attacks because it performs well against long-range correlated perturbations, but we did not say that in our submission. Such a claim could be made if one showed that a new attack is stronger than existing ones and that a new defense protects against this new attack. We do not claim that however. Instead, and to the best of our ability, we transparently evaluate regularized and adversarially trained models against pre-existing white-box and transfer attacks in Section 4.4. 

The LRC attack is nothing but a natural prototype for low frequency perturbations, as opposed to existing attacks which we have shown to mainly corrupt the short range (high frequency) structure of signals. As stated in the conclusion, devising further (e.g. gradient-based) low frequency attacks is an interesting direction of future research.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJgbuknzqm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Do you report attack success rate for a specific epsilon?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxBpoR5tm&amp;noteId=BJgbuknzqm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Oct 2018</span><span class="item">ICLR 2019 Conference Paper802 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Table 1 apparently shows areas under attack curves for varying epsilon ("The white-box and transfer attack accuracies are averaged over attack strengths in the range ∈ [0, 32] for MNIST and  ∈ [0, 8] for CIFAR10, i.e. the reported accuracies represent the integrated area under the attack curve."). This makes it hard to compare to previous work such as Madry et al 2017, who report attack success rate for the largest value of epsilon. Does the paper report the attack success rate for epsilon=8 specifically?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkl2bNy8q7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Please read the main text, we give a very precise explanation about what we report and we establish fair comparisons to the best of our abilities.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxBpoR5tm&amp;noteId=Bkl2bNy8q7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper802 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Oct 2018</span><span class="item">ICLR 2019 Conference Paper802 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Firstly, we would like to emphasize that the text is very precise about what the numbers reported in the tables. As stated in the first paragraph of Section 4.4 as well as in the caption of Table 1, we report white-box and transfer attack accuracies averaged over attack strengths in the range [0, 32] for MNIST and [0, 8] for CIFAR10.

Secondly, we establish a fair comparison between regularized models and adversarially trained ones, in that we train each architecture with various different training methods, including PGD-augmented training suggested in Madry et al. ***. In fact, for PGD and FGSM adversarial training, we trained models with each integer epsilon in the range [0, 32] for MNIST and [0, 8] for CIFAR10 and report results for the best performing one. The hyperparameters of the best performing models are reported in Section 7.2 in the Appendix. 

To the best of our knowledge, Madry et al. used different architectures and possibly different data preprocessing and data augmentation schemes. 

As a side note, we believe that an even more realistic performance measure should give less weight to larger perturbations which are easier to detect and give relatively more weight to smaller ones that are harder to detect. The averaged attack accuracies we report give equal weight to different perturbation strengths.


***: We assume that by “Madry et al. 2017” you meant [Madry et al., “Towards Deep Learning Models Resistant to Adversarial Attacks” 2017]</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ryxGk0jM9Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>How is this significantly different from previous broken defenses based on gradient regularization?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxBpoR5tm&amp;noteId=ryxGk0jM9Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Oct 2018</span><span class="item">ICLR 2019 Conference Paper802 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Regularizing the norm of the gradient of the output log probability with respect to the input has been tried many times and does not work as a defense against adversarial examples.

This work essentially proposes to use a Mahalanobis norm ( g^T A g) rather than a squared L2 norm (g^T g) for the gradient penalty. Why would this be any better?

Gradient regularization does not work because it is based on derivatives and thus is designed to resist only infinitesimal perturbations. It cannot "see" the way that finite-sized perturbations cross relu boundaries and so on. Using a Mahalanobis norm rather than an L2 norm doesn't address this fundamental limitation of gradient regularization. All it does is penalize the gradient more in some directions than others.

If anything, using a Mahalanobis norm seems like it should create more opportunities for adversarial attacks to succeed in the directions that were downweighted.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJgQCEy8cQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our work is a strict generalization of previous work and regularization was proven to be equivalent to robust optimization in certain settings.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyxBpoR5tm&amp;noteId=rJgQCEy8cQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper802 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Oct 2018</span><span class="item">ICLR 2019 Conference Paper802 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">- How is this significantly different from previous defenses based on gradient regularization?

To the best of our knowledge, gradient regularization as a method to improve adversarial robustness has been studied in two other concurrent works, both of them are cited [A.S. Ross &amp; F. Doshi-Velez, “Improving Adversarial Robustness and Interpretability of DNNs by Regularizing Input Gradients”, 2017] and [C.J. Simon-Gabriel et al. “Adversarial Vulnerability of Neural Networks Increases With Input Dimension”, 2018].

Firstly, our work goes a lot further in terms of theoretical justification for gradient regularization than both of these: we follow a principled approach to derive structured gradient regularization as a tractable approximation to training with correlated perturbations.

Secondly, our structured gradient regularizer (SGR) is a strict generalization of gradient norm (GN) regularization: while GN provides an approximation to training with white noise, SGR provides an approximation to training with arbitrarily correlated noise. Moreover, regularization has been shown to be equivalent to robust optimization in certain settings, see below.


- Why would gradient regularization w.r.t. Mahalanobis distance (i.e. SGR) be any better than gradient regularization w.r.t. L2 norm?

Firstly, gradient norm regularization based on L2 norm assumes isotropic white-noise, whereas Mahalanobis-distance based SGR operates with arbitrarily correlated noise.

Secondly, SGR can leverage the fact that adversarial examples might live in low-dimensional subspaces. Quoting from [Moosavi-Dezfooli et al, “Universal adversarial perturbations”, 2017]: “We hypothesize that the existence of universal perturbations fooling most natural images is partly due to the existence of such a low-dimensional sub-space that captures the correlations among different regions of the decision boundary.” SGR can leverage this by penalizing gradients that lie within such a subspace more strongly than gradients that lie outside it.


- Gradient-norm regularization has been tried many times and does not work as a defense against adversarial examples.

First of all, could you please provide references to papers where gradient regularization was the main method of defense (i.e. where it was not just used as a baseline) and was shown not to work?

Secondly, this statement is unqualified: to be precise, you need to (i) state how you measure performance, i.e. how you define whether some method “works” and (ii) what kind of threat model you assume, i.e. what kind of “adversarial examples” you want to robustify against. E.g. gradient-based or gradient-free, white-box or transfer/black-box attacks, whether the perturbations are constrained in magnitude or whether they are constrained by the counting-norm etc. In your statement you seem to make specific assumptions, which is why it is not true in the generality in which it was formulated.

For instance, if we take transfer attack accuracies as our measure of robustness and PGD transfer attacks as the threat-model, corresponding to the bold-face numbers in Table 1, then SGR and GN are statistically on par with PGD and FGSM trained models on CIFAR10, compare the bold-face numbers in each row.


- Gradient regularization does not work because it is based on derivatives and thus is designed to resist only infinitesimal perturbations.

There is a large body of work on the equivalence of regularization and robust optimization (adversarial training is a special case of robust optimization against a pointwise adversary that independently perturbs each example):

[Bertsimas and Copenhaver, “Characterization of the equivalence of robustification and regularization in linear and matrix regression” 2018], showed that in linear regression robust optimization for matrix-norm uncertainty sets and regularization are exactly equivalent. There is also a variety of settings for robust optimization under more general uncertainty sets in which regularization provides upper and lower bounds. See also [El Ghaoui and Lebret, “Robust solutions to least-squares problems with uncertain data” 1997].

[Xu et al., “Robustness and regularization of support vector machines“ 2009] established equivalence of robust optimization and regularization for Support Vector Machines.

More recently, [Gao et al., “Wasserstein distributional robustness and regularization in statistical learning” 2017] showed that Wasserstein-distance based distributionally robust stochastic optimization (Wasserstein-DRSO) is first order equivalent to gradient regularization.

These works clearly contradict your statement that gradient regularization does not work.

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>