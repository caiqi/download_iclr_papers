<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Incremental Hierarchical Reinforcement Learning with Multitask LMDPs | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Incremental Hierarchical Reinforcement Learning with Multitask LMDPs" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Hye6uoC9tm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Incremental Hierarchical Reinforcement Learning with Multitask LMDPs" />
      <meta name="og:description" content="Exploration is a well known challenge in Reinforcement Learning. One principled way of overcoming this challenge is to find a hierarchical abstraction of the base problem and explore at these..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Hye6uoC9tm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Incremental Hierarchical Reinforcement Learning with Multitask LMDPs</a> <a class="note_content_pdf" href="/pdf?id=Hye6uoC9tm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019incremental,    &#10;title={Incremental Hierarchical Reinforcement Learning with Multitask LMDPs},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Hye6uoC9tm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Exploration is a well known challenge in Reinforcement Learning. One principled way of overcoming this challenge is to find a hierarchical abstraction of the base problem and explore at these higher levels, rather than in the space of primitives. However, discovering a deep abstraction autonomously remains a largely unsolved problem, with practitioners typically hand-crafting these hierarchical control architectures. Recent work with multitask linear Markov decision processes, allows for the autonomous discovery of deep hierarchical abstractions, but operates exclusively in the offline setting. By extending this work, we develop an agent that is capable of incrementally growing a hierarchical representation, and using its experience to date to improve exploration.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Reinforcement learning, hierarchy, linear markov decision process, lmdl, subtask discovery, incremental</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We develop an agent capable of incrementally growing a hierarchical representation, and using its experience to date to improve exploration.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_r1gHwX8ZTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting ideas; contribution is marginal over previous work; lots of questions about hierarchy construction and utility</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hye6uoC9tm&amp;noteId=r1gHwX8ZTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper394 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper394 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Major comments:

This paper builds on previous work in hierarchical LMDPs and extends the core ideas to an online setting.  Essentially, we incrementally construct a hierarchy by adding new states to upper-level MDPs every once in a while; these are loosely initialized and the parameters are then refined with additional experience.

Overall, I felt that this paper lacked a substantial enough contribution. 

* The key contributions over previous work seems to be entirely contained in Sec. 3.1 and 3.2: (1) when we have visited k new states, add a new state to the hierarchy, and (2) initialize its parameters with intuitive values.

* To me, this level of contribution is below the bar for ICLR.  The ideas seem simplistic and likely to work only in the simplest of domains.

* The improvement over previous work is marginal.

* I think the paper is lacking in clarity.  I do not think I could re-implement the paper, given the level of detail presented.

* I was very disappointed in the experiments.  Not only were they on gridworld-like domains (see below), but it was not clear if the improvements were significant in any way.

* While I thought the discussion in Sec. 3.3 was interesting, it didn't seem to be a "contribution"; it felt like some adhoc thoughts.

* Keeping per-state counts is only workable in small state space domains.

Minor comments:

Please fix the formatting of your citations.

There are numerous typos and spelling errors.  Please correct them.

I am strongly opposed to the use of gridworlds, or anything like them, in modern RL research.  While many ideas work fine in small, toy domains, they simply do not scale.  As a field, we need to move past them and focus more on algorithms and ideas that have more practical relevance.

Pros:
+ Core ideas seem promising
+ Leveraging mathematical structure is a great strategy for constructing algorithms with desirable properties

Cons:
- Very limited experimental results
- Not clear if improvements are significant
- Hierarchy construction seems to be too limited to work in any reasonably sized problem
- No evidence, theoretical or otherwise, is given to suggest that this particular hierarchy construction method is any better than any other method

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJxYDLd52m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This work extend previous work with multitask linear MDP for discovery of deep hierarchical abstractions from offline setting to a online setting. It lacks clarify in presentation and has limited experiments and anlaysis. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hye6uoC9tm&amp;noteId=HJxYDLd52m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper394 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper394 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">pros:
The experiment results demonstrate better performance of the proposed method.
Cons: 
1. It does not compare with any existing HRL (with simple adaptation to multitask settings), except for Q learning under different exploration strategies. 

2. Also the method is tested only on one type of domain-gridworld, which seems very limited, especially for supporting the claims, including “the proposed method can uncover a deep hierarchical abstraction of task ensemble in a complete online fashion” and “the process of learning the hierarchical structure online does not significantly slow learning”

3. There is no implementation details of the proposed in a algorithmic form.

4. There is also no ablation test or discussion of parameters that affect the performance of the experiment results, for example, learning rate, size of the domain. 

5. There are a lot of notations used without definition. The paper should be more self-contained by providing accurate definition and citations.

For example, what are are Interior state,  boundary state, finite exit problem
Is V(s) computed with based on R_i or R, or both?
What is the control cost refer to? Is it related to R_i or R somehow?

6. Some references are cited wrongly. 
Machado et al (2018) cited is not about HRL. Please double check
The references should be updated to reflect the latest information, for example,
It might be better to cite the AAAI17 version of the option-critic architectures 
Between MDPs and Semi-MDP: A framework for Temporal Abstraction in Reinforcement learning should use the 1999 version 

 
7. Minor issues:
task it is -&gt; task is
Z^l \in R^{N\times N}
 Howver -&gt; however
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rylvQpg5n7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice idea and combination of methods. Difficult to assess significance</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hye6uoC9tm&amp;noteId=rylvQpg5n7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper394 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper394 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This work builds on the ICML paper from Saxe et al (2017) in which the compositionality property of LMDPs was exploited to solve multi-task hierarchies. The paper extends this work by proposing a method that learns incrementally such hierarchies instead of pre-defining them by design. Some experimental results illustrate the method on two toy problems, a 1D corridor and a corridor of rooms.

The paper deals with an interesting and hard problem. Learning hierarchies while solving an MDP is a much harder problem than solving the flat MDP or solving the hierarchical MDP. The authors leverage the compositionality of optimal controls of the LMDP framework to learn incrementally the hierarchies. Surprisingly, the proposed method not only learns those hierarchies, but also is more effective in terms of exploration.

On the positive side, the main idea is very interesting and has a lot of potential. The authors combine many techniques under the powerful framework of LMDPs such as hierarchical RL, low-rank factorization, and count-based exploration. The authors do a good job describing their approach (at a higher level).

On the negative side, the paper looks a bit incremental, given the prior existing work. I also found the paper unclear in many aspects lacking some relevant technical details (see below). The narrative is sometimes superficial or focused mainly on intuitions and analogies. Overall, it is difficult to assess the significance of this work and the results give the impression of limited applicability, beyond the described toy problem.

1- First, in order to combine optimal controls, the LMDPs need to be solved for each different boundary state, i.e., do you require to solve as many LMDPs as possible states? If that is the case, I don't think it makes sens to talk about exploration/exploration trade-off, since you really need to visit all states a priori.

2- I cannot understand what is learned and what is required a priori. the authors state that if "the multi-scale structure of the domain is known a priori, the decomposition (...) explicitly specified". What does exactly that mean? If what the method does is an incremental version of the low-rank factorization proposed in Earle at al (2018), I think the presentation can be better described in those terms.

2- Regarding exploration/exploitation tradeoff. From section 3, it seems that authors focus on a particular spatial problem and define already some exploration choices. But this means that the choice about when a state is integrated in the current MDP is already done, so no real trade-off exists?

3- The narrative in Section 3.2 is not very rigorous. The authors just mention the computational problem to keep consistency between layers and then just argue that "in practice" using count-based exploration everything works. I think a more principle approach is necessary.

4- Experiments I: what do the authors means by "exploration"? Is it just Boltzmann exploration? I can think of an exploration strategy that would choose an unseen state with probability 1 and would bring the agent to the goal in one shot.

5- Experiments II: I like the benchmark but, how does the result depend on the structure of the problem? What happens if I the rooms have very different sizes?

6- I miss some references that are very relevant to this work:

- the ICAPS paper "Hierarchical Linearly-Solvable Markov Decision Problems" by Jonson et al. seems to be the first proposing a hierarchical embedding of LMDPs.

- other factorization techniques exist, e.g., "Incremental Stochastic Factorization for Online Reinforcement Learning", Barreto et al (AAAI' 16), to uncover an MDP structure.

There are also some minor grammar mistakes:

"passive dynamics then become" -&gt; "passive dynamics then becomes"
"reward function r" -&gt; "reward function R"
"Howver" -&gt; "However"
"the room contains" -&gt; "the room that contains"
"have simply add" -&gt; "have simply added"
"spacial and temporal" -&gt; "spatial and temporal"
...</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>