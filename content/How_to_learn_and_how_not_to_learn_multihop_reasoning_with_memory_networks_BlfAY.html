<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>How to learn (and how not to learn) multi-hop reasoning with memory networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="How to learn (and how not to learn) multi-hop reasoning with memory networks" />
        <meta name="citation_author" content="Jifan Chen" />
        <meta name="citation_author" content="Greg Durrett" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1lf43A5Y7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="How to learn (and how not to learn) multi-hop reasoning with memory..." />
      <meta name="og:description" content="Answering questions about a text frequently requires aggregating information from multiple places in that text. End-to-end neural network models, the dominant approach in the current literature..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1lf43A5Y7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>How to learn (and how not to learn) multi-hop reasoning with memory networks</a> <a class="note_content_pdf" href="/pdf?id=B1lf43A5Y7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=jf_chen%40utexas.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="jf_chen@utexas.edu">Jifan Chen</a>, <a href="/profile?email=gdurrett%40cs.utexas.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="gdurrett@cs.utexas.edu">Greg Durrett</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Answering questions about a text frequently requires aggregating information from multiple places in that text. End-to-end neural network models, the dominant approach in the current literature, can theoretically learn how to distill and manipulate representations of the text without explicit supervision about how to do so. We investigate a canonical architecture for this task, the memory network, and analyze how effective it really is in the context of three multi-hop reasoning settings. In a simple synthetic setting, the path-finding task of the bAbI dataset, the model fails to learn the correct reasoning without additional supervision of its attention mechanism. However, with this supervision, it can perform well. On a real text dataset, WikiHop, the memory network gives nearly state-of-the-art performance, but does so without using its multi-hop capabilities. A tougher anonymized version of the WikiHop dataset is qualitatively similar to bAbI: the model fails to perform well unless it has additional supervision. We hypothesize that many "multi-hop" architectures do not truly learn this reasoning as advertised, though they could learn this reasoning if appropriately supervised.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">NLP, Reading Comprehension, Memory Networks, Multi-hop Reasoning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Memory Networks do not learn multi-hop reasoning unless we supervise them.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Sklli21Nam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1lf43A5Y7&amp;noteId=Sklli21Nam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1428 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1428 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1gLEr-63Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Mostly trivial claims</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1lf43A5Y7&amp;noteId=r1gLEr-63Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1428 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1428 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper claims that multi-hop reasoning (that is required in bAbI and WikiHop) is (1) not easy to learn directly and (2) direct supervision of the hop is often necessary. The paper also claims that (3) doing well on WikiHop doesn't necessarily mean the model is actually learning to hop.

The paper is easy to understand. I also agree with the claims. However, I think the claims are mostly trivial.

(1) and (2) seem to be well-known to the community. In fact, the original Memory networks paper by Weston et al. (2015) uses strong supervision to solve bAbI, and the follow-up paper, End-to-end memory networks, attempts to solve bAbI without strong supervision, in which the authors were clearly aware of the fact that strong supervision helps the model to learn multi-hop much more easily. Furthermore, there exist numerous models, e.g. Kumar et al. (2016) that the authors cite, that do very good multi-hop reasoning on bAbI. Since many of these models can be considered as variants of end-to-end memory networks, the authors' claim that strong supervision is critical is not well-supported. Also, I feel that the paper is not comprehensively reviewing these related works.

Lastly, (3) could be a helpful and interesting observation of WikiHop dataset / Memory network, but pointing out the flaw of a dataset or the model alone does not seem to have enough contribution for ICLR.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Byxo2ZUq3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting investigation but insufficient proposition and results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1lf43A5Y7&amp;noteId=Byxo2ZUq3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1428 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1428 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes to investigate the well-known problem of memory network learning and more precisely the difficulty of the attention learning supervision with such models. In the introduction, I strongly agree with the statement saying that while end-to-end memory network has been proposed, it is still very difficult to train such model with an off-the-shelf adaptive gradient descent algorithm and an end-to-end supervised loss.

The paper proposed to use a model with a 2-level attentive encoding of the memory blocks corresponding to a word and a sentence level. The authors start investigating in section 3 the use of an attention supervision. The authors investigate this attention supervision on the path-finding task of the Babi20 dataset and the Wikihop set of the QAngoroo dataset.

As secondary supervision signal, the authors proposed to use a 'pseudo-gold chain' reasoning information using the co-occurrences between the named entities of the questions and answers with the passages. It can be argued that this pseudo-gold reasoning chain is mainly possible because of the synthetic nature of the synthesis of the dataset which has been produced using a structured knowledge base.

In a sense, supervising attention in such way was already suggested in [Bordes and Weston 2015], the novelty seems very limited to me while the analysis provided by this work might be useful as an interesting starting point for further analysis and propositions in this domain.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1l11J3YsQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting analysis, but...</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1lf43A5Y7&amp;noteId=H1l11J3YsQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1428 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">21 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1428 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies the behavior of memory network in the task of multi-hop reasoning. Although memory network was advertised to be capable of multi-hop reasoning, this paper argues that memory network fails to learn reasonable multi-hop reasoning. However, by incorporating the supervision of reasoning path by encouraging attentions to appropriate sentences, memory network was able to perform multi-hop reasoning.

The analysis in the paper is interesting. Especially, some of them I found very interesting is
(i) Attention targets show that memory network does not attend to the appropriate sentences.
(ii) On WikiHop, `NoText` (which doesn’t read given document at all) achieves 59.7, which is only 5.1 lower than SOTA.
(iii) Encouraging attention to appropriate sentence leads to a dramatic performance gain (on bAbI path-finding and Wikihop Masked).

However, I found some limitations as follows.

First of all, bAbI QA and Wikihop are insufficient to draw attention.
- bAbI QA is synthetic — If current models are struggling with synthetic datasets, it’s great to work on synthetic datasets, since working on the easier dataset and later move on to the harder, real dataset makes sense. However, bAbI QA was solved a while ago (2 years ago, in this venue) and people are less interested in synthetic multi-hop reasoning now.
- WikiHop is pretty noisy — the authors of the original WikiHop paper has mentioned only 36% of questions have a unique multi-step answer (9% have a single-step answer, and 55% either have multiple possible answers or are noisy). In addition, this papers shows the model gets 59.7% without document, which means this task is not for multi-hop reasoning.

Second, this paper studies memory network in particular, but memory network is not used for multi-hop reasoning in a real dataset. For example, on Wikihop Masked, Memory network without supervision achieves 14.2 which means it doesn’t work at all. Even after adding supervision, it is worse than standard QA models (models designed for the single-hop task).

Lastly, the analysis of the attention targets without supervision does not give a new intuition about the incapability of the model in multi-hop reasoning, because the performance of the model is already bad (on bAbI path-finding and Wikihop Masked, since Wikihop standard seems to be meaningless).

To summarize, their motivation and the idea of encouraging attention to the right sentences is neat. In particular, since memory network has drawn a lot of attention, this study might give a new intuition to people who have been working on memory network. However, some limitations mentioned above made this paper not sufficient to be presented at ICLR main conference.

I think the authors can try one of these to make the paper better.

(i) Choose a pair of dataset and the model which the model performs reasonably on the dataset. (For example, bAbI except for path-finding &amp; memory network, or Wikihop &amp; Wikihop SOTA models), and try the same analysis. Then, the story will be “Though memory network performs well on this task, it turns out that it is not doing right multi-hop reasoning”. Also, it would be awesome if adding supervision can lead to a even higher performance by doing appropriate reasoning, but it shouldn’t be necessary.
(ii) Try the same analysis in more widely used tasks and models. I think a pair of (Wikihop Masked, Wikihop Masked SOTA model) is sufficient.

Even though the authors do not revise the paper (since it would take too long to revise), I think this paper is worth to be presented at ICLR workshop.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>