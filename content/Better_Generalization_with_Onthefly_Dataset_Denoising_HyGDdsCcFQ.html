<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Better Generalization with On-the-fly Dataset Denoising | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Better Generalization with On-the-fly Dataset Denoising" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HyGDdsCcFQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Better Generalization with On-the-fly Dataset Denoising" />
      <meta name="og:description" content="Memorization in over-parameterized neural networks can severely hurt generalization in the presence of mislabeled examples. However, mislabeled examples are to hard avoid in extremely large..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HyGDdsCcFQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Better Generalization with On-the-fly Dataset Denoising</a> <a class="note_content_pdf" href="/pdf?id=HyGDdsCcFQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019better,    &#10;title={Better Generalization with On-the-fly Dataset Denoising},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HyGDdsCcFQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=HyGDdsCcFQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Memorization in over-parameterized neural networks can severely hurt generalization in the presence of mislabeled examples. However, mislabeled examples are to hard avoid in extremely large datasets. We address this problem using the implicit regularization effect of stochastic gradient descent with large learning rates, which we find to be able to separate clean and mislabeled examples with remarkable success using loss statistics. We leverage this to identify and on-the-fly discard mislabeled examples using a threshold on their losses. This leads to On-the-fly Data Denoising (ODD), a simple yet effective algorithm that is robust to mislabeled examples, while introducing almost zero computational overhead. Empirical results demonstrate the effectiveness of ODD on several datasets containing artificial and real-world mislabeled examples.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">dataset denoising, supervised learning, implicit regularization</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We introduce a fast and easy-to-implement algorithm that is robust to dataset noise.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HkgFSQ1j2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Simple and interesting work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyGDdsCcFQ&amp;noteId=HkgFSQ1j2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper365 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper365 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=HkgFSQ1j2Q" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:

The authors propose a simple empirical method for cleaning the dataset for training. By using the implicit regularization property of SGD-based optimization method, the authors come up with a method of setting a threshold for the training loss statistics such that the examples that show losses above the threshold are regarded as noisy examples and are discarded. Their empirical results show that ODD (their method) can outperform other baselines when artificial random label noise is injected. They also show ablation studies on the hyperparameters and show the final result seems to be robust to those parameters. 

Pros:
- The method is very simple
- The empirical results, particularly on the synthetic noisy training data, seems to be encouraging.
- The ablation study argues that the method is robust to the hyperparameters, p, E, and h.

Cons:
- I think the results remains to be highly empirical. While it is interesting to see the division of the loss statistics in Figure 2, I am not very convinced about the real usage of the proposed method. The result in Table 5 shows that ODD can outperform ERM for real world datasets, but the improvement seems to be marginal. Moreover, the hyperparameter p was set to 30 for that experiment, but how did the authors choose that parameter? Clearly, if you choose wrong p, I think the performance will degrade, and it is not clear how you can choose p in real applications. The ablation studies are only with synthetic noisy label data, so I think the result is somewhat limited. 
- 

I think the paper shows interesting results, but my concern is that it seems to be quite empirical. The positive results are particularly on the synthetic data case. 


</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkeVN3FnpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Simple baselines like ODD could serve as drop-in replacement for ERM</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyGDdsCcFQ&amp;noteId=SkeVN3FnpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper365 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper365 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review. The goal of our paper is to show that in the case of deep learning with noisy supervision, a simple method, ODD, can significantly outperform sophisticated alternatives, such as MentorNet and Ren’s Meta Validation Optimization (both in ICML18). We believe ODD can serve as a drop-in replacement for ERM for robust deep learning, and the findings could lead to more theoretical insights.

Q: Improvement on real datasets seems marginal.

We respectfully disagree: while the improvement on ODD over ERM on WebVision seems marginal (around .5% top-1 error), the improvement of Inception ResNet-v2 over ResNet-50 is also only 1-2%. However, Inception ResNet-v2 increases training time from 2 days to 6 days (with 8 V100 GPUs). ODD, on the other hand, drops about 15% of the data and thus saves training time for both models. 

ODD can also be easily implemented, making it appealing to practitioners who train their models on real-world datasets. These datasets can contain an unknown source of label noise which can hurt generalization.

Despite its simplicity, ODD outperforms all the state-of-the-art methods on datasets with various types of label noise, and is competitive to the state-of-the-art on clean datasets. This includes CIFAR100 (“clean” version) as well as WebVision (a dataset based on noisy web supervision). ODD also demonstrates that even datasets which are assumed to be “clean”, such as CIFAR100, actually contain a lot of mislabeled examples. Our method could alleviate the process of tedious label cleaning, as a lot of the noisy labels are automatically detectable.


Q: How to choose p?
In Table 11 (appendix), we show that the performance of ODD is not much affected for various settings of p = 10, 30, 50, 80 on WebVision; similar arguments could be made for clean ImageNet. Even as ODD requires tuning p, it bypasses the more sophisticated hyperparameters, such as the amount of noise / weight between different objectives / extra MentorNet architecture proposed by previous methods. In practice we find that p ranging from 10 to 50 gives reasonable results.

Q: Ablation studies only with synthetic noisy label data.

Table 11 &amp; 12 show results for various settings of p on WebVision - a real-world dataset with noisy labels. It would be impossible to measure the amount of “real-world noise” let alone control the amount in an ablation study. We therefore conducted experiments on synthetic data in order to precisely control the amount of noise. The same methodology is used in both the MentorNet paper and Ren’s ICML18 paper.

Therefore, we believe that practitioners can apply ODD as a drop-in replacement for ERM for their (potentially noisy) datasets, so it should be of interest to ICLR. The implicit regularization effect can be used to filter out potentially mislabeled data and to improve generalization. Explaining why large learning rates would have this effect is also interesting, but could require more assumptions than is realistic at this point. We believe this is an interesting open problem for the theoretical community.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BkxMcDc5n7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting paper with sufficient empirical experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyGDdsCcFQ&amp;noteId=BkxMcDc5n7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper365 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper365 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=BkxMcDc5n7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper aims to remove potential examples with label noise by discarding the ones with large losses in the training procedure. The idea also applies to the setting where instances may contain large noise. The proposed method may have an implicit trade-off between the robust to label noise and feature noise, which explains why the proposed method also has good performances on instance-dependent label noise. The paper is well-written and has sufficient experiments. 

The discussions in Section 2.2 is unclear for me. What is "p_n(\ell)"? What is "p-th percentile of a distribution"? How reasonable thresholds are derived for the uniform label noise? Why the method will generalize to other types of label noise?

===
After reading the rebuttal, it is still unclear of how to determine the thresholds for finding incorrect labels. The authors empirically demonstrated a procedure to statistically find a threshold under the assumption that the label noise is uniform. However, theoretical guarantees are lacking. The extension to other types of label noise is also very intuitive. Although the proposed method is simple and effective, the lack of an effective method for choosing the threshold is a major concern for real-world applications. Are there some other ways to determine the threshold? For example, cross-validation method?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJg3dpt26m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Uniform random noise is used to analyze the regularization effect of SGD, which generalizes across multiple types of noise.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyGDdsCcFQ&amp;noteId=BJg3dpt26m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper365 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper365 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your review!

Q: What is “p_n(\ell)”
This is “the estimated (negative log-likelihood) loss distribution of uniform label noise”, as described before Equation (1).

Q: "p-th percentile of a distribution D"
This means “p” percent of the samples “s” from the distribution D are lower than the value “V”, so P(s &lt; V) = p.

Q: How reasonable thresholds are derived for the uniform label noise?
The reason why we used uniform label noise is because this is the simplest type of noise. Moreover, the entropy of this label noise is highest, so any other type of label noise would have smaller entropy. As a consequence, they are harder to separate than uniform label noise. Therefore, the threshold for other types of noise (including real world ones), should be higher than the 1/K-th percentile of “p_n(\ell)”, where K denotes the number of labels (to reduce false positives), motivating the percentile hyperparameter “p”.

Q: Why does the method generalizes to other types of noise?
In our analysis, we assume a label is either “correct / clean” or “incorrect / noise”. We wish to demonstrate that by leveraging the implicit regularization effect of large learning rate SGD, it is easier to separate the “correct” and “incorrect” labels. 

The uniformly random noise is used to gain “ground-truth” knowledge about the “correct / incorrect” information, since we do not have this information in real-world datasets. For other types of noise, only the distribution of “correct” and “incorrect” labels is changed; this should not affect SGD’s ability to separate them. 

The experiments on CIFAR100 "clean" dataset also supports this claim (Figure 1), as the examples that are treated as "incorrect" by ODD do contain more "incorrect" labels.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Hyx00AV5n7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>BETTER GENERALIZATION WITH ON-THE-FLY DATASET DENOISING</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyGDdsCcFQ&amp;noteId=Hyx00AV5n7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper365 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper365 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents ODD, a method that rejects incorrectly labeled / noisy examples from training on the fly. The motivation is sound, that with the capacity of modern neural networks, it's easy to memorize the mislabeled data and thus hurt generalization. If we could reject such mislabeled data, we may be able to get a more generalizable model. The authors made an observation that when training with large learning rate, examples with correct labeling and incorrect labeling exhibits different loss distributions. The authors further noticed that the loss distribution of incorrectly labeled examples can be simulated using eq.(1). Therefore, by setting a threshold that corresponds to a percentile of the incorrectly labeled loss distribution, the authors are able to reject incorrect examples.

Some comments:
1. Eq.(1) basically assumes all the noise is uniformly distributed among classes. What if only 2 classes are easily mislabeled while others are fine?
2. Section 4.1.3 and Section 4.4 Sensitivity to Noise are confusing. Please clarify the importance and rationale for such analysis.
3. Cosine schedule is used in the experiments. However, since the method does not work well with small learning rate, why not using a fixed large learning rate and decrease it after noise rejection? Also, in section 4.4 Sensitivity to E, the analysis of the sensitivity to the number of epochs is coupled with a changing learning rate. It would be better to see an experiment with the two decoupled.
4. The loss of an example is averaged over h epochs. It will better to clarify how the simulated distribution generated in such case since the distribution is dependent on fc(.), which is changed between two epochs.
5. Except for the first experiment, all other experiments are only compared with ERM, the vanilla algorithm. It would be better to show a comparison with other methods.
6. Please show a precision/recall of the examples that are marked as "noise" by the method.
7. I assume this method will remove a lot of hard examples. How does this affect training? Does this make the network more error-prone to harder instances?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1xTMRF2Tm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Addressing individual comments (continued)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyGDdsCcFQ&amp;noteId=H1xTMRF2Tm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper365 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper365 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Q4: How to merge statistics from multiple epochs?

We merely take the average over the log-likelihood loss of the same example (which could be different due to data augmentation). The loss statistics does not change significantly over one epoch of training.

Q6: Precision-Recall for examples classified as noise.

We evaluated precision and recall for examples classified as noise on CIFAR10 and CIFAR100 for different noise levels (1, 5, 10, 20, 30, 40), with hyperparameter settings in Appendix A.1. The recall values are around 0.87 where as the precision values range from 0.88 to 0.92. This demonstrates that ODD is able to achieve good precision/recall with default hyperparameters at different noise levels. We update these results in the Appendix. 

Q7: What is the amount of hard example removed? How does it affect error on “harder” instances?

In Figure 5, Table 10 and Table 11, we demonstrate the amount of examples classified as “hard” / “noisy”. The amount varies, but generally is smaller on clean datasets than noisy datasets (15% on WebVision, and 2.5% on ImageNet). 

The error on there harder instances would definitely be higher, since ODD stopped training them after E epochs. However, we demonstrate that this could even improve generalization, since these “hard” examples could be mislabeled, and therefore not worth learning in the first place. This is in contrast to other approaches such as “hard example mining” which assumes the “hard examples” are not mislabeled.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJxAl0YhTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Addressing individual comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HyGDdsCcFQ&amp;noteId=rJxAl0YhTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper365 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper365 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the review! One of our goals in this paper is to demonstrate that with the right principles, simple methods (ODD) can perform much better than complicated ones (based on Meta Learning). We answer Q5 first to address the comparison with other methods.

Q5: Performance comparison with other methods.

We compared the numbers reported by MentorNet on other experiments. MentorNet is the state-of-the-art method to address robust deep learning (in ICML18). However, our results show that MentorNet can be outperformed by switching to a cosine learning rate schedule alone. Other related methods are already outperformed by MentorNet from the comparisons in the MentorNet paper.

MentorNet requires tuning of a wide range of hyperparameters, including the pretrained denoising schedule, the architecture of the Mentor network, and even the individual dropout ratio for each epoch of training. This is much more complicated compared to ODD, which makes little changes to standard ERM training, and only introduces a few hyperparameters that are interpretable and easy to tune. 

Therefore, we believe that ODD can serve as a simple but effective baseline for future work in robust supervised deep learning. Future work in this direction can benefit from the implicit regularization principles found in this paper.

Q1: What if the actual noise model is not uniformly random?

Thanks for mentioning this! In real applications, the actual noise model is unlikely to be uniformly random; in fact, we have no information about the noise model at all! ODD makes no assumptions about the noise transition matrix or the amount of noise.

We assume uniformly random noise because of two reasons. First, this case is very easy to analyze and simulate, and we can evaluate the performance easily across multiple noise levels. Second, this noise has higher entropy than any other type of noise, making it easier to separate than clean labels. We use this analysis to select suitable threshold values that balances precision and recall reasonably as controlled by hyperparameter p.

Q2: Clarify the rationale for such analysis

Section 4.1.3 evaluates ODD’s performance when the dataset inputs are imbalanced, which is very common in real-world datasets (such as WebVision). By combining several labels into one, we prevent any method that tries to balance the dataset simply by counting the number of labels. CIFAR50 considers randomly merging the labels, and CIFAR20 considers merging the labels that are semantically related. Our positive results demonstrate that ODD is capable of dealing with unbalanced datasets as well.

Section 4.4 evaluates ODD’s sensitivity to the hyperparameters (p, E) as well as the portion of uniform noise in the dataset. We demonstrate that ODD is generally insensitive to changes of hyperparameters, and that ODD performs well under all the uniform noise levels. This suggests that we could bypass the process of estimating the amount of noise (which is unknown in realistic cases) in the dataset and tune (p, E) directly for ODD. While uniform noise is unrealistic, the amount of data subject to this type of noise can be controlled, unlike real-world noise.

Q3: Why not use large learning rate and decay? Why not decouple E and learning rate?

In Figure 3 and Table 7, we demonstrate the performance of the stepwise learning rate schedule. For the stepwise learning rate schedule, we used a large learning rate at first, and set E to the epoch where we first decrease the learning rate. From Figure 3, the smaller learning rate will start to overfit immediately if ODD is not performed. This forces E to be related to the learning rate schedule for the stepwise case. Moreover, the results from cosine outperform stepwise consistently, so we use cosine as a strong baseline to compare with.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>