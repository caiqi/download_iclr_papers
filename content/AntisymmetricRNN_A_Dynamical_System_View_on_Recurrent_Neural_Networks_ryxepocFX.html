<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ryxepo0cFX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks" />
      <meta name="og:description" content="Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ryxepo0cFX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks</a> <a class="note_content_pdf" href="/pdf?id=ryxepo0cFX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019antisymmetricrnn:,    &#10;title={AntisymmetricRNN: A dynamical System View on Recurrent Neural Networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=ryxepo0cFX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=ryxepo0cFX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Recurrent neural networks have gained widespread use in modeling sequential data. Learning long-term dependencies using these models remains difficult though, due to exploding or vanishing gradients. In this paper, we draw connections between recurrent networks and ordinary differential equations. A special form of recurrent network called the AntisymmetricRNN is proposed under this theoretical framework, which is able to capture long-term dependencies thanks to the stability property of its underlying differential equation. In comparison to existing approaches for improving RNN trainability, which often incur significant computation overhead, AntisymmetricRNN achieves the same goal by design. We showcase the advantage of this new architecture through extensive simulations and experiments. AntisymmetricRNN exhibits much more predictable dynamics. It outperforms regular LSTM models on tasks requiring long-term memory, and matches the performance on tasks where short-term dependencies dominate despite being much simpler.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BJl23nE5h7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A novel RNN architecture</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxepo0cFX&amp;noteId=BJl23nE5h7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper768 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper768 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper introduces antisymmetric RNN, a novel RNNs architecture that is motivated through ordinary differential equation (ODE) framework. Authors consider a first order ODE and the RNN that results from the discretization of this ODE. They show how the stability criteria with respect to perturbation of  the initial state results in an ODE  in a trainability criteria for the corresponding  RNN. This criteria ensures that there are  no exploding/vanishing gradients. Authors then propose a specific parametrization, relying on antisymmetric matrix to ensure that the stability/trainability criteria is respected. They also propose a gated-variant of their architecture.  Authors evaluate their proposal on pixel-by-pixel MNIST and CIFAR10 where they show they can outperforms an LSTM.

The paper is well-written and pleasant to read. However, while the authors argue that their architecture allows to mitigate vanishing/exploding gradient, there is no empirically verification of this claim. In particular, it would be nice to visualize how the gradient norm changes as the gradient is  backpropagated in time, compare the gradient flows of Antisymmetric RNN with a LSTM or report the top eigenvalue of the jacobian for the different models.

In addition,  the analysis for the antisymmetric RNN assumes no input is given to the model. It is not clear to me how having an input at each timestep affects those results?

A few more specific questions/remarks:
-	Experimentally, authors find that the gated antisymmetric RNN sometime outperforms its non-gated counterpart. However, one motivation for the gate mechanism is to better control the gradients flow. It is unclear to me what is the motivation of using gate for the antisymmetric RNN ?
-	as the proposed RNN relies on a antisymmetric matrix to represent the hidden-to-hidden transition matrix, which has less degree of liberty, can we expect the antisymmetric RNN to have same expressivity as a standard RNN. In particular, how easily can an antisymmetric RNN forgets information ?
-	On the pixel-by-pixel MNIST, authors report the Arjosky results for the LSTM baseline.
Note that some papers reported better performance for the LSTM baseline such as Recurrent Batch Norm (Cooijman et al., 2016) .

Antisymmetric RNN appears to be well-motived architecture and seems to outperforms previous RNN variants that also aims at solving exploding/vanishing gradient problem. Overall I lean toward acceptance, although I do think that adding an experiment explicitly showing that the gradient does not explode/vanish would strengthen the paper. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryghF-viT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxepo0cFX&amp;noteId=ryghF-viT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper768 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper768 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your constructive feedback. The paper is updated with the suggested changes. In particular, a new Figure 2 visualizing the eigenvalues of the end-to-end Jacobian is included.

“empirical verification of mitigation of vanishing/exploding gradient”? We included a new Figure 2 on the mean and standard deviation of the eigenvalues of the end-to-end Jacobian matrices for LSTMs and AntisymmetricRNNs with different diffusion constants. They are computed on the networks trained for the padded CIFAR10 dataset with time steps T in {100, 200, 400, 800}. As a quick summary, the eigenvalues for LSTMs quickly approaches zero as time steps increase, indicating vanishing gradients as they back-propagate in time. This explains why LSTMs fail to train at all on this task. AntisymmetricRNNs with a broad range of diffusion, on the other hand, have eigenvalues centered around 1. It is worth noting though as the diffusion constant increases to large values, AntisymmetricRNNs run into vanishing gradients as well. The diffusion constant plays an important role in striking a balance between the stability of discretization and non-vanishing gradients.

“how do inputs affects the analysis”? Our analysis in Section 3 on the stability of an ODE is valid with inputs. In Equation 9 where we calculate the Jacobian matrix, the inputs only affect the diagonal matrix. As long as the diagonal matrix is bounded, which is true for derivatives of most activation functions, the Jacobian matrix still satisfies the critical criterion with inputs. Figure 5 in Appendix D shows the simulation with independent standard Gaussian input. Although the dynamics become slightly noisier comparing with those in Figure 1, the trend remains the same.

“the motivation of using gate for the antisymmetric RNN”? We see AntisymmetricRNN and AntisymmetricRNN w/ gating as discretizations of two different ODEs under the same theoretical framework. Gating provides a mechanism for the underlying ODE to have more degrees of freedom and to capture more complex dynamics. Experimental results show that AntisymmetricRNN performs better on pMNIST while AntisymmetricRNN w/ gating works well on the other tasks. 

“expressivity of AntisymmetricRNN”? Structural constraint on the weight matrix could limit the expressivity of AntisymmetricRNN. However, we do not observe performance degradation in our empirical studies. We hypothesize it is due to over-parametrization in these networks. An AntisymmetricRNN can outperform other RNN models with fewer model parameters.

“how easily can an antisymmetric RNN forgets information”. The diffusion term can be regarded as a mechanism for AntisymmetricRNNs to forget inputs in the past. As shown in the newly added Figure 2, when the diffusion constant increases, the eigenvalues of the end-to-end Jacobian decreases, resulting in shrinking gradient w.r.t. inputs in the past. In our current formulation, the diffusion factor is a constant across all the time steps and dimensions, but we could extend it to be time-dependent and/or data-dependent in future work. 

“better baseline in Cooijman et al., (2016)”? Thanks for the pointer. We added that in the footnote. We decide to keep the LSTM baseline reported by Arjovsky et al., (2016) because it has a higher accuracy on the more challenging pMNIST task than that in Cooijman et al., (2016) (92.6% vs 90.2%). We added the 92.6% accuracy in the footnote. Cooijman et al., (2016) is very relevant to our paper and we have added it to the related work section. It would be interesting to compare a “batch-normalized AntisymmetricRNN” with the batch-normalized LSTM in future work.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BklTkn-9hQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good paper with original work, experiments could be improved</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxepo0cFX&amp;noteId=BklTkn-9hQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper768 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper768 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this paper, the authors provide a new approach to analyze the behavior of
RNNs by relating the RNNs with ODE numerical schemes. They provide analysis on
the stability of forward Euler scheme and proposed an RNN architecture called
AntisymmetricRNN to solve the gradient exploding/vanishing problem. 

The paper is well presented although more recent works in this direction
should be cited and discussed. Also, some important issues are omitted and not
explained. 
For example, the analysis begins with "RNNs with feedback" rather than vanilla
RNN, since vanilla RNN does not have the residual structure as eq(3). The
authors should note that clearly in the paper. 

Although there are previous works relating ResNets with ODEs, such as [1],
this paper is original as it is the first work that relates the stability of
ODE numerical scheme with the gradient vanishing/exploding issues in RNNs. 

In general, this paper provides a novel approach to analyze the gradient
vanishing/exploding issue in RNNs and provides applicable solutions, thus I
recommend to accept it. 


Detailed comments:

The gradient exploding/vanishing issue has been extensively studied these
years and more recent results should be discussed in related works.
Author mentioned that existing methods "come with significant computational
overhead and reportedly hinder representation power of these models". However
this is not true for [2] which achieves full expressive power with
no-overhead. 
It is true that "orthogonal weight matrices alone does not prevent exploding
and vanishing gradients", thus there are architectural approaches that can
bound the gradient norm by constants [3]. 

The authors argued that the critical criterion is important in preserving the
gradient norm. However, later on added a diffusion term to maintain the
stability of forward Euler method. Thus the gradient will vanish
exponentially w.r.t. time step t as: (1-\gamma)^t. Could the authors provide
more detailed analysis on this issue? 

Since eq(3) cannot be regarded as vanilla RNN, it would be better begin the
analysis with advanced RNN architectures that fit in this form, such as
Residual RNN, Statistical Recurrent Units and Fourier Recurrent Units. 

Why sharing the weight matrix of gated units and recurrent units? Is there any
other reason to do this other than reducing the number of parameters?

More experiment should be conducted on real applications of RNN, such as
language model or machine translation. 


[1] Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond finite layer
neural networks: Bridging deep architectures and numerical differential
equations. In ICML, pp. 3276–3285, 2018.  

[2] Zhang, Jiong, Qi Lei, and Inderjit S. Dhillon. "Stabilizing Gradients for
Deep Neural Networks via Efficient SVD Parameterization." In ICML, pp.
5806-5814, 2018.

[3] Zhang, Jiong, Yibo Lin, Zhao Song, and Inderjit S. Dhillon. "Learning Long
Term Dependencies via Fourier Recurrent Units." In ICML, pp. 5815-5823, 2018. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1lcTWPj6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxepo0cFX&amp;noteId=H1lcTWPj6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper768 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper768 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the detailed comments and for pointing us to the latest work on the Spectral RNN and Fourier Recurrent Units. We have updated the paper accordingly. 

“diffusion breaks the critical criterion”? We want to emphasize that the critical criterion describes a condition of stability of the underlying ODE w.r.t. initial values, while the diffusion term is necessary to stabilize the forward Euler discretization of the ODE. The AntisymmetricRNN does run into issues of vanishing gradient when the diffusion factor is set to large values, with order (1-c\epsilon\gamma)^t. Here \epsilon is the step size of Euler discretization, \gamma is the diffusion factor and c captures the derivatives of the hidden activation. Due to the small step size and the bounded derivatives, we find AntisymmetricRNN can tolerate a broad range of diffusion factors, as shown in the new Figure 2 added on the eigenvalues. 

“begin the analysis with advanced RNN architectures that fit in this form”. We have added discussion of more advanced recurrent architectures that fit in the “residual connection” form.

“Why sharing the weight matrix of gated units and recurrent units”? The weight matrix is shared between the gated units and recurrent units to satisfy the critical criterion. When the weight matrix is shared, the Jacobian matrix has the form of (D_1 + D_2) M, where D_1 and D_2 are diagonal matrices and M is an antisymmetric matrix. On the other hand, if the gated units and recurrent units use different weights, then the Jacobian matrix has the form of D_1 M_1 + D_2 M_2. Even if both M_1 and M_2 are antisymmetric, the eigenvalues of the Jacobian matrix can have real parts, thus breaking the criticality.

“Conduct experiment on language models and machine translation”? We conducted experiments on the pixel-by-pixel image tasks as the benchmark datasets for studying long-range dependence to demonstrate the effectiveness of the proposed method. We would like to study the performance of AntisymmetricRNN on language models and machine translation in future work.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJexpMXwh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>capacity of long-term storage?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxepo0cFX&amp;noteId=BJexpMXwh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper768 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper768 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This is an interesting paper which proposes a novel angle on the problem of learning long-term dependencies in recurrent nets. The authors argue that most of the action should be in the imaginary part of the eigenvalues of the Jacobian J=F' of the new_state = old_state + epsilon F(old_state, input) incremental type of recurrence, while the real part should be slightly negative. If they were 0 the discrete time updates would still not be stable, so slightly negative (which leads to exponential loss of information) leads to stability while making it possible for the information decay to be pretty slow. They also propose a gated variant which sometimes works better. 

This is similar to earlier work based on orthogonal or unitary Jacobians of new_state = H(old_state,input) updates, since the Jacobian of H(old_state,input) = old_state + epsilon F( old_state,input) is I + epsilon F'. In this light, it is not clear why the proposed architecture would be better than the partially orthogonal / unitary variants previously proposed. My general concern with this this type of architecture is that they can store information in 'cycles' (like in fig 1g, 1h) but this is a pretty strong constraint. For example, in the experiments, the authors did not apparently vary the length of the sequences (which would break the trick of using periodic attractors to store information). In practical applications this is very important. Also, all of the experiments are with classification tasks with few categories (10), i.e., requiring only storing 4 bits of information. Memorization tasks requiring to store many more bits, and with randomly varying sequence lengths, would better test the abilities of the proposed architecture.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJgebfDiTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxepo0cFX&amp;noteId=rJgebfDiTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper768 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper768 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comments and feedback.

“Connection to prior work on orthogonal/unitary weights”? Thanks to the reviewer for bringing up another angle to connect this work to the prior work on orthogonal/unitary weights. While prior work reaches unitary Jacobian by constraining the weight matrices to be orthogonal/unitary with linear activation (the condition breaks if nonlinear activation is used), unitary Jacobian is reached in AntisymmetricRNN with the residual connection and constraining f’ to have imaginary eigenvalues. Unitary/orthogonal matrices have eigenvalues that lie on the unit circle. Antisymmetric matrices have eigenvalues of the form i\lambda where \lambda is arbitrary. This implies that the dimension of the possible transformation is much larger (the whole imaginary axis). Therefore, antisymmetric networks are more expressive than unitary ones. There are three advantages of our approach: 1) our condition can be easily achieved with the antisymmetric weight parameterization, with no computational overhead; 2) our condition takes nonlinear activations into consideration; 3) we empirically demonstrate that our formulation is more expressive than constraining the weight matrix to be orthogonal/unitary, as shown in Table 1. Moreover, we expect the connections between RNNs and the ODE theory to serve as a framework to inspire new RNN architectures in the future.

“store information in 'cycles'”. The behavior of the network in phase space is not repetitive. Similar manifolds are obtained when one looks at Lorenz systems for example, which is a simplification of the weather system. The phase diagrams suggest that the network never blows or decays but it is important to note that it does not repeat itself and samples different points in space.

We thank the reviewer for suggesting the tasks with more categories and varying sequence lengths. It is definitely worth studying the performance of AntisymmetricRNN on tasks such as copy and addition in future work.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>