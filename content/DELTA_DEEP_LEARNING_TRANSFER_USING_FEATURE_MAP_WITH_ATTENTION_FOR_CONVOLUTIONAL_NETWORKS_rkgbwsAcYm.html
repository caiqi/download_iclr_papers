<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>DELTA: DEEP LEARNING TRANSFER USING FEATURE MAP WITH ATTENTION FOR CONVOLUTIONAL NETWORKS | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="DELTA: DEEP LEARNING TRANSFER USING FEATURE MAP WITH ATTENTION FOR CONVOLUTIONAL NETWORKS" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rkgbwsAcYm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="DELTA: DEEP LEARNING TRANSFER USING FEATURE MAP WITH ATTENTION FOR..." />
      <meta name="og:description" content="Transfer learning through fine-tuning a pre-trained neural network with an extremely large dataset, such as ImageNet, can significantly accel- erate training while the accuracy is frequently..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rkgbwsAcYm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>DELTA: DEEP LEARNING TRANSFER USING FEATURE MAP WITH ATTENTION FOR CONVOLUTIONAL NETWORKS</a> <a class="note_content_pdf" href="/pdf?id=rkgbwsAcYm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019delta:,    &#10;title={DELTA: DEEP LEARNING TRANSFER USING FEATURE MAP WITH ATTENTION FOR CONVOLUTIONAL NETWORKS},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rkgbwsAcYm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Transfer learning through fine-tuning a pre-trained neural network with an extremely large dataset, such as ImageNet, can significantly accel- erate training while the accuracy is frequently bottlenecked by the lim- ited dataset size of the new target task. To solve the problem, some regularization methods, constraining the outer layer weights of the tar- get network using the starting point as references (SPAR), have been studied. In this paper, we propose a novel regularized transfer learn- ing framework DELTA, namely DEep Learning Transfer using Fea- ture Map with Attention. Instead of constraining the weights of neu- ral network, DELTA aims to preserve the outer layer outputs of the target network. Specifically, in addition to minimizing the empirical loss, DELTA intends to align the outer layer outputs of two networks, through constraining a subset of feature maps that are precisely selected by attention that has been learned in an supervised learning manner. We evaluate DELTA with the state-of-the-art algorithms, including L2 and L2-SP. The experiment results show that our proposed method outper- forms these baselines with higher accuracy for new tasks.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">transfer learning, deep learning, regularization, attention, cnn</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">improving deep transfer learning with regularization using attention based feature maps</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">11 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SylfiAgETX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We are running additional experiments as requested and will continue update our responses.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgbwsAcYm&amp;noteId=SylfiAgETX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper237 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper237 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We want to thank all the anonymous reviewers for your hard work and the insightful comments. As you may see below in our response, we are running additional experiments as requested. We decide to first provide our response based on the data that we have and we will continue update our responses. Thank you for your patience and we look forward to your further comments and discussions.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ryl3-sh937" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>main contributions: bringing in the concept from teacher-student, same task to different task in transfer learning by modifying the weights of outer layers</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgbwsAcYm&amp;noteId=ryl3-sh937"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper237 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper237 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary
The paper describes using the technique of modifying the weights for the outer layers, used in teacher-student network for same task, to transfer learning for different tasks by modifying the loss function and pre-training using target network labels to emphasize the neurons that are considered important for prediction. The technique seems to be no more/slightly better than the Lsquare SP, but exceeds when used with attention.

Improvements
- the amount of training time needed to pre-train using the L-square FE and target labels should be mentioned as it seems that for large network, and large data, this can be a factor
- The choice of Resnet, at least one of the more recent networks for object detection (Inception, YOLO etc.)  would be a good add</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJxStoxV6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for the review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgbwsAcYm&amp;noteId=HJxStoxV6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper237 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper237 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review and encouraging comments. We summarize Reviewer 1’s major concerns as following two questions and we try to respond these two answers accordingly.

Q1. “the amount of training time needed to pre-train using the L-2 FE and target labels should be mentioned as it seems that for large network, and large data, this can be a factor”

Response: Thanks for the comment. We totally agree that the time-consumption of L2-FE adaption and attention learning should be considered as the overhead of our method. Indeed, the time spent by L2-FE adaption and attention learning is no more than 50% of overall training time.  For example, DELTA (w/o Attention) requires 139 minutes on Caltech30 task transferring from Resnet-101 pre-trained model, while DELTA (with Attention) consumes 197 minutes (42% more than DELTA w/o Attention which doesn’t need L2-FE adaption and attention learning). Furthermore, L2-SP takes124 minutes and L2 spends 115 minutes on the same task in the same settings. It is thus reasonable to conclude the extra time consumption on L2-FE adaption and attention learning does not add a significant overhead to common deep transfer learning practices. 

When we further breakdown such time overhead, we found that the major overhead is due to the attention learning, where for each filter forward inference is needed to estimate the contribution of such filter to the overall accuracy. It should not be a significant performance bottleneck even for a large dataset, as the number of filters needed to evaluate might be fixed with given scratch for transfer learning.  Note that one key contribution made in this manuscript is to improve the deep transfer learning via feature-map based regularization through introducing attention mechanism. All in all, many thanks for your comments. We are revising the manuscript, including the discussion on time consumption, accordingly.

Q.2 The choice of Resnet, at least one of the more recent networks for object detection (Inception, YOLO etc.)  would be a good add

Response: Thanks for comments. We agree to incorporate more results and neural network architectures. We are revising the manuscript with supplementary experiments on both new architecture (inception v3) and datasets(CUB-200-2011[1], Food-101[2]). The results of above experiments will be reported in our new version.

[1] C.Wah,S.Branson,P.Welinder,P.Perona,andS.Belongie. The caltech-ucsd birds-200-2011 dataset. California Institute of Technology, 2011. 6, 7, 8
[2] L. Bossard, M. Guillaumin, and L. Van Gool. Food-101–mining discriminative components with random forests. In ECCV, 2014. 6, 8 </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJx9CoFoTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Re: Thanks for the review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgbwsAcYm&amp;noteId=rJx9CoFoTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper237 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper237 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Few follow-ups:

1. Is each filter in each layer done independently or we are looking at combinations (through the succeeding layers)?
2. On this being not an issue for large data set, can you please put some numbers for Resnet 101 # of filters considered in eq. 6, times the number of instances in the train set used, for all the train datasets considered?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkxcAhkkAm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to "Few follow-ups"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgbwsAcYm&amp;noteId=SkxcAhkkAm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper237 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper237 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review. 
    1. Each filter (channel) is independently evaluated. Though it is time consuming, we only need to evaluate all these filters for one time, prior to the transfer learning.
    2. We considered filters(we use output channels indeed because we constrained the behaviors) of layer conv2_x, conv3_x, conv4_x, conv5_x. They are with 256, 512, 1024, 2048 channels separately,  totally 3840 channels. We used 256 instances(four mini batches) in the training set for all the training datasets.  </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BkguWT5PaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Comparison to similar literature published previously</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgbwsAcYm&amp;noteId=BkguWT5PaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper237 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper237 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Can you also explain similarity of this method with "PAYING MORE ATTENTION TO ATTENTION: IMPROVING THE PERFORMANCE OF CONVOLUTIONAL NEURAL NETWORKS VIA ATTENTION TRANSFER" published in ICLR 17 in particular the activation based attention transfer in the context of teacher-student network, and explain how the main idea in your submission is step above that?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BylowA8YTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to "Comparison to similar literature published previously"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgbwsAcYm&amp;noteId=BylowA8YTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper237 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper237 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for the comment. The work  (Zagoruyko &amp; Komodakis, 2016) was cited in our original submission. We fully agree that it is highly  relevant to our paper. The connections between these works have been briefly stated at the last paragraph of related work in our original paper. Below we would like to elaborate the major differences. 

We have different goals. Zagoruyko &amp; Komodakis aim to obtain a different (and smaller) network to perform the same task.  While DELTA use a similar attention mechanism, we aim to obtain a network to perform a different task. With the different goal, we have to adopt a similar but distinct approach for attention to enable efficient knowledge transfer. 
 
Methodologies. Our definition of attention is based on the discriminant capacity of each filter in the network trained on the source domain as applied to the target domain.  In this approach, we are able to assign the weights to the regularizer for EACH filter for optimizing knowledge transfer. With such attention (accuracy contribution of each filter), DELTA assign higher weights to the useful filters and re-use filters that do not classify the target data well (i.e., to make those filter  freely optimized).  

On the other hand, Zagoruyko &amp; Komodakis defined attention as the data representation obtained by teacher networks, where student networks are guided to obtain similar representation on the same data. Attention transfer considers attention as an objective that student needs to learn from teacher. It implements attention as the activation map, i.e, the aggregation of feature maps generated by all filters (in the same map) while we assign weight to each filter so that some of the filters can be re-used for new tasks. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_H1gy9nqOh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting approach but not yet clearly demonstrating a significant boost in performance</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgbwsAcYm&amp;noteId=H1gy9nqOh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper237 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper237 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Authors present a new regularisation approach named DELTA (Deep Learning Transfer using feature map with attention). What it does is preserving the outer layer outputs of the target network (in a transfer learning scenario) instead of constraining the weights of the neural network. I am not sure how this approach helps preserve the semantics. Authors state that the distance between source/target networks is characterised by DELTA using their outer layer outputs. This distance is then used in the loss function and through back-propagation incorporates knowledge from the source network. The results demonstrate some marginal improvement in the datasets used when compared with L^2 and L^2-SP.
More importantly I think the paper needs some attention in its format as the concepts are not very clear. It has some elements of novelty but not yet there.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1xLZTlE6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for the review </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgbwsAcYm&amp;noteId=B1xLZTlE6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper237 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper237 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review and constructive comments. We summarize the major concerns of Reviewer 2 as following:

Q1. There needs evidence of (statistically) significant performance boosting

Response: Thanks for your comments. In our experiments, we run DELTA and other baselines for multiple times under various settings, then estimate the mean accuracy with error bars in Tables 1. We further compare the worst-case performance (the lowest accuracy) of DELTA to the best-case performance (the highest accuracy) of rest baselines under each setting. The results suggest that DELTA can always perform better than the baseline algorithms, as its worst-case performance is marginally better than the best-case performance of baselines in our experiments. To address your comments, we will include supplementary experiments on some new neural architectures (inception v3) and datasets (CUB-200-2011[1], Food-101[2]). The new results will be compared and reported in our incoming revision.

Q2. What is the major contributions made in this paper.

Response:  Thanks for your comment. Our key innovation is the concept that we call "unactivated channel re-usage”. Specifically we demonstrate by experiments and case studies that, when we perform transfer learning in CNN, some of the convolution channels are useless in transfer learning. In contrast to the common belief that lower level convolution channels are for common feature extraction, higher level channels are for task-specific feature extraction, we demonstrate that there are channels that are useful (and useless) at all different levels. 

Our technical contribution is to find an approach to identify those “transferable channels” and preserve them through regularization and identify those “untransferable channels” and reuse them, using an attention mechanism with feature map regularization. 

Compared to existing deep transfer learning paradigms reusing the weights of outer layers of source networks, DELTA intends to constrain on the difference of the outputs (e.g., feature maps) rather than the weights between source/target networks. Compared to the knowledge distillation alike solution (if they are adopted for transfer learning), DELTA proposes a novel attention mechanism to make target network stay focused on the important features with high discriminant powers for knowledge transfer. To the best of knowledge, we are the first to regularize the divergence of the feature maps outputted by the outer layers of the source/target networks, with attention mechanisms, for deep transfer learning.

Using image classification and case studies we demonstrate the usefulness of our insights. In addition, we plan to further demonstrate the utility of the proposed methods by adding supplementary experiments on both new architecture(inception v3) and datasets(CUB-200-2011[1], Food-101[2]). Those results will be reported in our new version.

[1] C.Wah,S.Branson,P.Welinder,P.Perona,andS.Belongie. The caltech-ucsd birds-200-2011 dataset. California Institute of Technology, 2011. 6, 7, 8
[2] L. Bossard, M. Guillaumin, and L. Van Gool. Food-101–mining discriminative components with random forests. In ECCV, 2014. 6, 8 </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJxvZDiN3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The paper proposes a normalization procedure improving transfer learning by considering together the output layers of the source and target deep nets. The contribution is rather limited, the experiments are reasonable but not always complete, the use of language satisfactory but can be improved. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgbwsAcYm&amp;noteId=rJxvZDiN3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper237 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper237 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This is a reasonable paper based on a simple intuition. The authors have noticed that some of the state of the art methods (they use Li et al - ICML18 as the main reference) are using only some simple normalization for improving the transfer learning and as such they propose preserving the outer layer output of the target network and aligning it with the one of the source network. On top of that they also propose modeling the difference of feature maps considering an attention mechanism obtain through supervised learning. 

The idea in itself is interesting and valuable. However, I have had some difficulty in understanding precisely how the "behavior" is really regularized. While I understand what is depicted in Figure 1 I'm not completely sure this really means that the network behavior is regularized rather than simply correlating the two outputs. In the evaluation, the authors present in Figure 4 some qualitative examples but I would have expected to see some quantitative evaluation of this. I would have liked to see experiments on some larger datasets that are commonly used in computer vision (e.g., Caltech 256 is rather old even if it has been used in Li et al.). The quantitative results in Table 1 and 2 indicate some slight improvement but I'm not completely convinced that this is really significant in the end. The results in Figure 4 tend to show that with the attention mechanism there is a central bias and most of the results tend to be concentrated on the center of the image (in this case the result might also be correct but the examples presented are not too eloquent). 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJxVC6xNaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for the review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgbwsAcYm&amp;noteId=SJxVC6xNaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper237 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper237 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review and encouraging comments. We summarize and respond Reviewer 3’s concerns as follow.

Q1. I have had some difficulty in understanding precisely how the "behavior" is really regularized. While I understand what is depicted in Figure 1 I'm not completely sure this really means that the network behavior is regularized rather than simply correlating the two outputs

Response: Thanks for your comments. Yes, one intuition of our regularizer for deep transfer learning is to ensure the “behavior” of outer layers of the target network being similar to the source one. In this context,  “behavior” suggests functional preservation where similar inputs will produce similar outputs.  To measure similarity, however, is difficult.  Using feature map is a reasonable choice but such choice has many problems. First of all, feature maps/outer layer outputs are large with noise and redundancy. We have to understand which parts of feature maps (outputs) would help for the classification on the target task (rather than source task). In this way, an attention mechanism has been proposed to re-weight the output of each filter in out layers. Then, with the obtained attention, we characterize the divergence of feature maps/outputs between source/target networks as the attention-weighted squared-Euclidian distance between the feature maps. In this way, the regularizer to “correlate” the output has been designed. We further leverage some optimization paradigms, such as starting point as reference, to accelerate the deep transfer learning with better efficiency and effectiveness. Please refer to our methodologies section for details.


Q2. the authors present in Figure 4 some qualitative examples but I would have expected to see some quantitative evaluation of this. I would have liked to see experiments on some larger datasets that are commonly used in computer vision


Response: Thanks for your comments. Yes, eventually, handling large target dataset is an interesting topic in deep transfer learning research. Actually, we usually assume the target dataset size is relevantly small. As was stated in the first paragraph of the first section, the deep transfer learning studied here is originally motivated by the need of deep learning from small datasets. In such cases, CNNs are not be able to learn generalizable features from the small training sets, while weights of pre-trained networks with rich features learned from large datasets are assumed to help. To address your comments, we will include the results of supplementary experiments on both new architecture (inception v3) and datasets (CUB-200-2011[1], Food-101[2]). The new results will be reported in our incoming revision.

[1] C.Wah,S.Branson,P.Welinder,P.Perona,andS.Belongie. The caltech-ucsd birds-200-2011 dataset. California Institute of Technology, 2011. 6, 7, 8
[2] L. Bossard, M. Guillaumin, and L. Van Gool. Food-101–mining discriminative components with random forests. In 
ECCV, 2014. 6, 8 


Q3. The results in Figure 4 tend to show that with the attention mechanism there is a central bias and most of the results tend to be concentrated on the center of the image

Response: Thanks for your comments for catching the possible location bias. In our experiments we do not find evidence that our attention mechanism has a strong central bias. It pays attention to the parts of feature maps that are with discriminant capacities. The attention illustrated on dog images is just a coincidence. In our revised version, we plan to present more attention results to avoid the impression of central bias. 

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>