<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>UNSUPERVISED MONOCULAR DEPTH ESTIMATION WITH CLEAR BOUNDARIES | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="UNSUPERVISED MONOCULAR DEPTH ESTIMATION WITH CLEAR BOUNDARIES" />
        <meta name="citation_author" content="Yihan Hu" />
        <meta name="citation_author" content="Heng Luo" />
        <meta name="citation_author" content="Yifeng Geng" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1fs4oRqKm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="UNSUPERVISED MONOCULAR DEPTH ESTIMATION WITH CLEAR BOUNDARIES" />
      <meta name="og:description" content="Unsupervised monocular depth estimation has made great progress after deep&#10;  learning is involved. Training with binocular stereo images is considered as a&#10;  good option as the data can be easily..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1fs4oRqKm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>UNSUPERVISED MONOCULAR DEPTH ESTIMATION WITH CLEAR BOUNDARIES</a> <a class="note_content_pdf" href="/pdf?id=H1fs4oRqKm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=y4hu%40eng.ucsd.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="y4hu@eng.ucsd.edu">Yihan Hu</a>, <a href="/profile?email=heng.luo%40horizon.ai" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="heng.luo@horizon.ai">Heng Luo</a>, <a href="/profile?email=yifeng.geng%40horizon.ai" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="yifeng.geng@horizon.ai">Yifeng Geng</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Unsupervised monocular depth estimation has made great progress after deep
learning is involved. Training with binocular stereo images is considered as a
good option as the data can be easily obtained. However, the depth or disparity
prediction results show poor performance for the object boundaries. The main
reason is related to the handling of occlusion areas during the training. In this paper,
we propose a novel method to overcome this issue. Exploiting disparity maps
property, we generate an occlusion mask to block the back-propagation of the occlusion
areas during image warping. We also design new networks with flipped
stereo images to induce the networks to learn occluded boundaries. It shows that
our method achieves clearer boundaries and better evaluation results on KITTI
driving dataset and Virtual KITTI dataset.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">monocular depth estimation, unsupervised learning, image warping</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">This paper propose a mask method which solves the previous blurred results of unsupervised monocular depth estimation caused by occlusion</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rklMtztoam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1fs4oRqKm&amp;noteId=rklMtztoam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper31 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper31 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJxvQ5L537" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting "fix" to Godard et al 2017</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1fs4oRqKm&amp;noteId=BJxvQ5L537"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper31 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper31 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: This work is closely based on Godard et al 2017. The latter work does not handle occlusions well, as they give rise to artefacts and duplicates due to the use of backwards bilinear sampling (which is key to the method however, since it is locally sub-differentiable). Such artefacts, in turn, result in blurred disparity maps once trained on. The present work fixes this, and correspondingly obtains good experimental results, by defining an occlusion mask (section 3.2), and a way to get rid of the blurriness (section 3.3).
The experimental validation of the fix is carried out similarly to Godard et al 2017, with identical losses, on the same KITTI dataset, using the same splits, but also on the virtual KITTI dataset.

The paper is structured alright. The presentation is rather sloppy overall, with agrammatical sentences (noun-gerund-infinitive confusions, mainly) which repeatedly get in the way of understanding, sloppy character spacing and punctuation, references from the main text to the supplementary material. Individual paragraphs are exempt of errors and read smoothly.
The experimental evaluation seems adequate. 
The related work section is uncomfortably similar to Godard et al 2017, from which it borrows the presentation structure, and mostly the order of introduction of individual references.

It almost seems like this is an obligatory fix to Godard et al 2017, and might in the future be considered a standard correction to it. Overall, the scientific contribution appears to be slim.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BklsR2G92X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Relatively incremental novelty while better visualization results.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1fs4oRqKm&amp;noteId=BklsR2G92X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper31 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper31 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary &amp; Pros: 
This paper proposes monocular depth estimation with awareness of occlusion regions and edge blurriness during training. For occlusion region, it uses predicted disparity for estimating a mask map. For buryness, it modifies the network output and loss comparison strategy. The two strategies combined together yield sharper edge predictions. Though results are rather incremental for KITTI, the numbers are more significant with virtual KITTI having dense ground truth due to more pixels are evaluated around edge areas.

Cons: 
1: The work misses several relative works in unsupervised field for handling occlusion and boundary: 

Computing occlusion in unsupervised learning with videos has already been explored by optical flow prediction. 
[1] Meister et.al AAAI 2018: UnFlow: Unsupervised Learning of Optical Flow with a Bidirectional Census Loss.  
[2] Wang et.al CVPR 2018: Occlusion Aware Unsupervised Learning of Optical Flow
In this paper, computing occlusion mask for stereo pair could be a special case of optical flow. I would say the author may directly apply the same strategy to obtain same mask using disparity. 

Handling edge and boundary for blurriness using monocular videos  was also mentioned in 
[1] Yang et.al CVPR 2018: LEGO: Learning Edge with Geometry all at Once by Watching Videos
The author may also apply an non-local constraint for improve the edge quality as well. Maybe should compare a strategy such as dense crf, bilateral or learning edge affinity for clear boundary learning as well. 

2: Other than computing mask, the major novelty of the paper is an engineered modification of  network input and output for computing loss, which I think is not sufficient for ICLR. I would suggest this to be a workshop paper.  It needs more significant contributions not only handle boundaries, maybe consider improvements of absolute estimation results. Experiments over KITTI dataset also shows a rather incremental improvement over Godard's method.



</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HkejUk54nQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Difficult to read; method unclear</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1fs4oRqKm&amp;noteId=HkejUk54nQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper31 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper31 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a method for learning to perform monocular depth estimation, using calibrated pairs of stereo images - without supervision from ground truth depth maps. It is heavily based on Godard et al., CVPR 2017, and focuses on sharp ('clear') reconstruction of occluding boundaries.

My first problem with this paper is the poor quality of English, which makes it difficult to understand. I do not think this submission can be accepted for publication at ICLR, if only because of this.

Technically speaking, if I understand correctly, the key idea is:  At each optimization step, consider the predicted depth map. The points that are predicted to be visible in only one of the two images according to this depth map are 'disabled' during gradient back propagation, so that they do not influence the network optimization. The motivation is that the loss function assumes that each pixel in the first image is also visible in the corresponding second image of the stereo pairs - which is not true along the occluding boundaries. There is another technique, which (I think) consists in flipping the images and their roles, but I do not understand why it should improve learning - besides providing more training data.

The proposed method sounds weird to me:  If I understand correctly, there is no guarantee that learning will converge to anything meaningful.  For example, the optimization could introduce more occlusions than there really is, to disable the influence of more pixels without being penalize?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>