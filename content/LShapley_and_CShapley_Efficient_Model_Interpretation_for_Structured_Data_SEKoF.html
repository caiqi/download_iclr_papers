<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=S1E3Ko09F7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="L-Shapley and C-Shapley: Efficient Model Interpretation for..." />
      <meta name="og:description" content="Instancewise feature scoring is a method for model interpretation, which yields, for each test instance, a vector of importance scores associated with features. Methods based on the Shapley score..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_S1E3Ko09F7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data</a> <a class="note_content_pdf" href="/pdf?id=S1E3Ko09F7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 15 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019l-shapley,    &#10;title={L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=S1E3Ko09F7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=S1E3Ko09F7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Instancewise feature scoring is a method for model interpretation, which yields, for each test instance, a vector of importance scores associated with features. Methods based on the Shapley score have been proposed as a fair way of computing feature attributions, but incur an exponential complexity in the number of features.  This combinatorial explosion arises from the definition of Shapley value and prevents these
methods from being scalable to large data sets and complex models. We focus on settings in which the data have a graph structure, and the contribution of features to the target variable is well-approximated by a graph-structured factorization.  In such settings, we develop two algorithms with linear complexity for instancewise feature importance scoring on black-box models.  We establish the relationship of our methods to the Shapley value and a closely related concept known as the Myerson value from cooperative game theory. We demonstrate on both language and image data that our algorithms compare favorably with other methods using both quantitative metrics and human evaluation.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Model Interpretation, Feature Selection</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We develop two linear-complexity algorithms for model-agnostic model interpretation based on the Shapley value, in the settings where the contribution of features to the target is well-approximated by a graph-structured factorization.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">10 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HygvWsscTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>UPDATED: Four New Experiments Based on the Suggestions of the Reviewers</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1E3Ko09F7&amp;noteId=HygvWsscTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper484 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper484 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have added four experiments in the updated version of the paper based on the suggestions of three reviewers. The first experiment compares human evaluation on top words selected by our algorithms and KernelSHAP, and also compares human evaluation on masked reviews. The second experiment evaluates how the rank produced by our algorithms correlates with the rank of the Shapley value. The third experiment evaluates the sensitivity of our algorithms to the size of neighborhood. The last experiment empirically evaluates the statistical dispersion of sampling-based algorithms. The first experiment has been added to Section 5.3 in the main paper while the rest are added to Appendix C,D and E.

There are also some other minor changes addressing the concern of Reviewer 3 in the length of the paper. We have deferred the detailed description of data sets and models into the appendix. We have shortened Section 4.3 which describes the connection with related work. We also reduced the number of text examples for visualization in the appendix.

We again express our sincere thanks to all the reviewers, who have helped build our manuscript into a better and more complete shape!</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SyxgP3dn2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A new method for computing Shapely values</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1E3Ko09F7&amp;noteId=SyxgP3dn2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper484 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper484 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes two methods for instance-wise feature importance scoring, which is the task of ranking the importance of each feature in a particular example (in contrast to class-wise or overall feature importance).  The approach uses Shapely values, which are a principled way of measuring the contribution of a feature, and have been previously used in feature importance ranking.

The difficulty with Shapely values is they are extremely (exponentially) expensive to compute, and the contribution of this paper is to provide two efficient methods of computing approximate Shapely values when there is a known structure (a graph) relating the features to each other.

The paper first introduces the L(ocal)-Shapely value, which arises by restricting the Shapely value to a neighbourhood of the feature of interest.  The L-Shapely value is still expensive to compute for large neighbourhoods, but can be tractable for small neighbourhoods.

The second approximation is the C(onnected)-Shapely value, which further restricts the L-Shapely computation to only consider connected subgraphs of local neighbourhoods.  The justification for restricting to connected neighbourhoods is given through a connection to the Myerson value, which is somewhat obscure to me, since I am not familiar with the relevant literature.  Nonetheless, it is clear that for the graphs of interest in this paper (chains and lattices) restricting to connected neighbourhoods is a substantial savings.

I have understood the scores presented in Figures 2 and 3 as follows:

For each feature of each example, rank the features according to importance, using the plugin estimate for P(Y|X_S) where needed.
For each "percent of features masked" compute log(P(y_true | x_{S\top features})) - log(P(y_true | x)) using the plugin estimate, and average these values over the dataset.

Based on this understanding the results are quite good.  The approximate Shapely values do a much better job than their competitors of identifying highly relevant features based on this measure.  The qualitative results are also quite compelling, especially on images where C-Shapely tends to select contiguous regions which is intuitively correct behavior.

Comparing the different methods in Figure 4, there is quite some variability in the features selected by using different estimators of Shapley values.  I wonder is there some way to attack the problem of distinguishing when a feature is ranked highly when its (exact) Shapley value is high versus when it is ranked highly as an artifact of the estimator?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hyep4ji56Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1E3Ko09F7&amp;noteId=Hyep4ji56Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper484 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper484 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the detailed and encouraging comments! Based on the suggestions from the reviewer, we have included an experiment in the updated version that measures the correlation between L-Shapley, C-Shapley and the Shapley value. 

“Understanding of the evaluation metric”:

The evaluation metric we use is the following: 
log(P(y_pred | x)) - log(P(y_pred | x_{top features MASKED})). The reviewer's understanding is in general correct except that we use the predicted label instead of the true label in the data set, because we hope to find key features for why the model makes its own decision.  

“I wonder is there some way to attack the problem of distinguishing when a feature is ranked highly when its (exact) Shapley value is high versus when it is ranked highly as an artifact of the estimator?”

We have added a new experiment in the updated version to address the problem of how the rank of features correlates with the rank produced by the true Shapley value. We sample a subset of test data from Yahoo! Answers with 9-12 words, so that the underlying Shapley scores can be accurately computed. We employ two common metrics, Kendall's Tau and Spearman's Rho to measure the similarity (correlation) between two ranks. We have observed a high rank correlation between our algorithms and the Shapley value. See the figure in the link below, and also Appendix C for more details: 
<a href="https://drive.google.com/open?id=1oWsWyA4IkDIbaOjwOOwMAYJzu6kUuQSa" target="_blank" rel="nofollow">https://drive.google.com/open?id=1oWsWyA4IkDIbaOjwOOwMAYJzu6kUuQSa</a></span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SkeChVXt27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Novel methods for Shapley value estimation seem theoretically sound, could benefit from slightly more extensive evaluation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1E3Ko09F7&amp;noteId=SkeChVXt27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper484 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper484 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper provides new methods for estimating Shapley values for feature importance that include notions of locality and connectedness. The methods proposed here could be very useful for model explainability purposes, specifically in the model-agnostic case.  The results seem promising, and it seems like a reasonable and theoretically sound methodology.  In addition to the theoretical properties of the proposed algorithms, they do show a few quantitative and qualitative improvements over other black-box methods.  They might strengthen their paper with a more thorough quantitative evaluation.

I think the KernelSHAP paper you compare against (Lundberg &amp; Lee 2017) does more quantitative evaluation than what’s presented here, including human judgement comparisons.  Is there a way to compare against KernelSHAP using the same evaluation methods from the original paper?

Also, you mention throughout the paper that the L-shapley and C-shapley methods can easily complement other sampling/regression-based methods.  It's a little ambiguous to me whether this was actually something you tried in your experiments or not.  Can you please clarify?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyloPoi96m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1E3Ko09F7&amp;noteId=SyloPoi96m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper484 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper484 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the detailed suggestions and encouraging comments! We have included an experiment with human evaluation in the updated version. Below we respond to Reviewer 1’s questions in details. 

“Is there a way to compare against KernelSHAP using the same (human) evaluation methods from the original paper?”

We agree with the reviewer that human evaluation is important in this area, and we have added a new experiment with human evaluation in the updated version. 

In KernelSHAP paper, the authors designed experiments to argue for the use of Shapley value instead of LIME, which shows Shapley value is more consistent with human intuition on a data set with only a few number of features. Both KernelSHAP and our algorithms are ways of approximating Shapley value when there is a large number of features, under which case the exact same experiment is difficult to replicate. 

We have designed two experiments by ourselves involving human evaluation for our methods and KernelSHAP on IMDB in the updated version. We assume that the key words contain an attitude toward a movie and can be used to infer the sentiment of a review. In the first experiment, we ask humans to infer the sentiment of a review within a range of -2 to 2, given the key words selected by different model interpretation approaches. Second, we also ask humans to infer the sentiment of a review with top words being masked, where words are masked until the predicted class gets a probability score of 0.1. In both experiments, we evaluate the consistency with truth, the agreement between humans on a single review by standard deviation, and the confidence of their decision via the absolute value of the score. We observe L-Shapley and C-Shapley take the lead respectively in two experiments. See the table and an example interface in the links below, and also Section 5.3 for more details: 
<a href="https://drive.google.com/open?id=1aHZPP0ZAdyODgTEFLRrQAKyS4uJ8h-XS" target="_blank" rel="nofollow">https://drive.google.com/open?id=1aHZPP0ZAdyODgTEFLRrQAKyS4uJ8h-XS</a>
https://drive.google.com/file/d/1_HOR28DGlKqEQVplGahv47o2xPe5lT5e/view?usp=sharing

“It's a little ambiguous to me whether you tried to complement other sampling/regression-based methods in your experiments or not. Can you please clarify?”

In the experiments, we didn't combine our approach with sampling based methods as the number of model evaluations is already small enough in the setting (linear in the number of features). </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HyenmOMRoX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice addition to Shapley literature, but could be strengthened</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1E3Ko09F7&amp;noteId=HyenmOMRoX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper484 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper484 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes two approximations to the Shapley value used for generating feature scores for interpretability. Both exploit a graph structure over the features by considering only subsets of neighborhoods of features (rather than all subsets). The authors give some approximation guarantees under certain Markovian assumptions on the graph. The paper concludes with experiments on text and images.

The paper is generally well written, albeit somewhat lengthy and at times repetitive (I would also swap 2.1 and 2.2 for better early motivation). The problem is important, and exploiting graphical structure is only natural. The authors might benefit from relating to other fields where similar problems are solved (e.g., inference in graphical models). The approximation guarantees are nice, but the assumptions may be too strict. The experimental evaluation seems valid but could be easily strengthened (see comments).

Comments:

1. The coefficients in Eq. (6) could be better explained.

2. The theorems seem sound, but the Markovian assumption is rather strict, as it requires that a feature i has an S that "separates" over *all* x (in expectation). This goes against the original motivation that different examples are likely to have different explanations. When would this hold in practice?

3. While considering chains for text is valid, the authors should consider exploring other graph structures (e.g., parsing trees).

4. For Eqs. (8) and (9), I could not find the definition of Y. Is this also a random variable representing examples?

5. The authors postulate that sampling-based methods are susceptible to high variance. Showing this empirically would have strengthened their claim.

6. Can the authors empirically quantify Eqs. (8) and (9)? This might shed light as to how realistic the assumptions are.

7. In the experiments, it would have been nice to see how performance and runtime vary with increased neighborhood sizes. This would have quantified the importance of neighborhood size and robustness to hyper-parameters.

8. For the image experiments, since C-Shapley considers connected subsets, it is perhaps not surprising that Fig. 4 shows clusters for this method (and not others). Why did the authors not use superpixels as features? This would have also let them compare to LIME and L-Shapley.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1xadhicT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 3 (Details)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1E3Ko09F7&amp;noteId=B1xadhicT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper484 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper484 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1. “Coefficients in Eq. (6)”

The coefficients are derived from Myerson value, which can be interpreted as the Shapley value for the coalition game with a graph structure. The details can be found in the proof of Theorem 2. In particular, Equation (22) in the Appendix provides the concrete procedure of derivation.

2. "The Markovian assumption is rather strict." 

We thank the reviewer for addressing this point. We agree with the reviewer that Markovian assumption introduces bias in explanation, which aims for a better bias-variance trade-off when approximating Shapley values on structured data. Theorem 1 and Theorem 2 quantify the introduced bias under the setting when the Markovian assumption is approximately true. We also show on real data such an approximation achieves a better bias-variance trade-off empirically when the number of model evaluations is linear in the number of features. 

3. "Use other graph structures like parse trees on language." 

The reviewer made a very bright proposal. As the current paper focuses on the study of the generic setting where data with graph structure, we only use the simplest possible model on language to demonstrate the validity of the proposed algorithms. But the proposed idea can be a promising future direction. The authors have been thinking along the same direction for a while. One question one could ask is whether there exists a better solution concept in coalitional game theory under the setting of a parse tree. Related literature includes [1] and [2] if the reviewer is interested to think about this further.

4. "Y in Eqs. (8) and (9)." 

We assume the model has the form P_m(Y|X). Y is the response variable from the model.

5. “The authors postulate that sampling-based methods are susceptible to high variance. Show this empirically.”

We have added an experiment in the updated version addressing the statistical dispersion of estimates of the Shapley value produced by sampling-based methods. Two commonly used nonparametric metrics are introduced to measure the statistical dispersion between different runs of a common sampling-based method, as the number of model evaluations is varied. Figure in the link below shows the variability of SampleShapley and KernelSHAP as a function of the number of model evaluations:
<a href="https://drive.google.com/file/d/1yUvJ_Jqn2Bg16U-poEtMcTGfWifIcQ3_/view?usp=sharing" target="_blank" rel="nofollow">https://drive.google.com/file/d/1yUvJ_Jqn2Bg16U-poEtMcTGfWifIcQ3_/view?usp=sharing</a>
See also Appendix E for details.

6. "Empirically quantify Eqs. (8) and (9)." 

While we agree with the reviewer that a good empirical quantification of quantities in Eqs. (8) and (9) can verify the assumptions in practice, it is rather difficult to get a reliable estimate of the conditional mutual information (or similar quantities) in the high dimensional regime. We have added one experiment in the updated version to validate the correlation between our algorithms and the Shapley value directly, which partially reflects the conclusion of our theorem. See the figure in the link below and Appendix C for details: https://drive.google.com/file/d/1oWsWyA4IkDIbaOjwOOwMAYJzu6kUuQSa/view?usp=sharing

The better performance on real data in terms of log-odds ratio decay when top features are masked may also be viewed as a partial empirical evidence on the fact that the introduced bias is not as big as the reduced variance.

7. “it would have been nice to see how performance and runtime vary with increased neighborhood sizes” 

We have included a section for sensitivity analysis of our algorithms in the updated version. We study how correlation between the proposed algorithms and the Shapley value vary with the radius of neighborhood, the only hyper-parameter in our algorithms. A plot of model evaluations against the radius of neighborhood is also included. See the figures in the link below, and also Appendix D for details:
https://drive.google.com/open?id=1perbCh7oH95j3uDp6jNEM0vPvUvcUkZ8
https://drive.google.com/file/d/1f5yBIwxd85tyxQKB5gBlRtBX4pRe0noL/view?usp=sharing

8. "Not use superpixels as features."

We agree with the reviewer that using superpixels may lead to better visualization results. However, this leads to a performance decay in terms of the change in log-odds ratio when a fixed number of pixels are masked. The same issue has been addressed in [3]. For fairness of comparison, we use the raw pixels as features for all methods.

[1] Winter, Eyal. "A value for cooperative games with levels structure of cooperation." International Journal of Game Theory 18.2 (1989): 227-240.
[2] Faigle, Ulrich, and Walter Kern. "The Shapley value for cooperative games under precedence constraints." International Journal of Game Theory 21.3 (1992): 249-266.
[3] Lundberg, Scott M., and Su-In Lee. "A unified approach to interpreting model predictions." Advances in Neural Information Processing Systems. 2017.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkeAS2s5pm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 3 (Summary)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1E3Ko09F7&amp;noteId=rkeAS2s5pm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper484 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper484 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the detailed comments and encouraging title! We have included three experiments in the updated version to address Point 5, 6,and 7 of the reviewer’s comments,  and also omit unnecessary details in the original paper. We will respond the reviewer's comments concretely below. 

“The paper is generally well written, somewhat lengthy and at times repetitive (I would also swap 2.1 and 2.2 for better early motivation)”
Based on the reviewer’s request, we have shortened the paper by deleting unnecessary repetitions and details in Section 4.3 and the experiment section, and putting some of them to appendix. For example, the description of datasets is deferred to the appendix.  As a replacement, we have included a new experiment with human evaluation. On the other hand, we still keep the order of 2.1 and 2.2. The main reason is that it seems more natural to explain how importance of a feature subset is quantified first (section 2.1) before we motivate the Shapley value, which incorporating interaction based on this quantification (section 2.2).</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BkgYeky15Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Very weak baselines</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1E3Ko09F7&amp;noteId=BkgYeky15Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Oct 2018</span><span class="item">ICLR 2019 Conference Paper484 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I just wanted emphasize that the baselines used in this paper are very weak.  To the best of my knowledge, no one has claimed that any of the provided baselines (LIME, KernelSHAP, or SampleSHAP) are remotely close to SOTA for, or even capable of, interpreting neural networks in the manner demonstrated here, as the original papers focused on simpler models, such as SVM, or image models with superpixel preprocessing. 

The authors (partially) address this in the results section: 

"We emphasize that our focus is model-agnostic interpretation, and we omit the comparison with interpretation methods requiring additional assumptions or specific to a certain class models, like Integrated Gradients (Sundararajan et al., 2017), DeepLIFT (Shrikumar et al., 2017), LRP (Bach et al., 2015) and LSTM-specific methods (Karpathy et al., 2015; Strobelt et al., 2018; Murdoch &amp; Szlam, 2017)."

Even if limited to model-agnostic interpretation, a very simple, strong baseline is leave one out - black out a variable and see how much the prediction changes - which is well established in both NLP (<a href="https://arxiv.org/pdf/1612.08220.pdf)" target="_blank" rel="nofollow">https://arxiv.org/pdf/1612.08220.pdf)</a> and vision (https://arxiv.org/abs/1311.2901). This method would perform significantly better than the provided baselines (the baseline examples in the bottom two rows of Figure 4 are the worst I've seen in any paper).

I'd also argue that gradient-based methods should be compared against, such as gradient times input or integrated gradients. While not truly model-agnostic, they only require the model to be differentiable, thus apply to all neural nets, and all models considered in this paper.

Moreover, even if not directly comparable, I'd argue that at least some model-specific techniques should be included as well, in order to see how much is lost by moving from a custom method to a model-agnostic one.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rylA6HRv57" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response: Baselines not weak; Model-specific comparison not necessary in paper, but available in the reply</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1E3Ko09F7&amp;noteId=rylA6HRv57"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper484 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Oct 2018</span><span class="item">ICLR 2019 Conference Paper484 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We first thank the reader for reading and greatly appreciate his/her time for writing such detailed reviews:)

In summary, the reader proposes two suggestions: 
1. The current baselines, including KernelSHAP and LIME, are weak, compared to methods like 'leave-one-out'. 
2. The authors should compare with model-specific techniques, including ‘integrated gradients’. 

The short reply is:
1. Leave-one-out is not as strong as KernelSHAP, both theoretically and experimentally.
2. We do not compare with model-specific approaches in the paper as we focus on model agnostic interpretation. See the anonymous link at the end for a comparison made specifically for the reader.

Below are the concrete details:

We have different opinions on the first point (to a certain extent). In particular, KernelSHAP is stronger than 'leave-one-out':
a. Based on the source code of KernelSHAP (<a href="https://github.com/slundberg/shap/blob/master/shap/explainers/kernel.py)," target="_blank" rel="nofollow">https://github.com/slundberg/shap/blob/master/shap/explainers/kernel.py),</a> KernelSHAP considers 'masking each word' when computing importance scores, as long as the number of samples is super-linear in the number of features. 
b. Shapley value further incorporates the interaction between features when the number of samples is larger than d (the number of features), which is not the case for leave-one-out.
c. Experimentally, Leave-one-out is not as good as KernelSHAP when more than one features are masked in terms of the decay in log likelihood.

Secondly, the focus of this work is on model-agnostic interpretation, and thus we did not include comparison with model-specific methods in the paper. Model-specific methods can have superior performance in some cases while suffer a performance decay in other cases: For example, Integrated Gradients can have comparable performance to L-Shapley on CNNs, but perform not as well as other methods on LSTM with comparable complexity. Comparing our methods with all model-specific methods for various models will be an unnecessary use of time and also distract readers from the focus of the paper:  efficient approximations of Shapley value, as a model-agnostic method for model interpretation. Being MODEL-AGNOSTIC can be important in some practical settings where models are not specified or multiple models are used. 

Nevertheless, it does no harm to compare one or two model-specific methods in the reply as suggested by the reader. The reader proposes to compare our methods with Gradient X Input, DeepLIFT and Integrated Gradients. Given the inferior performance of Gradient X Input and the complexity of implementing DeepLIFT, we only compare with Integrated Gradient on NLP tasks, where the time complexity of integrated gradients is controlled to be (approximately) the same as L-Shapley for each sample: 
https://drive.google.com/file/d/1UYp2lKDXt-ORgL5vKsU35K5SMa-GQSrs/view?usp=sharing</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>