<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Don't Settle for Average, Go for the Max: Fuzzy Sets and Max-Pooled Word Vectors | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Don't Settle for Average, Go for the Max: Fuzzy Sets and Max-Pooled Word Vectors" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SkxXg2C5FX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Don't Settle for Average, Go for the Max: Fuzzy Sets and Max-Pooled..." />
      <meta name="og:description" content="Recent literature suggests that averaged word vectors followed by simple post-processing outperform many deep learning methods on semantic textual similarity tasks. Furthermore, when averaged word..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SkxXg2C5FX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Don't Settle for Average, Go for the Max: Fuzzy Sets and Max-Pooled Word Vectors</a> <a class="note_content_pdf" href="/pdf?id=SkxXg2C5FX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019don't,    &#10;title={Don't Settle for Average, Go for the Max: Fuzzy Sets and Max-Pooled Word Vectors},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SkxXg2C5FX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Recent literature suggests that averaged word vectors followed by simple post-processing outperform many deep learning methods on semantic textual similarity tasks. Furthermore, when averaged word vectors are trained supervised on large corpora of paraphrases, they achieve state-of-the-art results on standard STS benchmarks. Inspired by these revelations, we push the limits of word embeddings even further. We propose a novel fuzzy bag-of-word (FBoW) representation for text that contains all the words in the vocabulary simultaneously but with different degrees of membership, which are derived from similarities between word vectors. We show that max-pooled word vectors are only a special case of fuzzy BoW and should be compared via fuzzy Jaccard index rather than cosine similarity. Finally, we propose DynaMax, a completely unsupervised and non-parametric similarity measure that dynamically extracts and max-pools good features depending on the sentence pair. This method is both efficient and easy to implement, yet outperforms current baselines on STS tasks by a large margin when word vectors are trained unsupervised. When the word vectors are trained supervised to directly optimise cosine similarity, our measure is still comparable in performance despite being unrelated to the original objective.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">word vectors, sentence representations, distributed representations, fuzzy sets, bag-of-words, unsupervised learning, word vector compositionality, max-pooling, Jaccard index</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Max-pooled word vectors with fuzzy Jaccard set similarity are an extremely competitive baseline for semantic similarity; we propose a simple dynamic variant that performs even better.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">10 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BygtQLlm6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Please stay tuned</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkxXg2C5FX&amp;noteId=BygtQLlm6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1058 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1058 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Reviewers and Readers,

We were absolutely thrilled that our work received such a positive assessment. 
We would like to apologise for the delay in our replies; we are in fact working very hard to run additional analyses to quantitatively support our replies to each Reviewer.

These include :
- significance tests (Reviewer 3)

- comparisons of different choices for the universe matrix U and a short story on how we arrived at each of them (Reviewer 2)

- certain experiments to support our response to Review 1

We expect to post very detailed replies by Monday at the latest. We hope you all have a nice weekend and please stay tuned.


Best wishes,


ICLR 2019 Conference Paper1058 Authors
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rylBW7K1pX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Very polished paper, simple but effective.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkxXg2C5FX&amp;noteId=rylBW7K1pX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1058 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1058 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This is one of the best papers I reviewed so far this year (ICLR, NIPS, ICML, AISTATS), in terms of both the writing and technical novelty.

Writing: the author provided sufficient context and did comprehensive literature survey, which made the paper easily accessible to a larger audience. And the flow of this paper was very smooth and I personally enjoyed reading it.

Novelty: I wouldn't say this paper proposed a groundbreaking innovation, however, compared to many other submissions that are more obscure rather than inspiring to the readers, this paper presented a very natural extension to something practitioners were already very familiar with: taking an average of word vectors for a sentence and measure by cosine similarity.  Both max pooling and Jaccard distance are not something new, but the author did a great job presenting the idea and proved it's effectiveness through extensive experiments. (disclaimer: I didn't follow the sentence embedding literature recently, and I would count on other reviewers to fact check the claimed novelty of this paper by the authors)

Simplicity: besides the novelty mentioned above, what I enjoyed more about this paper is it's simplicity. Not just because it's easy to understand, but also it's easy to be reproduced by practitioners.

Quibbles: the authors didn't provide error bar / confidence interval to the results presented in experiment session. I'd like to know whether the difference between baselines and proposed methods were significant or not.

Miscellaneous: I have to say the authors provided a very eye-catching name to this paper as well, and the content of the paper didn't disappoint me neither. Well done :)
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rke4o1cOTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkxXg2C5FX&amp;noteId=rke4o1cOTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1058 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1058 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Reviewer,

We would like to thank you for such a positive assessment of our work. 
We were especially thrilled the Reviewer found our paper to be among the best they reviewed this year.

Regarding the significance analysis, unfortunately, the SentEval toolkit [1] we're using does not support this functionality.
Moreover most works known to us, including some of the most prominent works published at ICLR, do not conduct significance analysis for STS benchmarks ([2], [3], [4], [5]). Admittedly, some other works apply the Fisher's z test, which we believe is not appropriate in this setting. More appropriately, some apply the William's t test [6] or Steiger's z test [7] for correlated correlations. To the best of our knowledge, these tests require that data comes from a normal distribution, which is not case for STS. Although we have done a similar analysis using Steiger's z, we have to refrain from reporting (potentially) statistically invalid results and are looking to obtain further evidence that these tests can in fact be applied here. We are also looking into alternative (non-parametric) methods and will let the Reviewer know when our analysis is complete.

[1] Alexis Conneau and Douwe Kiela (2018). SentEval: An Evaluation Toolkit for Universal Sentence Representations. <a href="http://arxiv.org/abs/1803.05449" target="_blank" rel="nofollow">http://arxiv.org/abs/1803.05449</a>
[2] John Wieting, Mohit Bansal, Kevin Gimpel and Karen Livescu. ICLR 2016.
[3] Sanjeev Arora, Yingyu Liang and Tengyu Ma. A Simple but Tough-to-Beat Baseline for Sentence Embeddings. ICLR 2017.
[4] Jiaqi Mu and Pramod Viswanath. All-but-the-Top: Simple and Effective Postprocessing for Word Representations. ICLR 2018.
[5] Sandeep Subramanian Adam Trischler, Yoshua Bengio and Christopher J Pal. Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning. ICLR 2018.
[6] Williams, E. J. (1959). The comparison of regression variables. Journal of the Royal Statistical Society, Series B, 21, 396-399.
[7] Steiger, J. H. (1980). Tests for comparing elements of a correlation matrix. Psychological Bulletin, 87(2), 245-251.


Again, thank you very much and please do not hesitate to contact for with any further queries/clarifications.


Best wishes,


ICLR 2019 Conference Paper1058 Authors
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1lC2zgqhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting and simple idea</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkxXg2C5FX&amp;noteId=S1lC2zgqhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1058 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1058 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This submission presents a simple model for sentence representation based on max-pooling of word vectors. The model is motivated by fuzzy-set theory, providing both a funded pooling scheme and a similarity score between documents. The proposed approach is evaluated on sentence similarity tasks (STS) and achieves very strong performance, comparable to state-of-the-art, computationally demanding methods.

Pros:
+ The problem tackled by this paper is interesting and well motivated. Fast, efficient and non-parametric sentence similarity has tons of important applications (search, indexing, corpus mining).
+ The proposed solution is elegant and very simple to implement. 
+ When compared to standard sentence representation models, the proposed approach has very good performance, while being very efficient. It only requires a matrix vector product and a dimension-wise max.
+ The paper is very well written and flows nicely.
+ Empirical results show significant differences between different word vectors. The simplicity of this approach makes it a good test bed for research on word vectors.

Cons:
- Nothing much, really. 
- Eq. (3) is awkward, as it is a sequence of equalities, which has to be avoided. Moreover, if U is the identity, I don't think that the reader really need this Eq...

I have several questions and remarks that, if answered would make the quality of the presentation better:

* In infersent, the authors reported the performance of a randomly-initialized and max-pooled bi-lstm with fasttext vectors as the input lookup. This can be seen as an extreme case of the presented formalism, where the linear operator U is replaced by a complicated non linear function that is implemented by the random LSTM. Drawing that link, and maybe including this baseline in the results would be good.

* Related to this previous question, several choices for U are discussed in the paper. However, only two are compared in the experiments. It would be interesting to have an experimental comparison of:
- taking U = W
- taking U = I
- taking U as the principal directions of W
- taking U as a random matrix, and comparing performance for different output dimensions. 

Overall, this paper is a very strong baseline paper. The presented model is elegant and efficient. I rate it as an 8 and await other reviews and the author's response.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJx3gDVvpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkxXg2C5FX&amp;noteId=HJx3gDVvpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1058 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1058 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Reviewer,

We would like to thank you for such a kind assessment of our work and so many positive comments.
The Reviewer has asked some fascinating questions and we are jumping straight to them.


* On InferSent

Absolutely, in principle the linear operator U can be replaced by any non-linear function, such as a (deep) neural network. But because InferSent is a Bi-LSTM, the membership vector for a word w_t would depend on the membership vectors of words w_(t-1) and w_(t+1). By contrast, in our fuzzy *bag*-of-words model all the memberships vectors are computed separately and independently of each other.
We genuinely feel these "fuzzy sequences" have a great research potential but have to leave them to future work.

Randomly initialised InferSent uses GloVe vectors for its embeddings layer, followed by a randomly initialised Bi-LSTM. However, we see from [1] (Table 4) that its performance on STS14 is only 0.39, when averaged GloVe vectors already attain 0.54, while avg. fastText and word2vec both score above 0.63.
In other words, random InferSent is very unlikely to be a good baseline for unsupervised semantic textual similarity. Of course, the trained InferSent is a very strong model and we already compare against it in Table 1.

[1] Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. 
Supervised Learning of Universal Sentence Representations from Natural Language Inference Data. EMNLP 2017, pp. 670â€“680


* Different choices for U (the universe matrix)

We were very excited the Reviewer suggested the random matrix. We didn't mention this in the paper but in fact we tried all of the following universes:

-------------------------------------------------------------------------------------
GloVe|                                  STS12   STS13   STS14   STS15   STS16	
--------------------------------------------------------------------------------------
Avg.                                        52.1      49.6      54.6       56.1      51.4

W (top 100K)                         58.6     48.2       62.8       69.3      69.4
DynaMax                               58.2     53.9       65.1       70.9      71.1
Random 300x300	                57.0	     49.5       64.9       70.4     70.8
Identity 300                           57.7     51.4       65.9       70.7      70.4
SVD basis                               58.1     51.8       66.1       70.7      71.0
SVD (top 200 vec)                 57.0     49.5       64.6       69.4      69.9


For GloVe vectors the methods are basically the same but DynaMax gets good improvement over the max-pooled word vectors (identity matrix) with most other word vectors (Figures 1 &amp; 2).

We chose to focus on DynaMax and max-pooled vectors exclusively because only these 2 universes are non-parametric and deterministic.
We ourselves feel that DynaMax is probably the strongest and safest choice overall for any kind of word vectors.
However, it is sensible to start with just max-pooled word vectors because they avoid matrix multiplication altogether.
We will be linking our code repository after the anonymity period and hope the community discovers universes that we haven't so far.

Also, we agree that sequence of equalities in Eq. 3 is awkward, this is purely to emphasise the origins of max-pooled word vectors. We will consider how to alter this equation while keeping this message.

Again, we would like to thank the Reviewer for such a positive assessment and great questions.
Please do not hesitate to contact us for any further queries/clarifications.


Best wishes,


ICLR 2019 Conference Paper1058 Authors
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SyeN7H9S3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper presents (a) a fuzzy bag of word representation and (b) DynaMax, a similarity measure that max pools salient features from sentence pairs.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkxXg2C5FX&amp;noteId=SyeN7H9S3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1058 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1058 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Strengths:
- Good coverage of related work
- Clear presentation of the methods
- Evaluation using established SemEval datasets


Weaknesses:
1. It is not entirely clear what is the connection between fuzzy bag of words and DynaMax. In principle DynaMax can work with other methods too. This point should be elaborated a bit more.
2. It is claimed that the this paper shows that max-pooled word vectors are a special case of fuzzy bag of words. This is not correct. The paper shows how to "convert" one to the other. 
3. It is also claimed that point 2 above implies that max-pooled vectors should be compared with the fuzzy Jaccard index instead of cosine similarity. There is no proof or substantial justification to support this. 
4. Some relevant work that is missing:
- De Boom, C., Van Canneyt, S., Demeester, T., Dhoedt, B.: Representation learning for very
short texts using weighted word embedding aggregation. Pattern Recognition Letters 80,
150â€“156 (2016)
- Kenter, T., De Rijke, M.: Short text similarity with word embeddings. In: International on
Conference on Information and Knowledge Management. pp. 1411â€“1420. ACM (2015)</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByeCpELDTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkxXg2C5FX&amp;noteId=ByeCpELDTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1058 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1058 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Reviewer,

We would like to thank you for your assessment of our paper and positive comments regarding presentation and coverage of related work.
The Reviewer additionally had some concerns which we would like to address.


1.
As we explain in Section 2.2.3, the universe of DynaMax contains only the word embeddings from the 2 sentences being compared. If x1, x2,...xk and y1, y2,...,yl are word embeddings for sentences 1 and 2 respectively, then U = [x1; x2;...xk; y1; y2;...;yl]. This construction is also shown in Algorithm 1 (Lines 5-7).

We are not quite sure what the Reviewer meant by "In principle DynaMax can work with other methods too".

The DynaMax-Jaccard (DMJ) similarity has 3 components. The "dynamic" universe U (Section 2.2.3), the max-pooling operation (Eq. 2), and finally the fuzzy Jaccard index (Section 2.2.1). All 3 components are reflected and implemented in Algorithm 1.
Below we show the change in performance when max-pooling is replaced by average and when fuzzy Jaccard is replaced by cosine similarity.

GloVe                                STS12   STS13   STS14   STS15   STS16	

DynaMax Jaccard            58.2      53.9      65.1       70.9      71.1
DynaMax Cosine             58.2      53.6      63.2       67.2      67.4

DynaAvg. Jaccard            43.5      37.0      38.8       45.3      39.4
DynaAvg. Cosine	           40.0      39.1      38.3       39.7      31.2

We see that all 3 components in DynaMax-Jaccard are very important. When we replaced Jaccard with cosine, the performance dropped. When we replaced max with average it fell even further. Unfortunately, there is only a limited number of combinations we could report in the paper.


2. &gt; "... max-pooled word vectors are a special case of fuzzy bag of words. This is not correct."

Please allow us to elaborate why max-pooled vectors are in fact a special case of fuzzy BoW:

We deliberately left the matrix U unspecified in the definition of FBoW (Section 2.2, Eq. 1 &amp; 2).
When U=W, then U represents "concrete" words. We said this was the most intuitive (but not the only) choice.

In some cases, we no longer have concrete words but instead the words are "abstract". We acknowledge this in Section 2.2.2. In case of max-pooled vectors, U is the identity matrix I. However, we can always assign text labels to the rows of I, for example 'dim1', 'dim2', ..., 'dim300'.
These "words" represent abstract concepts learned by a neural network in its representations. In fact, there has been some work to figure out what these dimensions could contain (e.g. [1])
More generally, for any vector we can always generate a text label for that vector, and take that as a word.
The fuzzy BoW is then fuzzy with respect to these abstract words (concepts).

We will clarify this in the updated manuscript.

[1] Yulia Tsvetkov, Manaal Faruqui, and Chris Dyer. Correlation-based Intrinsic Evaluation of Word Vector Representations.
Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP, pp. 111â€“115


3. "... max-pooled vectors should be compared with the fuzzy Jaccard index instead of cosine similarity. There is no proof or substantial justification to support this. "

We appreciate there is no proof but we respectfully disagree there is no substantial justification. As discussed above, max-pooled vectors are a special case of fuzzy BoW and so the fuzzy Jaccard index is fully justified for this representation. Empirically fuzzy Jaccard outperforms cosine similarity on most tasks (Figure 1).


4. Thanks for bringing these works to our attention. We're happy to cite them where appropriate.


Overall, we showed that word embeddings by themselves (without any weights, tricks or supervision) are still a formidable baseline for semantic textual similarity. We reported up to 20-point increase on standard benchmark datasets. We also tried to rekindle the interest in fuzzy set theory, which is quite underrepresented in the mainstream ML research.

In addition to our replies, we hope the Reviewer can take these contributions into account and perhaps reconsider their score.

Again, thank you very much for your assessment and please do not hesitate to contact us with any further queries.


Best wishes,



ICLR 2019 Conference Paper1058 Authors</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rygwTXlCiX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Small typo in Algorithm 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkxXg2C5FX&amp;noteId=rygwTXlCiX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1058 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">24 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1058 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Readers,

We have spotted a small typo in the manuscript. 
In Algorithm 1, Line 3 the vector with all zeros z should have dimensions 1 x (k+l) and not 1 x d.
The dimension of the zero vector has to match the dimension of other vectors in the max-pooling operation, all of which are 1 x (k+l) after the projection onto U.
Alternatively, we can keep zero vector to be 1 x d but then we need to project it as well in Lines 8 and 9, i.e. compute zU^T.
However, the latter would be a useless computation, so we prefer the first option.

We apologise for any inconveniences this typo might have caused and will fix it in the next version of the manuscript.


Best wishes,


ICLR 2019 Conference Paper1058 Authors</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1e6Hyfos7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>About the matrix U</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkxXg2C5FX&amp;noteId=r1e6Hyfos7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">22 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1058 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">This work is very good, I am very interested. Fuzzy set theory is a very effective tool, which can explain and describe the uncertainty of data. The matrix U in this paper is very important, indicating the degree of membership of the elements in the fuzzy set, so the element value in U should be [0,1]. However, when U=W (W represents the word representation), the elements are a real number, which can be positive or negative. If U is unconstrained, the overall interpretability will be discounted.
In addition, in the experimental part, if the author can give an example of U, it will be more perfect.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJgsHga3i7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good question, thanks!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkxXg2C5FX&amp;noteId=SJgsHga3i7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1058 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">23 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1058 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your kind feedback and interest in our paper.
We also believe fuzzy set theory is a very useful framework and we were excited it worked well in this setting.

Onto your question, we briefly discuss the choice of R as opposed to [0, 1] in Sections 2.1 and 2.2.4 but will extend these sections in a revised version of the manuscript.


The universe matrix U is a (K x 300) matrix, i.e. the universe contains K entities and each row of U is an embedding of a single entity. When U=W, the entities are words and the rows of U are simply the word embeddings.

Now we want to convert a singleton {w} into a fuzzy set. We compute the membership values:

mu = [sim(w, u1), sim(w, u2), ..., sim(w, uK)]

We see that the membership values actually come from the similarity function sim(w, u) and not from the matrix U directly, so what's really important is whether values of sim(w, u) are in [0, 1] or R.

In our work, sim(w, u) is the dot product w * u, which does indeed take any real value. Below we discuss why dot product is a reasonable choice.

1. 
Intuitively, it's all the same up to the scale. We can easily map any real number into (0, 1) using, e.g. the 
logistic function s(x) = 1/(1+e^-x) and vice versa. So it's really not that important whether the values are in (0, 1)
or in R. In fact, there are many various extensions of fuzzy set theory that work with more general ranges than [0,1]. 
For a quick reference: <a href="https://en.wikipedia.org/wiki/Fuzzy_set#Extensions" target="_blank" rel="nofollow">https://en.wikipedia.org/wiki/Fuzzy_set#Extensions</a>

2.
The membership function for multisets (bags) takes values in N (i.e. the non-negative integers), so these values are already outside [0, 1]. We see that the standard [0, 1]-fuzzy sets are incompatible with multisets, that's why we constructed the most general sensible thing. Sets, bags, and [0,1]-fuzzy sets are all just a special case of fuzzy BoW.
Interestingly, since we always max-pool with a zero vector, fuzzy BoW will not contain any negative membership values.
This was not our intention, just a by-product of the model. As discussed in 1., negative values are fine. They just mean
the element is "really" not in the set.


3.
Still, why did we choose dot product and not, say, sim(w, u) = max(cos(w, u), 0)?
We know that cosine similarity is in [-1, 1], so sim(w, u) will be in [0, 1].
One reason is that word vectors are usually trained to maximise some kind of dot product in their objectives.
A more practical reason is simply because dot product works much better.

Turns out, if we normalise word embeddings we will get the same fuzzy BoW as if we used max(cos(w, u), 0)
(simply because dot product is the same as cosine similarity for normalised vectors).

Below we give the results for GloVe vectors

			         STS12	  STS13	  STS14	  STS15	  STS16	

Avg.		         52.1	   49.6	   54.6	   56.1	   51.4
Avg. norm	         47.1	   44.9	   49.7	   52.0	   44.0

DynaMax	         58.2	   53.9	   65.1	   70.9	   71.1
DynaMax norm	 53.7	   47.8	   59.5	   66.3	   62.9

We see here that normalisation hurts both averaged word vectors and well as max-pooled word vectors.
However, DynaMax norm still significantly outperforms Avg. norm. It even outperforms Avg. without norm on all but one tasks.

Hopefully, the above taken together explains why the membership values can indeed be any real numbers.
Again, thank you very much for a good question. 
We will add this discussion as well as results for more word vectors into a separate section in the Appendix.

Please do not hesitate to ask us any additional questions.

Best wishes,

ICLR 2019 Conference Paper1058 Authors</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>