<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ByghKiC5YX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Greedy Attack and Gumbel Attack: Generating Adversarial Examples..." />
      <meta name="og:description" content="We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ByghKiC5YX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data</a> <a class="note_content_pdf" href="/pdf?id=ByghKiC5YX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019greedy,    &#10;title={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=ByghKiC5YX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=ByghKiC5YX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Adversarial Examples</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">20 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_H1e_TlAhTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Can the reviewers please clarify the contribution(s)?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByghKiC5YX&amp;noteId=H1e_TlAhTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper481 Area Chair1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper481 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">As defined in this paper, an adversarial attack is just solving an optimization problem. For discrete sequence inputs, the paper considers a constrained discrete optimization problem. Discrete optimization is well studied and greedy algorithms for discrete optimization are also well-known and well-studied methods. They are obvious to machine learning practitioners as well. The particular greedy algorithm the authors use seems to be effective for this problem and does not require any special tricks.

Could the reviewers especially please comment on the following questions:

1. Is the Gumbel algorithm proposed necessary here or, more generally, is a new discrete optimization algorithm needed here?

2. The discussion section of the paper says:
"We have proposed a probabilistic framework for generating adversarial examples on discrete data, based on which we have derived two algorithms. Greedy Attack improves the state-of-the-art across several widely-used language models, and Gumbel Attack provides a scalable method for real-time generation of adversarial examples."

The paper claims to improve the state of the art. Can any of the reviewers comment on whether the paper advanced the state of the art in discrete optimization? Or, more generally, how should we read the claim above? Since a standard greedy algorithm works, there can't be anything special about this particular optimization problem that standard methods can't handle.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkgBBnMApX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Efficiency of Gumbel Attack; Difference and Connections in Discrete Optimization and Adversarial Attack.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByghKiC5YX&amp;noteId=BkgBBnMApX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper481 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper481 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the area chair for reading the reviews and our rebuttals carefully. We will answer the questions of Area Chair from the authors’ perspective. 

The area chair proposes two questions:
1) Why do we need Gumbel as a new discrete optimization algorithm?
2) Have we improved “the state-of-the-art” in discrete optimization?

The short reply is
1)  Gumbel attack is efficient. The efficiency can be a practical concern in the setting of adversarial attack.
2) We propose better algorithms in terms of accuracy or efficiency in the regime of adversarial attack. This regime is not exactly the same as discrete optimization. 

We address the details below. 
1)  
1.1 Gumbel attack is efficient both in terms of the number of model evaluations and in terms of real time. First, no model evaluation is required during the attack stage. Also, Figure 4 in the manuscript provides a comparison of real-time efficiency, which shows Gumbel attack is orders-of-magnitude faster. (Gumbel attack is around 10^-2 seconds per sample while FGSM, Delete-1 Score and other methods are between 10^-1s and 1s per sample on Yahoo! Answers.)
1.2 In practice, attackers may not be able to conduct many model evaluations to attack a real system.
1.3 It may also help design more efficient adversarial training algorithms. 

2)
2.1 We first address the difference between Greedy attack and standard greedy methods. 
The most standard greedy methods choose the first perturbation by evaluating models d * V times, where d is the length of the sentence/paragraph and V is the size of dictionary, and choose the next perturbation with complexity (d-1) *V, etc. Greedy attack follows a two-stage procedure motivated from a probabilistic framework, and takes O(d + k*V) evaluations in total (k being the number of perturbations). Moreover, Greedy attack is easier to parallelize. Given the efficiency concern of adversarial attacks, it can be more practical. 
2.2 The area of adversarial attack is not exactly the same as discrete optimization. 
We formulate the problem of adversarial attack as a constrained discrete optimization problem. The true constraint here is that “humans will not change their decisions”, which we approximate by constraining the number of perturbed words.  Experiments involving human subjects have been carried out to validate the effectiveness of approximation. 
2.3 We only show the superior performance of our algorithms to algorithms in adversarial attack ([1-4]), and we do not have the intention to claim it achieves the state-of-the-art in discrete optimization. 

[1] Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. Black-box generation of adversarial text sequences to evade deep learning classifiers. arXiv preprint arXiv:1801.04354, 2018.
[2] Jiwei Li, Will Monroe, and Dan Jurafsky. Understanding neural networks through representation erasure. arXiv preprint arXiv:1612.08220, 2016.
[3] Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard Harang. Crafting adversarial input sequences for recurrent neural networks. In Military Communications Conference, MILCOM 2016-2016 IEEE, pp. 49–54. IEEE, 2016.
[4] Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, and Wenchang Shi. Deep text classification can be fooled. arXiv preprint arXiv:1704.08006, 2017.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJxl0Dtjpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>UPDATED: Elaborate Section 3.1 and add new human evaluation based on the reviewers’ suggestions.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByghKiC5YX&amp;noteId=BJxl0Dtjpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper481 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper481 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We have elaborated the Greedy attack with a clearer presentation in Section 3.1. First, we have adopted the common notations and marked each expectation with a subscript to indicate the source of the expectation. Second, in the updated version, we have added a detailed explanation of the approximation in Equation (5, 7).  To summarize, when one assumes other features are perturbed adversarially, the Greedy Attack can be interpreted as maximizing a lower bound of the original objectives.

We have added another experiment to compare various algorithms with human evaluation on the IMDB movie review data set. On each instance, we increase the number of words to be perturbed until the prediction of the model changes. Then we ask humans to label original texts and perturbed texts. Greedy attack yields the best performance in the experiment. Please see Appendix B of the updated version for details.

We again express our sincere thanks to all the reviewers, who have provided very useful suggestions for helping build our manuscript into a better shape.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJekkfSq3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Important task; very poorly written</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByghKiC5YX&amp;noteId=BJekkfSq3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper481 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper481 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper addresses the problem of generating adversarial examples for discrete domains like text. They propose two simple techniques:
1) Greedy: two stage process- first stage involves finding the k words in the sentence/paragraph to perturb and second step changes the word in the positions identified in step 1.
2) Gumbel: first approach amortized over datasets where first and second steps are parametrized and learned over the dataset with the loss being the probability of flipping the decision.
Specifically, for the Gumbel approach, the authors use the non-differentiable top-k-argmax output to train the module in the second step which is not ideal and it would be better to train both first and second steps jointly in an end-to-end differentiable manner.

The results show that Greedy approach is able to significantly affect the accuracy of the systems compared to other adversarial baselines. Mturk evaluation shows that for tasks like sentiment analysis, humans weren't as confused as the systems were when the selected words were changed which is encouraging. However, the Gumbel method performs poorly compared to other baselines.
Moreover, a thorough analysis of why Greedy is doing better than some gradient based adversarial attacks is needed in the paper because it is unclear what is causing their greedy approach to perform well; is it the two-stage nature of the process?

My major gripe with the paper is that it is egregiously difficult to read in parts and is poorly written. There are dangling conditional bars in many equations (5, 7, Greedy attack etc.), unclear "expectation (E)" signs and many other confusing notational choices which make the math difficult to parse. I am not even sure if those equations are correctly conveying the idea they are meant to convey. I found  the algorithms to be more clearly written and realize that the text in the models and equations is unnecessarily complicated. The argument about approximation to the objective by considering the i positions independently is not convincing and their is nothing in the paper to show if the assumption is reasonable.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1lXIwYsT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByghKiC5YX&amp;noteId=B1lXIwYsT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper481 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper481 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thanks the reviewer for the comments, and the explanations on the motivation of the task. We have improved the clarity of Section 3.1 based on the reviewer’s suggestions. Below we respond in detail to the reviewer’s comments:

“the Gumbel method performs poorly compared to other baselines”

We agree that the performance of the Gumbel method is comparable to previous methods. However, its running time is significantly shorter than all the previous methods and our Greedy Attack method (See Figure 4). Thus, Gumbel Attack is the most efficient one across all methods even after taking into account the training stage. The efficiency of generating adversarial examples is an important factor for large-scale data.  

"what is causing their greedy approach to perform better” than “some gradient based adversarial attacks”?

While gradient-based methods have led to several successful algorithms in the continuous domain (e.g., natural images), they have been observed to be less effective compared to discrete methods (e.g., [1]). It is mainly because gradient based methods focus on the sensitivity of response to each feature in the infinitesimal space, while perturbation is carried out in discrete space. 

“it is egregiously difficult to read in parts and is poorly written”

We apologize for the difficulty of reading and have addressed the problem carefully. First, we have adopted the common notations and marked each expectation with a subscript to indicate the source of the expectation. Second, in the updated version, we have added a clearer and more detailed explanation of the approximation in Equation (5, 7).  To summarize, when one assumes other features are perturbed adversarially, the Greedy Attack can be interpreted as maximizing a lower bound of the original objectives.

“The argument about approximation to the objective by considering the i positions independently is not convincing”

We agree with the reviewer that this is an unnecessary assumption and have removed it from our framework (but still keep it in the design of Gumbel attack.) The independence assumption is used in Gumbel Attack for the sake of efficiency. This can be interpreted as a constraint on the search space so that decisions can be made in parallel. It can be a promising future direction to consider a framework where features are perturbed sequentially, with a termination gate [2] to control when to stop the perturbation. The latter enables the use of variable sizes of perturbation, instead of top-k perturbation.

[1] Gao, Ji, et al. "Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers." arXiv preprint arXiv:1801.04354 (2018)
[2] Shen, Yelong, et al. "Reasonet: Learning to stop reading in machine comprehension." Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2017.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hkxb5b3kpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Why is the task important?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByghKiC5YX&amp;noteId=Hkxb5b3kpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper481 Area Chair1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper481 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Can you please clarify why you say the task is important? It is very easy to generate errors for models of text. Attackers would not need the methods in this paper to produce Yelp reviews that a state-of-the-art text sentiment classifier got wrong. They would not need any knowledge of machine learning at all to find errors for these text classifiers.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJepNwO7pX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Adversarial examples help find systems' blind spots</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByghKiC5YX&amp;noteId=SJepNwO7pX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper481 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper481 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The task is important because it focuses on small perturbations to text that change the classifier decision. These changes, if undetected by humans,  exhibit clear brittleness of a classifier and will aid in better design of a more robust classifier. This is the motivation for generating adversarial attacks in general: perturb the input ever so slightly in a manner that is in general undetectable to the human eye but results in drastic change in model's predictions. This has implications ranging from interpretablility and reliability of the model to security and privacy issues.
In the Mturk experiments, the authors do demonstrate that a lot of the perturbations produced by their models do not change the human's decision on sentiment classification but does change the model prediction.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJgwIBu7am" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Motivating adversarial example research</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByghKiC5YX&amp;noteId=BJgwIBu7am"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Nicholas_Carlini1" class="profile-link">Nicholas Carlini</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper481 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Disclaimer: I have not read the paper. This comment is solely intended to respond to the AC asking why the problem domain is important.]

Adversarial example research papers have always had to deal with this question: why is this interesting? we already know classifiers make mistakes!

There are at least a few common counter arguments:

-  Yes, models make mistakes, but they are on average quite good. The interesting property of adversarial examples is that you can take an arbitrary input, that is very clearly Class A, and make the model produce the label for Class B. You can do this even when the object in Class A is the most A-like in the entire dataset. And even if class B resembles nothing like class A. That's what makes the domain interesting.

- Why bother trying to find strong attacks if random noise might work? The main counter-argument here is that random noise often has to have a significantly larger distortion than adversarial noise. With Gaussian noise with sigma=0.2 on ImageNet, models still reach modest (50%+) accuracy. Adversarial noise with norm 20x smaller can reduce model accuracy to &lt;1%.

- Is this actually a security problem? It depends on the situation. For a nice treatment of this question see <a href="https://arxiv.org/abs/1807.06732" target="_blank" rel="nofollow">https://arxiv.org/abs/1807.06732</a></span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HyxnjDahaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>thanks for your comment</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByghKiC5YX&amp;noteId=HyxnjDahaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper481 Area Chair1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper481 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for weighing in, Nicolas, but I'm not sure I understand your argument.

Neither of your first two points should surprise us when the models have substantial test error.

To put it another way, 50% of random sigma=.2 perturbations are misclassified for ImageNet and adversarially chosen errors can be 20x closer than these randomly found errors. Of *course* the nearest error is going to be significantly closer than randomly found errors, the nearest error is, by construction, the nearest error! Why is 20x closer unusually close in a high (~150,000) dimensional space? 
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryeJ9zR3pQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>I agree with your perspective</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByghKiC5YX&amp;noteId=ryeJ9zR3pQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Nicholas_Carlini1" class="profile-link">Nicholas Carlini</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper481 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">There's actually a paper under submission that makes exactly this argument ( <a href="https://openreview.net/forum?id=S1xoy3CcYX" target="_blank" rel="nofollow">https://openreview.net/forum?id=S1xoy3CcYX</a> ). I definitely agree that it should not be surprising that models have such low accuracy when you adversarially select noise to maximize classification error rate in light of this phenomenon.

I don't think this is actually contradictory to adversarial examples as a line of research. In particular, one of the main reasons I see adversarial example work as interesting is that it gives us an estimate of the worst-case accuracy. Just like average-case accuracy is useful in many situations (and standard 'accuracy-on-test-set' measures do this for us), worst-case accuracy is also useful in other cases.

To relate it back to this paper's topic of discrete data (again, I haven't read the paper) a classifier for malware that worked 100% of the time on "normal" data would be useless if it worked 0% of the time on adversarial data---because the only data it will ever see, malware, is by definition adversarial. The same argument applies to spam, and to a lesser extent various other written text attempting to avoid detection (e.g., the recent hate speech detectors).

Just to clarify, though: it sounds like your "Why is the task important?" question is generally directed at the adversarial example research as a whole. Is this right? There are, by my count, at least 60 papers under submission to ICLR this year that focus explicitly on the problem of adversarial examples (explaining their existence, approaches for generating them, and approaches for defending against them). Would you have a similar complaint about any of these other papers?</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_SkxNEow7Tm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Motivation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByghKiC5YX&amp;noteId=SkxNEow7Tm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper481 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper481 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Area Chair and Anonymous Reader:

Thanks for your questions on the motivation of adversarial attack for discrete data. Below we briefly explain the motivation, followed by the evidence that simple random perturbation does not work. 

In summary, the area chair and another reader posed the following questions：

1. Why does one need to study the phenomenon of adversarial examples on discrete data?
2. Why is this paper worth reading?
3. Do simple methods like random perturbation work on text data?   

In short, our reply is

1. Robustness is an important criterion for models on discrete data. The generation of adversarial examples can be used to evaluate robustness or even improve robustness.
2. In this paper, our goal is to propose methods with better performance (Greedy attack) or with higher efficiency (Gumbel attack).
3. We provide evidence that simple methods like random perturbation do not work. 

Below are concrete details:

Robustness is an important criterion for the application of machine learning models in critical areas such as medicine, financial markets, recommendation systems, and criminal justice. Adversarial examples have been used to evaluate the (adversarial) robustness of models (e.g., [1, 2, 5]) and have also been applied to train robust models (e.g., [3, 4]).

The phenomenon of adversarial examples was first found in state-of-the-art deep neural network models for classifying images (e.g., [5, 6, 2]), where small perturbations unobservable by human can easily fool neural networks. Similar to image data, the problem of adversarial perturbation on discrete data can be defined as altering the prediction of a model via minimal perturbation to an original sample (e.g., [7-14]).  

While there have been many pioneered and interesting papers in this area (e.g., [7-14]), we proposed Greedy attack, a method to increase the misclassification rate of a model with a comparable scale of perturbation, and Gumbel attack, a method to improve the efficiency of generating adversarial examples, (It just happens to be fashionable :) ).

It is natural to ask how the simplest algorithm, random perturbation, works before one is persuaded to read our paper. We compare our methods with random perturbation on the test set of the IMDB movie review dataset used in our paper. For each instance, we randomly sample k positions in the sentence, and replace them with randomly sampled words. The average consistency of the predictions of the model from the perturbed and the original instances is 99.9% after k = 10 words are changed, and 92%, 90.4% after k = 50, 100 words are changed respectively. See the following link for a plot of comparison: <a href="https://drive.google.com/file/d/1T6UJQPz4iDFqsK9XQZ0nYv-bBcYxWraP/view?usp=sharing." target="_blank" rel="nofollow">https://drive.google.com/file/d/1T6UJQPz4iDFqsK9XQZ0nYv-bBcYxWraP/view?usp=sharing.</a> 
We conclude that random perturbation does not work. 

[1] Carlini, Nicholas, and David Wagner. "Towards evaluating the robustness of neural networks." 2017 IEEE Symposium on Security and Privacy (SP). IEEE, 2017.
[2] Agarwal, Chirag, et al. "An Explainable Adversarial Robustness Metric for Deep Learning Neural Networks." arXiv preprint arXiv:1806.01477 (2018).
[3] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. ICLR (2018).
[4] Alex Kurakin, Ian Goodfellow, Samy Bengio. Adversarial machine learning at scale. ICLR 2017. 
[5] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. ICLR, 2015.
[6] Moosavi-Dezfooli, Seyed-Mohsen, Alhussein Fawzi, and Pascal Frossard. "Deepfool: a simple and accurate method to fool deep neural networks." CVPR, 2016.
[7] Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. Black-box generation of adversarial text sequences to evade deep learning classifiers. IEEE Security and Privacy Workshops (SPW), 2018.
[8] Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2021–2031, 2017.
[9] Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, and Wenchang Shi. IJCAI, 2018. 
[10] Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard Harang. Crafting adversarial input sequences for recurrent neural networks. In Military Communications Conference, MILCOM 2016-2016 IEEE, 2016.
[11] Suranjana Samanta and Sameep Mehta. Towards crafting text adversarial samples. arXiv preprint arXiv:1707.02812, 2017.
[12] Minhao Cheng, Jinfeng Yi, Huan Zhang, Pin-Yu Chen, and Cho-Jui Hsieh. Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples. arXiv preprint arXiv:1803.01128, 2018.
[13] Javid Ebrahimi, Anyi Rao, Daniel Lowd, Dejing Dou. Hotflip:White-box adversarial examples for text classification. ACL, 2018. 
[14] Jiwei Li, Will Monroe, Dan Jurafsky. Understanding neural networks through representation erasure.  arXiv preprint arXiv:1612.08220, 2016.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1x-WZR2T7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for elaborating</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByghKiC5YX&amp;noteId=H1x-WZR2T7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper481 Area Chair1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper481 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for elaborating on the motivation you have for the work. It is very helpful.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Skg9KAvmTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>I second the importance</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByghKiC5YX&amp;noteId=Skg9KAvmTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper481 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper481 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I'd like to second the importance of this work. Of course random perturbations at some point will also do the trick - but the same is true in computer vision applications where often small amounts of Gaussian noise lead to misclassifications. Nonetheless, many people in CV study adversarial perturbations as a means to understand what concepts network models have learnt and how susceptible they really are. Minimum adversarial perturbations are often several orders of magnitude smaller than random noise in CV, and the same seems to be true on discrete data like text.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1xno5Dz6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Indeed, why is this problem important?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByghKiC5YX&amp;noteId=B1xno5Dz6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper481 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Random perturbations are enough to fool text classifiers so why are the authors doing this? Because Gumbel-Softmax is fashionable?</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SJx8hts16X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>writing quality is extremely important</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByghKiC5YX&amp;noteId=SJx8hts16X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper481 Area Chair1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper481 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">A poorly written manuscript is sufficient reason, by itself, to recommend rejecting a paper.

Can you clarify how detrimental these writing problems are? Are they problems at the section and organizational level? The paragraph level in constructing clear prose? The sentence level? All of the above? Is the logical structure of the argument well-organized and easy to follow?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1exhKOmp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Poor mathematical exposition</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByghKiC5YX&amp;noteId=H1exhKOmp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper481 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper481 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Organizationally, the paper is fine and sections are presented in a logical manner. But as I mentioned in my review, the mathematical exposition is certainly non-conventional and maybe even wrong. Their notations (Expectation symbols, conditional symbols etc.) have serious issues and I found their arguments about approximation and assumptions hard to follow. I'm not convinced about their argument of  approximating their proposed schemes by considering each position independently (partly because I can't clearly follow their argument) and moreover I believe that their original non-approximated probabilistic (unclear) formulation is unnecessary because it doesn't add anything to the paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_S1gSsdb537" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Exciting advance in discrete adversarial attacks</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByghKiC5YX&amp;noteId=S1gSsdb537"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper481 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper481 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this work the authors introduce two new state-of-the-art adversarial attacks on discrete data based on a two-stage probabilistic process: the first step identifies key features which are then replaced in the second step through choices from a dictionary.

Overall the manuscript is very well written and easy to follow. The evaluation is extensive and contains all previous attacks I am aware of. The greedy attack outperforms all prior work by a large margin while the Gumbel attack works on par with the previous state-of-the-art while being significantly faster. 

I only have a few questions and remarks:

* What’s the “random attack” baseline in these tasks? In computer vision it’s often sufficient to add a little bit of salt-and-pepper noise or Gaussian noise to change the model decision.

* Another thing I am wondering is what the human evaluation scores would be on adversarials from other adversarial attacks? Adversarial attacks in general (e.g. in computer vision) can work in two ways: one being actually changing the semantic content (thus also “fooling humans) while the other changes background features / add noise to which humans are pretty insensitive (unless you add too much of it). The greedy attack does seem to change some semantics as can be seen in the increased error rate of humans (which is pretty rare for computer vision adversarials). It might be that other attacks are rather changing words or characters which are not as semantically meaningful, as would be revealed by the accompanying human scores.

* Are you planning to release the code? Will it be part of CleverHans or Foolbox?

Overall, I find this work to be a really exciting advance on discrete adversarial attacks.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1x8tPKiam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByghKiC5YX&amp;noteId=S1x8tPKiam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper481 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper481 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the encouraging comments and the help in addressing the importance of the task!

What’s the “random attack” baseline in these tasks? In computer vision it’s often sufficient to add a little bit of salt-and-pepper noise or Gaussian noise to change the model decision.

We define “random attack” as randomly sample k positions in the sentence, and replace them with randomly sampled words. We run random perturbation on the test set of the IMDB movie review dataset used in our paper. The average consistency of the predictions of the model from the perturbed and the original instances is 99.9% after k = 10 words are changed, and 92%, 90.4% after k = 50, 100 words are changed respectively. See the following link for a plot of comparison with our algorithms (on the first five words): <a href="https://drive.google.com/file/d/1T6UJQPz4iDFqsK9XQZ0nYv-bBcYxWraP/view?usp=sharing." target="_blank" rel="nofollow">https://drive.google.com/file/d/1T6UJQPz4iDFqsK9XQZ0nYv-bBcYxWraP/view?usp=sharing.</a> 
We conclude that random perturbation does not work.  

“What the human evaluation scores would be on adversarials from other adversarial attacks?” 

We have added another experiment to compare various algorithms with human evaluation on the IMDB movie review data set. On each instance, we increase the number of words to be perturbed until the prediction of the model changes. Then we ask humans to label original texts and perturbed texts. Greedy attack yields the best performance in the experiment. Please see Appendix B of the updated version for details.

“Are you planning to release the code? Will it be part of CleverHans or Foolbox?”

Yes, we plan to release the code. We will either release the code in a stand-alone github repository or merge it into CleverHans.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJlmF5xq27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Novel probabilistic framework for making adversarial attacks on deep networks with discrete valued inputs; flexible framework that allows solving the trade-off between attack success rate and computation time</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByghKiC5YX&amp;noteId=HJlmF5xq27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper481 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper481 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors proposed a novel probabilistic framework to model adversarial attacks on deep networks with discrete inputs such as text. The proposed framework assumes a two step construction of an adversarial perturbation: 1) finding relevant features (or dimensions) to perturb (Eq. 3); 2) finding values to replace the features that are selected in step 1 (Eq. 4). The authors approximate some terms in these two equations to make the optimization easier. For example, it is *implicitly* assumed that given the i-th feature is removed from consideration, the probability of attack success does not change *on average* under probabilistic *adversarial* attack on other features (Eq. 5). It is not clear why that should hold and under what conditions that assumption would be reasonable (given that the attacks on other features are adversarial, although being probabilistic). 
The proposed framework allows one to solve the computation vs. success rate trade-off by either estimating the best attack from the network (called greedy attack Eq. 6) or using a parametric estimation that does not require model evaluation (called Gumbel attack). Experimental results suggest that Gumbel attack has better or competitive attack rate on models developed for text classification while having the most computationally efficiency among other methods. It is also noticeable that the greedy attack achieves the best success rate with a large margin among all the tested methods. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hkx5iwFoam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ByghKiC5YX&amp;noteId=Hkx5iwFoam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper481 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper481 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the detailed and encouraging comments! 

To address the reviewer’s concern on Equation 5, we have added a more rigorous and detailed explanation of the approximation. Roughly, when one assumes other features are perturbed adversarially, the Greedy Attack can be interpreted as maximizing a lower bound of the original objectives. Details can be found in Section 3.1 of the updated version. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>