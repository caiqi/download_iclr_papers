<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>SALSA-TEXT : SELF ATTENTIVE LATENT SPACE BASED ADVERSARIAL TEXT GENERATION | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="SALSA-TEXT : SELF ATTENTIVE LATENT SPACE BASED ADVERSARIAL TEXT GENERATION" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1liWh09F7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="SALSA-TEXT : SELF ATTENTIVE LATENT SPACE BASED ADVERSARIAL TEXT..." />
      <meta name="og:description" content="Inspired by the success of self attention mechanism and Transformer architecture&#10;  in sequence transduction and image generation applications, we propose novel self&#10;  attention-based architectures to..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1liWh09F7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>SALSA-TEXT : SELF ATTENTIVE LATENT SPACE BASED ADVERSARIAL TEXT GENERATION</a> <a class="note_content_pdf" href="/pdf?id=B1liWh09F7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019salsa-text,    &#10;title={SALSA-TEXT : SELF ATTENTIVE LATENT SPACE BASED ADVERSARIAL TEXT GENERATION},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=B1liWh09F7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Inspired by the success of self attention mechanism and Transformer architecture
in sequence transduction and image generation applications, we propose novel self
attention-based architectures to improve the performance of adversarial latent code-
based schemes in text generation. Adversarial latent code-based text generation
has recently gained a lot of attention due to their promising results. In this paper,
we take a step to fortify the architectures used in these setups, specifically AAE
and ARAE. We benchmark two latent code-based methods (AAE and ARAE)
designed based on adversarial setups. In our experiments, the Google sentence
compression dataset is utilized to compare our method with these methods using
various objective and subjective measures. The experiments demonstrate the
proposed (self) attention-based models outperform the state-of-the-art in adversarial
code-based text generation.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Self-attention, Transformer, generative adversarial networks, GAN, neural text generation, NTG, generative models</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a self-attention based GAN architecture for unconditional text generation and improve on previous adversarial code-based results.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Byeums64pX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good experimental design and clarity of writing, but limited novelty, experimentation, and analysis</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1liWh09F7&amp;noteId=Byeums64pX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1204 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1204 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary of the paper: This work builds on the adversarial autoencoder (AAE) proposed by Makhzani et al 2015 and the adversarially regularized auto encoder (ARAE) proposed by Kim et al. 2017. It does this by integrating Vaswani et al. 2017’s transformer model and self-attention (Parikh et al. 2016 and many others) and the AAE and ARAE methodologies. The authors call this SALSA (Self Attentive Latent Space Based Adversarial Text Generation). Basically SALSA combines these parts that are successful on their own and shows on the Google Sentence Compression dataset that ARAE and AAE can be beaten with the inclusion/injection of Transformer + self-attention. Their models show longer and higher-quality sentence generation with more diversity. For some of the BLEU/Human Eval metrics though, ARAE seems to perform better. SALSA seems to aid in alleviating mode collapse (something ARAE struggles dramatically to do).

Preliminary Comments:
  * Paper is easy-to-read and organized.
  * Experimental design is well-described especially in how the dataset was preprocessed; something many papers in this area fail to do.
  * Evaluation metrics are discussed in detail, which is a plus.

Drawbacks:
  * Novelty is limited: SALSA is an aggregation of already proposed methodologies. 
  * The experimentation could’ve been more robust. 
        - It seems as though there’s only one model that is evaluated here. Trying different models (varying HPs or just random seeds) or providing justification of how this specific model was chosen would be helpful. If I missed this, please correct me.
        - Varying the dimensionality of the noise vector, how often it is copied would be helpful.
        - Discussion on why copying the noise vector T times rather than generating a smaller dimensional noise vector (or the same dimensional noise vector) at each step would strengthen the paper.
  * Statistics of the human evaluation, standard errors/variances of how the humans evaluated the sentences would strengthen the results more.
  * The discussion of experimental results leaves a little to be desired. I’d like to know what a 0.1 improvement in grammaticality, semantic consistency, fluency etc means. Standard errors like I mentioned before and/or examples of this would go a long way to helping that.
  * The authors discuss how SALSA aids in ‘better performance’ on long/complex sentences. A definition of what is long and complex in conjunction with evaluation across BLEU/Self-BLEU/PPL/R-PPL/3 Human metrics would strengthen the paper dramatically.
  * Looking at SALSA-ARAE vs ARAE in Table 2, ARAE has much higher BLEU scores. Some discussion at why this is the case would also improve the paper.

Minor Issues:
  * There are many typos in the text, so another proof-read would be good.
  * Figures and tables are not space-efficient and do not need to be so large; placed in the manners they are. This empty space could be used to add more discussion/further experimentation. Figure 1 can be rotated to give you 6-8 lines more. Figures 2 and 3 can be put side by side without reducing the font too much.
  * The bolding of certain numbers in the tables are confusing on first glance because you’re obviously comparing SALSA-AAE to AAE and SALSA-ARAE to ARAE, but the tables have both without a divider, so on first glance I was confused (wasn’t the best performance in the row). So including a double lined divider on each table would be helpful.
  * A few papers have been listed multiple times in the references (AAE and ARAE papers).
  * Related work section: an inclusion of a few of the more recent GAN papers for text is missing. I think some discussion of other VAE approaches to text such as Bowman et al. 2016: Generating Sentences from a Continuous Space among others would be good. I think citing Kingma and Welling 2014 Auto-Encoding Variational Bayes and Rezende et al. 2014 Stochastic Backpropagation and Approximate Inference in Deep Generative models is important in the first mention of VAEs.

Concluding Thoughts:
  * This paper has limited model novelty and doesn’t overcome this by having great experiments or analysis.
  * This paper has the building blocks for great experimentation and analysis, and if those are completed well, has the ability to make strong claims regarding the impact of SALSA on a variety of metrics.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJxY3Mc337" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Strong experimental results but the technical novelty is limited</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1liWh09F7&amp;noteId=rJxY3Mc337"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1204 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1204 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Building on prior work  exploring Adversarial Autoencoder (AAE) and Adversarial Regularized Autoencoder 
(ARAE) for text generation (Cifka et al., 2018), this paper proposes to employ widely adopted Self Attention 
and Transformer to enhance the modeling capabilities of the generator and discriminator in AAE and ARAE 
for text generation. Experiments on the Google Sentence Compression dataset show that, AAE and ARAE 
enhanced with self attention and Transformer generate long and high-quality sentences with much higher 
diversity than the corresponding models without attention, although ARAE without attention achieves higher 
BLEU and human evaluation scores.

This paper is well written and organized, the experiments are well executed, and the results are convincing,
which demonstrates the effectiveness of Self Attention and Transformer that have been widely adopted in 
natural language processing recently.

Major:

1. The technical novelty of this paper is very limited. It is a trivial combination of Self Attention, Transformer, 
and previous models for text generation. The experimental results are not surprising. 

2. The experimental results in Table 2-5 just list the performance of AAE and ARAE with/without attention. 
In-depth analyses about the performance of different model variants will make the paper much more 
interesting.

3. The current model generates sentences by fixing the length to 50 first. It is important to explore other more
natural modeling variants  for generating sentences of variable lengths.

4. In Fig. 1, it is surprising that one 100-d noise vector passed to a fully connected layer is copied T times. 
Is it better to use a different noise vector in each step?

5. On page 8, the last sentence above Section 5 is inappropriate. Table 2 and 5 clearly show that ARAE has 
the best performance on BLEU and human evaluation scores, although it lacks generation diversity.

In conclusion, this paper has well-executed experiments. But the novelty is very limited, the model design might 
need more exploration, and detailed analyses of the results are required.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1xT4GfKnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea but not novel enough and lack of insights</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1liWh09F7&amp;noteId=H1xT4GfKnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1204 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1204 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Contributions:

The main contribution of this paper is the incorporation of the Transformer architecture for adversarial latent-code-based text generation. Specifically, Transformer is used for the encoder and decoder design of AAE and ARAE. This is different from previous GAN models designed for text generation, where LSTM is used for generator, and LSTM or CNN is used for discriminator.

Weaknesses:

(1) Novelty: I think the novelty of this paper is somehow limited. Basically, instead of using LSTM and CNN, the main novelty is the usage of Transformer for encoder and decoder design. From the modeling perspective, there is no novelty, using the same AAE and ARAE models proposed by previous work. 

It the paper is well-written and well-executed, this paper still has the chance to be a good paper, however, I think this paper is quite thin. The contents seems not enough to support a paper.

(2) Presentation: 

First, in the related work section 2.4, I think the authors should discuss related works not just AAE &amp; ARAE. SeqGAN, RankGAN, TextGAN, LeakGAN and MaskGAN is another research line that needs to be discussed.

Second, there is no need to make Figure 1 &amp; 2 &amp; 3 this large. It really just wastes a lot of spaces. Table 1 also wastes a lot of spaces. If getting rid of these, the contents in this paper is really thin. 

Third, there are several typos in the paper. Please proofread the paper carefully.

(3) Evaluation: Please see my detailed comments listed in the Questions below. 

Questions:

(1) This paper is interested in adversarial latent-code-based text generation. I have a general question about this paper. In the introduction section, the authors mentioned the problem of exposure bias, and then say GAN is a promising approach to alleviate this problem. This is true. However, doing adversarial learning on the latent code seems not help alleviate this exposure bias problem, since still no sequence-level guidance is provided. The model is still generating next word based on previous ground-truth words, with the difference being that an adversarially learned latent code is also injected. Can the authors justify why this model is useful and how it can alleviate exposure bias?

(2) I think the Transformer used in this work can be also adopted for SeqGAN etc. Do the authors think SeqGAN, MaskGAN etc will also benefit from using Transformer?  

(3) In section 4.3, the authors show qualitatively SALSA generates longer and more diverse text. Can the authors provide some intuition why this is the case?

(4) I think the sample size used in the human evaluation is too small. Only 18 sentences are used. A larger-scale human evaluation is needed. How reliable and what are the variances of the scores in Table 5? 

Minor issues:

(1) AAE and ARAE has been cited twice in the references. Please correct it. 

(2) Not all the boldings in Table 2-5 are correct. Usually, bolding is used for the best performing model. Please correct them. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>