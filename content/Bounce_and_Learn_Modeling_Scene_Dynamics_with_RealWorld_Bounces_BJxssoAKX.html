<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Bounce and Learn: Modeling Scene Dynamics with Real-World Bounces | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Bounce and Learn: Modeling Scene Dynamics with Real-World Bounces" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJxssoA5KX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Bounce and Learn: Modeling Scene Dynamics with Real-World Bounces" />
      <meta name="og:description" content="We introduce an approach to model surface properties governing bounces in everyday scenes. Our model learns end-to-end, starting from sensor inputs, to predict post-bounce trajectories and infer..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJxssoA5KX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Bounce and Learn: Modeling Scene Dynamics with Real-World Bounces</a> <a class="note_content_pdf" href="/pdf?id=BJxssoA5KX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019bounce,    &#10;title={Bounce and Learn: Modeling Scene Dynamics with Real-World Bounces},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BJxssoA5KX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We introduce an approach to model surface properties governing bounces in everyday scenes. Our model learns end-to-end, starting from sensor inputs, to predict post-bounce trajectories and infer two underlying physical properties that govern bouncing - restitution and effective collision normals. Our model, Bounce and Learn, comprises two modules -- a Physics Inference Module (PIM) and a Visual Inference Module (VIM). VIM learns to infer physical parameters for locations in a scene given a single still image, while PIM learns to model physical interactions for the prediction task given physical parameters and observed pre-collision 3D trajectories.  To achieve our results, we introduce the Bounce Dataset comprising 5K RGB-D videos of bouncing trajectories of a foam ball to probe surfaces of varying shapes and materials in everyday scenes including homes and offices. Our proposed model learns from our collected dataset of real-world bounces and is bootstrapped with additional information from simple physics simulations. We show qualitative and quantitative results on our newly collected dataset and outline open challenges for learning to model real-world bounces.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">intuitive physics, visual prediction, surface normal, restitution, bounces</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SJloGnWRnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Might be Good but Difficult to Evaluate:  No Comparison to Existing Methods.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJxssoA5KX&amp;noteId=SJloGnWRnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper651 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper651 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors present both a dataset of videos of a real-world foam ball bouncing and a model to learn the trajectory of the ball at collision (bounce) points in these videos.  The model is comprised of a Physics Inference Module (PIM) and a Visual Inference Module (VIM).  The PIM takes in both a vector of physical parameters (coefficient of restitution and collision normal) and a point cloud representation of the pre-bounce trajectory, and produces a point cloud representation of the post-bounce trajectory (or, rather, an encoded version of such).  The VIM takes in an image and ground-truth bounce location and produces the physical parameters of the surface at that location.

I find the paper well-written and clear.  The motivation in the introduction is persuasive and the related work section is complete.  However, the authors are introducing both a new training paradigm (to my knowledge unused in the literature) and a new model, and without any existing baselines to compare against I find it a bit difficult to understand how well the model works.  

Overall, the authors’ model is somewhat complicated and not as general as it initially seems.  To justify this complication I would like to see more convincing results and benchmarking or application to more than one single dataset (e.g. non-spheres bouncing).

Here are some specific concerns:

1)  I could not find a link to an open-sourced version of the dataset(s).  Given that the authors emphasize the dataset as a main contribution of the paper, they should open-source it and make the link prominent in the main text (apologies if I somehow missed it).

2)  The authors claim in multiple places that the model is trained end-to-end, but this does not seem to be the case.  Specifically, the PIM is pre-trained on an auxiliary dataset from simulation.  The trajectory encoder also seems to be pre-trained (though I could be wrong about that, see my question below).  Furthermore, there is a bit of hand-holding:  The PIM uses ground-truth state for pre-training, and the VIM gets the ground-truth bounce location.  In light of this, the model seems a lot less general and end-to-end than implied in the abstract and introduction.

3)  No comparison to existing baselines.  I would like to see how the authors’ model compares to standard video prediction algorithms.  The authors could evaluate their model with respect to pixel loss (after ground-truth rendering) and compare to a video prediction algorithm (such as PredNet by Lotter, Kreiman, &amp; Cox, 2016).  Given that the authors’ method uses some extra “privileged” information (as described in point 2), it should far out-perform algorithms that train only on video data, and such a result would strengthen the paper a lot.

4)  Table 1 is not a very convincing demonstration of performance.  Regardless of baselines, the table does not show confidence intervals.  I would love to see training curves with errorbars of the models on the most important metrics (e.g. Dist and COR Median Absolute Error).

I also was confused about a couple of things:

1)  How was the PointNet trajectory encoder trained?  I did not see this mentioned anywhere.  Were gradients passed through from the PIM?  Was the same network used for both the simulation and real-world data?

2)  The performance of the center-based model in Table 1 seems surprisingly low.  The center-based model should be as good at the Train core, Fix traj. enc. model, since it has access to the ball’s position.  Why is it worse?  Is the VIM at fault?  Or is the sphere-fitting sub-optimal?  How does it compare on the simulated data with ground truth physical parameters?

3)  Lastly, the color-scheme is a bit confusing.  It looks like the foam ball in the videos was rainbow-colored.  However, in the model outputs in trajectory figures time is also rainbow-colored.  This was initially a bit confusing.  Perhaps grayscale for the model outputs would be clearer.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByekAQQo6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author Response for AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJxssoA5KX&amp;noteId=ByekAQQo6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper651 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper651 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for their feedback. We address the concerns of the reviewer below.

1) “The authors are introducing both a new training paradigm (to my knowledge unused in the literature) and a new model, and without any existing baselines to compare against I find it a bit difficult to understand how well the model works.”
        We agree that due to the novelty of our training paradigm, model and data, there is a lack of existing literature/baselines to compare against. This is an unavoidable challenge we face. However, in order to better provide context for the performance of our models, we have conducted extensive quantitative and qualitative experiments and compared to relevant baselines (as also noted by other reviewers) including: (a) experiments dissecting the proposed model to localize the performance gains obtained due the PointNet trajectory encoders; (b) training the PIM on real-world data; and (c) a ground truth normals based experiment for reference. Overall, we hope that our proposed approach can also serve as a useful baseline for future work in this direction. 

2) “Overall, the authors’ model is somewhat complicated and not as general as it initially seems.  To justify this complication I would like to see more convincing results and benchmarking or application to more than one single dataset (e.g. non-spheres bouncing).”
        As previously noted by the reviewer, prior work along the lines of estimating physical parameters and learning models of physics from real-world data is extremely scarce. Therefore, there are no relevant datasets that can directly be used to benchmark our approach, which also emphasizes the need for such a dataset. 
In the nascent stages of this field, we believe that addressing the problem with a spherical probe object provides a good starting point. Non-spherical probe objects introduce additional complexity making exploration in this direction more challenging. For example, results in [a] show how much the physical properties vary across the surface of an object. The controlled setup of a spherical probe object ensures that the outcomes of bounces are dependent only on the physical properties of one object. However, we agree that non-spherical probe objects could definitely be an interesting and essential next step to pursue as future work. 

Specific concerns:
1) “A link to open source version of dataset is not available”
        The double-blind submission of ICLR constrains the ability for us to provide the dataset publicly without revealing our identity. The data will be made publicly available with the final version of the paper. 

2) “The authors claim in multiple places that the model is trained end-to-end, but this does not seem to be the case.  Specifically, the PIM is pre-trained on an auxiliary dataset from simulation.  The trajectory encoder also seems to be pre-trained (though I could be wrong about that, see my question below).  Furthermore, there is a bit of hand-holding:  The PIM uses ground-truth state for pre-training, and the VIM gets the ground-truth bounce location.  In light of this, the model seems a lot less general and end-to-end than implied in the abstract and introduction.”
        The PIM (including the trajectory encoder) is pretrained initially using simulation data. The VIM+PIM pipeline is then finetuned in end-to-end manner on the real data. In the abstract/introduction, we refer to this end-to-end training. It is true that the PIM uses simulation parameters in the pretraining phase and the VIM uses the ground truth location to index the feature maps. However, the training is still “end-to-end”  in the conventional usage of the term, since the model is fully differentiable and the gradients for the objective in Equation 3 are computed w.r.t all the parameters of both the VIM and PIM. This is analogous to pretraining on ImageNet and finetuning with added parameters for other tasks which is also referred to as end-to-end training.  


(Continued below)

</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1eXfEXiaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>(continuation of ) Author Response for AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJxssoA5KX&amp;noteId=r1eXfEXiaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper651 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper651 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">3) “The authors could evaluate their model with respect to pixel loss (after ground-truth rendering) and compare to a video prediction algorithm (such as PredNet by Lotter, Kreiman, &amp; Cox, 2016).”
        The goal of our work was to investigate whether real-world data can be used to learn models of physics and also simultaneously estimate physical parameters in real-world scenes. We do not, however, deal with the realistic rendering of the predicted outputs from the learned physics model. Therefore, we cannot directly compare to future-prediction models like PredNet [Lotter et al], since we do not predict the pixels in the future frames. 

4) “I would love to see training curves with errorbars of the models on the most important metrics (e.g. Dist and COR Median Absolute Error)”
        We have now computed the error bars for the Forward prediction distance error and COR Median absolute error over multiple training/testing runs with different initializations. These results confirm the conclusions of our ablative study.
Experiment			Dist (Mean, Std)		COR Med Abs Err (Mean, Std)
Center based			28.2, 0.005			0.173, 0.01
Fix core and traj. enc. 	        38.4, 0.008			0.258, 0.008
Train core and traj. Enc.	24.7, 0.004			0.169, 0.006
Train core, Fix traj. Enc.	21.9, 0.006			0.158, 0.01


Clarifications:
1) “How was the PointNet trajectory encoder trained? Were gradients passed through from the PIM?  Was the same network used for both the simulation and real-world data?”
        Yes, the PointNet trajectory encoder is actually part of the PIM in our proposed approach. The gradients for the trajectory encoder are computed with respect to the objectives mentioned in Equations (2) and (3). 
Yes, the same network is used for simulation and real-world data.

2) “The performance of the center-based model in Table 1 seems surprisingly low. Is the VIM at fault?  Or is the sphere-fitting sub-optimal?”
        In theory, if accurate centers and point clouds are available, both models should perform similarly. The sphere-fitting in our data is sub-optimal due to the noise in the stereo-depth estimates. We believe that this highlights the advantage of using a PointNet-based model to avoid dealing with hand-crafted estimates of centers.

[a] Jui-Hsien Wang, Rajsekhar Setaluri, Dinesh K. Pai, and Doug L. James. Bounce maps: An improved restitution model for real-time rigid-body impact. ACM Transactions on Graphics (Proceedings of SIGGRAPH 2017), 36(4), July 2017. doi: <a href="https://doi.org/10.1145/3072959.3073634." target="_blank" rel="nofollow">https://doi.org/10.1145/3072959.3073634.</a>
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_HkegBTQcnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>a well evaluated solution to an interesting and challenging problem</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJxssoA5KX&amp;noteId=HkegBTQcnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper651 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper651 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a method for inferring physical properties of the world (specifically, normals and coefficients of restitution) from both visual and dynamic information.  Objects are represented as trajectories of point clouds used under an encoder/decoder neural network architecture.  Another network is then learned to predict the post bounce trajectory representation given the prebounce trajectory representation given the surface parameters.  This is used both to predict the post bound trajectory (with a forward pass) but also to estimate the surface parameters through an optimization procedure.  This is coupled with a network which attempts to learn these properties from visual cues as well.  This model can be either pretrained and fixed or updated to account for new information about a scene.

The proposed model is trained on a newly collected dataset that includes a mixture of real sequences (with RGB, depth, surface normals, etc) and simulated sequences (additionally with physical parameters) generated with the help of a physics engine.  It is compared with a number of relevant baseline approaches and ablation models.  The results suggest that the proposed model is effective at estimating the physical properties of the scene.

Overall the paper is well written and thoroughly evaluated.  The problem is interesting and novel, the collected dataset is likely to be useful and the proposed solution to the problem is reasonable.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1gfIEmspm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author Response for AnonReviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJxssoA5KX&amp;noteId=H1gfIEmspm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper651 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper651 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for their time and appreciation of our work.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ryeBDwsMs7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Great work; Important and interesting problem; Missing some details</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJxssoA5KX&amp;noteId=ryeBDwsMs7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper651 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper651 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Paper summary:
The paper proposes to predict bouncing behavior from visual data. The model has two main components: (1) Physics Interface Module, which predicts the output trajectory from a given incoming trajectory and the physical properties of the contact surface. (2) Visual Interface Module, which predicts the surface properties from a single image and the impact location. A new dataset called Bounce Dataset is proposed for this task.

Paper strengths:
- The paper tackles an interesting and important problem.
- The data has been collected in various real scenes.
- The idea of training the physics part of the network with synthetic data and later fine-tuning it with real images is interesting.
- The experiments are thorough and well-thought-out.

Paper weaknesses:
- It would be more interesting if the dataset was created using multiple types of probe objects. Currently, it is only a ball.

- It is not clear how the evaluation is performed. For instance, the length of the groundtruth and predicted trajectories might be different. How is the difference computed?

- The impact location (x,y) corresponds to multiple locations in 3D. Why not using a 3D point as input? It seems the 3D information is available for both the real and synthetic cases.

- Why is it non-trivial to use a deconvolution network for predicting the output point cloud trajectory?

- The length of the input trajectory can vary, but it seems the proposed architecture assumes a fixed-length trajectory. I am wondering how it handles a variable-length input.

- How is the bounce location encoded in VIM?

- I don't see any statistics about the objects being used for data collection. That should be added to the paper.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJe2hHXj6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author Response for AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJxssoA5KX&amp;noteId=SJe2hHXj6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper651 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper651 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for their appreciation of our work. We address the reviewer’s concerns here:

1) “It would be more interesting if the dataset was created using multiple types of probe objects. Currently, it is only a ball.”
        We agree that the eventual goal for research in this direction should be to generalize to multiple types of probe objects. We discuss this further in the response to the review from AnonReviewer2. (<a href="https://openreview.net/forum?id=BJxssoA5KX&amp;noteId=ByekAQQo6Q" target="_blank" rel="nofollow">https://openreview.net/forum?id=BJxssoA5KX&amp;noteId=ByekAQQo6Q</a> )

2)“The length of the groundtruth and predicted trajectories might be different. How is the difference computed?”
        The evaluation is not dependent on the length of the trajectories recorded. The distance between the predicted center and the ground-truth center is computed at timestep 10 (0.1 seconds post-bounce). All trajectories in the dataset have length greater than 10 timesteps.

3) “The impact location (x,y) corresponds to multiple locations in 3D. Why not using a 3D point as input? It seems the 3D information is available for both the real and synthetic cases.”
        In the physics model, the 3D collision point is currently used since the point cloud is represented with collision as origin. In the VIM model, using the 3D points is similar to using a 2D (x,y) points since we eventually need to extract visual features from 2D input images.

4) “Why is it non-trivial to use a deconvolution network for predicting the output point cloud trajectory?”
        There is very limited work on generating point clouds from embeddings. Integrating a deconvolution model would have added an additional obstacle to an already challenging problem. Furthermore, it would make localizing the errors more difficult.

Some relevant literature that demonstrate the challenges of generating point clouds:
[1] Achlioptas, Panos, et al. "Representation learning and adversarial generation of 3D point clouds." arXiv preprint arXiv:1707.02392 (2017).
[2] Insafutdinov, Eldar, and Alexey Dosovitskiy. "Unsupervised Learning of Shape and Pose with Differentiable Point Clouds." arXiv preprint arXiv:1810.09381 (2018).
[3] Lin, Chen-Hsuan, Chen Kong, and Simon Lucey. "Learning efficient point cloud generation for dense 3D object reconstruction." arXiv preprint arXiv:1706.07036 (2017).
[4] Achlioptas, Panos, et al. "Learning Representations and Generative Models for 3D Point Clouds." (2018).


5) “The length of the input trajectory can vary, but it seems the proposed architecture assumes a fixed-length trajectory. I am wondering how it handles a variable-length input.”
        We observed that 10 frames before and after the collision contain sufficient information. Therefore, we used these 20 frames in the proposed model. For videos where more frames are available, we use only the 10 frames before and after collision. 

6) “How is the bounce location encoded in VIM?”
        The bounce location is used to index the feature map which is the output of the VIM. We present this in Subsection 3.2 “Training” paragraph - $\rho_{x,y}$ is obtained by indexing the output $\mathcal{V}(I)$.

7) “I don't see any statistics about the objects being used for data collection. That should be added to the paper.”
        Thank you for the suggestion. That would indeed be informative. We shall add this to the final version of the paper since this would require some additional effort to label the objects. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>