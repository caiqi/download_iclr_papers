<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>LARGE BATCH SIZE TRAINING OF NEURAL NETWORKS WITH ADVERSARIAL TRAINING AND SECOND-ORDER INFORMATION | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="LARGE BATCH SIZE TRAINING OF NEURAL NETWORKS WITH ADVERSARIAL TRAINING AND SECOND-ORDER INFORMATION" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1lnJ2Rqt7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="LARGE BATCH SIZE TRAINING OF NEURAL NETWORKS WITH ADVERSARIAL..." />
      <meta name="og:description" content="Stochastic Gradient Descent (SGD) methods using randomly selected batches are widely-used to train neural network (NN) models. Performing design exploration to find the best NN for a particular..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1lnJ2Rqt7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>LARGE BATCH SIZE TRAINING OF NEURAL NETWORKS WITH ADVERSARIAL TRAINING AND SECOND-ORDER INFORMATION</a> <a class="note_content_pdf" href="/pdf?id=H1lnJ2Rqt7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019large,    &#10;title={LARGE BATCH SIZE TRAINING OF NEURAL NETWORKS WITH ADVERSARIAL TRAINING AND SECOND-ORDER INFORMATION},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=H1lnJ2Rqt7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Stochastic Gradient Descent (SGD) methods using randomly selected batches are widely-used to train neural network (NN) models. Performing design exploration to find the best NN for a particular task often requires extensive training with different models on a large dataset,  which is very computationally expensive. The most straightforward method to accelerate this computation is to distribute the batch of SGD over multiple processors. However, large batch training often times leads to degradation in accuracy, poor generalization, and even poor robustness to adversarial attacks.  Existing solutions for large batch training either do not work or require massive hyper-parameter tuning. To address this issue, we propose a novel large batch training method which combines recent results in adversarial training (to regularize against ``sharp minima'') and second order optimization (to use curvature information to change batch size adaptively during training). We extensively evaluate our method on Cifar-10/100, SVHN, TinyImageNet, and ImageNet datasets, using multiple NNs, including residual networks as well as compressed networks such as SqueezeNext.  Our new approach exceeds the performance of the existing solutions in terms of both accuracy and the number of SGD iterations (up to 1\% and $3\times$, respectively). We emphasize that this is achieved without any additional hyper-parameter tuning to tailor our method to any of these experiments.
</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">adversarial training, large batch size, neural network</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Large batch size training using adversarial training and second order information</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_S1xtFQylaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clearly an unfinished paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1lnJ2Rqt7&amp;noteId=S1xtFQylaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1021 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1021 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose using information from the Hessian to grow the batch size as the training progresses. It is well-known that larger batch sizes can be used for later stages of optimization (ie, <a href="https://arxiv.org/abs/1711.00489," target="_blank" rel="nofollow">https://arxiv.org/abs/1711.00489,</a> https://arxiv.org/abs/1706.05699), but they are missing motivation as to why use Hessian information for this.

Furthermore, the description of the algorithm is lacking detail and is essentially unreproducible in current form.

The main description of their method is Algorithm 1 box, which suggests to grow batch size when "eigenvalue" is much smaller than previous eigenvalue. Is that the top eigenvalue? How is it estimated? Why is that the criterion? Note that for stochastic least squares problem one benefits from later batch sizes in later stages of optimization even though Hessian doesn't change.

In section Section 4.3 they start talking a bit about computing Hessian, referring to non-existent figure 6 for details of block approximation.

Authors mention that Hessian computation is not supported in major frameworks but don't provide explanation of how they compute it (did they not use a major framework for ImageNet experiments?).

Note that a single row of Hessian (hence full Hessian) can be computed in all major frameworks by differentiating an element of the gradient. IE, in PyTorch https://gist.github.com/apaszke/226abdf867c4e9d6698bd198f3b45fb7, and also eigenspectrum of Hessian can be approximated -- https://github.com/noahgolmant/pytorch-hessian-eigenthings</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Bkl9upa53m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting work, but the theoretical part is not strong enough</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1lnJ2Rqt7&amp;noteId=Bkl9upa53m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1021 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1021 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies the large batch size training of neural networks, and incorporates adversarial training and second-order information to improve the efficiency and effectiveness of the proposed algorithm. In particular, the authors use second-order information to automatically generate the step size and batch size in each iteration, and apply adversarial training as a regularization method to improve the test performance. Finally, the authors demonstrate their algorithm and compare it with the baseline algorithms on a wide range of datasets. This paper is clearly written and has the following strength:

1.	This paper proposes an adaptive method for SGD training, which proves its convergence for strongly convex optimization.
2.	This paper incorporates the adversarial training and robust optimization into the adaptive SGD training, and shows that this combination significantly improves the test performance.
3.	The authors perform experiments on different datasets, which show that the proposed method enjoys less training time and higher accuracy when using large batch size.

However, this paper also has the following weakness:

1.	The theoretical analysis is somewhat trivial, and the assumption on the objective function is rather strong, which is not consistent with the nonconvex loss functions that are widely applied in training neural networks.
2.	Theorem 1 provides convergence rate of SGD on strongly convex objective functions. However, the authors do not carefully characterize the learning rate to ensure that the loss function achieve \epsilon-accuracy. Moreover, in order to make the last term in (5) be smaller than \epsilon, the learning rate \eta_0 should be in the order of O(\epsilon), which is no longer a tuning-free parameter.
3.	The authors mention that the proposed algorithm converges faster than basic SGD, but it is not clearly demonstrated from Theorem 1.
4.	I am confused about how to determine the number of iterations for different algorithm as shown in Table 1s and 2? Do you stop each algorithm when they attain the same training error on the training dataset?
5.	It is also confused that the number of iterations for ABS and ABSA are relatively larger than that of BL (Tables 1 and 2), but the training time of BL is longer than those of ABS and ABSA as reported in Table 3?
6.	Some minor flaws. In (9) it should be \|\nabla L(\Theta)\|^2; in Lemma 3, the expectation on the left side should be taken conditioned on \theta_t.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SkxjN0yHsQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Promising numerical results, but lacks clear description and explanation of the algorithm</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1lnJ2Rqt7&amp;noteId=SkxjN0yHsQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1021 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1021 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Based on my understanding, this paper describes a novel approach for addressing the large batch training problem. The authors propose increasing the batch size based on reductions in the largest eigenvalue of the Hessian. This is combined with adversarial training using the fast gradient sign method to reduce the total number of iterations required for training and improve generalization performance. Unfortunately, although the numerical results seem quite promising, the algorithm and its explanation and details are not described clearly in the paper, which makes me lean towards rejection. I describe this more fully below:

1. Description of the Algorithm

The description of the algorithm in Section 3.1 is simply not clear, and lacks clear exposition motivating why the algorithm ought to work. To add to this confusion, there appear to be some inconsistencies between the (brief) description of the method and the description given in the Main Contributions and Limitations section in the Introduction. 

As an example, in Section 3.1, the approach for computing the eigenvalue of the Hessian is not described. Which eigenvalue is computed? How is this done? What is the batch size used in this computation? Is it computed over the full training set? The Limitations section briefly describes this (power iteration to tolerance &lt;= 10^-2), but this should be elaborated on in Section 3.1. In fact, the limitations should not be discussed until a clear description of the algorithm is given.

The introduction makes this even more confusing by claiming the second order information is computed by “backpropagating the Hessian operator”. This seems to imply that the 3rd derivative information is computed for second-order information. Later in the Introduction, the authors claim to use Hessian matvecs to perform the power iteration. I believe that the authors mean that the Hessian-vector product is obtained by differentiating the product g’v (a scalar quantity). 

In addition, it was not described how the learning rate is changed in the algorithm. Later in the experiments, none of the additional hyperparameters in the procedure are given, such as the duration factor, kappa, the hyperparameters in the adversarial training, and more. This all ought to be included for completeness.

2. Questions about Details of the Algorithm

If it is indeed the case that the authors are using power iteration to compute the largest eigenvalue, why not use Lanczos method as it typically works better for symmetric matrices? In addition, if the intention was to compute the largest eigenvalue of the Hessian, one must be wary that the power iteration/Lanczos method computes the eigenvalue with largest magnitude (the absolute value of lambda), which may mean that it’s possible that the algorithm is utilizing negative curvature information rather than positive curvature information (particularly in the earlier epochs), which may contradict their intuition based on flat minima. This needs to be addressed.

Secondly, there is no explanation as to why increasing the batch size would lead to consistent decrease in the eigenvalues of the Hessian. This is certainly not true for all optimization problems. Even if the flat minima/sharp minima hypothesis is assumed, is it possible for the iterates after increasing the batch size to still tend towards sharper minimizers after being in a flat region? This intuition and explanation needs to be expanded on (and argued for) in order for the algorithm to make any conceptual sense.

Lastly, why is the duration factor needed to increase the batch size if the eigenvalue condition fails? if the duration factor is removed, how does the batch size evolve? Is it necessary? How is the duration factor tuned?

3. Inconsequential Theoretical Results

The authors also prove a theorem bounding the expected optimality gap with adaptive batch sizes. On closer look, this is a simple adaptation of the result by Bottou, Curtis, and Nocedal [2] and does not utilize any of the algorithmic mechanisms described in the paper. Hence, the theoretical result is not novel, does not provide any additional insight on the algorithm, and could be applied to any adaptive/changing batch size SG algorithm. In my opinion, this ought to be removed. (Assumption 2 is also mentioned in the main paper, but is only described in the Appendix.)

4. Additional Considerations

The paper is missing much work done by Nocedal’s group on increasing batch sizes (some of which utilize the L-BFGS approximation to the Hessian); see [1, 3].

Other relevant work by Sagun, Bengio, and others on large batch training, flat minima, and the Hessian in deep learning ought to be included as well; see [4-7]. 

Lastly, the algorithm demonstrates some significant improvements on the number of iterations. However, efficiency with respect to epochs is not discussed. It may make sense to plot test loss/error against epochs and batch size against iterations for clarity.

Typos/Grammatical Errors:
- Page 2: Should not state “(We refer to this method as ABS)”, easier to include by including (ABS) after Adaptive Batch Size in the beginning of the bullet point.
- Page 6: Section 4: “information” not “informatino”
- Page 6: Section 4: “the” not “teh”
- Page 7: Section 4.1: “confirms” not “confirming”
- Page 7: Section 4.1: no “a” in “a very consistent performance”

Summary:

Overall, although the paper presents some promising numerical results, it lacks a detailed description and explanation of the algorithm to be worthy of publication. It leaves many aspects of the algorithm open to the reader’s interpretation, and I do not believe I could reproduce the results with the information provided. The manuscript needs significant changes to the detail, structure, and writing before it can be considered for publication.

References:
[1] Bollapragada, Raghu, et al. "A progressive batching L-BFGS method for machine learning." arXiv preprint arXiv:1802.05374(2018).
[2] Bottou, Léon, Frank E. Curtis, and Jorge Nocedal. "Optimization methods for large-scale machine learning." SIAM Review 60.2 (2018): 223-311.
[3] Byrd, Richard H., et al. "Sample size selection in optimization methods for machine learning." Mathematical programming134.1 (2012): 127-155.
[4] Chaudhari, Pratik, et al. "Entropy-sgd: Biasing gradient descent into wide valleys." arXiv preprint arXiv:1611.01838(2016).
[5] Jastrzębski, Stanisław, et al. "DNN's Sharpest Directions Along the SGD Trajectory." arXiv preprint arXiv:1807.05031(2018).
[6] Sagun, Levent, et al. "Empirical Analysis of the Hessian of Over-Parametrized Neural Networks." arXiv preprint arXiv:1706.04454 (2017).
[7] Zhu, Zhanxing, et al. "The Regularization Effects of Anisotropic Noise in Stochastic Gradient Descent." arXiv preprint arXiv:1803.00195 (2018).</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>