<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learning Preconditioner on Matrix Lie Group | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learning Preconditioner on Matrix Lie Group" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Bye5SiAqKX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learning Preconditioner on Matrix Lie Group" />
      <meta name="og:description" content="We study two types of preconditioners and preconditioned stochastic gradient descent (SGD) methods in a unified framework. We call the first one the Newton type due to its close relationship to..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Bye5SiAqKX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning Preconditioner on Matrix Lie Group</a> <a class="note_content_pdf" href="/pdf?id=Bye5SiAqKX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 13 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019learning,    &#10;title={Learning Preconditioners on Lie Groups},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Bye5SiAqKX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=Bye5SiAqKX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We study two types of preconditioners and preconditioned stochastic gradient descent (SGD) methods in a unified framework. We call the first one the Newton type due to its close relationship to Newton method, and the second one the Fisher type as its preconditioner is closely related to the inverse of Fisher information matrix. Both preconditioners can be derived from one framework, and efficiently learned on any matrix Lie groups designated by the user using natural or relative gradient descent. Many existing preconditioners and methods are special cases of either the Newton type or the Fisher type ones. Experimental results on relatively large scale machine learning  problems are reported for performance study. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">preconditioner, stochastic gradient descent, Newton method, Fisher information, natural gradient, Lie group</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a new framework for preconditioner learning, derive new forms of preconditioners and learning methods, and reveal the relationship to methods like RMSProp, Adam, Adagrad, ESGD, KFAC, batch normalization, etc.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_r1lQAt9vT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>List of revisions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bye5SiAqKX&amp;noteId=r1lQAt9vT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper109 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper109 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1, Section 4.4, add a note on preconditioned gradient norm clipping and its relationship to trust region method due to its importance in practice.

2, Section 6.3, add a few comments on complexity comparison with KFAC. KFAC requires inversion of symmetric matrices, and thus might fail to scale up to large scale problems since generally, it is difficult to efficiently inverse a matrix in parallel (to our knowledge and experiences). Our methods require back substitution, which is as computationally cheap as matrix multiplication on GPU (given enough resources for parallelization). Thus, our methods could scale up to large scale problems.  

Please check code (in our pytorch implementation, misc dir)
<a href="https://github.com/lixilinx/psgd_torch/blob/master/misc/benchmark_mm_trtrs_inv.py" target="_blank" rel="nofollow">https://github.com/lixilinx/psgd_torch/blob/master/misc/benchmark_mm_trtrs_inv.py</a>
for details. For linear system with dimension 1024, back substitution is about 300 times faster than matrix inversion on 1080 ti GPU. For dimension 8192, back substitution is about 2000 times faster.  

3, Sections 7.2 and 7.3, fine tune performance of SGD, momentum and Adam, especially on the language modeling task since momentum and Adam perform poorly due to the sparsity of gradients. For this task, we found that:

Momentum: diverge when step size &gt; =0.2; converges when step size &lt;= 0.05. Convergence is too slow. Then we tried to clip the updates as in clipped SGD method to avoid divergence when large step size is used. Still, momentum method performs the worst.
 
Adam: Fine tune its damping factor improves performance. Its performance is rather sensitive to the damping factor. As the momentum method, Adam also destroys the sparsity of gradients and performs not so well.

Sparse Adam: it only updates the 1st and 2nd moments and model parameters when their corresponding gradients are not zeros. This slightly improves performance, but still far from SGD’s performance.
 
One may argue that SGD is a special case of momentum and Adam, and thus they should perform as well as SGD after fine tuning all their parameters. Well, we do not agree for two reasons. First, jointly fine tuning all these parameters are too expensive. For example, Adam has four parameters to tweak. Second, as their names suggest, certain parameters are expected to have their typical values. For example, a momentum method with momentum 0 is just SGD, but not a typical momentum method.

4, Appendix A, a short note showing that our preconditioners preserve the sparsity of gradients in the language modeling task. 

5, Scattered minor revisions and clarifications in the text. Many are due to reviews’ comments, and we appreciate it.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1xklEYyaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>natural gradient can be derived from different metrics</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bye5SiAqKX&amp;noteId=B1xklEYyaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper109 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper109 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Just a quick response to AnonReviewer1's comment stating that 'I've never heard of the natural gradient being defined using a different metric than the Fisher metric'. This is not true. Please check Amari's classic paper, Natural Gradient Works Efﬁciently in Learning, sections 3.3, 3.4, 7, and 8, for examples of natural gradient on Lie groups. 

Actually, considering that some readers might not be familiar with natural gradient, we have a note at the end of section 4.1 of our paper to remind the difference between a natural gradient derived from the Fisher metric and a natural gradient derived from a tensor metric. 

We thank reviewers for their time and efforts, and will improve our paper accordingly.


</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hyg6oT30h7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>explanation could use more work, but a solid idea that seems to work in practice</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bye5SiAqKX&amp;noteId=Hyg6oT30h7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper109 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 08 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper109 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=Hyg6oT30h7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Author proposes general framework to use gradient descent to learn a preconditioner related to inverse of the Hessian, or the inverse of Fisher Information matrix, where the inverse may take a particular form, ie, Kronecker-factored form like in KFAC. I have tracked down the implementation of this method by author from earlier paper Li 2018 and verified that it works and speeds up convergence of convolutional networks in terms of number of iterations needed. In particular, Kronecker Factored preconditioner using approach in the paper worked better in terms of wall-clock time on MNIST LeNet5, comparing against an existing PyTorch implementation of KFAC from César Laurent.


Some comments on the paper:

Section 2
The key seems to be equation 8. The author provides loss function, the minimum is what is achieved by inverse of the Hessian. Given the importance of the formula, it feels like proof should be included (perhaps in Appendix).

Justification of the criterion is relegated to earlier work in Li (<a href="https://arxiv.org/pdf/1512.04202.pdf)," target="_blank" rel="nofollow">https://arxiv.org/pdf/1512.04202.pdf),</a> but I failed to fully grasp the motivation. There are simpler criteria being introduced, such as criterion 1, equation 17, which simply minimizes the difference between predicted gradient delta and observed, why not use that criterion?

The justification is given that using inverse Hessian may "amplify noise", which I don't buy. When using SGD to solve least-square regression, dividing by Hessian does not have a problem of amplifying noise, so why is this a concern here?


Section 3

The paper should make it clear that empirical Fisher matrix is used, unlike "unbiased estimate of true Fisher" which used in many natural gradient papers.

Section 4
Is "Lie group" used anywhere in the derivations? It seems the same algebra holds even without that assumption. The motivation for using "natural gradient for learning Q" seems to come from Amari. I have not read that paper, how important it is to use the "natural" gradient for learning Q? What if we use regular gradient descent for Q?

Section 7
Figure 1 showed that Fisher-type criterion didn't work for toy problem, it would be more informative if it used square root of Fisher-type criterion. The square root comes out of regret-analysis (ie, AdaGrad uses square root of gradient covariance)
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HklSR4lZpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Responses to AnonReviewer3’s comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bye5SiAqKX&amp;noteId=HklSR4lZpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper109 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper109 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Comment 1]: The key seems to be equation 8. ...it feels like proof should be included...
[Response 1]: We believe this equation is thoroughly studied in Li’s work.
    

[Comment 2]: Justification of the criterion
[Response 2]: Let us consider three cases to compare these criteria.
 
Case 1, noiseless gradient, positive definite Hessian. All preconditioners in Li’s work are equivalent, leading to the same secant equation, delta g = H * delta x.
 
Case 2, noiseless gradient, indefinite Hessian. Only criterion c3 in Li’s work can guarantee positive definiteness of the preconditioner. One may point out that other criteria can yield positive definite preconditioner under Wolfe conditions. But the resultant preconditioner is remotely related to the Hessian. We are seeking a preconditioner whose eigenvalues are the inverse of the absolute eigenvalues of Hessian to precondition the Hessian perfectly.
      
Case 3, noisy gradient, positive definite or indefinite Hessian. Only criterion c3 in Li’s work leads to a preconditioner that still corresponds to the secant equation delta g = H * delta x for math optimization, i.e., eq. (9) shown in our paper.

      
[Comment 3] The justification is given that using inverse Hessian may "amplify noise", which I don't buy...
[Response 3]: Gradient noise amplification may cause little concern for well-conditioned problems. Your least-square regression problem might fall into this case. But it could lead to divergence for ill-conditioned problems, e.g., learning recurrent networks requiring long term memory. Fig. 6 in Li’s work shows one such example. 

Using SGD as the base line, a good preconditioner actually suppresses the gradient noise. Our Tensorflow implementation gives one RNN training example using batch size 1. SGD fails to converge with batch size 1, although it converges with much large batch sizes. Our methods converge well with batch size 1 since the preconditioners also suppress gradient noises.

  
[Comment 4]: The paper should make it clear that empirical Fisher matrix is used...
[Response 4]: We will emphasize it in the revised paper. We already emphasized it in our implementation packages. 


[Comment 5]: Is "Lie group" used anywhere in the derivations...
[Response 5]: These are great questions. We choose to learn the preconditioner on Lie group due to our years of practices in neural network training. Properties of Lie group are repeatedly exploited by our methods. For example, you mentioned that ‘same algebra holds even without that assumption’. Well, it is true because Q and Q + delta Q are already on the same Lie group. Otherwise, this is not necessarily true. For example, if you constrain Q to be a band matrix, generally, you may not able to write delta Q as -(step size)*R*Q, where R is a band matrix similar to Q.

Why natural gradient? Once we decide to learn the preconditioner on the Lie group, then gradient on the Lie group is just the natural gradient derived from a tensor metric. On the theoretical aspects, both Amari and Cardoso give a lot of justifications for natural gradient, i.e., equivariant property, fast convergence, etc. In practice, it helps a lot as we can use normalized step size to update the preconditioner. We rarely feel the need to tune this step size (0.01 as default value and works well).  

Can we use regular gradient descent? Let us consider two cases.

Case 1, Q is on a Lie group. Yes, we can use regular gradient descent. But the updating step size may require fine tuning for each specific problem. Convergence could be slow when initial values for Q is either too large or too small. Precautions are required to prevent Q converging to singular matrices.   

Case 2, Q is not on any Lie group. Regular gradient descent still works. Similar difficulties are: how to choose the updating step size; how to determine the initial value. For example, the authors have considered preconditioner with form P = (scalar)*I + U*U^T. For math optimization, we already know how to update this preconditioner (limited-memory BFGS). For stochastic optimization, the authors failed to find an efficient and yet tuning-free updating methods for such preconditioner. However, we do not exclude the existence of such preconditioner updating methods. 


[Comment 6]: ...it would be more informative if it used square root of Fisher-type criterion...
[Response 6]: We used square root Fisher-type preconditioner. We will clarify it in the revised paper. 

By the way, our Pytorch implementation gives demo showing the usage of both square root and regular Fisher type preconditioners. For small scale problems like MNIST, the Fisher type preconditioner may perform better. For large scale problems, the square root Fisher type preconditioner seems more numerically robust, less picky on the damping factor. So we use the square root Fisher type preconditioner in experiment 2 and 3.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJexSy6s2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Possibly interesting ideas but badly presented and justified, and with poor experimental design</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bye5SiAqKX&amp;noteId=rJexSy6s2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper109 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper109 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a preconditioned SGD method where the preconditioner is adapted by performing some type of gradient descent on some secondary objective "c".  The preconditioner lives in one of a restricted class of invertible matrices (e.g. symmetric, diagonal, Kronecker-factored) constituting a Lie group (which is where the title comes from). 

I think the idea of designing a preconditioner based on considerations of gradient noise and as well as the Hessian is interesting. However most of that work was done in the Li paper, and including the design of "c".  This paper's contribution seems to be to work out some of the details for various restricted classes of matrices, to construct a "Fisher version" of c, and to run some experiments. 

The problem is that I don't really buy the original motivation for the "c" function from the Li paper, and the newer Fisher version of c proposed in this paper doesn't seem to have any justification at all.  I also find that the paper in general doesn't do a good job of explaining its various choices when designing the algorithm.  This could be somewhat forgiven if the experimental results were strong, but unfortunately they are too limited, and marred by overly-simplistic baselines that aren't properly tuned.


More detailed comments below

Title:

I think the title is poorly chosen.  The paper doesn't use Lie groups or their properties in any significant way, and "learning" is a bad choice of words too, since it involves generalization etc (it's not merely the optimization of some function).  A better title would be "A general framework for adaptive preconditioners" or something.

Intro:

Citation of Adagrad paper is broken

The literature review contained in the intro needs works. I wouldn't call methods like quasi-Newton methods "convex optimization methods".  Those algorithms were around a long time ago before "convex optimization" was a specific topic of study and are probably *less* associated with the convex optimization literature than, say, Adagrad is. And methods like Adagrad aren't exactly first-order methods either. They use adaptively chosen preconditioners (that happen to be diagonal) which puts them in a similar category to methods like LBFGS, KFAC, etc.

It's not clear at this point in the paper what it means for a preconditioner to be "learned on" something.  

Section 2:

The way you discuss quadratic approximations is confusing.  Especially  the sentence "is the sum of approximation error and constant term independent of theta" where you then go on to say that a_z does depend on theta.  I know that this second theta is the "current theta" separate from the theta as it appears in the formula for the approximation but this is really sloppy. Usually people construct the quadratic approximation in terms of the *change in theta* which makes such things cleaner.

You should explain how eqn 8 was derived since it's so crucial to everything that follows.  Citing a previous paper with no further explanation really isn't good enough here.  Surely with all of the notation you have already set up it should be possible to motivate this criterion somehow. The simple fact that it recovers P = H^-1 in the noiseless quadratic case isn't really good enough, since many possible criteria would do the same.

I've skimmed the paper you cited and their justification for this criterion isn't very convincing.  There are other possible criteria that they give and there doesn't seem to be a strong reason to prefer one over the other.


Section 3:

The way you define the Fisher information matrix corresponds to the "empirical Fisher", since z includes the training labels.  This is different from the standard Fisher information matrix.

How can you motivate doing the "replacement" that you do to generate eqn 12? Replacing delta theta with v is just notation, but how can you justify replacement of delta g with g + lambda v?  This isn't a reasonable approximation in any sense that I can discern. Once again this is an absolutely crucial step that comes out of nowhere.  Honestly it feels contrived in order to produce a connection to popular methods like Adam.

Section 4: 

The prominent use of the abstract mathematical term "Lie group" feels unnecessary and like mathematical name-dropping. Why not just talk about certain "classes" of invertible matrices closed under standard operations (which would also help people that don't know what a Lie group is)?  If you are going to invoke some abstract mathematical framework like Lie groups it needs to actually help you do something you couldn't otherwise. You need to use some kind of advanced Theorem for Lie groups. 

Without knowing the general form of R equation 18 is basically vacuous. *any* matrix (in the same class) could be written this way.

I've never heard of the natural gradient being defined using a different metric than the Fisher metric.  If the metric can be arbitrary then even standard gradient descent is a "natural gradient" too (taking the Euclidean metric).  You could argue for a generalized definition that would include only parametrization independent metrics, but then your particular metric wouldn't obviously work.


Section 6:

Rather than comparing to Batch Normalization you would be better off comparing to the old centering and normalization work of Schraudolph et al which the former was based on, which is actually a well-defined preconditioner.

Section 7: 

You really need to sweep over the learning rate parameters for optimizers like SGD with momentum or Adam.   Otherwise the comparisons aren't very interesting. 

"Tikhonov regularization" should just be called L2-regularization

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hyxv3jbM6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Responses to AnonReviewer1’s comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bye5SiAqKX&amp;noteId=Hyxv3jbM6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper109 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper109 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Comment 1]: …most of that work was done in the Li paper…
[Response 1]: Our contributions include: propose a new framework for learning preconditioners on Lie groups; predict useful new preconditioners and optimization methods; reveal its relationships to many existing methods (ESGD, batch normalization, KFAC, Adam, RMSProp, Adagrad); compare Newton and Fisher type preconditioners; implementations and empirical performance study.

[Comment 2]: … I don't really buy the original motivation for the "c" function…doesn't seem to have any justification at all…
[Response 2]: We are willing to know the reasons.

[Comment 3]: …experimental results…too limited…overly-simplistic baselines…aren't properly tuned…
[Response 3]: You may find more comparison results on small scale problems like MNIST related in our implementation packages. The image recognition and NLP tasks considered in the paper are representative, and baselines already achieved reasonable performance. Still, we are willing to fine tune them and update the results during the rebuttal period.

[Comment 4]: …title is poorly chosen…doesn't use Lie groups…in any significant way… "learning" is a bad choice of words…merely the optimization of some function…
[Response 4]: Solving for the optimal preconditioner is a tracking problem since generally the Hessian changes along with parameters, and also an estimation problem due to the existence of gradient noises. So we think ‘learning’ is a proper word. Lie group provides a concise framework for our study, and enables efficient learning via natural gradient descent.

[Comment 5]: I wouldn't call methods like quasi-Newton methods "convex optimization methods"...less associated with the convex optimization literature…
[Response 5]: Quasi-Newton methods are derived assuming nonnegative definite Hessian, and are taught in convex optimization textbooks.

[Comment 6]: Citation of Adagrad paper is broken... methods like Adagrad aren't exactly first-order methods...
[Response 6]: We state that Adagrad is a variation of SGD. We do not state that it is a first-order method.

[Comment 7]: The way you discuss quadratic approximations is confusing...a_z...really sloppy...people construct the quadratic approximation in terms of the *change in theta*...
[Response 7]: We will explicitly point out that a_z only contains higher order approximation errors in the revised paper. 
You can construct quadratic approximation in terms of either theta or the change in theta.

[Comment 8]: You should explain how eqn 8 was derived...Citing a previous paper...isn't good enough…I've skimmed the paper you cited... justification for this criterion isn't very convincing... 
[Response 8]: We believe these topics are thoroughly addressed in the cited paper. This is a conference paper with recommend page length 8. Nevertheless, we reviewed important facts in the background section, e.g., Eq. (9), the correspondence to Newton method regardless of the existence of nonconvexity and gradient noises.

[Comment 9]: The way you define the Fisher information matrix corresponds to the "empirical Fisher"...
[Response 9]: We will emphasize it in the revised paper. We already emphasized it in our implementations.

[Comment 10]: How can you motivate doing the "replacement" …how can you justify replacement of …  This isn't a reasonable approximation in any sense... comes out of nowhere…it feels contrived to…
[Response 10]: The math in the paper is clear. No approximation is involved here.

[Comment 11]: … use of the abstract mathematical term "Lie group" feels unnecessary…mathematical name-dropping…Why not … "classes" of invertible matrices closed under standard operations…You need to use some kind of advanced Theorem for Lie groups.
[Response 11]: Matrix Lie group is the precise term here. We use its properties to design the preconditioners and their learning rules.

[Comment 12]: … equation 18 is basically vacuous…
[Response 12]: It is Amari’s natural gradient or Cardoso’s relative gradient on the Lie group.

[Comment 13]: I've never heard of the natural gradient being defined using a different metric than the Fisher metric…your particular metric wouldn't obviously work.
[Response 13]: Please check Amari’s work on natural gradient.

[Comment 14]: Rather than comparing to Batch Normalization you would be better off comparing to the old centering and normalization work of Schraudolph…
[Response 14]: Please give further details like Schraudolph’s paper, link, code implementation, etc.

[Comments 15]: You really need to sweep over the learning rate parameters for optimizers like SGD...
[Response 15]: We already searched the learning rates in a large range for these methods. We are further refining the results of SGD, momentum and Adam, and update the paper during the rebuttal period.

[Comment 16]: "Tikhonov regularization" should just be called L2-regularization
[Response 16]: We will call it L2-regularization in the revised paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1e9NsdXsm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Solid work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bye5SiAqKX&amp;noteId=S1e9NsdXsm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper109 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper109 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors suggest and analyse two types of preconditioners for optimization, a Newton type and a Fisher type preconditioner.

The paper is well written, the analysis is clear and the significance is arguably given. The authors run their optimizers on a synthetic benchmark data set and on imagenet.
The originality is not so high as the this line of research exists for long. 
The "Lie" in the title is (technically correct, but) a bit misleading, as only matrix groups were used.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1gF-Z1Za7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Responses to AnonReviewer2’s comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Bye5SiAqKX&amp;noteId=B1gF-Z1Za7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper109 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper109 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">[Comment 1]: The "Lie" in the title is (technically correct, but) a bit misleading, as only matrix groups were used.
[Response 1]: We will use ‘matrix Lie group’ in the title after revision. In the text, we already point out that Lie group in the paper refers to the matrix Lie group.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>