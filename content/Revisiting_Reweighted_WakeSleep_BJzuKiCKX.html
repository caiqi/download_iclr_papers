<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Revisiting Reweighted Wake-Sleep | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Revisiting Reweighted Wake-Sleep" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJzuKiC9KX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Revisiting Reweighted Wake-Sleep" />
      <meta name="og:description" content=" Discrete latent-variable models, while applicable in a variety of settings, can often be difficult to learn. Sampling discrete latent variables can result in high-variance gradient estimators for..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJzuKiC9KX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Revisiting Reweighted Wake-Sleep</a> <a class="note_content_pdf" href="/pdf?id=BJzuKiC9KX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019revisiting,    &#10;title={Revisiting Reweighted Wake-Sleep},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BJzuKiC9KX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value"> Discrete latent-variable models, while applicable in a variety of settings, can often be difficult to learn. Sampling discrete latent variables can result in high-variance gradient estimators for two primary reasons: 1) branching on the samples within the model, and 2) the lack of a pathwise derivative for the samples. While current state-of-the-art methods employ control-variate schemes for the former and continuous-relaxation methods for the latter, their utility is limited by the complexities of implementing and training effective control-variate schemes and the necessity of evaluating (potentially exponentially) many branch paths in the model. Here, we revisit the Reweighted Wake Sleep (RWS; Bornschein and Bengio, 2015) algorithm, and through extensive evaluations, show that it circumvents both these issues, outperforming current state-of-the-art methods in learning discrete latent-variable models. Moreover, we observe that, unlike the Importance-weighted Autoencoder, RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent-variable models as well. Our results suggest that RWS is a competitive, often preferable, alternative for learning deep generative models.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">variational inference, approximate inference, generative models, gradient estimators</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Empirical analysis and explanation of particle-based gradient estimators for approximate inference with deep generative models.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Syg9R7FF6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Discrete latent variables, stochastic control flow and probabilistic programming</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJzuKiC9KX&amp;noteId=Syg9R7FF6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper459 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper459 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewers for their time and for appreciating our arguments about why RWS is preferable to IWAE-based approaches as we increase number of particles.

We would like to re-emphasize an important implication from our paper: RWS is a good inference algorithm for models that have _stochastic control flow_: i.e. models where latent variables can be instantiated dynamically, and different branching paths explored, based on the choice from a random variable[4-7]. Note that this is orthogonal to a large number of recent work that employ discrete latent variables in deep generative models[8-10]---where either the discrete variable is transformed via continuous relaxations, or, marginalized out entirely. The crucial difference is that none of these approaches address models where the model execution itself is determined via the discrete choices (if-statements or for-loops), as opposed to simply passing it through to a neural network---what is done with the discrete choice matters.

This is best illustrated in the domain of universal probabilistic programs (like Pyro [1]) which can contain arbitrary continuous and discrete latent variables, and where latent variables are instantiated dynamically and defined by running the program (or generative model). Stochastic control flow is a feature of such models [2] and allows the definition of expressive models, with potentially infinite number of latent variables [3], as mentioned in the discussion.

Universal (or higher-order) probabilistic programs form the largest family of samplable distributions and thus are a powerful tool to model data. Amortized inference and model parameter learning in such probabilistic programs, however, is typically only done using variational methods in the VAE/IWAE family of algorithms (as summarized in our manuscript). We’re trying to say: RWS is a simple and often superior algorithm to use in this model family.

Our point about probabilistic programs, hard selection and stochastic branching is illustrated by our choice of experiments (GMM and AIR). However, we will more strongly emphasize this point in the updated manuscript.

[1] <a href="http://pyro.ai/" target="_blank" rel="nofollow">http://pyro.ai/</a>
[2] http://pyro.ai/examples/intro_part_i.html#Universality:-Stochastic-Recursion,-Higher-order-Stochastic-Functions,-and-Random-Control-Flow
[3] Quote from [2]
    "For example, we can construct recursive functions that terminate their recursion
     nondeterministically, provided we take care to pass pyro.sample unique sample names
     whenever it’s called."
[4] GMM models in this work
[5] Tree-structured latent variables in https://arxiv.org/abs/1806.07832
[6] Memory-based models in https://arxiv.org/abs/1709.07116
[7] AIR-like models in this work and https://arxiv.org/abs/1806.01794
[8] DVAE++ - https://arxiv.org/abs/1802.04920
[9] DVAE#  - https://arxiv.org/abs/1805.07445
[10] VQ-VAE - https://arxiv.org/abs/1711.00937
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJepNja0hQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice experimental discoveries</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJzuKiC9KX&amp;noteId=BJepNja0hQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper459 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper459 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper conducts an extensive set of experiments on RWS and compares it against a set of benchmarks such as GMM and IWAE. The main contribution of the paper is the fact revealed by these experiments, that RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent variable models as well. The performance of RWS will increase significantly if we increase the number of particles. 

The experimental part is written in an inspiring way, and I enjoyed reading it. However, there should be stronger baselines incorporated. for example, <a href="https://arxiv.org/abs/1805.07445." target="_blank" rel="nofollow">https://arxiv.org/abs/1805.07445.</a> Also, I think the authors could try to emphasize more on the shortcomings of RWS discovered by the GMM experiments, and how defensive importance sampling fixes it. There are several other parts in the paper that indicates interesting facts, diving deeper into it could possibly lead to more interesting findings.

In all, I would consider these comparison results important to be somewhere in the literature, but because its lack of rigorous analysis and explanation for the observations, I personally think these observations alone are not novel enough to be an ICLR paper. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BylkLVYKa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJzuKiC9KX&amp;noteId=BylkLVYKa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper459 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper459 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Baselines: As set out in our overall response, we aim to show that RWS is a better choice for inference in models that have stochastic control flow, where the choice from the discrete latent variables matters explicitly. In our GMM example, the cluster identity is such a choice, and in AIR, the stopping condition for the loop is another such choice. The work done in DVAE++, DVAE#, and other such approaches do not really handle this general class of problems well---by typically requiring enumeration of all possible branches and choices.

GMM: We will include a more detailed description of how defensive sampling ameliorates issues discovered in the GMM experiments in the updated manuscript.

Theoretical Rigour: We will include a more comprehensive discussion of the theoretical basis of why RWS is better than IWAE in the updated manuscript. Briefly, the justification for why RWS does not suffer from the "tighter bounds" problem is due to RWS's use of self-normalised importance sampling to compute the gradient of proposal parameters---resulting in both the asymptotic bias and variance decreasing linearly with number of samples.

Empirical Rigour: Our experiments strongly support our hypotheses:
  a. Unlike IWAE, RWS performs better with more particles, both in terms of the generative model and inference network, and
  b. It allows for effective and easy application to models where the choice from the discrete random variables affects model expansion or computation---something that requires expensive enumeration with continuous relaxations, or extremely finicky and unreliable construction with control-variate methods.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SylgzBTohX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Revisiting Reweighted Wake-Sleep</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJzuKiC9KX&amp;noteId=SylgzBTohX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper459 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper459 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This manuscript investigates the performance of Reweighted Wake-Sleep (RWS) framework for learning deep generative models with discrete latent variables. It gives a clear introduction to variational autoencoder based models for scenarios with discrete latent variables, including IWAE and also models based on continuous relaxations of discrete variables. The paper performs several experiments, which suggest that RWS is more appropriate for discrete latent variables than other methods such as IWAE. Especially, increasing the number of particles, unlike IWAE, always enhances the performance of RWS.

While this paper investigates an important problem, and also offers interesting observations, it lacks a rigorous analysis of why the RWS performance is consistently better than IWAE. More precisely, the propositions should be stated in more formal language and they should be accompanied with a minimal rigorous justification.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJeBGBtFaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJzuKiC9KX&amp;noteId=BJeBGBtFaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper459 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper459 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The key formal justification is relatively straightforward: RWS, unlike IWAE, does not suffer from the “tighter bounds” problem. On the contrary, RWS uses self-normalized importance sampling to estimate the gradient with respect to \phi. Both the asymptotic bias and variance of a self-normalized importance sampling estimator decrease linearly in number of particles. This means that increasing number of particles improves our gradient estimator and thus the optimization procedure.

We will explain this in more detail in the updated manuscript.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HJxxyGwinm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting experimental paper but more insights are expected</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJzuKiC9KX&amp;noteId=HJxxyGwinm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper459 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper459 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Main idea:
This paper studies a problem of the importance weighted autoencoder (IWAE) pointed out by  Rainforth 18, that is, tighter lower bounds arising from increasing the number of particles improve the learning of the generative model, but worsen the learning of the inference network. The authors show that the reweighted wake-sleep algorithm (RWS) doesn't suffer from this issue. Moreover, as an alternative to control variate scheme and reparameterization trick, RWS doesn't suffer from high variance gradients, thus it is particularly useful for discrete latent variable models.   
To support the claim, they conduct three experiments: 1) on ATTEND, INFER, REPEAT, a generative model with both discrete and continuous latent variables; 2) on MNIST with a continuous latent variable model; 3) on a synthetic GMM.

Clarity issues:
1. "branching" has been used many times, but AFAIK, this seems not a standard terminology. What do "branching on the samples", "conditional branching", "branching paths" mean?
2. zero-forcing failure mode and delta-WW: I find this part difficult to follow. For example, the following sentence 
"the inference network q(z|x) becomes the posterior for this model which, in this model, also has support at most {0, . . . , 9} for all x". 
However, this failure mode seems an interesting finding, and since delta-WW outperforms other methods, it deserves a better introduction. 

Questions:
1. In Fig 1 (right), how do you estimate KL(q(z|x) || p(z|x))?
2. In Sec 4.2, why do you say IWAE learns a better model only up to a point (K = 128) and suffers from diminishing returns afterwards?  
3. In Fig 4, why WS doesn't achieve a better performance when K increasing?

Experiments:
1. Since the motivating story is about discrete latent variable models, better baselines should be compared, e.g. RBM, DVAE, DVAE++, VQ-VAE etc. 
2. All experiments were on either on MNIST or synthetic data, at least one large scale experiment on discrete data should be made to verify the performance of RWS. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJxb9BFYaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJzuKiC9KX&amp;noteId=rJxb9BFYaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper459 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper459 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Clarity issues:

By stochastic branching we refer to the evaluation of generative models where discrete latent variables are used to select which part of the model is going to be evaluated next. For example, in AIR, this decides when the program halts, in GMM the cluster index decides which likelihood function is evaluated. Another example of this are probabilistic context-free grammars (PCFGs) where discrete variables are used to describe which production rule is used (for example <a href="https://arxiv.org/abs/1806.07832)." target="_blank" rel="nofollow">https://arxiv.org/abs/1806.07832).</a> This is in contrast with modeling approaches where the discrete latent variable is merely an input to a neural network that doesn’t distinguish it from a non-discrete latent variable since it does not explicitly use the discreteness to model distinct modes of the data. (also see our general comment)

We will clarify the “zero-forcing” failure mode and delta-WW in the updated manuscript.

Questions:

To estimate KL(q(z|x) || p(z|x)), we take the difference of the log likelihood estimated by a 5000-particle IWAE bound and the ELBO estimated by 5000 Monte Carlo samples.

The statement that “IWAE learns a better model only up to a point” is justified by the IWAE curve in the middle of Figure 2: the decreasing slope indicates that improvements in marginal log probability decrease with increasing numbers of particles. This is even more pronounced in Figure 1, where IWAE performance decreases for k &gt; 10.

In Fig 4, WS actually does achieve better performance as K increases - the final value of the learning curve goes down, although only very slightly.

Experiments:

Regarding experiments, RBM/DVAE/++/# and VQ-VAE allow learning models with discrete latent variables in general; however, the resulting discreteness cannot be used for directing the control flow of a generative model (see also response to AnonReviewer3 and our general comment).
    - In the DVAE family of algorithms, learning in discrete latent variable models is achieved by a continuous relaxation. This prevents using these variables as hard branching conditions.
    - In the VQ-VAE algorithm, the discrete latent variable is explicitly designed to be used to select an embedding and it is deterministic. This limits the use of a discrete latent variable (cannot be used to model a cluster identity or stopping of a while loop). 

Even though we do not have experiments on large-scale real-world datasets, AIR is a non-trivial model, and using it can be seen as a large-scale experiment - taking several days (and several GPUs) to obtain results summarized in Figure 1. Similarly, to the best of our knowledge, ours is the first reported result of an MNIST model trained with IWAE with 512 particles.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>