<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Small nonlinearities in activation functions create bad local minima in neural networks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Small nonlinearities in activation functions create bad local minima in neural networks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rke_YiRct7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Small nonlinearities in activation functions create bad local..." />
      <meta name="og:description" content="We investigate the loss surface of neural networks. We prove that even for one-hidden-layer networks with " slightest"="" nonlinearity,="" the="" empirical="" risks="" have="" spurious="" local="" minima="" in="" most="" cases.="" our..."="" />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rke_YiRct7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Small nonlinearities in activation functions create bad local minima in neural networks</a> <a class="note_content_pdf" href="/pdf?id=rke_YiRct7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019small,    &#10;title={Small nonlinearities in activation functions create bad local minima in neural networks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rke_YiRct7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We investigate the loss surface of neural networks. We prove that even for one-hidden-layer networks with "slightest" nonlinearity, the empirical risks have spurious local minima in most cases. Our results thus indicate that in general "no spurious local minima" is a property limited to deep linear networks, and insights obtained from linear networks are not robust. Specifically, for ReLU(-like) networks we constructively prove that for almost all (in contrast to previous results) practical datasets there exist infinitely many local minima. We also present a counterexample for more general activations (sigmoid, tanh, arctan, ReLU, etc.), for which there exists a bad local minimum. Our results make the least restrictive assumptions relative to existing results on local optimality in neural networks. We complete our discussion by presenting a comprehensive characterization of global optimality for deep linear networks, which unifies other results on this topic.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">spurious local minima, loss surface, optimization landscape, neural network</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We constructively prove that even the slightest nonlinear activation functions introduce spurious local minima, for general datasets and activation functions.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_H1gdFd3epm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rke_YiRct7&amp;noteId=H1gdFd3epm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper458 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper458 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors present some theoretical results on the loss surface of neural networks. Their main results are:

(1) They consider a 1 layer hidden neural network where the single nonlinearity is ReLU / ReLU-like. Here they prove that as long as a linear model cannot fit the data, then there exits a local minimum strictly inferior to the global one (They can then scale the parameters to get infinitely many local optima).

The key idea is to construct a local minima whose risk value is the same as the local least squares solution. Then to construct a set of parameters that has smaller risk value than this local optima. The proof technique is interesting.

(2) They construct a particular dataset for which a one hidden layer neural net with other nonlinear activations (sigmoid, tanh, etc.) also has local optima.

I think this theorem is a bit less interesting since the dataset given has only two data points. I think it is less interesting to prove suboptimality of neural nets in small sample size settings. 

(3) Global optimailty of linear networks. The authors show that deep linear networks (i.e. y = W1 W2 W3...W5 x) have only global minima or saddle points.

I'm not familiar enough with the field to know the significant of this result. The deep linear network  just seems like an artificial construction (i.e. in practice one would simply condense W1...W5 to one W) to study nonconvexity / local optima, no one would use it in practice.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HkeS3_Q02X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review of "Small nonlinearities in activation functions create bad local minima in neural networks"</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rke_YiRct7&amp;noteId=HkeS3_Q02X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper458 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper458 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Paper represents theoretical analysis of the loss surface of neural networks. The authors supplied interesting results about local minima properties of neural networks. The paper is written quite well and easy to follow. Furthermore, authors made a comprehensive literature survey and connected their paper to already existing literature. The proofs seem correct (please note that paper is quite long and it requires more time then conference review period, hence, I stated “seem correct”). 
Although, paper provides novel theorems, I have several concerns, and these are:
•	Isn’t it clear that (generally speaking) a non-convex problem will have many local minima? Previous paper in neural network community is (in my opinion) not theorems. I believe one should read those statements such that in practice (please note here practice means that architectures, e.g. resnet50, inceptionv3, used in day to day life) researchers don’t not observe those “really bad” local minima.
•	In ML literature, we have convex machines which are guaranteed to converge to global optimum. Given image dataset, or text datasets supervised deep learning in in general much better than convex methods e.g. SVMs. The paper clearly shows that there exist, in some cases, exponentially many local minima however  current training methods are able to find better solutions than convex methods (I am completely aware that functions classes are different however success metric, accuracy, is the same). Hence, how relevant are the results without taking in to account architectural choices or optimization methods for deep learning? May be structural risk minimisation is a better approach than empirical risk minimization for quantifying the performance of deep neural networks,
In conclusion, paper is interesting however I believe it need to be improved. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hkgdn1QB3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>elegant construction; interesting phenomenon</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rke_YiRct7&amp;noteId=Hkgdn1QB3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper458 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper458 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
The authors provide a clean and easily understood sufficient
condition for spurious local minima to exist in networks with
a hidden layer using ReLUs or leaky ReLUs.  This condition,
that there is not linear transformation with zero loss,
is satisfied for almost all inputs with more examples than
input variables.

The construction is elegant.  The mathematical writing in the paper,
especially describing the proof of Theorem 1, is very nice -- they
expose the main ideas effectively.

I do not know of another paper using a similar proof, but I have not
studied the proofs of the most closely related papers prior to doing
this review, so I have limited ability to vouch for this paper's
technical novelty.

The authors also show that networks using many other popular
activation functions have spurious local minima for a very
simple dataset.  All of these analysis are unified using a
simple, if technical, set of conditions on activation function.

Finally, the authors prove a somewhat technical theorem about
optima in deep linear networks, which generalizes some
earlier treatments of this topic, providing an checkable
condition for global minimality.

There is extensive discussion of related work.  I am not aware of
related work not covered by the authors.

In some cases, when the authors discuss previous work, they write as
if restriction to the realizable case is an assumption, when it seems
to me to be more of a constraint.  In other words, it seems harder to
prove the existence of spurious minima in the realizable case.
They seem to acknowledge this after their statement of their Theorem 2,
which also uses a realizable dataset.

Also, a few papers, including the Venturi, et al paper cited by
the authors, have analyzed whether spurious local minima exist
in subsets of the parameter space, including those likely to
be reached during training with different sorts of initializations.
In light of this work, the authors might want to tone down claims
about how their work shows that results about linear networks do
not generalize to the non-linear case.  In particular, to make
their construction work in the case of wide networks, they
need an overwhelming majority of the hidden units to be "dead",
which seems as it is unlikely to arise from training with
commonly used initializations.

Overall, I think that this paper makes an interesting and
non-obvious contribution on a hot topic.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>