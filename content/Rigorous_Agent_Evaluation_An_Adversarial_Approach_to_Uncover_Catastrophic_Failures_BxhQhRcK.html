<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1xhQhRcK7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Rigorous Agent Evaluation: An Adversarial Approach to Uncover..." />
      <meta name="og:description" content="This paper addresses the problem of evaluating learning systems in safety critical domains such as autonomous driving, where failures can have catastrophic consequences. We focus on two problems:..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1xhQhRcK7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures</a> <a class="note_content_pdf" href="/pdf?id=B1xhQhRcK7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019rigorous,    &#10;title={Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=B1xhQhRcK7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=B1xhQhRcK7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">This paper addresses the problem of evaluating learning systems in safety critical domains such as autonomous driving, where failures can have catastrophic consequences. We focus on two problems: searching for scenarios when learned agents fail and assessing their probability of failure. The standard method for agent evaluation in reinforcement learning, Vanilla Monte Carlo, can miss failures entirely, leading to the deployment of unsafe agents. We demonstrate this is an issue for current agents, where even matching the compute used for training is sometimes insufficient for evaluation. To address this shortcoming, we draw upon the rare event probability estimation literature and propose an adversarial evaluation approach. Our approach focuses evaluation on adversarially chosen situations, while still providing unbiased estimates of failure probabilities. The key difficulty is in identifying these adversarial situations -- since failures are rare there is little signal to drive optimization. To solve this we propose a continuation approach that learns failure modes in related but less robust agents. Our approach also allows reuse of data already collected for training the agent. We demonstrate the efficacy of adversarial evaluation on two standard domains: humanoid control and simulated driving. Experimental results show that our methods can find catastrophic failures and estimate failures rates of agents multiple orders of magnitude faster than standard evaluation schemes, in minutes to hours rather than days.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">agent evaluation, adversarial examples, robustness, safety, reinforcement learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We show that rare but catastrophic failures may be missed entirely by random testing, which poses issues for safe deployment. Our proposed approach for adversarial testing fixes this.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">10 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_H1l_8M3gRm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Paper Updated</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xhQhRcK7&amp;noteId=H1l_8M3gRm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1393 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1393 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Reviewers,

Thank you for the constructive feedback. All reviews expressed that we were formulating and tackling a significant problem, and that the experimental results were compelling. There were also positive comments about the soundness and novelty of the approach. We hope our work leads to an increased focus on robustness and adversarial examples in RL (and in general beyond norm-ball perturbations).

We have updated our paper to incorporate reviewer feedback. In particular, we added a paragraph at the end of section 4.1 to explain why classical baselines would not work in our context. We added section 4.4 to discuss practical considerations: lower bounds on statistical efficiency, as well as heuristics we use to robustify our method. We have revamped the exposition in section 3.3 to explain one of the key novelties of our approach: the continuation approach to learning FPPs. The other novelties were motivating an important, unaddressed problem, and the extension of the importance sampling framework to include stochasticity.

We believe these address the reviewer comments on statistical efficiency, baselines, and novelty. If the responses satisfy the reviewers, we hope they will consider raising their scores, or letting us know in what ways they think the paper should be improved. 

Thanks,
Authors
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1lWFTj03X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Effective application of an importance sampling framework to testing RL agent policies for rare failures</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xhQhRcK7&amp;noteId=B1lWFTj03X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1393 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1393 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
Proposes an importance sampling approach to sampling failure cases for RL algorithms. The proposal distribution is based on a function learned via a neural network on failures that occur during agent training. The method is compared to random sampling on two problems where the "true" failure probability can be approximated through random sampling. The IS method requires substantially fewer samples to produce failure cases and to estimate the failure probability.

Review:
The overall approach is technically sound, and the experiments demonstrate a significant savings in sampling compared to naive random sampling. The specific novelty of the approach seems to be fitting the proposal distribution to failures observed during training. 

I think the method accomplishes what it sets out to do. However, as the paper notes, creating robust agents will require a combination of methodologies, of which this testing approach is only a part. 

I wonder if learning the proposal distribution based on failures observed during training presents a risk of narrowing the range of possible failures being considered. Of course identifying any failure is valuable, but by biasing the search toward failures that are similar to failures observed in training, might we be decreasing the likelihood of discovering failures that are substantially different from those seen during training? One could imagine that if the agent has not explored some regions of the state space, we would actually like to sample test examples from the unexplored states, which becomes less likely if we preferentially sample in states that were encountered in training.

The paper is well-written with good coverage of related literature. I would suggest incorporating some of the descriptions of the models and methods in Appendix D into the main paper.

Comments / Questions:
* Sec 4.2: How are the confidence bounds for the results calculated?
* What are the "true" failure probabilities in the experiments?
* Sec 4.3: There is a reference to non-existant "Appendix X"

Pros:
* Overall approach is sound and achieves its objectives

Cons:
* Small amount of novelty; primarily an application of established techniques</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJxm6c4GaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Other details</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xhQhRcK7&amp;noteId=BJxm6c4GaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1393 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1393 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt; I think the method accomplishes what it sets out to do. However, as the paper notes, creating robust agents will require a combination of methodologies, of which this testing approach is only a part. 

Agreed, this an exciting direction for future work. We believe our work is essential for this goal - if we cannot test whether an agent is robust or not, we cannot hope to develop robust agents. Note that in section 4.3 we use the FPP in a simple way to identify more robust agents. We hope future work extends on this - one way is to learn the FPP online with the policy and apply it for adversarial training. This could yield large improvements in sample efficiency - if the FPP is 100x faster at failure search, the agent gets useful examples 100x as often.

&gt; I would suggest incorporating some of the descriptions of the models and methods in Appendix D into the main paper.


We’ve edited down the length of the paper, which allows to move some important details to the main paper. We’ll mention some details regarding the training + architecture of the failure probability predictor in the next update. Are there any specific details you would suggest we include?

&gt; Sec 4.2: How are the confidence bounds for the results calculated?
&gt; What are the "true" failure probabilities in the experiments?

The ground truth failure probabilities are obtained by running the VMC estimator for 5e6 episodes on Driving and 2e7 episodes on Humanoid. Right now, this is mentioned in the footnote at the bottom of page 7, with additional details in the appendix. Thanks for raising this - we’ve definitely tried to make these details as clear as possible, but also realize there’s a lot of such details, and may still be unclear. Please let us know if the writing could be clearer.

The confidence bands in Figure 1 represent 2 standard errors. Each plot is generated by running the estimators many times, and plotting the probability of an unreliable estimate. We use a conservative estimate for standard errors, where if p^ is the empirical mean over n trials for the probability parameter for a Bernoulli RV, SE(p^) = sqrt(max(p^, 0.1) * (1-p^) / n). The max is just to avoid overly narrow confidence bands when p^ is very close to 0 (i.e. when none of the estimates from the estimator are unreliable).

&gt; Sec 4.3: There is a reference to non-existant "Appendix X"

Thanks, fixed.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Byxi9c4fpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Addressing main concerns and novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xhQhRcK7&amp;noteId=Byxi9c4fpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1393 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018 (modified: 13 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1393 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the review and suggestions. We first address what we understand to be the main concerns in your review:

We believe there are two sources of novelty. (1) A long-term goal is robust RL agents. Testing agents when rewards are highly sparse is on the critical path to this goal. To our knowledge, this problem has gone unaddressed. Thus, one novelty is considering a practical and important class of rare event estimation problems. (2) Our setting is fairly different from classical settings. By exploiting its structure, we provide an effective approach, whereas prior approaches simply would not work.

&gt; Small amount of novelty; primarily an application of established techniques
&gt; The specific novelty of the approach seems to be fitting the proposal distribution to failures observed during training. 

We believe there are several novel ideas in our approach which are missing in this summary. These novelties aren’t just small changes - we don’t see how existing approaches could handle our setting (failure search and risk estimation, with binary failure signals) without them. Admittedly, we emphasized importance over novelty in writing the paper, and will edit for clarity.

The main novelty in the continuation approach is to learn the proposal distribution from a family of related, but weaker, agents. Our method goes beyond simply fitting a function to data. Fitting a proposal distribution to failures observed for the final agent would not work well. For example, in Humanoid, the final agent fails once every 110k episodes, and was trained for 300k episodes. If we run existing methods like the cross-entropy method on the final agent, we would need significantly more than 300k episodes of data to get a good proposal distribution. 

Another novel aspect is our extension of the standard importance sampling setup to include stochasticity. While this seems very fundamental, we are not aware of this in prior work. To reflect the practicalities of RL tasks, we separate controllable randomness (observed initial conditions) from unobservable, uncontrollable randomness (environment and agent randomness, or unobserved initial conditions). We show this changes the form of the minimum-variance proposal distribution (Proposition 3.2). Additionally, in our setup, the initial state distribution is arbitrary and unknown.

&gt; I wonder if learning the proposal distribution based on failures observed during training presents a risk of narrowing the range of possible failures being considered.

This is a good observation. In our humanoid experiments, we safeguard against this using a differentiable neural dictionary (Appendix D.1, moved to E.1 in the latest revision). This encourages higher failure probabilities for initial conditions far from those seen during training. Also see our response to R3 regarding statistical efficiency.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1guoXzK37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Timely topic, reasonable approach, and good experimental results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xhQhRcK7&amp;noteId=H1guoXzK37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1393 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1393 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposed an adversarial approach to identifying catastrophic failure cases in reinforcement learning. It is a timely topic and may have practical significance. The proposed approach is built on importance sampling for the failure search and function fitting for estimating the failure probabilities. Experiments on two simulated environments show significant gain of the proposed approaches over naive search. 

The reviewer is not familiar with this domain, but the baseline, naive search, seems like straightforward and very weak. Are there any other methods for the same problem in the literature? The authors may consider to contrast to them in the experiments. 

What is the certainty equivalence approach? A reference would be helpful and improve the presentation quality of the paper.

What is exactly the $\theta_t$ in Section 3.3? What is the dimension of this vector in the experiments? What quantities should be encoded in this vector in practice? 

I am still concerned about the fact that the FPP depends on the generalization of the binary classification neural network, although the authors tried to give intuitive examples and discussions. Nonetheless, I understand the difficulty. Could the authors give some conditions under which the approach would fail? Any alternative approaches to the binary neural network? What is a good principle to design the network architecture? 

Overall, this paper addresses a practically significant problem and has proposed reasonable approaches. While I still have concerns about the practical performance of the proposed methods, this work along the right track in my opinion.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1lgl9Iz6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarifying other details</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xhQhRcK7&amp;noteId=r1lgl9Iz6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1393 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1393 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt; What is the certainty equivalence approach? A reference would be helpful and improve the presentation quality of the paper.

The certainty equivalence approach is described on page 3. The term has a long history in economics and control, going back to work by Stephen Turnovsky. We will add a reference:
Stephen Turnovsky. Optimal Stabilization Policies for Stochastic Linear Systems: The Case of Correlated Multiplicative and Additive disturbances. Review of Economic Studies 1976. 43 (1): 191–94.

&gt; What is exactly the $\theta_t$ in Section 3.3? What is the dimension of this vector in the experiments? What quantities should be encoded in this vector in practice? 

In general, theta_t should contain any features which provide useful information about the failure probabilities of the policy, and are easy to condition on. In our experiments, theta_t encodes the training iteration, and the amount of noise applied to the policy (details in old appendix D.1, moved to E.1 in the upcoming version), so two dimensions. More features may improve performance, but this was just the simple thing we tried, and since the improvement was already so drastic, it didn’t seem there was much point pushing further.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1xLqdIM6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Addressing main concerns regarding practical performance and baselines</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xhQhRcK7&amp;noteId=r1xLqdIM6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1393 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1393 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">&gt; Overall, this paper addresses a practically significant problem and has proposed reasonable approaches. While I still have concerns about the practical performance of the proposed methods, this work along the right track in my opinion.

Thank you for the positive comments, and helpful feedback. Could you please explain what concerns you have about the practical performance of the proposed methods? How can we address these? We believe our approach is a large improvement over baselines, both in theory, and as supported by our experiments.

&gt; The reviewer is not familiar with this domain, but the baseline, naive search, seems like straightforward and very weak. Are there any other methods for the same problem in the literature?

We assume you are talking about failure search, and not failure rate estimation? In our original paper, we did compare our method with an additional baseline: a prioritized replay baseline. This does significantly better than naive search, but significantly worse than our proposed method. 

We seem to be the first to tackle this problem. The setting is sufficiently different from classical settings, so classical baselines would not work, as we explain in our response to R2. We’d be happy to compare to additional baselines though - are there are any other baselines you would suggest we include?

&gt; I am still concerned about the fact that the FPP depends on the generalization of the binary classification neural network, although the authors tried to give intuitive examples and discussions. Nonetheless, I understand the difficulty. Could the authors give some conditions under which the approach would fail? Any alternative approaches to the binary neural network? What is a good principle to design the network architecture? 

The main point we hope to convey is that approaches beyond VMC are crucial, and using an optimized adversary is a good idea in safety-critical settings. We can guarantee that we never do worse than VMC by over a small constant factor (see the discussion on statistical efficiency in our response to R3 for details). However, as you point out, details can influence how much improvement we observe in practice. These details can be application specific, and is not the focus of our paper, but we expand on some of these details below.

Our approach would not help if the neural network severely underestimates the failure probability of a large fraction of failure cases. This could occur for initial states that are very different from all the initial states we have seen during training. We could mitigate this issue: (1) In the humanoid domain, we use a differentiable neural dictionary. The DND outputs higher failure probabilities for points very far from those seen during training. (2) Since we train on weaker agents, we tend to overestimate the failure probabilities. In general, a guiding principle is to output higher failure probabilities for examples we are uncertain about.

We included architectural details in Appendix D.1, but will move the key ideas to the main paper in the next update. Does this address your concerns? We are happy to provide more details if that helps.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJl92-Odhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Relevant, convincing experiments with a potential weak point in the method</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xhQhRcK7&amp;noteId=BJl92-Odhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1393 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1393 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=BJl92-Odhm" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">PAPER SUMMARY
-------------

The paper proposes a method for evaluating the failure probability of a learned agent, which is important in safety critical domains. 

Using plain Monte Carlo for this evaluation can be too expensive, since discovering a failure probability of epsilon requires on the order of 1/epsilon samples. Therefore the authors propose an adversarial approach, which focuses on scenarios which are difficult for the agent, while still yielding unbiased estimates of failure probabilities. 

The key idea of the proposed approach is to learn a failure probability predictor (FPP). This function attempts to predict at which initial states the system will fail. This function is then used in an importance sampling scheme to sample the regions with higher failure probability more often, which leads to higher statistical efficiency.
Finding the FPP is itself a problem which is just as hard as the original problem of estimating the overall failure probability. However, the FPP can be trained using data from different agents, not just the final agent to be evaluated (for instance the data from agent training, containing typically many failure cases). The approach hinges on the assumption that these agents tend to fail in the same states as the final agent, but with higher probability. 

The paper shows that the proposed method finds failure cases orders of magnitude faster than standard MC in simulated driving as well as a simulated humanoid task. Since the proposed approach uses data acquired during the training of the agent, it has more information at its disposal than standard MC. However, the paper shows that the proposed method is also orders of magnitudes more efficient than a naive approach using the failure cases during training.


REVIEW SUMMARY
--------------

I believe that this paper addresses an important problem in a novel manner (as far as I can tell) and the experiments are quite convincing.
The main negative point is that I believe that the proposed method has some flaws which may actually decrease statistical efficiency in some cases (please see details below).


DETAILED COMMENTS
-----------------

- It seems to me that a weak point of the method is that it may also severly reduce the efficiency compared to a standard MC method. If the function f underestimates the probability of failure at certain x, it would take a very long time to correct itself because these points would hardly ever be evaluated. It seems that the paper heuristically addresses this to some extent using the exponent alpha of the function. However, I think there should be a more in-depth discussion of this issue. An upper-confidence-bound type of algorithm may be a principled way of addressing this problem.

- The proposed method relies on the ability to initialize the system in any desired state. However, on a physical system, where finding failure cases is particularly important, this is usually not possible. It would be interesting if the paper would discuss how the proposed approach would be used on such real systems.

- On page 6, in the first paragraph, the state is called s instead of x as before. Furthermore, the arguments of f are switched.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1lea0SWpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Statistical efficiency and other technical concerns</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xhQhRcK7&amp;noteId=H1lea0SWpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1393 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018 (modified: 13 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1393 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for taking the time to write very thoughtful comments.

&gt; "I believe that this paper addresses an important problem in a novel manner (as far as I can tell) and the experiments are quite convincing."

It sounds like we’re on the same page regarding the importance of the problem, novelty, and experimental sections. You raised some really good points about the technical section, which we discuss below.

&gt; "The main negative point is that I believe that the proposed method has some flaws which may actually decrease statistical efficiency in some cases... It seems to me that a weak point of the method is that it may also severely reduce the efficiency compared to a standard MC method."

Theoretically, we can ensure that our method never does more than 2x worse than standard MC.

(1) Here’s an intuitive approach for limiting slowdown by a constant factor. We can run both standard MC and our estimator in parallel. If standard MC finds at least a few failures, we can use standard MC. If not, we can use our method. This incurs a slow-down of 2x in the worst case, while remaining orders of magnitudes better in safety critical domains such as the ones we test. Neufeld et al, which we mention in our related works, give even better guarantees when combining stochastic estimators.

(2) Moreover, any method for variance reduction or choosing proposal distributions can be worse in certain cases. This is true for cross-entropy method, subset simulation, control variates, baselines, to name a few. Yet these methods are used in practice, with great success. Requiring 0 slowdown may be too demanding -- we suspect an analogue of the no free lunch theorem might hold -- but we can limit slowdown by a constant factor. We will make all this more clear in the manuscript.

In practice, we employ safeguards to protect us from the issues you describe. (1) For the humanoid experiment we used a Differentiable Neural Dictionary described in Appendix E.1 (Pritzel at al, 2017), this was in D.1 in the original version. A DND is a kNN classifier in feature space, but uses a learned pseudo-count to output higher failure probabilities when the query point is far from training points. Intuitively, the DND model outputs higher failure probabilities for points on which it is uncertain, related to UCB. (2) We trained the FPP on weaker agents. So our method typically over-estimates failure probabilities. (3) Even so, if f underestimates the probability of failure at several points x, it will still typically converge much faster than standard MC. If all x are underestimated by at most a factor of k, then our method slows down on the order of sqrt(k). We show experimentally that our method does orders of magnitude better so this slowdown is not bad.

&gt; "The proposed method relies on the ability to initialize the system in any desired state. However, on a physical system, where finding failure cases is particularly important, this is usually not possible. It would be interesting if the paper would discuss how the proposed approach would be used on such real systems."

Our method actually does not initialize the system at arbitrary states. We only assume that the initial state x is sampled from some (unknown) distribution. Further, the initial system state only needs to be partially observable and the unobserved details can be absorbed into Z. We will make this more clear in the paper - does this address your concern?

&gt; "On page 6, in the first paragraph, the state is called s instead of x as before. Furthermore, the arguments of f are switched."

Thanks for spotting this, we will fix this.

References (also cited in the original paper):
James Neufeld, Andras Gyorgy, Csaba Szepesvari, Dale Schuurmans. Adaptive Monte Carlo via Bandit Allocation. In ICML 2014.
Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adria Puigdomenech, Oriol Vinyals, Demis Hassabis, Daan Wierstra, and Charles Blundell. Neural episodic control. In ICML 2017.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ByeinoLypQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarification regarding Proposition 3.2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1xhQhRcK7&amp;noteId=ByeinoLypQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1393 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1393 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the specific feedback and helpful comments. We wanted to quickly clarify the correctness of Proposition 3.2, since it seemed to be a major point in your review.

&gt; "It seems to me that Proposition 3.2 is wrong. In the proof it is written E[U^2] = E[W^2 c(X,Z)], which is wrong since U^2 = W^2 c^2(X,Z). This means that the proposal distribution Q_f* is not in fact the optimal proposal distribution. This is problematic because the entire approach is justified using this argument."

We believe the proof is correct, but this point is indeed subtle, and we’ll clarify it in the paper. In our case c(X, Z) is a Bernoulli random variable. So c^2(X, Z) = c(X, Z), as c(·, ·) is either 0 or 1 and in both cases the square is the identity. This means E[U^2] = E[W^2 c^2(X,Z)] = E[W^2 c(X,Z)]. In the case where c represents an arbitrary distribution, the optimal proposal distribution is more difficult to compute and is a worthwhile question for future work. 

We also note that the standard analysis of the optimal proposal distribution under importance sampling does not account for unobserved stochasticity, which we model in Z. This is why the optimal proposal distribution we derive (for Bernoulli random variables) differs from the standard case.

Please let us know if this addresses your concern.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>