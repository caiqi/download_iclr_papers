<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learnable Embedding Space for Efficient Neural Architecture Compression | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learnable Embedding Space for Efficient Neural Architecture Compression" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=S1xLN3C9YX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learnable Embedding Space for Efficient Neural Architecture..." />
      <meta name="og:description" content="We propose a method to incrementally learn an embedding space over the domain of network architectures, to enable the careful selection of architectures for evaluation during compressed..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_S1xLN3C9YX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learnable Embedding Space for Efficient Neural Architecture Compression</a> <a class="note_content_pdf" href="/pdf?id=S1xLN3C9YX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 15 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019learnable,    &#10;title={Learnable Embedding Space for Efficient Neural Architecture Compression},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=S1xLN3C9YX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=S1xLN3C9YX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We propose a method to incrementally learn an embedding space over the domain of network architectures, to enable the careful selection of architectures for evaluation during compressed architecture search. Given a teacher network, we search for a compressed network architecture by using Bayesian Optimization (BO) with a kernel function defined over our proposed embedding space to select architectures for evaluation. We demonstrate that our search algorithm can significantly outperform various baseline methods, such as random search and N2N (Ashok et al.,2018). The compressed architectures found by our method are also better than the state-of-the-art manually-designed compact architecture ShuffleNet (Zhang et al., 2018). We also demonstrate that the learned embedding space can be transferred to new settings for architecture search, such as a larger teacher network or a teacher network in a different architecture family, without any training.
</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Network Compression, Neural Architecture Search, Bayesian Optimization, Architecture Embedding</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a method to incrementally learn an embedding space over the domain of network architectures, to enable the careful selection of architectures for evaluation during neural architecture search (NAS).</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">12 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BJxRNWmx6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting idea but...</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xLN3C9YX&amp;noteId=BJxRNWmx6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1455 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1455 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this work, the authors propose a new strategy to compress a teacher neural network. Briefly, the authors propose using Bayesian optimization (BO) where the accuracy of the networks is modelled using a Gaussian Process function with a squared exponential kernel on continuous neural network (NN) embeddings. Such embeddings are the output of a bidirectional LSTM taking as input the “raw” (discrete) NN representations (when regarded as a covariance function of the “raw” (discrete) NN representations, the kernel is a deep kernel).

The authors apply this framework for model compression. In this application, the search space is the space of networks obtained by sampling reducing operations on a teacher network. In applications to CIFAR-10 and CIFAR-100 the authors show that the accuracies of the compressed network obtained through their method exceeds accuracies obtained through other methods for compression, manually compressed networks and random sampling.

I have the following concerns/questions:

1)	The authors motivate their work in the introduction by discussing the importance of learning a good embedding space over network architectures to “generate a priority ordering of architectures for evaluation”. Within the proposed BO framework, this would require the optimization of the expected improvement in a high-dimensional and discrete space (the space of NN architectures), which “is non-trivial”. In this work, the authors do not try to solve this general problem, but specialize their work to model compression, which has a much lower dimensional search space (space of networks obtained by sampling reducing operations on a teacher network). For this reason, I believe the presentation and motivation of this work is not presented clearly. Specifically, while I agree that the methods and results in this paper can be relevant to the problem of getting NN embeddings for a larger search space, this should be discussed in the conclusion/discussion as future direction, rather than as motivating example. Generally, I think the method should be described in the context of model compression rather than as a general method for neural architecture search (NAS) method (in my understanding, its use for NAS would be unfeasible). 

2)	I have been wondering why the authors optimize the kernel parameters by maximizing the predictive GP posterior rather than maximizing the GP log marginal likelihood as in standard GP regression?

3)	The sampling procedure should be explained in greater detail. How many reducing operations are sampled? This would be important to fully understand the random search method the authors consider for comparison in their experiments. I expect that the results from that method will strongly depend on the sampling procedure and different choices should probably be explored for a fair comparison. Do the authors have any comment on this?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BylTSOvc6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to the reviewer's questions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xLN3C9YX&amp;noteId=BylTSOvc6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1455 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1455 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the useful feedback! Here is our response:

*** Response to the question about the motivation and the presentation of this paper: ***

We thank the reviewer for the suggestion about the presentation of the paper. We have edited the introduction to motivate our method more in the context of model compression. We also include exploring its application to the general NAS problem as our future work in the conclusion section.


*** Response to the question about the using the log marginal likelihood as the objective function: ***

We agree that the log marginal likelihood is the standard objective function in previous works on kernel learning. However, we do not use the log marginal likelihood for the following two reasons:

(1) We empirically find that maximizing the log marginal likelihood yields worse results than maximizing the predictive GP posterior. Here are the results:

CIFAR-100		                        Accuracy		#Params		Ratio		        Times		f(x)
VGG-19	        Log Marginal	69.90%±0.69%	1.50M±0.68M	0.9254±0.3382	16.14x±9.22x	0.9422±0.0071
	                Ours	                71.41%±0.75%	2.61M±0.61M	0.8699±0.0306	7.99x±1.99x	0.9518±0.0158
						
ResNet-18	Log Marginal	72.80%±1.11%	1.72M±0.18M	0.8467±0.0160	6.57x±0.67x	0.9033±0.0094
	                Ours	                73.83%±1.11%	1.87M±0.08M	0.8335±0.0073	6.01x±0.26x	0.9123±0.0151
						
ResNet-34	Log Marginal	73.11%±0.57%	3.34M±0.48M	0.8435±0.0224	6.47x±0.89x	0.9059±0.0134
	                Ours	                73.68%±0.57%	2.36M±0.15M	0.8895±0.0069	9.08x±0.59x	0.9246±0.0076

'Log Marginal' refers to training the LSTM by maximizing the log marginal likelihood. 'Ours' refers to maximizing p(f|D).

(2) Also, when using the log marginal likelihood, we observe the loss is numerically unstable due to the log determinant of the covariance matrix in the log likelihood. The training objective usually goes to infinity when the dimension of the covariance matrix is larger than 50, even with smaller learning rates, which may harm the search performance.

Therefore, we train the LSTM parameters by maximizing the predictive GP posterior.


*** Response to questions about the sampling procedure: ***

Here are the details about how we sample one compressed architecture. This sampling procedure is used in both the ‘Random Search’ baseline and the optimization of the acquisition function in our method.

(1) For layer removal, only layers whose input dimension and output dimension are the same are allowed to be removed. Each removable layer can be removed with probability p_1.  However, if the probability is fixed, the diversity of sampled architectures would be reduced. For example, if we fix p_1 to 0.5, a compressed architecture with over 70% layers removed can hardly be generated. To encourage the diversity of random samples, p_1 is first randomly drawn from the set P_1={0.3, 0.4, 0.5, 0.6, 0.7} at the beginning of generating a new compressed architecture.

(2) For layer shrinkage, we divide layers into groups and for layers in the same group, the number of channels are always shrunken with the same ratio. The layers are grouped according to their input and output dimension. This is to make sure the network is still valid after the layer shrinkage. The shrinkage ratio for each group is drawn from the uniform distribution U(0.0, 1.0).

(3) For adding skip connections, only when the output dimension of one layer is the same as the input dimension of another layer, the two layers can be connected. When there are multiple incoming connections for one layer, the outputs of source layers are added up to form the input for that layer. For each pair of connectable layers, a connection can be added between them with probability p_3. Similar to p_1 in layer removal, p_3 is not fixed but randomly drawn from the set P_3={0.003, 0.005, 0.01, 0.03, 0.05} at the beginning of generating a compressed architecture. Values in P_3 are relatively small, because we found in experiments that adding too many skip connections empirically harm the performance of compressed architectures.

Combining all these three kinds of randomly sampled operations, a compressed architecture is generated from the teacher architecture. We have tried to include more values in the set P_1 and P_3 but that does not yield any improvement in the performance.


</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Bygrx6CthX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting idea but the paper needs further work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xLN3C9YX&amp;noteId=Bygrx6CthX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1455 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1455 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper describes a new neural architecture search strategy based on Bayesian optimization to find a compressed version of a teacher network. The main contribution of the paper is to learn an embedding that maps from a discrete encoding of an architecture to a continuous latent vector such that standard Bayesian optimization can be applied. 
The new proposed method improves in terms of compressing the teacher network with just a small drop in accuracy upon an existing neural architecture search method based on reinforcement learning and random sampling.


Overall, the paper presents an interesting idea to use Bayesian optimization on high dimensional discrete problems such as neural architecture search. I think a particular strength of this methods is that the embedding is fairly general and can be combined with various recent advances in Bayesian optimization, such as, for instance, multi-fidelity modelling.
It also shows on some compression experiments superior performance to other state-of-the-art methods.

However, in its current state I do not think that the paper is read for acceptance:

- Since the problem is basically just a high dimensional, discrete optimization problem, the paper misses comparison to other existing Bayesian optimization methods such as TPE [1] / SMAC [2] that can also handle these kind of input spaces. Both of these methods have been applied to neural architecture search [3][4] before. Furthermore, since the method is highly related to NASBOT [5], it would be great to also see a comparison to it.

- I assume that in order to learn a good embedding, similar architectures need to be mapped to latent vector that are close in euclidean space, such that the Gaussian process kernel can model any correlation[7]. How do you make sure that the LSTM learns a meaningful embedding space? It is also a bit unclear why the performance f is not used directly instead of p(f|D). Using f instead of p(f|D) would probably also make continual training of the LSTM easier, since function values do not change.

- The experiment section misses some details:
  - Do the tables report mean performances or the performance of single runs? It would also be more convincing if the table contains error bars on the reported numbers.
  - How are the hyperparameters of the Gaussian process treated?
  
- The related work section misses some references to Lu et al.[6] and Gomez-Bombarelli et al.[7] which are highly related.

- What do you mean with the sentence  "works on BO for NAS can only tune feed-forward structures" in the related work section? There is no reason why other Bayesian optimization should not be able to also optimize recurrent architectures (see for instance Snoek et al.[8]). 

- Section 3.3 is a bit confusing and to be honest I do not get the motivation for the usage of multiple kernels. Why do the first architectures biasing the LSTM? Since Bayesian optimization with expected improvement samples around the global optimum, should not later evaluated, well-performing architectures more present in the training dataset for the LSTM?


[1] Algorithms for Hyper-Parameter Optimization
    J. Bergstra and R. Bardenet and Y. Bengio and B. Kegl
    Proceedings of the 25th International Conference on Advances in Neural Information Processing Systems (NIPS'11)

[2] Sequential Model-Based Optimization for General Algorithm Configuration
    F. Hutter and H. Hoos and K. Leyton-Brown
    Proceedings of the Fifth International Conference on Learning and Intelligent Optimization (LION'11)

[3] Towards Automatically-Tuned Neural Networks
    H. Mendoza and A. Klein and M. Feurer and J. Springenberg and F. Hutter
    ICML 2016 AutoML Workshop

[4] Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures
    J. Bergstra and D. Yamins and D. Cox
    Proceedings of the 30th International Conference on Machine Learning (ICML'13)

[5] Neural Architecture Search with Bayesian Optimisation and Optimal Transport
    K. Kandasamy and W. Neiswanger and J. Schneider and B. P{\'{o}}czos and E. Xing
    abs/1802.07191

[6] Structured Variationally Auto-encoded Optimization
    X. Lu and J. Gonzalez and Z. Dai and N. Lawrence
    Proceedings of the 35th International Conference on Machine Learning

[7] Automatic chemical design using a data-driven continuous representation of molecules
    R. Gómez-Bombarelli and J. Wei and D. Duvenaud and J. Hernández-Lobato and B. Sánchez-Lengeling and D. Sheberla and J. Aguilera-Iparraguirre and T. Hirzel. and R. Adams and A. Aspuru-Guzik
    American Chemical Society Central Science

[8] Scalable {B}ayesian Optimization Using Deep Neural Networks
    J. Snoek and O. Rippel and K. Swersky and R. Kiros and N. Satish and N. Sundaram and M. Patwary and Prabhat and R. Adams
    Proceedings of the 32nd International Conference on Machine Learning (ICML'15)</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1ectNwqam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Questions about TPE [1], SMAC [2] and their applications to NAS [3][4]:</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xLN3C9YX&amp;noteId=S1ectNwqam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1455 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1455 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the detailed feedback. Here is our response to questions about  TPE [1], SMAC [2] and their applications to NAS [3][4]:

*** Our key contribution ***

We would like to emphasize that our key contribution is a novel method that incrementally learns an embedding space for the architecture domain, i.e., a unified representation for the configuration of architectures, which includes the number of layers, the type and configuration parameters of each layer and how layers are connected to each other. The learned embedding space can be used to compare architectures with complex skip connections and multiple branches and we can combine it with any Sequential Model-Based Optimization (SMBO) method to search for desired architectures. In this work, we define the kernel function (similarity metric between the configuration of architectures) over this incrementally larned space and apply Bayesian optimization to search for desired architectures. The focus of our work is not the use of Bayesian optimization (or some other SMBO methods) but how the embedding or the representation for the configuration of architectures itself can be learned over time. Other than the Gaussian process regression used in this paper, our method can be combined with more sophisticated SMBO methods such as TPE [1] and SMAC [2]. But this is beyond the focus of this work.

 *** Details about TPE and SMAC ***

TPE [1] is a hyperparameter optimization algorithm based on a tree of Parzen estimator. In TPE [1] and its application to NAS [4],  they use Gaussian mixture models (GMM) to fit the probability density of the hyperparameter values, which indicates that they determine the similarity between two architecture configurations based on the Euclidean distance in the original hyperparameter value domain. However, instead of comparing architecture configurations in the original hyperparameter value domain, we transform architecture configurations into our learned embedding space and compare them in the learned embedding space. Also in [1] and [4], each architectural hyperparameter is optimized independently of others and it is almost certainly the case that the optimal values of some hyperparameters depend on settings of others. This issue can be solved by applying TPE over our learned unified representation for all the configuration parameters.

SMAC [2] is a random-forest-based Bayesian optimization method. In SMAC [2] and its application to NAS [3], they compare two architecture configurations with a combined kernel that is *manually* defined based on the Euclidean distance or the Hamming distance between corresponding configuration parameter values. However, we compare two architecture configurations with an *automatically* learned kernel function defined over a ‘data-driven’ embedding space that is incrementally learned during the optimization. [3] can possibly benefit from our work by replacing their manually defined kernel with our learned kernel function.

*** Our method is complementary to TPE and SMAC ***

Both TPE and SMAC focus on improving SMBO methods while our novelty is not in the use of Bayesian optimization methods. Our main contribution is the incrementally learning of an embedding to represent the configuration of network architectures such that we can carry out the optimization over the learned space instead of the original domain of the value of configuration parameters. Our method is complementary to TPE and SMAC and can be combined with them when being applied to NAS.

*** [3] and [4] do not tune how the layers are connected to each other. ***

Also, TPE [1] and SMAC [2] have been applied to neural architecture search [3][4] before, however the connections between layers in the architectures tuned in [3] and [4] are fixed while we allow the addition of skip connections to optimize how the layers are connected. We believe optimizing how the layers are connected is crucial for the performance of the architecture and we have validated this in the ablation study (Table 5 in Appendix 6.3).
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryglzcbg0m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to comparison to existing methods</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xLN3C9YX&amp;noteId=ryglzcbg0m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1455 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1455 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I do follow the intuition of the paper that, compared to existing methods, the proposed method learns an embedding in order to allow for measuring similarities between architectures in a latent space rather than in the much more complicated original space.
It is true that existing BO methods are complementary to the presented approach, however, for me it remains open whether the learned embedding is actually helpful for Bayesian optimization and improves upon methods that only operate in the original input space. 
I still feel that the paper would be much more convincing, if it contains an experiment that shows that BO in the original space (e.g TPE) is outperformed by Bayesian optimization that uses the latent embedding.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1eeX4vcTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Questions about NASBOT [5] and questions about the LSTM training objective</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xLN3C9YX&amp;noteId=S1eeX4vcTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1455 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1455 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
*** Response to the question about NASBOT [5]: ***

Yes, our work is related to NASBOT as mentioned in the related work. Different from our incrementally learned embedding space for the architecture domain, their proposed OTAMANN distance is a *manually defined* distance metric between architectures and can also be used to compare architectures with different topologies. But we find it is non-trivial to integrate OTAMANN distance into our pipeline. Their public implementation is customized to their search space (searching for architectures from the scratch), which is significantly different from our search space (searching for compressed architectures based on a teacher network). Also, to compute OTAMANN distance, one needs to *manually define* a layer label mismatch cost matrix but in their implementation, they treat the residual block as a special layer type while in our work, a residual block is not specially treated but broken down into several layers with skip connections. This makes it hard to integrate OTAMANN distance into our pipeline. We are looking into their code and trying our best for this.

*** Response to “How do you make sure that the LSTM learns a meaningful embedding space?”: ***

The predictive GP posterior guides our choice of the architectures for evaluation at each search step, therefore we learn a meaningful embedding space by updating the LSTM weights θ to maximize \Sum_i log p(f(xi) | f(D \ xi); θ), which is a measurement of how accurate the posterior distribution is. The higher the value of p(f(xi) | f(D \ xi); θ) is, the more accurately the posterior distribution characterizes the statistical structure of the function f and the more the function f is consistent with the GP prior. Thus we define the loss function (Eq 5) based on p(f|D).


*** Response to “It is also a bit unclear why the performance f is not used directly instead of p(f|D).”: ***

We agree that a meaningful embedding space should be predictive of the function value (the performance of the architecture). However directly training the LSTM by regressing the function value with a Euclidean loss does not let us directly evaluate how accurate the posterior distribution characterizes the statistical structure of the function. As we have mentioned above, the posterior distribution guides our search process by influencing the choice of architectures for evaluation at each step. Therefore, we believe p(f|D) is a more suitable training objective for our search algorithm than regressing the value of f. To validate this, we have tried changing the objective function from maximizing p(f|D) to regressing the value of f with a Euclidean loss and here are the results:


CIFAR-100		                Accuracy	        #Params	        Ratio	                Times	        f(x)
VGG-19	        Euclidean	70.95%±1.07%	2.47M±1.26M	0.8771±0.0627	9.62x±4.55x	0.9453±0.0092
	                Ours	        71.41%±0.75%	2.61M±0.61M	0.8699±0.0306	7.99x±1.99x	0.9518±0.0158
						
ResNet-18	Euclidean	71.67%±0.67%	1.62M±0.27M	0.8560±0.0243	7.07x±1.09x	0.8917±0.0137
	                Ours	       73.83%±1.11%	1.87M±0.08M	0.8335±0.0073	6.01x±0.26x	0.9123±0.0151
						
ResNet-34	Euclidean	72.87%±1.11%	2.49M±0.60M	0.8834±0.2814	8.90x±2.04x	0.9127±0.0103
	                Ours	       73.68%±0.57%	2.36M±0.15M	0.8895±0.0069	9.08x±0.59x	0.9246±0.0076

'Euclidean' refers to training the LSTM by regressing the value of f with a Euclidean loss. 'Ours' refers to maximizing p(f|D).

We observe that maximizing p(f|D) consistently yields better results than regressing the value of f with a Euclidean loss.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BygS45WxAm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to additional experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xLN3C9YX&amp;noteId=BygS45WxAm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1455 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1455 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for providing the additional experiments. This is very valuable information. Could you add that to the appendix?</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SygyWQPqT7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to other questions ; Updated results of multiple runs</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xLN3C9YX&amp;noteId=SygyWQPqT7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1455 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1455 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Here is our response to other questions:

*** Response to questions about experimental details: ***
We re-run the experiments for 3 times and update the results in the paper (please check the PDF). In Table 1, we show the mean and standard deviation of the results for ‘Ours’ and ‘Random Search’. We observe that after multiple runs, the average performance of our method also outperforms all the baselines as before.

The mean of the Gaussian process prior is set to zero. The Gaussian noise variance is set to 0.05. The kernel width parameter $\sigma$ (defined in Eq 4) in the RBF kernel is set as $\sigma^2=0.01$.


*** Response to questions about related work: ***

Thanks for suggesting the related work. We have updated the paper and added [6] and [7] in the related work section. For your convenience, here is the text about [6] and [7] in the paper: “Our work can also be viewed as carrying out optimization in the latent space of a high dimensional and structured space, which shares a similar idea with previous literature [6][7]. For example, [6] presents a new variational auto-encoder to map kernel combinations produced by a context-free grammar into a continuous and low-dimensional latent space.”

*** Response to “What do you mean with the sentence  "works on BO for NAS can only tune feed-forward structures" in the related work section?”: ***

We are sorry for the confusion of the term ‘feed-forward structures’ in this sentence. We have corrected the sentence to “However, most existing works on BO for NAS only show results on tuning network architectures where the connections between network layers are fixed, i.e., most of them do not optimize how the layers are connected to each other.” For example, [8] tunes the hidden size, the embedding size and other architectural parameters in the language model but it does NOT change how the layers in the model are connected to each other. Our results (Table 5 in Appendix 6.3) show that optimizing how the layers are connected (in this work, by adding skip connections) is crucial to the performance of the compressed network architecture.

The fundamental reason why previous works on BO for NAS do not optimize how the layers are connected is that there lacked a principled way to quantify the similarity between two architectures with complex skip connections, which is addressed by our proposed learnable embedding space. They can benefit our proposed method to be extended to optimize how the layers are connected.

*** Response to questions about the motivation of using multiple kernels: ***

Sorry for the confusion in Sec 3.3. We have edited Sec 3.3 to make the motivation more clear. The main motivation of training multiple kernels is to encourage the search algorithm to explore more diverse architectures. We only evaluate 160 architectures during the whole search process so it is possible the learned kernel is overfitted to the training samples and bias the following sampled architectures for evaluation. To encourage the search algorithm to explore more diverse architectures, we propose the usage of multiple kernels, motivated by the bagging algorithm, which is usually employed to avoid overfitting.

Regarding the statement about the first architecture biasing the LSTM, this statement is invalid in the current context and we have removed it from the paper. This was a conjecture at the early development stage of this work and we mistakenly put it here.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJx0P9-xAQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Question about the treatment of the GP hyperparameters</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xLN3C9YX&amp;noteId=HJx0P9-xAQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1455 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1455 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Does this mean you keep the hyperparameters  of the Gaussian process fixed? Why not adapting them by either optimizing the marginal log-likelihood or marginalizing over them as described in <a href="https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf" target="_blank" rel="nofollow">https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf</a></span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_B1e700_3sQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A nice paper questioned by the significance of the results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xLN3C9YX&amp;noteId=B1e700_3sQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1455 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">23 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1455 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Review:

This paper proposes a method for finding optimal architectures for deep neural networks based on a teacher network. The optimal network is found by removing or shrinking layers or adding skip connections. A Bayesian Optimization approach is used by employing a Gaussian Process to guide the search and the acquisition function expected improvement. A special kernel is used in the GP to model the space of network architectures. The method proposed is compared to a random search strategy and a method based on reinforcement learning.
	
Quality: 

	The quality of the paper is high in the sense that it is very well written and contains exhaustive experiments with respect to other related methods

Clarity: 

	The paper is well written in general with a few typos, e.g., 

	"The weights of the Bi-LSTM θ, is learned during the search process. The weights θ determines"

Originality: 

	The proposed method is not very original in the sense that it is a combination of several known techniques. May be the most original contribution is the proposal of a kernel for network architectures based on recurrent neural networks.

	Another original idea is the use of sampling to avoid the problem of doing kernel over-fitting. Something that can be questioned, however, in this regard is the fact that instead of averaging over kernels the GP prediction to account for uncertainty in the kernel parameters, the authors have suggested to optimize a different acquisition function per each kernel. This can be problematic since for each kernel over-fitting can indeed occur, although the experimental results suggest that this is not happening.
	
Significance:

	Why N2N does not appear in all the CIRFAR-10 and CIFAR-100 experiments? This may question the significance of the results.

	It also seems that the authors have not repeated the experiments several times since there are no error bars in the results.
	This may also question the significance of the results. An average over several repetitions is needed to account for the randomness in for example the sampling of the network architectures to learn the kernels.

	Besides this, the authors may want to cite this paper

	Hernández-Lobato, D., Hernandez-Lobato, J., Shah, A., &amp; Adams, R. (2016, June). Predictive entropy search for multi-objective Bayesian optimization. In International Conference on Machine Learning (pp. 1492-1501).	

	which does multi-objective Bayesian optimization of deep neural networks (the objectives are accuracy and prediction time).

Pros:

	- Well written paper.
		
	- Simply idea.

	- Extensive experiments.

Cons:
	
	- The proposed  approach is a combination of well known methods.

	- The significance of the results is in question since the authors do not include error bars in the experiments.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJei_UPcTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to the reviewer's questions; Updated the results of multiple runs; Clarification about the originality</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xLN3C9YX&amp;noteId=BJei_UPcTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1455 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1455 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the feedback and suggestions. We have addressed all the questions here:

*** Response to questions about the performance of N2N: ***

All the numbers of N2N are from their original paper but N2N did not test their method to compress ShuffleNet so we do not have the performance of N2N on ShuffleNet. N2N did not test their method under the setting VGG-19 on CIFAR-100 either. For ResNet-34 on CIFAR-100, N2N only provides results of layer removal (indicated by ‘N2N - removal’ in Table 1 in our paper) so for fair comparison, we compare  ‘N2N - removal’ with ‘Ours - removal’, which refers to only considering the layer removal operation in the search space. ‘Ours - removal’ also significantly outperforms ‘N2N - removal’ in terms of both the accuracy and the compression ratio.


*** Response to questions about experiment results: ***

We re-run the experiments for 3 times and update the results in the paper (please check the PDF). In Table 1, we show the mean and standard deviation of the results for ‘Ours’ and ‘Random Search’. We observe that after multiple runs, the average performance of our method also outperforms all the baselines as before.


*** Response to questions about the related work: ***

We have updated the paper and added this paper in related work. Also in the conclusion section, we think it’s an interesting future direction to combine their method with our proposed embedding space to identify the Pareto set of the architectures that are both small and accurate. Thanks for suggesting the related work!


*** Response to questions about the originality of our work: ***

We would like to emphasize that our key contribution is a novel method that incrementally learns an embedding space for the architecture domain, i.e., a unified representation for the configuration of architectures. The learned embedding space can be used to compare architectures with complex skip connections and multiple branches and we can combine it with any Sequential Model-Based Optimization (SMBO) method (we choose GP based BO algorithms in this work) to search for desired architectures. Based the learned embedding space, we present a framework of searching for compressed network architectures with Bayesian optimization (BO). The learned embedding provides a feature space over which the kernel function of BO is defined. Under this framework, we propose a set of architecture operators for generating architectures for search and a multiple kernel strategy to encourage the search algorithm to explore more diverse architectures.

We demonstrate that our method can significantly outperform various baseline methods, such as random search and N2N (Ashok et al.,2018). The compressed architectures found by our method are also better than the state-of-the-art manually-designed compact architecture ShuffleNet (Zhang et al., 2018). We also demonstrate that the learned embedding space can be transferred to new settings for architecture search, such as a larger teacher network or a teacher network in a different architecture family, without any training.


</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1lpeTl3YQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Typos in the Paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1xLN3C9YX&amp;noteId=B1lpeTl3YQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1455 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Sep 2018</span><span class="item">ICLR 2019 Conference Paper1455 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">In the right part of Table 2, 'Architecture Teacher #Params' should be 'Teacher Accuracy #Params' and 'Congiguration Teacher #Params' should be 'Configuration Accuracy #Params'.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>