<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Byey7n05FQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Plan Online, Learn Offline: Efficient Learning and Exploration via..." />
      <meta name="og:description" content="We propose a plan online and learn offline framework for the setting where an agent, with an internal model, needs to continually act and learn in the world. Our work builds on the synergistic..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Byey7n05FQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control</a> <a class="note_content_pdf" href="/pdf?id=Byey7n05FQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019plan,    &#10;title={Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Byey7n05FQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=Byey7n05FQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We propose a plan online and learn offline framework for the setting where an agent, with an internal model, needs to continually act and learn in the world. Our work builds on the synergistic relationship between local model-based control, global value function learning, and exploration. We study how local trajectory optimization can cope with approximation errors in the value function, and can stabilize and accelerate value function learning. Conversely, we also study how approximate value functions can help reduce the planning horizon and allow for better policies beyond local solutions. Finally, we also demonstrate how trajectory optimization can be used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation. This exploration is critical for fast and stable learning of the value function. Combining these components enable solutions to complex control tasks, like humanoid locomotion and dexterous in-hand manipulation, in the equivalent of a few minutes of experience in the real world.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">deep reinforcement learning, exploration, model-based</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a framework that incorporates planning for efficient exploration and learning in complex environments.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HklU137bCQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>additional reviewer input, after consideration of author responses ?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byey7n05FQ&amp;noteId=HklU137bCQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1318 Area Chair1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1318 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you to everyone for the detailed reviews and the authors for their detailed responses.

Now would be a great time to hear from the reviewers as to whether their concerns
have been addressed, and if they wish to make any score adjustments.

Thanks in advance for this additional input.
-- area chair
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r1grAPeCa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Summary of responses</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byey7n05FQ&amp;noteId=r1grAPeCa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1318 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1318 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank all the reviewers and the area chair for taking time to read our paper and providing feedback. The summary of our responses to common questions raised by the reviewers is below. We look forward to continued discussions to address any additional questions.

&gt;&gt;&gt; Source of dynamics model

In this work, we assume that we have access to the ground truth dynamics model. We *do not* believe that this is an unreasonable assumption, especially for our motivating problems. Good models based on knowledge of physics or through learning (system identification) are available in most engineering and robotics settings. Indeed, most successful robotics results have been through use of models or simulators (Boston Dynamics, Honda, OpenAI). The work is also directly relevant for fields where dynamics models are available (e.g. character animation in graphics) and simulation to reality transfer, which is gaining a lot of interest in robotic learning.

We also emphasize that knowing the dynamics does not make the problem trivial, and does not imply that one can simply “pre-solve” the MDP and deploy the solution. Some aspects of the MDP may be revealed only at deployment time, such as the states where we may want to concentrate the approximation power, or the reward function may be revealed only at deployment time (robot knows physics but does not know which task to solve). We thus feel that algorithms for real-time action selection is an important component to enable robots to behave competently in dynamic environments.

&gt;&gt;&gt; Novelty

POLO combines three threads of work into one coherent and elegant algorithm that produces impressive results. All reviewers have pointed out and noted that the motivation and presentation of the algorithm is clear and neat, and the results are impressive. While it may be easy to postulate that bringing together these threads of work is important, the specifics of how to do this to produce robust algorithms with impressive results is highly non-trivial and far from obvious. We feel that the quality of results should be taken into account when assessing the novelty. Indeed, one could argue that landmark results like AlphaGo and AlphaZero do not make deep contributions to any sub-field of RL/ADP, but it remains one of the most impressive feats due to bringing together different algorithmic sub-components and showing impressive results. We also note that the component of planning/MPC to explore, which we demonstrate in the maze example, has not been explored in continuous control.


We hope that the above clarifications also help to resolve other questions that were raised. In particular, our goal is *not* to bridge model-free and model-based RL methods; nor is to provide strong performance bounds for MPC. We make no claims about the former, and we merely use the latter as a motivation to develop a practical algorithm. While these are very important questions, they are not our focus and beyond the scope of the current submission. We kindly request that our paper be evaluated on the basis of results for the problem setting we study, as opposed to insights to other problem domains/settings.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJewsWF937" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Lucid paper with nice ideas, but problem setting not completely clear</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byey7n05FQ&amp;noteId=rJewsWF937"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1318 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1318 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper was a joy to read.   The description and motivation of the POLO framework was clear, smart, and sensible.  The fundamental idea is to explore the interplay between value-function estimation and model-predictive control and demonstrate how they benefit one another.  None of these ideas is fundamentally new, but the descriptions and their combination is very nice.

As I finished the paper, though, I was left with a lingering lack of understanding of the exact problem setting that is being addressed. The name is cute but didn't help clarify.    As I understand it:
- we have a correct dynamics model (I'm assuming that's what "nominal dynamics model" means) and a good trajectory optimization algorithm
- the agent has limited online cognitive capacity
- there is no opportunity for offline computation
If offline computation time were available, then we could run this algorithm (or your favorite other RL algorithm) in the agent's head before taking any actions in the actual world.   That does not seem to be the setting here, although it does seem to me that you might be able to show that POLO is a good algorithm for finding a value function, offline, with no actual interaction with the world.

So, fundamentally, this paper is about action under computational time constraints.   One strategy would be for the robot to use 7 of its cores to run your favorite approximate DP / RL algorithm in parallel with 1 core that's used for action selection.  Why is that worse than your algorithm 1?

Setting this question aside, I had some other comments:
- It is better *not* to use "trajectory optimization" and "model-predictive control" interchangeably.  I can use traj opt in other circumstances (e.g. with open loop trajectory following) and could use other planners for MPC.
- Some version of lemma 2 probably (almost certainly) already exists somewhere in the literature;  I'm sorry, though, that I can't point you to a concrete reference.
- The argument about MPC letting us approximate H Bellman backups is plausible, but seems somewhat subtle;  it would be good to elaborate it in some more detail.
- The set of assertions and experiments is very nice.
- Why are no variances shown in figure 3?   Why does performance seem to degrade after a certain horizon.

This paper doesn't seem really to be about learning representations.  I don't know if that's important to the ICLR decision-making.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkeXEnOipQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byey7n05FQ&amp;noteId=SkeXEnOipQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1318 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1318 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for taking time to review our paper and for the constructive feedback. We greatly appreciate the comment that you enjoyed the exposition, assertions, and results in the paper. We look forward to continued fruitful discussions!

==================
Problem Setting
==================
The agent knows the MDP dynamics, but the MDP can be very complex with some information about the MDP revealed only at deployment time. Hence, it is not feasible in general to “pre-solve” the MDP and simply deploy the solution. For instance, we may know the state distribution only at deployment time and hence not know where to concentrate the approximation power in policy gradient or dynamic programming methods. Also, the reward function may be revealed only at deployment time (the robot knows physics but doesn’t know which task to do until human command). This is the general premise of real-time MPC which has enjoyed tremendous success in controlling complex systems in engineering and robotics. At the same time, we note that if there is a possibility to pre-solve the MDP before deployment, POLO can be used for this purpose as well and our experiments show that POLO is more efficient than fitted value iteration.

=======================
Significance and novelty
=======================
First and foremost, we emphasize that POLO produces very impressive results for hard continuous control tasks as noted by all the reviewers. POLO requires 1 CPU hour as opposed to 500 CPU hours reported by OpenAI (our numbers with PG are similar as well, and we will include these with the final paper). While model-free RL obviously does not require access to a model, the overwhelming majority of results in RL (e.g. AlphaGo, Atari, MuJoCo) are in simulated environments where a model is available by design. Model based methods have also been very successful in robotics (e.g. Boston Dynamics’ Atlas, Honda’s Asimo, OpenAI’s dexterous hands). Thus, we believe that knowing the dynamics model is not a severe limitation. One can interpret POLO as a very strong model-based baseline that model-free RL algorithms can strive to compete with, or as a powerful vehicle with direct applicability for simulation to reality transfer, which is a topic of immense interest in robot learning.

POLO combines three important and deep threads of work: MPC, approximate dynamic programming, and exploration. The primary contribution of this work is to connect the three threads as opposed to making a contribution to any one. We believe that combining these threads of work into a simple and elegant algorithm that produces impressive results to be important and valuable. We emphasize that all reviewers found the motivation and presentation of the algorithmic framework to be elegant. Furthermore, combining MPC and uncertainty quantification to do efficient and targeted exploration has not been explored in the past in continuous control.

=======================
Reg. alternate approach
=======================
You are indeed correct that the core question is about action selection with bounded resources at run-time. In this setting, using any RL/DP algorithm on 7 cores, it is natural to focus the search process around the current state of interest due to limited resources. Thus, the suggested approach reduces to MPC -- 7 cores perform local rollouts which are then combined by the final core in some way -- either non-parametric blending with exponentiated costs (MPPI), a fitted form of iLQG, or some alternative. We show in our results that POLO outperforms trajectory centric RL which is synonymous with MPC.

=======================
Additional comments
=======================
We will include additional discussion about the following suggested components in the final version: (a) trajectory optimization vs MPC; (b) H-step Bellman backups; (c) error bars for the plots.
We agree that trajectory optimization has broader connotations than MPC. In this work, we used it in the context of real-time trajectory optimization which is synonymous with MPC. We will clarify the distinctions in the paper.
We also emphasize that Lemma 2 is not a primary contribution of the paper -- it primarily serves as a motivation for the algorithm we develop. We agree that prior work should have Lemma 2, since it is fairly elementary, and will include additional citations if we find the appropriate sources.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SJxZOqMtnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>limited insight and novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byey7n05FQ&amp;noteId=SJxZOqMtnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1318 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1318 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this paper, the authors propose POLO, a reinforcement learning algorithm which has access to the model of the environment and performs RL to mitigate the planning cost. For the planning, POLO uses the known model of the environment up to a fixed horizon H and then use an approximated value function in the leaf nodes. This way, instead of planning for an infinite horizon, the planning is factored to a shorter horizon, resulting in lower computation cost.

The novelty and motivation behind this approach is limited. Similar or even more general approach for discrete action space is introduced in "Sample-Efficient Deep RL with Generative Adversarial Tree Search" where they also learn the model of the environment and additionally consider the error due to the model estimation. There is also a clear motivation in the mentioned paper while I could not find a convincing one for the current paper. 
Putting the novel limitation aside,  both of these paper, the current paper, and the paper I mentioned, suffer from very lose estimation bounds. Both of these works bound somewhat similar (not the same) things via L_inf error of value function which in practice does not necessarily result in useful or insightful upper bounds (distribution dependent bound is desired). Moreover, with the assumption of knowing the environment model, the implication of the current work is significantly limited.

The authors do a good job of writing the paper and the paper is clear which is appreciatable.

In equation 6 the authors use log-sum-exp and claim it corresponds to UCB, but they do not provide any evidence to support their claim. 

In addition, the Bayesian linear regression in the tabular setting is firstly proposed in Generalization and Exploration via Randomized Value Functions and beyond tabular setting (the setting in the current paper) was proposed in Efficient Exploration through Bayesian Deep Q-Networks. 

The claims in this paper are not strong enough and the empirical study does not strongly support or provide sufficient insight. For example experiments in section 3.2 does not provide much insight beyond common knowledge.

While bridging the gap between model based and model free approaches in RL are significantly important research directions in RL, I do not find the current draft significant enough to shed sufficient light into this topic.



</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Byen-JFsa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to review (1/2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byey7n05FQ&amp;noteId=Byen-JFsa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1318 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1318 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for taking time to review our paper and for the feedback. We address your concerns below, and hope that our clarifications would help appreciate the work better. We look forward to continued fruitful discussions.

=======================
Significance &amp; Novelty
=======================
POLO combines three important and deep threads of work: MPC, approximate dynamic programming, and exploration. The primary contribution of this work is to connect the three threads of work as opposed to making a contribution to any one. We believe that combining them into a simple and elegant algorithm that produces impressive results is important and valuable. We emphasize that all reviewers found the motivation and presentation of the algorithmic framework to be elegant. While the combination of MPC and value function may seem straightforward, it has not found wide applicability in continuous control settings in the past. For example, Zhong et al. study the setting of learning value function to help MPC and found the contribution of the value function to be minimal in their settings. We also emphasize that combining MPC and uncertainty quantification to do efficient and targeted exploration has not been studied in the past in continuous control settings.

Our empirical study attempts to isolate individual benefits enabled by each component in the POLO framework. Firstly, we have clearly demonstrated that learned value functions can support short horizon MPC. This has not been explored extensively in controls applications, and most MPC works do not consider learning a value function using the interaction data. Secondly, we demonstrate the utility of uncertainty quantification and MPC for exploration, through the maze example. Further, we demonstrate that MPC accelerates value function learning. While individual components may have been suggested before (Bellman himself suggests using prior experience to reduce planning computation), we present all the benefits in one elegant framework that actually achieves very strong empirical results in practice as noted by other reviewers.

=======================
Known dynamics model
=======================
First and foremost, we emphasize that in the known dynamics setting our algorithm significantly outperforms model-free RL methods like policy gradient. While model-free RL obviously does not require access to a model, the overwhelming majority of results in RL are in simulated environments (e.g. AlphaGo, Atari etc.) where a model is available by design. Furthermore, the majority of successful results in robotics are also through model-based methods (eg Boston Dynamics' Atlas, Honda's Asimo, OpenAI's dexterous hands). Thus, one can interpret POLO as a very strong model-based baseline that model-free RL algorithms can strive to compete with, or as a powerful vehicle with direct applicability for simulation to reality transfer, which is a topic of immense interest in robot learning.

Furthermore, we wish to point out that knowing the dynamics does not make the problem trivial. Certain aspects of the MDP may be revealed only at run-time, thereby ruling out the possibility of pre-solving the MDP. For instance, we may not know the states to concentrate the approximation power of policy search or dynamic programming methods till deployment time. The reward function may also be revealed only at deployment time (robot knows physics, but does not know what task to do till the human tells it). Thus, having algorithms that can compute good actions at run-time is critical for a variety of settings, and we show in our results that POLO outperforms MPC.

Finally, we wish to point out that we make explicit the assumption of knowing the dynamics model, and do not even attempt to bridge model-free and model-based RL methods (as used in the connotation of recent papers). We feel that it is important to not judge the work on the basis of a problem we are not attempting to solve.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJxWryti67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to review (2/2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byey7n05FQ&amp;noteId=rJxWryti67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1318 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1318 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">=======================
Related Works
=======================
Thank you for pointing out the GATS paper, we have included a citation to it in our updated submission. As discussed earlier, the broad idea of combining planning and value function learning is not new. However, intuitions and lessons learned from discrete settings rarely transfer to continuous domains. For instance, global value or Q learning methods have not produced great results in continuous control with high-dimensional action spaces, while DQN performs very well in Atari which has a small number of discrete actions. Similarly, very different planning approaches are used in discrete action settings (e.g. UC-Trees) and continuous robotics problems (e.g. iLQG, PI^2, RRT). We emphasize that in the continuous control settings, we can synthesize controllers orders of magnitude more efficient than currently used approaches like PPO in the OpenAI dexterous hand work.

Thanks for pointing out the other papers studying Bayesian linear regression, we have included citations to those as well. We would like to emphasize that the computational view of Bayesian regression is not the contribution of this work. Rather, we use it as a means to perform uncertainty estimation and drive exploration in the POLO framework.

=======================
Answers to other questions
=======================
- Regarding equation 6, we actually *do not* claim that our approach corresponds to UCB. Rather, we only say that log-sum-exp is a risk seeking objective and corresponds to optimism in the face of uncertainty, and this broad heuristic has been used successfully in other works.
- Regarding Lemma 2, this is not a primary contribution of our work, and is fairly elementary. We use it primarily as a motivation for the practical algorithm we develop. We agree that the L_inf norm bounds are loose and tighter bounds would be great, but that is orthogonal to the main points of this paper.

=======================
Summary
=======================
In summary, we have presented an elegant framework and algorithm that offers tangible benefits in the space of continuous control. This enables solutions to complex control problems orders of magnitude more efficiently than currently used techniques. The work should be evaluated based on the clean presentation and strong empirical results as opposed to weak connections to problems and bounds we do not focus on.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_BJgIdkoD2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice results but lean technical contribution</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byey7n05FQ&amp;noteId=BJgIdkoD2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1318 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1318 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes to combine fitted value iteration with model predictive control (MPC) to speed up the learning process. The value iteration is the "Learn offline" subsystem while MPC is the "Plan online" subsystem. In addition, this paper also proposes an exploration technique that increases exploration if the multiple value function estimators disagree. The evaluation is complete and shows nice results.

However, I did not rank this paper high for two reasons. First, it is not clear to me how the model is acquired in MPC. Does the method learn the model? Does the method linearize the dynamics and assume a linear model? I am not sure. I suspect that the method just uses the simulator as the model. If it is the case, the method is not so useful because for complexity systems, such as humanoids, we do not know the model. And the comparisons with model-free learning algorithms are not fair because the paper assumes that the model is given. If this is not the case, I suggest that a more detailed description of MPC should be presented in Section 2.3.

Second, the technical contributions are lean. The three main components, 1) fitted value iteration, 2) MPC and 3) exploration based on multiple value function estimates, are not novel. The combination of them seems straight forward. For example, the H-step Bellman update (Section 2.3) is a blend between Monte-Carlo method and Q learning. It seems to be similar to the TD(\lambda) method. Thus, it is not surprising that it can accelerate convergence of value function.

For the above reasons, I would not recommend accepting this paper at this time.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Syl5JztsaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Byey7n05FQ&amp;noteId=Syl5JztsaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1318 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1318 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for taking time to review our paper, and for your analysis and review. We address your two concerns as follows:

=======================
Regarding source of model
=======================
In this work, we assume that we have access to the dynamics model of the environment. We do not believe that this is a severe limitation because reasonable dynamics models are known for a majority of complex engineered systems including robotics. Indeed, most success stories in robotics are through use of models/simulators. Notable examples include Boston Dynamics’ Atlas, Honda’s Asimo, and the recent in-hand manipulation results from OpenAI. There is also a growing body of work and interest in simulation to reality transfer in RL for robotics, and we believe that POLO would serve as a strong baseline method for this research direction. POLO is also complementary to fields like learning dynamics models and nonlinear system identification.

Furthermore, we also want to emphasize that knowing the dynamics does not make the problem trivial. Certain aspects of the MDP may be revealed only at run-time, thereby ruling out the option of essentially pre-solving the MDP before deployment time. For instance, we may not know the states to concentrate the approximation power of policy search or dynamic programming methods till deployment time. The reward function may also be revealed only at deployment time (robot knows physics, but does not know what task to do till the human tells it). Thus, having algorithms that can compute good actions at run-time is critical for a variety of settings, and we show in our results that POLO outperforms MPC.

=======================
Comparisons to model-free RL
=======================
Model-free RL does not assume explicit knowledge of the dynamics, which is certainly a weaker assumption that in the POLO case. However, model-free RL has predominantly been demonstrated only in simulated environments where a model is available by definition (e.g. AlphaGo, Atari, MuJoCo). We believe that POLO would be an important contribution for researchers studying simulation to reality transfer, since it is orders of magnitude more efficient than running model-free RL in simulators. We have updated the paper to reflect this comparison more accurately.

=======================
Significance and novelty
=======================
POLO combines three important and deep threads of work: MPC, approximate dynamic programming, and exploration. The primary contribution of this work is to connect the three threads of work, into a simple and elegant algorithm, as opposed to making a contribution to any one of the streams. We believe that combining the threads of work into a practical algorithm that produces impressive results to be important and valuable. We emphasize that all reviewers found the motivation and presentation of the algorithmic framework to be elegant. While the combination of MPC and value function may appear seemingly straightforward, it has not been found effective in continuous control in the past. For example, Zhong et al. study the setting of learning value function to help MPC and found the contribution of the value function to be minimal in their settings. We also emphasize that combining MPC and uncertainty quantification to do efficient and targeted exploration for continuous control has not been studied in the past.

=======================
Summary
=======================
To summarize, while each component of POLO is well studied, combining them into a practical algorithm that produces impressive results is far from obvious. Combining the different components allows POLO to synthesize and learn competent behaviors on the fly for high dimensional systems. While we assume to know the dynamics model, this is not an outlandish assumption given the prevalence of complex model based robotic control in the real world, and the growing body of work in learning dynamics models, intuitive physics, and simulation to reality transfer.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>