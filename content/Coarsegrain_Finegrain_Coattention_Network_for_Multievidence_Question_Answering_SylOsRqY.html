<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Syl7OsRqY7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Coarse-grain Fine-grain Coattention Network for Multi-evidence..." />
      <meta name="og:description" content="End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume the answer and evidence appear close together in a..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Syl7OsRqY7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering</a> <a class="note_content_pdf" href="/pdf?id=Syl7OsRqY7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 18 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019coarse-grain,    &#10;title={Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Syl7OsRqY7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=Syl7OsRqY7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume the answer and evidence appear close together in a single document. In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents. The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query. We implement these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input. On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">question answering, reading comprehension, nlp, natural language processing, attention, representation learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A new state-of-the-art model for multi-evidence question answering using coarse-grain fine-grain hierarchical attention.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Byxm3fiRpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Summary of updates</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syl7OsRqY7&amp;noteId=Byxm3fiRpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper340 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper340 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear reviewers,

We thank you sincerely for your feedback! We have updated the draft with the information below. A more detailed update can be found in our individual responses to the reviews.

1. We added additional experiments that demonstrate the effectiveness of our method on TriviaQA. In particular, reranking the BiDAF++ span extraction model with our model provides a gain of 3.1% EM and 3.0% F1 on the Wiki dev set.
2. We added the IDs of examples used in the error analysis to the Appendix.
3. We added references to related work in memory networks and query-based multi-document summarization.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HkxVLCtKnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Method that is well adapted to the task to be solved; clear and well written</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syl7OsRqY7&amp;noteId=HkxVLCtKnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper340 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper340 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a method for multi-hop QA based on two separate modules, which are called coarse-grained and fine-grained modules. The coarse-grained module reads all of the supporting documents for QA, whereas the fine-grained one reads the local context surrounding each candidate entity's mentions. Both modules are used to predict the score of a candidate entity being the answer, with the final result being the sum of the two scores.

This is a fine paper and achieves a new state of the art on the Qangaroo multi-hop QA dataset. The paper is clearly written, presents the models intuitively, while not foregoing technical detail should that be interesting to a reader. I appreciated the ablation results, as well as the qualitative analyses. The overall idea of encoding different levels of context is an important one, and I am glad that this paper shows that this approach works for a complex QA task.

There are two downsides to the paper. The first is that I am not sure it is really accurate to call the coarse-grained model as such, as it still seems to require passing every word in the supporting documents to an encoder. It seems to be more aimed at capturing global information from the supporting documents, rather than to make a quick, high-level pass at inference. The second weakness is that the coarse- and fine-grained modules barely interact at all, as their prediction scores are simply summed at the output layer. It is nice that even such a simple method of interaction already works so well, but I would have expected some exploration or comment on how more interactions could be enabled.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1xqtoV6p7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer 1 comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syl7OsRqY7&amp;noteId=H1xqtoV6p7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper340 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper340 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank you for your feedback, and are glad that you found the results to be useful and manuscript to be clearly written and meaningfully ablated and analyzed.


RE: the name “coarse-grain”

We chose the word “coarse” for the act of summarizing the entire document collection without observing the question. That is, the resulting summary must compact the documents into a high-level representation without observing the candidate answer (hence “coarse”). In contrast, the fine-grain module summarizes with respect to the candidate answer, hence the summary is more precise (hence “fine”).

RE: lack of interactions between fine-grain and coarse-grain modules

While the modules do not directly interact, the encoders and the embeddings are shared (please see Figure 1). We do agree that finding avenues for interactions is an interesting area for future work. For example, one can extract mention representations from the coattention representations of the coarse-grain module instead of the support encoder. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1xSi3WtnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good work; could be more interesting to see application in other related tasks</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syl7OsRqY7&amp;noteId=S1xSi3WtnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper340 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper340 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes an interesting coarse-grain fine-grain coattention network architecture to address multi-evidence question answering and achieves the new state-of-the-art results on the Qangaroo WikiHop dataset.  The main idea is to divide the task across the coarse-grain and fine-grain modules in a complimentary manner such that the coarse-grain module learns from efficient modeling of support documents and the query whereas the fine-grain module learns from associations of candidate mentions in the support documents with the query. 

The major strength of the model is observed with learning effective representations of larger numbers of long support documents and the state-of-the-art results are achieved without the use of pretrained contextualized embeddings. The main novelty lies in how the coattention and self-attention strategies are combined hierarchically to learn relevant representations in a complimentary fashion (rather than serial). Overall, the paper is very well-written and presents solid results with meaningful ablation study, quantitative and qualitative analyses. I have a few comments/suggestions:

- It would be interesting to see how the inclusion of pretrained contextualized embeddings such as ELMo, ULMFit, BERT would help the current model. 

- "This is likely because coreference resolution captures intra-document and inter-document dependencies more accurately than hierarchical attention." --&gt; Please clarify why this is the case.

- "We hypothesize that ways to reduce this type of error include using more robust pretrained contextual encoders (McCann et al., 2017; Peters et al., 2018) and coreference resolution." --&gt; I agree; also, it would be worth considering some commonsense knowledge to alleviate this issue, because the fact that Scotland is a part of UK and has a border with England should be learned. Here is a relevant work: "Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge" by Mihaylov and Frank, 2018.

- "The second type (28% of errors) results from questions that are not answerable. For example, the support documents do not provide the narrative location of the play “The Beloved Vagabond” for the query narrative location the beloved vagabond." --&gt; It would be great if you could release the set of unanswerable questions for the community.

- Please include the memory network-based QA works in the related work section because they involve some forms of reasoning. Also, I would suggest to cover the query-focused multi-document summarization area in the related work section because they also require evidence synthesis from multiple documents to address a query. It would be very interesting if authors can apply their model for the query-focused multi-document summarization task as well, as this would further validate the effectiveness of the proposed architecture for reasoning across multiple documents.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJeMGh4p6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer 2 comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syl7OsRqY7&amp;noteId=HJeMGh4p6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper340 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper340 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank you for your feedback, and are glad that you found the results to be useful and manuscript to be well-written and meaningfully ablated and analyzed.  

RE: incorporation of world knowledge and common-sense knowledge

Thank you for the suggestion!

Zellers et al. [1] find that contextual embeddings capture a surprising amount of common sense knowledge (please see the EMNLP 2018 presentation on new SWAG results [2]). Perhaps this accounts for the consistent gains brought about by CoVe [3], ELMo [4], and BERT [5] on a wide variety of NLP tasks.

Unfortunately we could not do these experiments in time due to computational constraints. In particular, on-the-fly evaluation of contextualized embeddings required too much memory given the large documents lengths in Qangaroo. We plan to look into practical ways of using these embeddings in future work.


RE: coref captures dependencies more accurately than hierarchical attention

We mean that the type of coref we use (e.g. entity matching) introduces very precise direct links, whereas links implicitly captured by soft hierarchical attention is gated and hence noisy. We will clarify this in the writing.


RE: list of unanswerable questions

We agree and will release a list of unanswerable questions found during our manual inspection of a subset of the dataset.


RE: reference to related works in memory network-based QA and query-focused multi-document summarization

Thank you bringing this to our attention. We will include these in our related works section. We’ll also explore the applicability of our method to query-focused multi-document summarization in the future.

After receiving our reviews, we experimented with our model on another task of reranking extractive question answering. We ran preliminary experiments in which we applied our model to rerank the outputs produced by (the merge version of) BiDAF++ [9] on TriviaQA [8]. In particular, we rerank the top-50 answers produced by BiDAF++ (let’s refer to the as the “N-best list”). We find that on the subset of the dev set in which the ground-truth answer is present in the N-best list (which is 86.8% of the data), reranking using our method improves exact match accuracy (EM) from 59.8% to 63.2% and F1 from 64.5% to 67.8%. On the subset in which the ground-truth answer is not present in the N-best list (which is 13.2% of the data), reranking using our method improves EM from 17.5% to 18.4% and F1 from 22.2% to 23.2%. On the entire dev set, reranking using our method improves EM from 54.0% to 57.1% and F1 from 58.7% to 61.7%. Hence, we think our method can be used to improve existing span-extractive models as a reranker (e.g. similar to [6-7]). We will add this result to the paper.

[1] <a href="https://arxiv.org/abs/1808.05326" target="_blank" rel="nofollow">https://arxiv.org/abs/1808.05326</a>
[2] https://drive.google.com/file/d/1nRJDlDNVsbBf75tmYIwZj48HM9l4kIxA/view?usp=sharing
[3] https://arxiv.org/abs/1708.00107
[4] https://arxiv.org/abs/1802.05365
[5] https://arxiv.org/abs/1810.04805
[6] https://arxiv.org/abs/1808.05759
[7] https://arxiv.org/abs/1711.05116
[8] https://arxiv.org/abs/1705.03551
[9] https://arxiv.org/abs/1710.10723
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Sklnu2iP3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>I am open to increasing my rating score as long as the authors could address my concerns and confusions below</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syl7OsRqY7&amp;noteId=Sklnu2iP3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper340 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper340 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper focuses on multi-choice QA and proposes a coarse-to-fine scoring framework. Where the coarse-grained answer scoring model computes the scores with the attention over the whole passages, and the fine-grained one only uses local contexts for each answer option (candidate).

The proposed approach was evaluated on the only dataset of WikiHop, and achieved large improvement over the other methods on the leaderboard. However, I found the paper lack of motivation about the designs of the coarse and fine scoring models. For example, why using self-attention after GRU and co-attention in the two answer scoring models?

Another concern I have is about the novelty. Besides the complicated model designs, the coarse and fine scoring models are both following some common ideas in previous work. And each model could achieve on-par results compared to previous baselines. This makes me feel that the whole approach looks more like model combination of two not-so-novel (and not very well-motivated) models.

Thirdly, the only evaluation on WikiHop brings more problems to the above two points. Since the motivation of the architecture design is not very clear, I am not sure whether the architectures could generalize to other benchmarks. Similar concern for the model combination approach.

Moreover, the proposed approach is a general architecture for multiple-choice datasets requiring multiple evidence. To verify its generalizability, I suggest the authors add further experiments on one dataset from the following ones: either multi-choice QA datasets like ARC and RACE/RACE-open, or other open-domain QA datasets like TriviaQA, by treating the re-ranking of answer predictions as multi-choice QA problems (like the approach in Evidence Aggregation for Open-Domain QA from ICLR2018).

A minor question: why the CFC w/o encoder could still work so well? At least the fine-grained scoring model should heavily rely on encoders. Otherwise, according to Eq (17), the fine-grained model cannot use any contextual information.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByeIepE667" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Part 1 response to reviewer 3 and additional results on TriviaQA reranking</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syl7OsRqY7&amp;noteId=ByeIepE667"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper340 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper340 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank you for your detailed feedback! Sorry about breaking this up into 2 parts, it seems we hit the post limit. The results for TriviaQA reranking is in part 2 of this response.

RE: motivation of design of model and novelty

One lesson from prior work on extractive question answering [2, 14] show that the initial encoding of the inputs are crucial to model performance (whereas subsequent decoding matter much less). The coarse-grain and fine-grain modules of the CFC emphasize two complementary approaches to build this initial encoding. The first approach is to read the document with respect to the question, and then find the appropriate answer. The initial encodings produced here are highly relevant to the question, but they do not capture the context in which the answer is mentioned. The second approach is to read the document with respect to the candidate answer, and then consider how relevant this answer is to the question. The initial encodings produced here precisely model the context in which the answer is mentioned, however they do not capture interactions with the question until later. We designed the CFC to incorporate these two complementary approaches into a single network. As a result, our model is able to retain and focus on different aspects of the input (this is shown in our analysis in sections 4.1 and 4.2) and achieve state-of-the-art results on a competitive reading comprehension task.


RE: motivation behind the particular usage of self-attention and coattention

We use co-attention to build codependent representations of two inputs. This has been crucial to extractive question answering as well as visual question answering [1-3]. We use self-attention in this work as a means to aggregate information over a variable-length sequence. Prior work have shown its effective across a wide range of NLP tasks [4-8]. An alternative method to information aggregation is pooling, which we compare against in our ablation studies and find to perform worse than self-attention. Our intuition is that self-attention allows for more flexibility in the model and is able to learn task-specific pooling strategies (e.g. emphasize certain regions of the input space over others, as oppose do maximization/minimization/average over features)

The particular design we choose is influenced by what kind of information aggregation we would like to perform. For the coarse-grain module, we would like to first interpret a supporting text with respect to the query, hence we apply coattention over the supporting text and the query. Next, we would like to summarize each variable length supporting text, hence we apply self-attention over each coattention output. Next, we would like to produce a summary of the document collection, which consists of a variable number of supporting texts, hence we apply self-attention again over the set of supporting text summaries. Finally, we query this summary using an answer summary, which we also produce using self-attention. For the fine-grain module, we must first summarize each variable length mention of the answer, hence we apply self-attention. Next, we would like to interpret these mentions with respect to the question, hence we apply coattention. Finally, we would like to summarize this reinterpretation to produce a score, hence we apply self-attention again.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hyxq-pVap7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Part 2 response to reviewer 3 and additional results on TriviaQA reranking</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Syl7OsRqY7&amp;noteId=Hyxq-pVap7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper340 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper340 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">RE: reranking existing question answering models, as well as other other tasks

Thanks for mentioning this! After considering your feedback we ran preliminary experiments in which we applied our model to rerank the outputs produced by (the merge version of) BiDAF++ [13] on TriviaQA [12]. In particular, we rerank the top-50 answers produced by BiDAF++ (let’s refer to the as the “N-best list”). We find that on the subset of the dev set in which the ground-truth answer is present in the N-best list (which is 86.8% of the data), reranking using our method improves exact match accuracy (EM) from 59.8% to 63.2% and F1 from 64.5% to 67.8%. On the subset in which the ground-truth answer is not present in the N-best list (which is 13.2% of the data), reranking using our method improves EM from 17.5% to 18.4% and F1 from 22.2% to 23.2%. On the entire dev set, reranking using our method improves EM from 54.0% to 57.1% and F1 from 58.7% to 61.7%. Hence, we think our method can be used to improve existing span-extractive models as a reranker (e.g. similar to [9-10]). We will add this result to the paper.

The setup in the ARC/RACE/RACE-open are such that
- The evidence is a collection of short sentences (a key challenge is IR)
- The answers are propositions whose truth values are determined by evidence in the sentences

In contrast, Qangaroo is such that
- The evidence is a collection of paragraphs obtained from relation-guided knowledge graph traversal
- The answers are plausible entities for the relation in interest

While we think both of these settings are interesting, our model is designed for the latter and unsuitable for the former since the the answers are not entities, so the fine-grain module does not have entity mentions to attach to. One can hypothetically extract entities from non-entity answers, however for ARC/RACE/RACE-open, multiple answer choices refer to the same entity (e.g. he did A vs. he did B). That is, the mentions are shared between answers and do not distinguish between answers. Consequently the output of the fine-grain module for answers that share entities would be the same.


RE: question on encoder-less ablation

The fine-grain module still has a bidirectional GRU inside the coattention (Please see eq. 18 and eq. 7)

[1] <a href="https://arxiv.org/abs/1606.00061" target="_blank" rel="nofollow">https://arxiv.org/abs/1606.00061</a>
[2] https://arxiv.org/abs/1611.01604
[3] https://arxiv.org/abs/1711.00106
[4] https://arxiv.org/abs/1801.10296
[5] https://arxiv.org/abs/1805.01052
[6] https://arxiv.org/abs/1707.07045
[7] https://arxiv.org/abs/1706.03762
[8] https://arxiv.org/abs/1805.09655
[9] https://arxiv.org/abs/1808.05759
[10] https://arxiv.org/abs/1711.05116
[11] https://arxiv.org/abs/1805.08092
[12] https://arxiv.org/abs/1705.03551
[13] https://arxiv.org/abs/1710.10723
[14] https://arxiv.org/abs/1611.01603</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>