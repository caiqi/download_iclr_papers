<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Rectified Gradient: Layer-wise Thresholding for Sharp and Coherent Attribution Maps | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Rectified Gradient: Layer-wise Thresholding for Sharp and Coherent Attribution Maps" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Hkemdj09YQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Rectified Gradient: Layer-wise Thresholding for Sharp and Coherent..." />
      <meta name="og:description" content="Saliency map, or the gradient of the score function with respect to the input, is the most basic means of interpreting deep neural network decisions. However, saliency maps are often visually..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Hkemdj09YQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Rectified Gradient: Layer-wise Thresholding for Sharp and Coherent Attribution Maps</a> <a class="note_content_pdf" href="/pdf?id=Hkemdj09YQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019rectified,    &#10;title={Rectified Gradient: Layer-wise Thresholding for Sharp and Coherent Attribution Maps},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Hkemdj09YQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=Hkemdj09YQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Saliency map, or the gradient of the score function with respect to the input, is the most basic means of interpreting deep neural network decisions. However, saliency maps are often visually noisy. Although several hypotheses were proposed to account for this phenomenon, there is no work that provides a rigorous analysis of noisy saliency maps. This may be a problem as numerous advanced attribution methods were proposed under the assumption that the existing hypotheses are true. In this paper, we identify the cause of noisy saliency maps. Then, we propose Rectified Gradient, a simple method that significantly improves saliency maps by alleviating that cause. Experiments showed effectiveness of our method and its superiority to other attribution methods. Codes and examples for the experiments will be released in public.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Interpretability, Attribution Method, Attribution Map</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a new attribution method that removes noise from saliency maps through layer-wise thresholding during backpropagation.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">14 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SJlHOIJnTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our Common Reply to All Reviewers</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkemdj09YQ&amp;noteId=SJlHOIJnTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper339 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper339 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the three reviewers for providing detailed and constructive feedback that will no doubt help us improve the quality of this work.

We addressed the reviewers’ comments in detail individually. We number the comments by ”C{number}” and answers by “A{number}” for ease of reference. We also refer to AnonReviewer{number} by “AN{number}”. We also uploaded our revised manuscript where we have reflected the reviewers’ comments and feedback.

The figures that we refer to in the answers are those in the revised version of the paper, not the original version. Figures have been added, removed, or modified during the revision process. Hence using figures in the original paper may lead to confusion.

To address the general concern about cherry-picking, we have repeated the qualitative experiments for 1.5k randomly chosen ImageNet images (both before final thresholding and after final thresholding). We believe that these results alleviate the cherry-picking concern.

There also has been repeated (explicit and implicit) questions on whether applying simple final threshold to baseline attribution maps is enough to replicate the benefits of RectGrad. To refute this concern, we applied 95 percentile final threshold to baseline attribution methods such that RectGrad and baseline attribution maps have similar levels of sparsity. Note that we did not apply the threshold q = 98, which was used in our RectGrad results. In the setting of q = 98 on baseline methods, RectGrad attribution maps are slightly less sparse than baseline attribution maps. This is because threshold is applied up to the first hidden layer, not the input layer in the RectGrad procedure. With this final threshold on baseline methods, we repeated the qualitative and quantitative experiments . We also repeated the qualitative experiments for the same 1.5k randomly chosen ImageNet images.

The links to anonymized Google drives containing the 1.5k random samples are listed in the comment below.

Please let us know if the Reviewers have any further questions or comments.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkgYnI1n6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Links to Samples on 1.5k Randomly Chosen ImageNet Images</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkemdj09YQ&amp;noteId=SkgYnI1n6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper339 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper339 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Note1: Samples for the same image have the same file name “{image number}.png” for ease of comparison between attribution maps with and without threshold / attribution maps for original image and adversarial image.

Note2: We did not include samples for adversarial attacks which failed to change the final decision.

Random Samples W/O Baseline Final Threshold
<a href="https://drive.google.com/drive/folders/1F9k-Jvxe1OppDoIDHV1SWLzugkpPQMkY?usp=sharing" target="_blank" rel="nofollow">https://drive.google.com/drive/folders/1F9k-Jvxe1OppDoIDHV1SWLzugkpPQMkY?usp=sharing</a>

Random Samples with Baseline Final Threshold
https://drive.google.com/drive/folders/1LQOWtvJPV9nnUCjB2VRNDgFGixmpAnJB?usp=sharing

Adversarial Attack Samples W/O Baseline Final Threshold
https://drive.google.com/drive/folders/1kkgVQs2jBWWqLW5CTrKhpBnpwAUhZDJs?usp=sharing

Adversarial Attack Samples with Baseline Final Threshold
https://drive.google.com/drive/folders/1MXrmQCgWgpf84Mj1f9Q8QrOfOncSbvlf?usp=sharing
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Hyx_l8y267" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Changes to the Paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkemdj09YQ&amp;noteId=Hyx_l8y267"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper339 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper339 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">(1) Added Section 4.1 which explains the rationale behind the propagation rule for RectGrad, moved Section 4.1 (Relation to Deconvolution and Guided Backprop.) to Section 4.2, and moved Section 4.2 (Useful techniques) to Appendix.

(2) Added Appendix A.1 which describes the procedure and results of a larger-scale version of the feature map occlusion experiment, moved Appendix A.1 (Qualitative Experiments) to A.2, and moved Appendix A.2 (Quantitative Experiments) to A.3.

(3) Moved “Training Dataset Occlusion” test from Section 5.2 (Qualitative Comparison with Baseline Methods) to Section 5.3 (Quantitative Comparison with Baseline Methods).

(4) Added "Noise Level" test in Section 5.3 (Quantitative Comparison with Baseline Methods).

(5) Added experimental results comparing RectGrad with baseline methods with final threshold.

(6) Revised main text to reflect Reviewer comments.

(7) Resized Figure 3 and removed redundant subfigures from Figure 4.

(8) Changed in-text reference to the proposed method from “Rectified Gradient” to “RectGrad”.

(9) Removed results on MNIST as we felt they added little to the paper.

(10) Moved attribution map visualization method to Appendix F.1.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SJlgP_d5h7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting work, but I believe a few questions need to be answered to make the paper strong enough for acceptance. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkemdj09YQ&amp;noteId=SJlgP_d5h7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper339 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper339 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary of the paper:
This paper proposed RectGrad, a gradient-based attribution method that tries to avoid the problem of noise in the attribution map. Further, authors hypothesize that noise is caused by the network carrying irrelevant features, as opposed to saturation, discontinuities, etc as hypothesized by related papers. 

The paper is well written and easy to read through. 

Strengths:
- Formally addresses a hitherto unanswered question of why saliency maps are noisy. This is an important contribution.
- RectGrad is easy to implement.

Questions for authors:
- Since the authors are saying that the validity of their hypothesis is “trivial”, it would be nice to have this statement supported by more quantitative, dataset-wide analyses on the feature map and training dataset occlusion tests. For e.g., what percentage of the test dataset shows attributions on the 10x10 occluded patch? 
- How does RectGrad compare with simply applying a final threshold on other attribution maps? How do the results on training data and feature occlusion change after such a threshold is applied? How do results on adversarial attacks change?
- Could this method generalize to non-ReLU networks?  
- Premise that auxiliary objects in the image are part of the background is not necessarily true. For instance, the hand in “lighter” is clearly important to know that the flame is from a lighter and not from a candle or some other form of fire. Similarly, the leaves in the “frog” example. 
- (Optional) As shown in (<a href="https://openreview.net/forum?id=B1xeyhCctQ)" target="_blank" rel="nofollow">https://openreview.net/forum?id=B1xeyhCctQ)</a> gradients on ReLU networks overlook the bias term. In the light of this, what is the authors’ take on whether a high bias-attribution is the cause for the noisy gradient-attribution? 
- (Optional) In some sense, RectGrad works because layers closer to the input may capture more focussed features than layers close to input which may activate features spread out all over the image. It would be interesting to see if RectGrad works for really small networks such as MobileNet (https://arxiv.org/abs/1801.04381) where such an explicit hierarchy of features may not be there. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkxmQ5JnTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to AnonReviewer2 Part 1/3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkemdj09YQ&amp;noteId=BkxmQ5JnTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper339 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper339 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the friendly and detailed review. Before reading our reply for your review, we politely ask you to read “Our Common Reply to All Reviewers” first.

C1: "Since the authors are saying that the validity of their hypothesis is “trivial”, it would be nice to have this statement supported by more quantitative, dataset-wide analyses on the feature map and training dataset occlusion tests. For e.g., what percentage of the test dataset shows attributions on the 10x10 occluded patch?"

A1: Before summarizing additional experiment results, we would like to make a clarification. Our hypothesis is comprised of two parts: (1) background features cause noise in saliency maps and (2) background features are trivial or irrelevant to the classification task. The Reviewer seems to be implying that we have claimed both parts (1) and (2) to be trivial. However, it is only part (1) that we have claimed to be trivial by the definition of gradient. We have never claimed part (2) is trivial.

We performed two additional experiments to demonstrate that DNNs do not filter out irrelevant features during forward propagation / that background feature activations are irrelevant to the classification task.

First, as the Reviewer suggested, we measured how much attribution is on the 10x10 occluded patch. Since we don’t have a criterion of how much attribution is trivial enough to be seen as “no attribution”, we instead summed all absolute attribution within the patch and took the average across the test dataset. We repeated this with other attribution methods and created a bar chart comparing the average. Results are shown in Figure 8. Ideally, there should be nearly zero attribution, but we observed that the saliency map assigned the most attribution to the patch among all attribution methods. This shows that DNNs do not filter our irrelevant features during forward propagation (by definition of gradient) and that other attribution methods alleviate this problem.

Second, we created segmentation masks for 10 correctly classified CIFAR10 images of each class (total 100 images) and repeated the feature map occlusion test. We recorded (class logit) – (largest logit among the other 9 classes) and took the average over all the images. Figure 9 in Appendix A.1 shows that the difference is generally positive throughout the occlusion process (i.e., the class does not change throughout the occlusion process), and this implies the irrelevance of background features to the classification task.

C2: "How does RectGrad compare with simply applying a final threshold on other attribution maps?"

A2: We found that noise can accumulate during backpropagation. Specifically, irrelevant features may have trivial gradient near the output layer; however, since gradient is calculated by successive multiplication, the noise can grow exponentially as gradient is propagated towards the input layer. This often results in confusing attribution maps which assign high attribution to entirely irrelevant regions (e.g. baseline methods assign high attribution to uniform background in "lighter" example in Figure 6), especially for deep networks such as Inception. In such situation, simply applying a final threshold does not work. RectGrad does not suffer from this problem since it thresholds irrelevant features at every layer and hence stops noise accumulation in the first place (Section 4.1 explains how RectGrad thresholds irrelevant features).

We have also surveyed samples for 1.5k randomly chosen ImageNet images, and we found them to be generally consistent with our claims in the paper. As we mentioned in “Our Common Reply to All Reviewers”,  we have uploaded the 1.5k samples in anonymous Google drives so that the Reviewers can inspect them if he/she is not convinced by the results in the paper. However, as it always is with DNN interpretability research, it is difficult to demonstrate the efficacy of the proposed method or erase the cherry-picking concern just with large-scale samples. We are aware of this fact and that is why we have conducted numerous additional quantitative experiments (e.g. A3). It would be very much appreciated if the Reviewer understands and takes this situation into account when reviewing the revised version of this work.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hkeud9k2TX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to AnonReviewer2 Part 2/3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkemdj09YQ&amp;noteId=Hkeud9k2TX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper339 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper339 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">C3: "How do the results on training data and feature occlusion change after such a threshold is applied? How do results on adversarial attacks change?"

A3: The final threshold setting is described in “Our Common Reply to All Reviewers”.

For the training data occlusion test, we created another bar chart summarizing the results after applying a final threshold (test and bar chart details are described in A1). For this test, we found using q = 95 final threshold led to trivially different averages. Hence we used a custom threshold for each baseline method such that they had similar average attribution in the patch as RectGrad. Figure 8 shows that RectGrad had smaller standard deviation than baseline methods. This indicates that RectGrad more consistently assigns near-zero attribution to the patch. Therefore, RectGrad has advantages over baseline methods regardless of whether final threshold is used or not.

For “feature occlusion”, we assume it is the Sensitivity experiment in Section 5, not feature map occlusion experiment in Section 3. We observed that after applying the final threshold, RectGrad still outperforms local attribution methods. RectGrad initially performed worse than global attribution methods after occluding approx. 10 patches. However, after final threshold, RectGrad now shows similar performance.

This was not requested by the Reviewer, but we measured how noisy each attribution maps are using the total variation metric, and we believe the results can further support our reply to C3. To measure how noisy attribution maps were before and after the final threshold, we measured the total variation of each attribution maps and took the average across the test dataset. Figures 12 and 13 show that even though the total variation reduces for baseline methods after final threshold, RectGrad outperforms baseline methods in both cases.

Finally, we applied final threshold to baseline attribution maps for adversarial attack images. Figures 7 and 11 show the results. We found that our observations still held (that is, RectGrad is more or as class-sensitive as baseline methods). Also, the baseline attribution maps with final threshold were still noisy.

With the above four additional experiments, we can draw the following conclusions:

(1) RectGrad more consistently assigns zero attribution to irrelevant features.
(2) At the same level of sparsity, RectGrad performs better than or similar to baseline methods in Sensitivity. This shows that RectGrad is better than or as good as other methods in selecting important features.
(3) At the same level of sparsity, RectGrad is more or as class-sensitive as baseline methods.
(4) At the same level of sparsity, RectGrad attribution maps are significantly less noisy than baseline attribution maps. 

(Conclusion 4 is quantitatively supported by A5 of our reply to AN1 where we answer "Since this paper claims to produce less noisy saliency maps, what does it mean quantitatively? Is it true that it produces less pixels on the background? If so, can we evaluate it with foreground-background segmentation annotation to prove that point?")

Given that attribution maps are ultimately used by humans to “visually” interpret DNN decisions, we believe sparsity / clarity is also an important factor in DNN interpretability. Noisy attribution maps such as “lighter” example in Figure 6 can hinder the user’s attempt to interpret DNN decisions. As our experiments show, baseline attribution maps are still noisy after final thresholding. On the same level of sparsity, RectGrad attribution maps are significantly less noisy but still better than or as good as baseline attribution maps in highlighting important features. Hence we believe RectGrad has a notable advantage over baseline attribution methods.

C4: "Could this method generalize to non-ReLU networks?"

A4: We direct the Reviewer to Section 4.1, where we explain the rationale behind the definition of RectGrad. The reasoning that RectGrad propagates gradient through units whose marginal effect on the output exceeds some thresholds applies to any DNN network structure, regardless of the type of activation function used. We corroborate this claim with experiment results on CIFAR10 with Sigmoid and TanH  DNNs. We used the same architecture as ReLU DNN and trained each for 40 epochs to achieve 70.5% and 75% test accuracy respectively. We observed that RectGrad PRR does not work as well as vanilla RectGrad for Sigmoid and TanH.

Please note that these results and discussions are not included in the current version of revised paper due to the 10-page limit. However, if the Reviewer feels this result is important enough, we will insert them in the Appendix.

Sigmoid
<a href="https://drive.google.com/drive/folders/1c_qLKm-uOB-Dcz5KVpvAnlFPHZL9I6FH?usp=sharing" target="_blank" rel="nofollow">https://drive.google.com/drive/folders/1c_qLKm-uOB-Dcz5KVpvAnlFPHZL9I6FH?usp=sharing</a>

TanH
https://drive.google.com/drive/folders/18bMsizZ-geHqMQp8pGSHbXGjJk8f1Pms?usp=sharing</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1e82cJ3a7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to AnonReviewer2 Part 3/3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkemdj09YQ&amp;noteId=S1e82cJ3a7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper339 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper339 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">C5: "Premise that auxiliary objects in the image are part of the background is not necessarily true."

A5: We agree with the reviewer that auxiliary objects may influence the DNN decision process. That is why we have the hyper parameter threshold percentile q so that the user can control the degree to which RectGrad emphasizes important features. We direct the reviewer to Figure 5 where we explore the effect of varying q. Attribution maps with q = 80 ~ 90 highlights the object of interest (the cabbage butterfly) along with auxiliary objects such as flowers or grass that may be helpful to the DNN in identifying the object. On the other hand, attribution maps with q &gt; 95 highlight features that may have been most influential to the final decision, namely the spots on the butterfly's wing. Auxiliary objects may not have been highlighted in the RectGrad attribution maps in the paper since we used a high threshold q = 98.

C6: "For instance, the hand in “lighter” is clearly important to know that the flame is from a lighter and not from a candle or some other form of fire. Similarly, the leaves in the “frog” example."

A6: For the “lighter” example, the attribution also highlights the cap/hood of the lighter which indicates that RectGrad correctly explains how the DNN distinguished the lighter flame from other forms of fire. Also lowering the threshold parameter q to 90 also highlights the hand. For the “frog” example, we found that RectGrad attribution highlighted the leaves only slightly for q = 80, which implies leaves may be irrelevant or trivial to the classification task. This explanation is plausible since other methods also assign relatively small attribution to the leaves.

Lighter image: <a href="https://drive.google.com/open?id=1mj0H5hdXUQRrcT0J-AJximd2eYfJhEIh" target="_blank" rel="nofollow">https://drive.google.com/open?id=1mj0H5hdXUQRrcT0J-AJximd2eYfJhEIh</a>

Frog image: https://drive.google.com/open?id=1VBxiO8iO-cKxwFvc5Fkw9r8sXTLSbRqK
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_BJlWRgW53Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper proposes a new method for producing saliency maps and my main concerned is the objective evaluation. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkemdj09YQ&amp;noteId=BJlWRgW53Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper339 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper339 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies how to better visually interpret a deep neural network. It proposes a new method to produce less noisy saliency maps, named RectGrad. RectGrad thresholds gradient during backprop in a layer-wise fashion in a similar manner to a previous work called Guided Backprop. The difference is that Guided Backprop employs a constant threshold, i.e. 0, while RectGrad uses an adaptive threshold based on a percentile hyper-parameter. The paper is well-written, including a comprehensive review of previous related works, an meaningful meta-level discussion for motivation, and a clear explanation of the proposed method. 

One of my biggest concern is regarding the experiment and evaluation section. Conclusions are drawn based on the visualization of a few saliency maps. I am not sure how much I can trust these conclusions as the conclusions are drawn in a handy-wavy manner the examples are prone to cherry-picking and . For example, this is the conclusion in the Adversarial Attack paragraph: “we can conclude that Rectified Gradient is  equally or more class sensitive than baseline attribution methods”. As pointed out by the paper, the conclusion can be drawn from Figure 8 in the main paper and Figure 10 in Appendix A.1. However, the proposed method tends to produce a saliency map with higher sparsity, therefore the difference may appear more apparent. It is stretching to conclude that it is more class sensitive without further quantitative validation. 

Evaluation appears to be a common concern to the work on saliency maps.  The existing quantitative evaluation in the paper seems disconnected to the visual nature of saliency maps. Concretely, when can we say one saliency map looks better than another? Since this paper claims to produce less noisy saliency maps, what does it mean quantitatively? Is it true that it produces less pixels on the background? If so, can we evaluate it with foreground-background segmentation annotation to prove that point? Though how to evaluate saliency maps remains an open question, I feel some discussion on this paper would make the paper more insightful. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1xU8sk3Tm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to AnonReviewer1 Part 1/2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkemdj09YQ&amp;noteId=B1xU8sk3Tm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper339 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper339 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the friendly and detailed review. Before reading our reply for your review, we politely ask you to read “Our Common Reply to All Reviewers” first.

C1: "One of my biggest concern is regarding the experiment and evaluation section. Conclusions are drawn based on the visualization of a few saliency maps. I am not sure how much I can trust these conclusions as the conclusions are drawn in a handy-wavy manner the examples are prone to cherry-picking. For example, this is the conclusion in the Adversarial Attack paragraph: “we can conclude that Rectified Gradient is equally or more class sensitive than baseline attribution methods”. As pointed out by the paper, the conclusion can be drawn from Figure 8 in the main paper and Figure 10 in Appendix A.1."

A1: We have surveyed samples for 1.5k randomly chosen ImageNet images, and we found them to be generally consistent with our claims in the paper. As we mentioned in “Our Common Reply to All Reviewers”,  we have uploaded the 1.5k samples in anonymous Google drives so that the Reviewers can inspect them if he/she is not convinced by the results in the paper. However, as it always is with DNN interpretability research, it is difficult to demonstrate the efficacy of the proposed method or erase the cherry-picking concern just with large-scale samples. We are aware of this fact and that is why we have conducted numerous additional quantitative experiments (e.g. AN2 A3). It would be very much appreciated if the Reviewer understands and takes this situation into account when reviewing the revised version of this work.

C2: "However, the proposed method tends to produce a saliency map with higher sparsity, therefore the difference may appear more apparent."

A2: We applied final threshold to baseline attribution methods such that RectGrad and baseline attribution maps have similar levels of sparsity. The final threshold details are described in Our Common Reply to All Reviewers. We then repeated all qualitative experiments on 1.5k randomly chosen ImageNet images (image links listed in comment above). We have observed that the conclusions still generally hold even after final threshold.

We also believe that the ability of our method to control the threshold hyper parameter to make the difference more/less apparent is an advantage, rather than a disadvantage. If the image was near the decision boundary in the first place, then the adversarial attack would change only a small portion of internal activation patterns. This may lead to attribution maps which are indistinguishable from attribution maps for original images. Baseline methods have no effective way of dealing with this except final threshold. However, as we have observed, final threshold still results in noisy attribution maps for baseline methods. For RectGrad, the user can control the threshold percentile q to visually understand which features really account for the change in DNN decision.

C3: "It is stretching to conclude that it is more class sensitive without further quantitative validation."

A3: To our knowledge, there is no previous work in quantitatively validating how class sensitive an attribution method is. There only exist qualitative methods: (1) comparing original attribution map with those produced with respect to another class (Smilkov et al. 2017, <a href="https://arxiv.org/abs/1706.03825)" target="_blank" rel="nofollow">https://arxiv.org/abs/1706.03825)</a> and (2) adversarial attack (Nie et al. 2018, https://arxiv.org/abs/1805.07039). We have chosen the latter method since it is deterministic; for the former method, there are 1000 other classes to choose from, and we thought this could lead to space trouble or cherry-picking concerns. Hence the large-scale adversarial attack samples are the best we can do to convince the Reviewer in the current situation.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJxYhsk367" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to AnonReviewer1 Part 2/2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkemdj09YQ&amp;noteId=BJxYhsk367"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper339 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper339 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">C4: "The existing quantitative evaluation in the paper seems disconnected to the visual nature of saliency maps."

A4: We assumed that the samples inserted in the paper would be enough to convince the Reviewers that RectGrad produces attribution maps with significantly less noise than baseline methods. That is why the quantitative experiments are focused on evaluating whether RectGrad attribution maps are truly informative, i.e., highlights features relevant to DNN decision. However the Reviewer does not seem to be convinced due to the cherry-picking concern. We address this concern in “Our Common Reply to All Reviewers.” If the Reviewer is still not convinced, we conducted additional quantitative experiments to prove that RectGrad significantly reduces noise in saliency maps. We summarize the results in the reply below.

C5: "Since this paper claims to produce less noisy saliency maps, what does it mean quantitatively? Is it true that it produces less pixels on the background? If so, can we evaluate it with foreground-background segmentation annotation to prove that point?"

A5: If the reviewer is not convinced by “Our Common Reply to All Reviewers”, we performed two additional quantitative experiments to prove that RectGrad reduces noise.

First, as the Reviewer suggested, we created segmentation masks for 10 correctly classified CIFAR10 images of each class (total 100 images) and measured how much attribution falls on the background. Specifically, we compared the sum of absolute value of attribution on the background. Figure 12 shows that the RectGrad assigns significantly less attribution to the background than baseline methods. Moreover, even with final threshold (so that RectGrad and baseline attribution maps have similar sparsity), RectGrad outperformed baseline methods.

We also measured how noisy each attribution maps are using the total variation metric, and we believe the results can further support our reply to C5. To measure how noisy attribution maps were before and after the final threshold, we measured the total variation of each attribution map and took the average across the test dataset. Figure 13 show that even though the total variation reduces for baseline methods after final threshold, RectGrad outperforms baseline methods in both cases.

To summarize, we have shown through (1) foreground-background segmentation annotation and (2) total variation metric that RectGrad attribution maps are significantly less noisy than baseline attribution maps both with and without final threshold.

C6: "Evaluation appears to be a common concern to the work on saliency maps." + "Though how to evaluate saliency maps remains an open question, I feel some discussion on this paper would make the paper more insightful." + "Concretely, when can we say one saliency map looks better than another?"

A6: We agree that attribution map evaluation is an important line of DNN interpretability research. However, we disagree with the Reviewer’s comment that “some discussion on this paper would make the paper more insightful.” The main contributions of this paper are (1) identifying why saliency maps are noisy and (2) proposing a solution. Proposal or discussion of evaluation metric for attribution methods are out of scope of this paper. Hence we believe that adding such content will cloud the focus of our paper.

Lastly, we are worried about the fact that the Reviewer did not mention Section 3 (Our Explanation for Saliency Maps) which we included in the list of our contributions. We believe this contribution is significant in our work for the following reason: this paper thoroughly investigates and proposes an answer to the question of why saliency maps are noisy. To the best of our knowledge, this question has never been answered previously. There are several studies which propose hypotheses (e.g. Smilkov et al. 2017, <a href="https://arxiv.org/abs/1706.03825;" target="_blank" rel="nofollow">https://arxiv.org/abs/1706.03825;</a> Shrikumar et al. 2017, https://arxiv.org/abs/1704.02685), but they do not conduct extensive experiments to corroborate their claims. Thus, we hope the Reviewer takes this contribution into account when reviewing the revised version of this paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_BJxGAKO6oQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Insightful observations, but results are less convincing.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkemdj09YQ&amp;noteId=BJxGAKO6oQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper339 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">24 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper339 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In the paper, the authors proposed a new saliency map method, based on some empirical observations about the cause of noisy gradients.
Specifically, through experiments, the authors clarified that the noisy gradients are due to irrelevant information propagated in the forward pass in DNN. Because the backpropagation follows the same pass, irrelevant feature are conveyed back to the input, which results in noisy gradients.
To avoid noisy gradients, the authors proposed a new backpropagation named Rectified Gradient (RectGrad). In RectGrad, the backward pass is filtered out if the product of the forward signal and the backward signal are smaller than a threshold. The authors claim that, with this modification in backpropagation, the gradients get less noisy.
In some experiments, the authors presented that RectGrad can produce clear saliency maps.

I liked the first half of the paper: the observations that irrelevant forward passes are causing noisy gradients seem to be convincing. The experiments are designed well to support the claim.
Here, I would like to point out, that noisy gradients in occluded images may be because of the convolutional structures. Each filter in convolution layer is trained to respond to certain patterns. Because the same filter is used for each of subimages, some filters can be activated occasionally on occluded parts. I think this does not happen if the network is densely connected without convolutional structures. The trained dense connection will be optimized to remove the effects of occluded parts. Hence, for such networks, the gradient will be zeros for occluded parts.

The second half of the paper (Sec.4 and 5) are not very much convincing to me.
Below, I raise several concerns.

1. There is no justification on the definition of RectGrad: Why Rl = I(al * Rl &gt; t) R(l+1)?
The authors presented Rl = I(al * Rl &gt; t) R(l+1) as RectGrad, that can filter out irrelevant passes. However, there is no clear derivation of this formula: the definition suddenly appears. If the irrelevant forward passes are causes of noisy gradients, the modification Rl = I(al &gt; t) R(l+1) seems to be more natural to me. It is also a natural extension to the ReLU backward pass Rl = I(al &gt; 0) R(l+1). Why we need to filter out negative signals in backward pass?

2. The experimental results are less convincing: Is RectGrad truly good?
In Sec.5.2, the authors presented saliency maps only on a few images, and claimed that they look nicely. However, it is not clear that those "nicely looking" saliency map are truly good ones. I expect the authors to put much efforts on quantitative comparisons rather than qualitative comparisons, so that we can understand that those "nicely looking" saliency maps are truly good ones.
Sec.5.3 presents some quantitative comparisons, however, the reported Sensitivity and ROAR/KAR on RectGrad are not significant. The authors mentioned that this may be because of the sparsity of RectGrad. However, if the sparsity is the harm, the underlying observations of RectGrad may have some errors. I think the current manuscript has an inconsistency between the fundamental idea (based on empirical observations) and the performance of RectGrad.

[Minor Concern]
In Sec.5, the authors frequently refer to the figures in appendix. I think the main body of the paper should be self-contatined. I therefore think that some of the figures related to main results should appear in the main part.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Sylw8nkhpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to AnonReviewer3 Part 1/3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkemdj09YQ&amp;noteId=Sylw8nkhpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper339 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper339 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the friendly and detailed review. Before reading our reply for your review, we politely ask you to read “Our Common Reply to All Reviewers” first.

C1: "I liked the first half of the paper: the observations that irrelevant forward passes are causing noisy gradients seem to be convincing. The experiments are designed well to support the claim. Here, I would like to point out, that noisy gradients in occluded images may be because of the convolutional structures. Each filter in convolution layer is trained to respond to certain patterns. Because the same filter is used for each of subimages, some filters can be activated occasionally on occluded parts. I think this does not happen if the network is densely connected without convolutional structures. The trained dense connection will be optimized to remove the effects of occluded parts. Hence, for such networks, the gradient will be zeros for occluded parts."

A1: Thank you for the compliments! Note that if we only use dense layers as the Reviewer suggested, we would not be able to achieve such high test accuracy. Hence the problem of noisy gradients for CNNs  would have to be addressed sooner or later. In addition, at the preliminary stage of this research, we also found that using fully connected layers does not solve this problem. We speculate this happens due to two reasons: (1) random initialization of weights and (2) lack of incentive for the network to "not remove forward signal from irrelevant features" or "zero out weights corresponding to irrelevant features". Correspondingly, we found that using l2 loss / weight decay k||w||^2 does remove noise from saliency maps. However, by the time the weight decay coefficient k was high enough to produce clear saliency maps, DNN lacked the expressiveness to achieve sufficiently high test accuracy. Therefore, we did not include this observation in the final version of our paper.

C2: "There is no justification on the definition of RectGrad: Why Rl = I(al * Rl &gt; t) R(l+1)? The authors presented Rl = I(al * Rl &gt; t) R(l+1) as RectGrad, that can filter out irrelevant passes. However, there is no clear derivation of this formula: the definition suddenly appears. If the irrelevant forward passes are causes of noisy gradients, the modification Rl = I(al &gt; t) R(l+1) seems to be more natural to me. It is also a natural extension to the ReLU backward pass Rl = I(al &gt; 0) R(l+1). Why we need to filter out negative signals in backward pass?"

A2: We direct the Reviewer to Section 4.1 where we explain the rationale behind the definition of RectGrad in the revised version of the paper. We also direct the Reviewer to A2 of our reply to AN2, where we answer "how does RectGrad compare with simply applying a final threshold on other attribution maps?".

C3: "The experimental results are less convincing: Is RectGrad truly good? In Sec.5.2, the authors presented saliency maps only on a few images, and claimed that they look nicely.”

A3: We direct the Reviewer to A1 of our reply to AN1, where we reply to "One of my biggest concern is regarding the experiment and evaluation section. Conclusions are drawn based on the visualization of a few saliency maps. I am not sure how much I can trust these conclusions as the conclusions are drawn in a handy-wavy manner the examples are prone to cherry-picking.”

We have also proven through additional quantitative experiments that RectGrad attribution maps are not only sparse, but significantly less noisy than baseline attribution maps. We direct the Reviewer to A5 of our reply to AN1, where we answer "since this paper claims to produce less noisy saliency maps, what does it mean quantitatively? Is it true that it produces less pixels on the background? If so, can we evaluate it with foreground-background segmentation annotation to prove that point?"
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryx6j31n6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to AnonReviewer3 Part 2/3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkemdj09YQ&amp;noteId=ryx6j31n6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper339 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper339 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">C4: “However, it is not clear that those "nicely looking" saliency map are truly good ones. I expect the authors to put much efforts on quantitative comparisons rather than qualitative comparisons, so that we can understand that those "nicely looking" saliency maps are truly good ones."

A4: The concern seems to be about whether the sparse attribution maps produced by RectGrad are truly meaningful. If this is not the case, then RectGrad should perform worse than baseline methods with final threshold. That is, RectGrad attribution maps should have no advantage over baseline attribution maps that are simply thresholded to have the same level of sparsity. To verify whether this is true, we applied final threshold so that baseline attribution maps have similar sparsity as RectGrad attribution maps and then repeated all the experiments. We also conducted additional quantitative experiments to support our claims.

The experimental settings and results are described in A3 of our reply to AN2, where we answer "how do the results on training data and feature occlusion change after such a threshold is applied? How do results on adversarial attacks change?". We also direct the Reviewer to A3 of our reply to AN1, where we reply to "it is stretching to conclude that it is more class sensitive without further quantitative validation."

C5: "Sec.5.3 presents some quantitative comparisons, however, the reported Sensitivity and ROAR/KAR on RectGrad are not significant."

A5: We would first like to point out that the result for KAR is not trivial. On the contrary, RectGrad shows the best performance among all attribution methods on KAR. We cite our description of the results on KAR in the paper: “next, Figure 17 shows KAR scores. Interestingly, all baseline attribution methods failed to exceed even the random baseline. Only Rectiﬁed Gradient had similar or better performance than the random baseline.” This confusion may have been caused by the fact that higher AUC indicates a poorer attribution method for ROAR while it indicates a better attribution method for KAR.

We address the Reviewer’s comments on Sensitivity and ROAR in the answer to the next comment.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1ldbTknTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to AnonReviewer3 Part 3/3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkemdj09YQ&amp;noteId=H1ldbTknTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper339 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper339 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">C6: "The authors mentioned that this (worse performance of RectGrad on ROAR and Sensitivity) may be because of the sparsity of RectGrad. However, if the sparsity is the harm, the underlying observations of RectGrad may have some errors. I think the current manuscript has an inconsistency between the fundamental idea (based on empirical observations) and the performance of RectGrad."

A6: The result on Sensitivity does not contradict but corroborates our claims on RectGrad. As we now explain in Section 4.1 (Rationale Behind the Propagation Rule for RectGrad), RectGrad theoretically (1) removes noise and (2) thresholds out features by order of importance (note that (1) is a consequence of (2)). The Sensitivity result shows that the logit when the features are occluded according to RectGrad for the first few patches drops faster than or as fast as those of other methods. This indicates that RectGrad has successfully captured the most important features, as we have claimed.

We believe the drop in performance after the first few patches is an inevitable consequence of sparsity. To see this, we have performed an additional experiment where we apply final threshold to baseline attribution methods so that RectGrad attribution maps and baseline attribution maps have similar levels of sparsity. We observed that at similar levels of sparsity, there is no significant difference between the performance of RectGrad and baseline methods.

Given that attribution maps are ultimately used by humans to “visually” interpret DNN decisions, we believe sparsity / visual quality is also an important factor in DNN interpretability. Noisy attribution maps such as “lighter” example in Figure 6 can hinder the user’s attempt to interpret DNN decisions. As our experiments show, baseline attribution maps are still noisy after final threshold. On the same level of sparsity, RectGrad attribution maps are significantly less noisy but still better than or as good as baseline attribution maps in highlighting important features (results described in reply to AN1, A5). Hence we believe RectGrad has a notable advantage over baseline attribution methods.

As for the explanation for ROAR in Appendix B.2, our intention was to show that this metric may not be suitable for objectively evaluating RectGrad. To explain why, we cite a sentence from Hooker et al. (2018, <a href="https://arxiv.org/abs/1806.10758)" target="_blank" rel="nofollow">https://arxiv.org/abs/1806.10758)</a> which proposed ROAR: “training the model from random initialization is crucial in order for the constant value for which we replaced the input to be considered “uninformative””. The assumption behind ROAR is that the occluded features do not influence the classification task. However, as we have shown in Appendix B.2, this does not seem to be the case for RectGrad.

[Minor Concern]

C7: “In Sec.5, the authors frequently refer to the figures in appendix. I think the main body of the paper should be self-contained. I therefore think that some of the figures related to main results should appear in the main part.”

A7: We were notified that ICLR had set a strict limit on paper length after drafting. We tried our best to insert as much figures related to the main results as possible into the main part. Moving more figures will cause the paper to violate the 10-page limit. We also decided to insert qualitative experiment figures in the main part since it is relatively easier to accurately describe quantitative results with words.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>