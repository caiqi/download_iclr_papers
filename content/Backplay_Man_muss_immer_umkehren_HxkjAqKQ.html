<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Backplay: 'Man muss immer umkehren' | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Backplay: 'Man muss immer umkehren'" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1xk8jAqKQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Backplay: 'Man muss immer umkehren'" />
      <meta name="og:description" content="A long-standing problem in model-free reinforcement learning (RL) is that it requires a large number of trials to learn a good policy, especially in environments with sparse rewards. We explore a..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1xk8jAqKQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Backplay: 'Man muss immer umkehren'</a> <a class="note_content_pdf" href="/pdf?id=H1xk8jAqKQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019backplay:,    &#10;title={Backplay: 'Man muss immer umkehren'},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=H1xk8jAqKQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">A long-standing problem in model-free reinforcement learning (RL) is that it requires a large number of trials to learn a good policy, especially in environments with sparse rewards. We explore a method to increase the sample efficiency of RL when we have access to demonstrations. Our approach, Backplay, uses a single demonstration to construct a curriculum for a given task. Rather than starting each training episode in the environment’s fixed initial state, we start the agent near the end of the demonstration and move the starting point backwards during the course of training until we reach the initial state. We perform experiments in a competitive four-player game (Pommerman) and a path-finding maze game. We find that Backplay provides significant gains in sample complexity with a stark advantage in sparse reward settings. In some cases, it reached success rates greater than 50% and generalized to unseen initial conditions, while standard RL did not yield any improvement.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Exploration, Games, Pommerman, Bomberman, AI, Reinforcement Learning, Machine Learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Learn by working backwards from a single demonstration, even an inefficient one, and progressively have the agent do more of the solving itself.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rJgq3KK7a7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>sensible method, but limited novelty and evaluation is lacking</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xk8jAqKQ&amp;noteId=rJgq3KK7a7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper137 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018 (modified: 15 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper137 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=rJgq3KK7a7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper presents a strategy for solving sparse reward tasks with RL by sampling initial states from demonstrations. The method, Backplay, starts by sampling states near the end of a demonstration trajectory, so that the agent will be initialized to states near the goal. As training progresses, the initial state distribution is incrementally shifted towards earlier steps in the demonstration, until the agent is trained starting from the original initial state. The authors further provide an analysis of the sample complexity of this method on a simple MDP. The method is demonstrated on a maze navigation task and a challenging game Pommerman.

The method is simple and sensible, but not particularly novel. As the authors pointed out, a very similar strategy for using demonstrations was previously presented in an OpenAI blogpost, Learning Montezuma’s Revenge from a Single Demonstration. However since that work was not published, it should not be held against this paper. That being said, sampling initial states from demonstrations is a tried-and-true strategy in RL, and the manually designed curriculum is also not particularly novel. Therefore the method is mainly a minor tweak to longstanding techniques. The paper has also acknowledges these previous works. As such, a more thorough evaluation with previous methods, such as those for automatic curriculum generation (e.g. Florensa et al. 2017 and Aytar et al. 2018) is vital, but is very much lacking in the current set of experiments.

This work can also benefit from a more diverse set of tasks to better evaluate the effectiveness of the method, and provide more insight on when such a strategy is beneficial. The experiments were conducted only on discrete grid world tasks, and additional experiments in continuous domains could be valuable. In the maze task, Backplay is not significantly better than uniform. Pommerman is a much more compelling task and shows more promising improvements from backflip. However, training seems to have been terminated fairly early, before the performance for most policies have converged. In particular, the standard dense policy in figure 3c seems to be doing pretty well, will it catch up to the backplay policy with more training? It is also pretty unexpected that the uniform policies are doing so poorly, worse than the standard policy for the Pommerman experiments. Do the authors have any intuition on why this might be the case?

In figure 3, what is the initial state distribution used to evaluate the various methods? Are all policies initialized to the original initial state of a task, or are initial states sampled from the demonstrations? Given the periodic drops in performance for the backplay policies, it appears that the initial states might be changing according the curriculum during evaluation. If that is the case, it might not be a fair comparison for the other policies, especially for the standard policies, which are trained under different initial states.

As detailed in the appendix, the sliding windows for the curriculum do not seem to have a lot of overlap. This might be a potentially problematic design decision, since the sudden change in the initial state distribution, may cause the policy to “forget” about strategies learned for previous initial states. Has the authors experimented with other more gradual transition strategies?

I think this paper in its current form does not yet meet the bar for ICLR. But this line of work could be a potentially promising direction. More thorough evaluation, better baselines, and more diverse tasks can help to strengthen this work. Further analysis on the effects of different initialization strategies could also make for a compelling contribution.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJeXOphc2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Request for some clarifications. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xk8jAqKQ&amp;noteId=rJeXOphc2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper137 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper137 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Thanks for your submission.

The  authors present a very elegant strategy of using Backplay, that learns a curriculum around a suboptimal demonstration. The authors show the technique reaches an upper bound on sample complexity especially in sparse reward environments. The strength of the paper is the ability to learn from even 10 sub-optimal demonstrator trajectory thereby achieving optimality in reaching the goal. The biggest limitation of the method as with other vanilla model free RL is the lack of generalization. 

A bit more motivation on the simplified assumption that function approximation would have been better. Although, such a simplification seems to be a natural candidate to be upper bounded by the longest shortest path from v_0 to v_*; consideration of such simplicaton on the neighbourhood structure of the graph with respect to the maximum vertex degree seems to be missing or cliques. Although, the authors comment about the strong assumptions being made to aid the analysis. 

The authors explain the analysis in a very precise and the analysis seems to work. Although, the part of the analysis where connections are drawn to the reciprocal spectral gap is not very clear. 

The authors discuss the limitation of the analysis in the case of the binary tree, that follows from the arguments before.

It will be great to see a more systematic approach to deciding how fast/slow the window should be updated to unify some of the findings from the empirical experiments as that seems to affect the way the agent trains using Backplay.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJlgUMez6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer3 "Request for some clarifications."</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xk8jAqKQ&amp;noteId=rJlgUMez6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper137 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper137 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks so much for your review.

We appreciate your recognition that one strength of the Backplay paper is the ability to learn from the very sub-optimal expert in the Gridworld. This is true to an even larger extent in more complex environments, and we stress that the fact that Backplay lets the agent do much better than the expert is a huge strength compared to most current LFD approaches, including both DAgger and GAIL. Additionally, note that when we gave Backplay more trajectories (in the 100 map Pommerman case), it did generalize. This is specified on page 7, just before section 5.

Can you please clarify your second paragraph? We are unsure what you mean.

Thanks for the kind words about our mathematical analysis. We think that this is a keen and unique strength of this paper compared to anything similar in the literature. We will strive in the next version to clarify the connections to the spectral gap.

With regards to discussing a more systematic approach to the window updates, while we do not expect the reviewers to read the Appendix in detail, we did include information about this in A.9. We will address your request in the next version by showing the results of further experiments and making this association more clear.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1xWghSch7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Minor contributions; not a novel idea with limited evaluation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xk8jAqKQ&amp;noteId=S1xWghSch7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper137 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper137 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents a method for increasing the efficiency of sparse reward RL methods through a backward curriculum on expert demonstrations. The method in the paper is as follows: assuming access to expert demonstration and a resettable simulator, the start state of the agent in the beginning of training is sampled from end of demonstration (close to the rewarding state) where the task of achieving the goal is easy. Then gradually through a curriculum this is shifted backwards in the demonstration, making the task gradually harder. 

The proposed method is closely related to 1) “Learning Montezuma’s Revenge from a Single Demonstration” a blog post and open-source code release by Salimans and Chen (OpenAI Blog, 2018) where they show that constructing a curriculum that gradually moves the starting state back from the end of a single demonstration to the beginning helps solve Montezuma’s revenge game 2) “Reverse Curriculum Generation for Reinforcement Learning” by Florensa et al. (CoRL 2017) , where they start the training to reach a goal from start states nearby a given goal state and gradually the agent is trained to solve the task from increasingly distant start states. 

The approach is evaluated on a pair of tasks, a maze environment and a stochastic four-player game, Pommerman. In the maze environment, they compare to vanilla PPO and Uniformly sampled starting points across the expert trajectory. The Backplay method outperforms the vanilla baseline, however, from the training curves (~3500 epochs) in the appendix A4, it looks like the Uniform sampling baseline is doing as well or better than the proposed method. As pointed out by the authors themselves the reverse curriculum does not seem necessary in this environment. Also, it is unclear to me whether the curves shown is comparable as the starting point of the agent, at least in the beginning of training, is close to the goal with higher success rate for the Backplay method compared to baselines. A good convincing assessment would be to report success rate against the same starting point for all methods preferably not from the starting point of the demonstrations to assess generalisation of these methods for which authors briefly report unsuccessful results. 

The Pommerman environment is more complex and the results reported are more interesting. Figure 3 shows the results on four different maps for which expert demonstrations are generated from a Finite-Machine Tree-search method (a competitive method in this environment). I’m slightly confused by the plots and the significant drops in performance once the curriculum is finished and agent encounters the start position of the demonstration trajectory (epoch 250). Is this affected by the schedule of the curriculum? Also, the choice of terminating training at epoch 550 is not clear as the method does not seem to have converged yet (the variance is quite high) and would be interesting to observe the dynamics of learning as the training proceeds and whether it converges to a stable policy at all. I am also slightly unclear regarding the performance difference between Standard method in Figure 3(c) and 3(d). If the Standard method is still the same baseline, vanilla PPO, why such huge performance difference? In my understanding, only the Uniform and Backplay methods should be affected by the quality of demonstrations? I believe this figure needs more explanation and clarity. I am also not clear on why Standard method is terminated at epoch 450 while other methods are trained until epoch 550. Figure 4 reports results of generalisation to 10 unseen maps but again the choice of terminating training after 550 epochs is not clear to me as the method again does not seem to have converged. 

Overall, the choice of parameters is not well motivated, these include the window size for sampling the start point, the schedule for shifting the start point, batch size (102400 seems large to me and this choice is never explained), horizon (in appendix A3 reported to be 1707 for Maze while in the main text it is reported as 200 steps), termination of training (3500 for Maze, Figure 7, and 550 in Pommerman, Figure 3). 

I commend the authors for honestly reporting their method’s shortcomings such as failure in generalisation, however, I find that the work lacks significance and quality. There is not much novelty in the proposed method and there is a clear lack of comparisons to existing sample efficient LfD techniques such as Generative Adversarial Imitation Learning (GAIL). I believe this paper requires substantial improvements for publication and is not up to the ICLR standards in its current form.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJxTSbz7aX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer AnonReviewer1: Specific points</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xk8jAqKQ&amp;noteId=BJxTSbz7aX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper137 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper137 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">“Also, it is unclear to me whether the curves shown is comparable as the starting point of the agent, at least in the beginning of training, is close to the goal with higher success rate for the Backplay method compared to baselines. A good convincing assessment would be to report success rate against the same starting point for all methods preferably not from the starting point of the demonstrations to assess generalisation of these methods for which authors briefly report unsuccessful results.”
- Starting at epoch 1750, the success rate is *always* relative to the same starting point for all methods. Taking this into account, the much better sample complexity for Backplay is apparent. We will improve the way that we display these results. (Unfortunately, no method generalized in this scenario, and so comparing to other starting points is fruitless.)

“I’m slightly confused by the plots and the significant drops in performance once the curriculum is finished and agent encounters the start position of the demonstration trajectory (epoch 250). Is this affected by the schedule of the curriculum?”
- The significant drop in performance was due to the agent having to start from the initial state *all* of the time. You can see similar but smaller versions of this drop throughout the training procedure for Backplay. We will clarify this further.

“Also, the choice of terminating training at epoch 550 is not clear as the method does not seem to have converged yet (the variance is quite high) and would be interesting to observe the dynamics of learning as the training proceeds and whether it converges to a stable policy at all.”
- We gave all of the models the same *total training time* (three days) rather than keeping the number of epochs constant. Given that this was confusing, we will change this in the next version by continuing training to an invariant number of epochs.

“I am also slightly unclear regarding the performance difference between Standard method in Figure 3(c) and 3(d). If the Standard method is still the same baseline, vanilla PPO, why such huge performance difference? In my understanding, only the Uniform and Backplay methods should be affected by the quality of demonstrations? I believe this figure needs more explanation and clarity.”
- The difference between 3c and 3d relative to Standard is only in the starting position of the agent. The board positions are not symmetric, and thus some starting positions are more advantageous than others wrt passage layout and item positioning. It is likely the latter that causes these learning curve differences as the agent often learns utilize the kick bomb item and it could have trouble if there aren’t any nearby.

“I am also not clear on why Standard method is terminated at epoch 450 while other methods are trained until epoch 550. Figure 4 reports results of generalisation to 10 unseen maps but again the choice of terminating training after 550 epochs is not clear to me as the method again does not seem to have converged.”
- See above re keeping the total training time invariant.

“Overall, the choice of parameters is not well motivated, these include the window size for sampling the start point, the schedule for shifting the start point, batch size (102400 seems large to me and this choice is never explained), horizon (in appendix A3 reported to be 1707 for Maze while in the main text it is reported as 200 steps), termination of training (3500 for Maze, Figure 7, and 550 in Pommerman, Figure 3).”
- We will motivate the HPs more clearly in the next iteration. Briefly: 
  a. We discussed the window size / starting point in A.9, but feel that a thorough treatment of this is out of scope as our paper presents these results for the first time in the literature and the standalone field of hyperparameter search is vast.
  b. We originally used a smaller batch size (see footnote on p16 in the appendix) and found that Standard suffered while Backplay worked no problem. In the interest of a more fair comparison, we increased the batch size until PPO was given sufficiently decorrelated samples to train well.
  c. The horizon of 1707 is a hyperparameter in PPO. The number of allowed steps (200) before the episode terminates is a different concept.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BkgXtgGQTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to reviewer AnonReviewer1: High Level</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xk8jAqKQ&amp;noteId=BkgXtgGQTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper137 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018 (modified: 13 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper137 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you so much for your review. We split this response into two parts so that we could address the high level review and then the individual parts:

"I find that the work lacks significance and quality. There is not much novelty in the proposed method and there is a clear lack of comparisons to existing sample efficient LfD techniques such as GAIL. I believe this paper requires substantial improvements for publication and is not up to the ICLR standards in its current form."
- We would like for the reviewer to revisit this comment and their feeling that our work lacks significance and novelty:

a. Novelty: Our approach is genuinely new. The closest comparison (OpenAI) debuted two weeks prior to ours and in the form of a blog post rather than a paper. The second closest technique (Florensa) required that the environment was both resettable and reversible (rather than just resettable) and they used random backward walks to generate the curriculum. These are not small differences. The Pommerman environment is not reversible - we cannot reverse the state past the point where an agent dies. Neither are other research environments like Starcraft or a robot manipulating breakable objects. Random backward walks are inefficient and use a lot of time exploring areas not useful for learning an optimal policy. However, in an effort to instill how impactful these differences are, we will run Florensa's technique on the Grid environment and report comparisons.

b. Significance: We debuted a technique that (1) achieves a higher success rate over a vastly smaller complexity than standard RL (Figure 4), while generalizing to some extent to unseen maps (page 8). It does this (2) given even a suboptimal expert's state trajectory and (3) in sparse reward environments. Further, we (4) included unique theoretical analysis detailing why this works. We think that these contributions are unmatched and humbly request that you reevaluate our score in light of them.

c. Comparisons: GAIL and most other LFD techniques are limited to being approximately as good as the expert demonstration. By relaxing the state assumption, our technique surpasses the expert’s performance as an upper bound. We did run experiments with DAgger and found that results were poor, even after further training with RL. This is briefly mentioned at the very end of the appendix. Would it be satisfactory to you if we run further experiments with DAgger and report those in the next iteration?

Given your review, we believe that we didn't convey this story well enough. However, we also believe that our paper makes two important contributions that will be valuable to the community: (i) We demonstrate that Backplay is a robust, lightweight, and simple strategy to overcome sparse rewards in resettable environments; (ii) We provide the first mathematical framework for this line of research s.t. that we can analyze imitation learning under simple (albeit strong) assumptions and consequently laying out interesting future research directions. 

We hope that after we fix our presentation and address your concerns, you will be willing to reevaluate your score. Thanks again!
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>