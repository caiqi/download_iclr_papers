<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learning Neural PDE Solvers with Convergence Guarantees | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learning Neural PDE Solvers with Convergence Guarantees" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rklaWn0qK7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learning Neural PDE Solvers with Convergence Guarantees" />
      <meta name="og:description" content="Partial differential equations (PDEs) are widely used across the physical and computational sciences. Decades of research and engineering went into designing fast iterative solution methods...." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rklaWn0qK7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning Neural PDE Solvers with Convergence Guarantees</a> <a class="note_content_pdf" href="/pdf?id=rklaWn0qK7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019learning,    &#10;title={Learning Neural PDE Solvers with Convergence Guarantees},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rklaWn0qK7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Partial differential equations (PDEs) are widely used across the physical and computational sciences. Decades of research and engineering went into designing fast iterative solution methods. Existing solvers are general purpose, but may be sub-optimal for specific classes of problems. In contrast to existing hand-crafted solutions, we propose an approach to learn a fast iterative solver tailored to a specific domain. We achieve this goal by learning to modify the updates of an existing solver using a deep neural network. Crucially, our approach is proven to preserve strong correctness and convergence guarantees. After training on a single geometry, our model generalizes to a wide variety of geometries and boundary conditions, and achieves 2-3 times speedup compared to state-of-the-art solvers.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Partial differential equation, deep learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We learn a fast neural solver for PDEs that has convergence guarantees.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Bkgt-vXc3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A Good and Solid Work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklaWn0qK7&amp;noteId=Bkgt-vXc3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1214 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1214 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper develops a method to accelerate the finite difference method in solving PDEs. Basically, the paper proposes a revised framework for fixed point iteration after discretization. The framework introduces a free linear operator --- the choice of the linear operator will influence the convergence rate. The paper uses a deep linear neural network to learn a good operator. Experimental results on Poisson equations show that the learned operator achieves significant speed-ups. The paper also gives theoretical analysis about the range of the valid linear operator (convex open set) and guarantees of the generalization for the learned operator. 

This is, in general, a good paper. The work is solid and results promising.  Solving PDEs is no doubt an important problem, having broad applications. It will be very meaningful if we can achieve the same accuracy using much less computational power.  Here, I have a few questions. 

1). Why didn’t you try the nonlinear deep network? Is it merely for computational efficiency? I expect that nonlinear networks might result in even better estimates of H and further reduce the number of fixed-point iterations, despite each operation of H will be more expensive. There might be some trade-off here. But I would like to see some empirical results and discussions.

2). The evaluation is only on Poisson equations, which are known to be easy. Have you tried other PDEs, such as Burger’s equations? I think your method will be more meaningful for those challenging PDEs, because they will require much more fine-grained grids to achieve a satisfactory accuracy and hence much more expensive. It will be great if your method can dramatically improve the efficiency for solving these equations. 

3). I am a bit confused about the statement of Th 3 --- the last sentence “H is valid for all parameters f and b if the iterator \psi converges …” I think it should be “for one parameter”. 

Miscellaneous:
1)	Typo. In eq. (7) 
2)	Section 3.3, H(w) should be Hw (for consistency)
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BygG_etiTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklaWn0qK7&amp;noteId=BygG_etiTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1214 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1214 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your helpful reviews and suggestions.

1) "Why didn’t you try the nonlinear deep network? Is it merely for computational efficiency? I expect that nonlinear networks might result in even better estimates of H and further reduce the number of fixed-point iterations, despite each operation of H will be more expensive. There might be some trade-off here. But I would like to see some empirical results and discussions."

The reason we did not use nonlinear deep networks is that it’s hard to prove correctness guarantees. Our linear iterator has provably correct fixed point while nonlinear iterators may have non-unique or incorrect fixed points. In addition, it is easy to prove convergence by spectral theory, while this is not the case for nonlinear operators.


2). "The evaluation is only on Poisson equations, which are known to be easy. Have you tried other PDEs, such as Burger’s equations? I think your method will be more meaningful for those challenging PDEs, because they will require much more fine-grained grids to achieve a satisfactory accuracy and hence much more expensive. It will be great if your method can dramatically improve the efficiency for solving these equations."

We did additional experiments on Helmholtz equations, \nabla^2 u + k^2 u = 0, which is known to be very challenging [1]. So far we have some preliminary results of Conv1 model in a square domain: we outperform traditional methods by a similar margin. The following show for different values of k, the ratio of computation cost compared to Jacobi in terms of layers / flops (same as Table 1). 
k = 1: 0.422 / 0.685
k = 2: 0.396 / 0.643
k = 3: 0.383 / 0.622

We leave more thorough analysis of the Helmholtz equation for future work.

[1] Oliver G. Ernst and Martin J. Gander. Why it is Difficult to Solve Helmholtz Problems with Classical Iterative Methods. Numerical analysis of multiscale problems, 2012.


3). "I am a bit confused about the statement of Th 3 --- the last sentence H is valid for all parameters f and b if the iterator \psi converges … I think it should be 'for one parameter'. "

In Theorem 1 and Lemma 1, we showed that if our iterator is convergent, it converges to the correct solution, hence it is valid. In Theorem 3, we showed that if the iterator is valid for some f and b, then the iterator is valid for every f and b. These combined implies that the iterator is valid for every f and b if it is convergent for one f and b. We will rephrase Theorem 3 to remove the confusion.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rklkZrg8nm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting, well-written paper</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklaWn0qK7&amp;noteId=rklkZrg8nm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1214 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1214 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">==Summary==
This paper is well-executed and interesting. It does a good job of bridging the gap between distinct bodies of literature, and is very in touch with modern ML ideas. 

I like this paper and advocate that it is accepted. However, I expect that it would have higher impact if it appeared in the numerical PDE community. I encourage you to consider this conference paper to be an early version of a more comprehensive piece of work to be released to that community.

My main critique is that the paper needs to do a better job of discussing prior work on data-driven methods for improving PDE solvers.
==Major comments==
* You need to spend considerably more space discussing the related work on using ML to improve PDE solvers. Most readers will be unfamiliar with this. You should explain what they do and how they are qualitatively different than your approach. 

* You do a good job 3.3 of motivating for what H is doing. However, you could do a better job of motivating the overall setup of (6). Is this a common formulation? If so, where else is it used?
* I’m surprised that you didn’t impose some sort of symmetry conditions on the convolutions in H, such as that they are invariant to flips of the kernel. This is true, for example, for the linearized Poisson operator. 

==Minor comments==

* Valid iterators converge to a valid solution. However, can’t there be multiple candidate solutions? How would you construct a method that would be able to find all possible solutions?

* In (9), why do you randomize the value of k? Wouldn’t you want to learn a different H depending on what computation budget you knew you were going to use downstream when you deploy the solver? 

* In future work it may make sense to learn a different H_i for each step i of the iterative solver. 

* When introducing iterative solvers, you leave it as an afterthought that b will be enforced by clamping values at the end of each iteration. This seems like a pretty important design decision. Are there alternatives that guarantee that u satisfies b always, rather than updating u in such a way that it violates G and then clamping it back? Along these lines, it might be useful to pose (2) with additional terms in the linear system to reflect G. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1eZ3PYjaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to AnonReviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklaWn0qK7&amp;noteId=H1eZ3PYjaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1214 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1214 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your helpful reviews and suggestions.

1) “You need to spend considerably more space discussing the related work on using ML to improve PDE solvers. Most readers will be unfamiliar with this. You should explain what they do and how they are qualitatively different than your approach.”

We will add more discussions in our updated paper. To the best of our knowledge, related works applying ML to PDEs directly fit the solution with deep networks, which have no correctness or generalization guarantees and are restricted to specific dimensions and geometries.


2) “You do a good job 3.3 of motivating for what H is doing. However, you could do a better job of motivating the overall setup of (6). Is this a common formulation? If so, where else is it used?”

This formulation is a novel idea that provides correctness guarantees by leveraging a hand designed solver: we modify the residual of a hand designed solver. Another idea that also modify the residual (but not of a hand designed solver) is conjugate gradient.


3) “I’m surprised that you didn’t impose some sort of symmetry conditions on the convolutions in H, such as that they are invariant to flips of the kernel. This is true, for example, for the linearized Poisson operator.”

Generalization, for our model, is almost for free because of our linear ConvNet setup. Therefore, we didn’t find strong reasons to restrict the network parameters and reduce dimensionality. Enforcing symmetry introduces unnecessary overhead.


4) “Valid iterators converge to a valid solution. However, can’t there be multiple candidate solutions? How would you construct a method that would be able to find all possible solutions?”

For most PDEs with Dirichlet boundary conditions (e.g. Possion, Helmholtz), the solution is always unique. Thus, a valid iterator should converge to the unique solution. We currently consider PDEs that have unique solutions.


==Minor comments==

5) “In (9), why do you randomize the value of k? Wouldn’t you want to learn a different H depending on what computation budget you knew you were going to use downstream when you deploy the solver?”

Our hope is to learn a generic solver for a type of PDE that can be applied to a variety of applications. Therefore, we train the model agnostic to downstream applications. Nonetheless practitioners who know their computation budget can certainly fine tune our iterator with a fixed k.

6) “In future work it may make sense to learn a different H_i for each step i of the iterative solver.”

Thank you for the suggestion. We can try in the future, e.g. there are some methods that take the history of the iteration into account, which means it has a different H for each step.


7) “When introducing iterative solvers, you leave it as an afterthought that b will be enforced by clamping values at the end of each iteration. This seems like a pretty important design decision. Are there alternatives that guarantee that u satisfies b always, rather than updating u in such a way that it violates G and then clamping it back? Along these lines, it might be useful to pose (2) with additional terms in the linear system to reflect G.”

This is the most straightforward way to satisfy the boundary condition, and most existing iterative solvers enforce boundary conditions with this reset operation. We will also explicitly add G into our update rule in our updated paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJg5yrONnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A linear method for speeding up PDE solvers with good empirical performances</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklaWn0qK7&amp;noteId=BJg5yrONnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1214 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1214 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
The authors propose a method to learn and improve problem-tailored PDE solvers from existing ones. The linear updates of the target solver, specified by the problem's geometry and boundary conditions, are computed from the updates of a well-known solver through an optimized linear map.  The obtained solver is guaranteed to converge to the correct solution and
achieves a considerable speed-up compared to solvers obtained from alternative state-of-the-art methods.   

Strengths:
Solving PDEs is an important and hard problem and the proposed method seems to consistently outperform the state of the art. I ve liked the idea of learning a speed-up operator to improve the performance of a standard solver and adapt it to new boundary conditions or problem geometries. The approach is simple enough to allow a straightforward proof of correctness. 

Weaknesses:
The method seems to rely strongly on the linearity of the solver and its deformation (to guarantee the correctness of the solution). The operator H is a matrix of finite dimensions and it is not completely clear to me what is the role of the multi-layer parameterization. Based on a grid approach, the idea applies only to one- or two-dimensional problems. 

Questions:
- in the introduction, what does it mean that generic solvers are effective 'but could be far from optimal'?  Does this refer to the convergence speed or to the correctness of the solution? 
- other deep learning approaches to PDE solving are mentioned in the introduction. Is the proposed method compared to them somewhere in the experiments? 
- given a PDE and some boundary conditions, is there any known method to choose the liner iterator T optimally? For example, since u* is the solution of a linear system, could one choose the updates to be the gradient descent updates of a least-squares objective such as || A u - f||^2?
- why is the deep network parameterization needed? Since no nonlinearities are present, isn t this equivalent to fix the rank of H?
- given the `  interpretation of H' sketched in Section 3.3, is there any relationship between the proposed accelerated update and the update of second-order coordinated descent methods (like Newton or quasi-Newton)?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJxZsitoa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklaWn0qK7&amp;noteId=BJxZsitoa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1214 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1214 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your helpful reviews and suggestions.

1) “The method seems to rely strongly on the linearity of the solver and its deformation (to guarantee the correctness of the solution). The operator H is a matrix of finite dimensions and it is not completely clear to me what is the role of the multi-layer parameterization. 
“why is the deep network parameterization needed? Since no nonlinearities are present, isn t this equivalent to fix the rank of H?”

Even though composition of linear functions is still linear, using d linear layers is better than one. On a grid with n^2 vertices: one convolution layer requires O(n^2) computations and have local receptive field; one fully-connected layer requires O(n^4) computations and have global receptive field; our deep U-Net architecture has O(n^2) computations but global receptive field. Our hope is that the deep U-Net architecture learns a linear function with both good computation properties (O(n^2)) and convergence properties, which is impossible for one layer models.

Our learned network H is a convolutional operator, which does not have low rank. A low rank H is unlikely to perform well because many different errors may be mapped to the same correction term, while a high rank H can correct different errors differently. Our parameterization learns a high rank H with O(n^2) computation.


2) “Based on a grid approach, the idea applies only to one- or two-dimensional problems.”

Our method generalizes without modification to any dimensional problems: simply replace 2-D convolution to k-D convolution.


3) “in the introduction, what does it mean that generic solvers are effective 'but could be far from optimal'?  Does this refer to the convergence speed or to the correctness of the solution?”

We meant that generic solvers like Jacobi are hand-designed and theoretically correct, but may not be optimal in terms of convergence speed. Designing a solver is a trade-off between computation-per-iteration and spectral radius. We would like to have the smallest spectral radius given computation budget. We verify in our experiments: human designed solvers (e.g. Jacobi) are not Pareto optimal, and are outperformed by our learned solvers. Similar observations have also been made in other fields: learned models outperform hand designed ones, e.g. Andrychowicz et al., 2016, Song et al, 2017.


4) “other deep learning approaches to PDE solving are mentioned in the introduction. Is the proposed method compared to them somewhere in the experiments?”

To the best of our knowledge, related works applying ML to PDEs directly fit the solution with deep networks, which have no correctness or generalization guarantees and are restricted to specific dimensions and geometries. Our algorithm is the first deep learning based method with provable correctness and generalization guarantees.


5) “given a PDE and some boundary conditions, is there any known method to choose the liner iterator T optimally? For example, since u* is the solution of a linear system, could one choose the updates to be the gradient descent updates of a least-squares objective such as || A u - f||^2?”

Actually, this is exactly the update rule for most existing methods (conjugate gradient, Jacobi, etc). We compared with these methods (conjugate gradient, Jacobi) in experiments and outperform them. For example, if we minimize the objective 1/2 u^T A u - u^T f, given that A is symmetric, positive-definite, this objective has a unique minimizer, and the derivative is exactly Au - f. If we perform gradient descent on this objective with learning rate 1, we get exactly the Jacobi update.

Gradient descent may not be optimal; improving it is undergoing active research (e.g. ADAM, Adagrad, “Learning to learn” [1]). We tackle a special class of optimization problems and design methods with both correctness guarantees and better performance.

[1] Andrychowicz, Marcin, et al. "Learning to learn by gradient descent by gradient descent. NIPS, 2016.


6) “given the `interpretation of H' sketched in Section 3.3, is there any relationship between the proposed accelerated update and the update of second-order coordinated descent methods (like Newton or quasi-Newton)?”

On a grid with k (= n^2) vertices, second order methods (e.g. Newton) have optimal convergence speed (solve linear equations with a single update), but poor computation complexity (O(k^3) to solve A inverse). Our method only require O(k) computation per-iteration, and we hope to achieve a good trade-off between convergence speed and computation budget by optimization.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1e_-B_MoX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Deep Multigrid</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklaWn0qK7&amp;noteId=B1e_-B_MoX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1214 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hello!
The similar idea is proposed in "Deep Multigrid: learning prolongation and restriction matrices" (<a href="https://arxiv.org/abs/1711.03825)," target="_blank" rel="nofollow">https://arxiv.org/abs/1711.03825),</a> where authors optimize parameters of the multigrid method with a neural network reformulation of the multigrid method and automatic differentiation tool. Also, almost the same objective function to measure parameters quality is used, but with explanation how does this objective relate to the spectral radius of the iteration matrix. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1g2mv84om" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Differences and problems with Deep Multigrid</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklaWn0qK7&amp;noteId=H1g2mv84om"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1214 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Oct 2018</span><span class="item">ICLR 2019 Conference Paper1214 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hello!

Thank you for pointing out this unpublished but relevant work! We were not aware of it and we will certainly add a reference. Deep Multigrid has some surface resemblance to our method, but there are major differences:

(1) Generalization: Deep Multigrid does not generalize to different grid size or different geometries. The learned prolongation and restriction operators are fully-connected layers, which need retraining for each grid size and geometry. Our model generalizes (both by design and in experiments) to very different grid sizes and geometries after training on a single example (Figure 1).

(2) Usability: Deep Multigrid only experimented on 1D grids, with no proposed generalization to 2D or 3D geometries. 
On 1D grid, the matrix A is tridiagonal, and Au = f can be solved exactly by Gaussian elimination in O(n) time [1]. Contrastly, our method applies without modification to any dimension (by using d-dimensional convolution).

(3) Flexibility: Deep Multigrid only learns prolongation and restriction operators. Our U-Net model is end-to-end: it implicitly includes smoothing, prolongation, and restriction. Our approach is simpler yet more general.

(4) Experiments: Deep multigrid does not compare runtime with state-of-the-art solvers. Our method is faster (wall-clock time and number of operations) than both Jacobi Multigrid and FEniCS.


[1] Randall J LeVeque. Finite difference methods for ordinary and partial differential equations: steady-state and time-dependent problems, volume 98. Siam, 2007.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>