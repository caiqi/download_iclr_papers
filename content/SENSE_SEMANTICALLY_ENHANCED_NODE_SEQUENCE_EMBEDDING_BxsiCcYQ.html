<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>SENSE: SEMANTICALLY ENHANCED NODE SEQUENCE EMBEDDING | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="SENSE: SEMANTICALLY ENHANCED NODE SEQUENCE EMBEDDING" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1x9siCcYQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="SENSE: SEMANTICALLY ENHANCED NODE SEQUENCE EMBEDDING" />
      <meta name="og:description" content="Effectively capturing graph node sequences in the form of vector embeddings is critical to many applications. We achieve this by (i) first learning vector embeddings of single graph nodes and (ii)..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1x9siCcYQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>SENSE: SEMANTICALLY ENHANCED NODE SEQUENCE EMBEDDING</a> <a class="note_content_pdf" href="/pdf?id=B1x9siCcYQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019sense:,    &#10;title={SENSE: SEMANTICALLY ENHANCED NODE SEQUENCE EMBEDDING},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=B1x9siCcYQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Effectively capturing graph node sequences in the form of vector embeddings is critical to many applications. We achieve this by (i) first learning vector embeddings of single graph nodes and (ii) then composing them to compactly represent node sequences. Specifically, we propose SENSE-S (Semantically Enhanced Node Sequence Embedding - for Single nodes), a skip-gram based novel embedding mechanism, for single graph nodes that co-learns graph structure as well as their textual descriptions. We demonstrate that SENSE-S vectors increase the accuracy of multi-label classification tasks by up to 50% and link-prediction tasks by up to 78% under a variety of scenarios using real datasets. Based on SENSE-S, we next propose generic SENSE to compute composite vectors that represent a sequence of nodes, where preserving the node order is important. We prove that this approach is efficient in embedding node sequences, and our experiments on real data confirm its high accuracy in node order decoding.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Semantic, Graph, Sequence, Embeddings</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Node sequence embedding mechanism that captures both graph and text properties.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_S1egN7wGam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting topics are introduced but some corrections and clarifications are necessary</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1x9siCcYQ&amp;noteId=S1egN7wGam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper646 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper646 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors introduce the problem of learning embeddings that consider both text information and graph structures, as well as the embedding of a sequence of nodes with embeddings.

However, the proposed algorithm, SENSE-S, is incremental in the sense of aggregating two simple structures. In the evaluation, it is compared only with the heuristic combination of node2vec and paragraph2vec, not with any existing work about the graph embeddings that incorporate node features even though they are mentioned in the related work.

Furthermore, the objective of node sequence embedding is not clear. What do we want to represent from the embedding of node sequences? It looks like we have to keep the node embeddings anyway, and then what is the problem of just storing node ordering instead of having representation? Or can we aggregate node embeddings in some way with storing the order of nodes? These kinds of questions can be raised, mainly because of uncertain objectives. The description of preserving both ordering and node properties is too vague.

Also, SENSE does not seem to have any connection with SENSE-S. Why is SENSE-S special to SENSE? Are they independent?

Finally, the authors claim that SENSE is necessary to overcome the space issue that needs q*d dimension. However, from Figure 5, it seems that the proposed algorithm actually needs O(q*d) dimensions to represent the sequence correctly. It is somewhat related to the question about i.i.d. assumption in Theorem 2, where embedding does not guarantee the orthogonality across the dimensions. 

* Details
- In the introduction, "first" is repeated in the last paragraph of Page 1.
- N_G(v) and N_T(\phi) are said to be independent, but it should be the assumption since they are not the fact.
- Eq. (2) is not aligned with Eq (3) or (4). Either one needs to be fixed or the derivation needs to be described.
- How SVM is used needs to be described. Usage of embedding might be different depending on the usage of RBF kernel or linear kernel.
- Using the smaller number of random walks for Citation Network because it is a larger dataset needs some explanation.
- The calculation on the improvement percentage is completely misleading. If the accuracy is improved from 95% to 96%, it is about 1% improvement, not 20% improvement based on the error rate calculation.
- How the training/validation/test sets are split needs more description. Is it split by nodes or edges?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1lqXr9cnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper presents two approaches: one called SENSE-S for embedding nodes in attributed networks; the other one called SENSE for embedding a sequence of nodes. SENSE-S follows the structure of Skip-gram model. The main difference is that SENSE-S considers both node and words in node content as input and output for learning their embedding. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1x9siCcYQ&amp;noteId=S1lqXr9cnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper646 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper646 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents two approaches: one called SENSE-S for embedding nodes in attributed networks; the other one called SENSE for embedding a sequence of nodes. SENSE-S follows the structure of Skip-gram model. The main difference is that SENSE-S considers both node and words in node content as input and output for learning their embedding. For generating embedding vector for a sequence of nodes, SENSE takes summation of cyclically shifted unit-vectors constructed by SENSE-S on nodes in a sequence.    

The paper is well written with a clear definition of the studied problem and a clear introduction of the presented methods. Evaluation was conducted on two real-world data sets (Wikipedia and citation network). It is an interesting idea to represent a sequence by the summation of cyclically shifted unit-vectors of nodes in a sequence. However, there are several concerns about the work presented in this paper. 
1) the evaluation of SENSE-S is not sufficient. The baseline methods used in comparison are the simple ones that take concatenation of vectors induced from text and graph, or use one for initializing the learning of the other.  There existing several approaches that learn node embedding vectors from attributed graph (considering both the node content text and graph topology structure), such as TADW [1], HSCA [2], PLANE [3],GAE[4], AANE[5], ANRL [6]. SENSE-S should be compared with these methods for showing its effectiveness. 
2) the embedding vector of a node sequence is evaluated by showing the decoding accuracy. It would be more interesting to show how these vectors can be used for some real applications. And, to have high decoding accuracy, the embedding dimension for sequences of 10 nodes should be up to 1024, which is quite expensive for computing and for storage, making the presented method unpractical in real-world applications.   


[1] C. Yang, Z. Liu, D. Zhao, M. Sun, E. Y. Chang, Network representation learning with rich text information. IJCAI, 2015
[2] D. Zhang, J. Yin, X. Zhu, C. ZHang, Homophily, structure, and content augmented network representation learning.  ICDM 2016. 
[3] T. M. V. Le and H. W. Lauw. Probabilistic latent document network embedding.  ICDM, 2014.
[4] Thomas N Kipf, Max Welling. Variational Graph Auto-Encoders. NIPS Workshop on Bayesian Deep Learning.  2016
[5] Xiao Huang, Jundong Li, Xia Hu. Accelerated attributed network embedding. SDM 2017.
[6] Zhen Zhang, Hongxia Yang, Jiajun Bu, Sheng Zhou, Pinggang Yu, Jianwei Zhang, Martin Ester, Can Wang. ANRL: Attributed Network Representation Learning via Deep Neural Networks. IJCAI, 2018
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_r11pKi1i7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea and fleshed-out experiments, but somewhat niche appeal.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1x9siCcYQ&amp;noteId=r11pKi1i7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper646 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper646 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes node embedding methods for applications where nodes are sequentially related. An example application is the "Wikispeedia" dataset, in which nodes are connected in a graph, but a datapoint (a wikispeedia "game") consists of a sequence of nodes that are visited. Each node is further attributed with textual information.

The methods proposed are most closely related to skipgrams, whereby the sequence of nodes are treated like words in a sentence. Then, node attributes (i.e., text) and node representations must be capable of predicting neighboring nodes/words. (Fig.s 1/2 are a pretty concise overview of the proposed architecture).

Positively, this is a quite sensible extension and modification of existing ideas in order to support a new (or different) problem setting.

Negatively, I'd say the applications for this technique are fairly niche, which may limit the paper's readership. The method is mostly fairly straightforward and not methodologically groundbreaking (probably borderline in terms of expected methodological contribution for ICLR). I also didn't understand whether the theoretical claims were significant.

The wikispedia/physics experiments feel a bit more like proofs-of-concept rather than demonstrating that the technique has compelling real-world uses. The experiments are quite well fleshed-out and detailed though.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>