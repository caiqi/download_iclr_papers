<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>The Problem of Model Completion | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="The Problem of Model Completion" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1xEtoRqtQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="The Problem of Model Completion" />
      <meta name="og:description" content="Motivated by the need for efficient techniques for shared model governance, we propose splitting deep learning model between multiple parties. The security guarantee of this technique is introduced..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1xEtoRqtQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The Problem of Model Completion</a> <a class="note_content_pdf" href="/pdf?id=H1xEtoRqtQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 14 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019the,    &#10;title={The Problem of Model Completion},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=H1xEtoRqtQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=H1xEtoRqtQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Motivated by the need for efficient techniques for shared model governance, we propose splitting deep learning model between multiple parties. The security guarantee of this technique is introduced as the problem of *model completion*: Given the entire training data set or an environment simulator, and a subset of the parameters of a trained deep learning model, how much training is required to recover the model’s original performance? We define a metric for evaluating the hardness of the model completion problem and study it empirically in both supervised learning on ImageNet and reinforcement learning on Atari and DeepMind Lab.  Our experiments show that (1) the model completion problem is harder in reinforcement learning than in supervised learning because of the unavailability of the trained agent’s trajectories, and (2) its hardness depends not primarily on the number of parameters of the missing part, but more so on their type and location.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">deep learning, reinforcement learning, multi-party computation</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We study empirically how hard it is to recover missing parts of trained models</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HJla2wojnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review: Problem motivation and analysis</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xEtoRqtQ&amp;noteId=HJla2wojnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper436 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper436 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes and studies the “model completion” problem: given a trained network (and the data on which is was trained), if a subset of the network is reinitialized from scratch, how many retraining iterations are needed to achieve the original network accuracy (or some percentage of it)? For a variety of networks and problems in both supervised and reinforcement learning, model-completion (MC) hardness is quantified for individual network layers/sections. The experiments are the core of the paper and are generally well documented and seem reproducible.

However, there are two issues that cloud the paper:
	1. The problem motivation (bounding the security of model splitting) is a bit odd. Has model splitting been proposed in the literature as a potential solution to shared model governance? Otherwise it feels like the problem setting was invented to justify the analysis in this paper: “the tail wagging the dog” as the saying goes…
	2. Model completion yet still be an interesting analytical tool for deep networks, but this requires a different evaluation. For instance, model completion provides a way to study how complicated different network layers are to learn or maybe to quantify how much of the inference task may be contained in each. (Though these concepts would need precise language and experimental evidence.) But how do these observations compare to other ways of obtaining similar observations? For instance, from the pruning literature, (Molchanov, 2017, ICLR, <a href="https://openreview.net/pdf?id=SJGCiw5gl)" target="_blank" rel="nofollow">https://openreview.net/pdf?id=SJGCiw5gl)</a> includes several figures detailing the statistics of individual network layers and how “prunable" are the filters in each.

This is largely an analytical paper, and I’ll readily acknowledge that it is difficult to pull a clear and insightful study out of a jumble of experimental observations (and hard to review such a paper too). But the limitations of the problem motivation (point #1) and (in my opinion) the misaligned focus of the analysis (point #2), hurt the clarity and significance of this paper. For it to really be a useful tool in understanding deep learning, some additional work seems to be needed.

Other notes:
	3. Pruning literature would be a reasonable comparison in the related work. For instance, (Han, ICLR, 2017, https://arxiv.org/abs/1607.04381) describes a dense-sparse-dense method where a (dense) model is pruned (sparse), after which the pruned connections are reinitialized and retrained (dense) leading to improved accuracy relative to the original dense model.
	4. Consider replacing the uncommonly used “ca.” with “~”, e.g. “~1000x” instead of “ca. 1000x”.
	5. The specifics about ImageNet in the intro to Section 3 should be moved to Section 4.
	6. In Section 3.2 paragraph 2, clarify if “loss” refers to test loss as stated in the intro to Section 3.
	7. In Figure 2 (alpha=0.9) and Figure 3 (alpha=1.0, bottom), why are the values constant?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rklIsV6_pQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We made the motivation of our paper more explicit and added related work on pruning</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xEtoRqtQ&amp;noteId=rklIsV6_pQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper436 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper436 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for your careful review! Let us address your concerns:

1. We acknowledge that the paper should be more upfront about its motivation (e.g. mention it in the abstract). We are not aware of previous work that discusses model splitting. Our motivation was to find a technique for multi-party computation (MPC) with limited computational &amp; communication overhead. With these constraints, model splitting is an obvious approach to try, and we didn’t think to lay claim to its "invention." We’ve adjusted the phrasing in the paper to make this more clear.

While model splitting is much faster as an MPC technique, its security guarantees are much weaker. The hardness of the model completion problem is at the heart of this MPC technique, and this is the reason we wanted to study it.

2. Thank you for pointing us to the pruning literature. We have added a paragraph to the related work section.

Regarding the two papers you referenced: Molchanov et al. (2017) evaluate different approaches for pruning trained models with the aim of making inference faster, dropping entire feature maps at a time. Figure 2 in their paper shows the distribution of importance of feature maps across layers (VGG-16). Their findings (e.g. that lower layers are more important) are compatible with ours. There are also other related papers that prune individual connections rather than entire feature maps (e.g. Changpinyo et al., <a href="https://arxiv.org/pdf/1702.06257.pdf)." target="_blank" rel="nofollow">https://arxiv.org/pdf/1702.06257.pdf).</a>

Han et al. (2016) improve the accuracy of models by training in three phases of which the second one adds a sparsity regularizer (how many of the connections are reduced to 0). However, they do not include an analysis on which parts of the model get pruned, making their results incomparable with ours.

All of these papers remove neurons across all layers, i.e. by dropping some but not all neurons in every layer. In contrast, in our experiments we remove entire layers at a time with all of their neurons.

Nevertheless, an interesting connection between our work and the pruning literature you pointed us to is the empirical evidence regarding the relative importance of different layers in the model. However, in contrast to our work the literature does not study what happens when removing an entire layer at a time.

4-6: Thank you for pointing this out. We have fixed this in the paper.
7. In Figure 2 constant values at 0.9 are due to the way learning rate schedule is updated (there's a jump at that point for all layers and they reach 0.9 in the same time). In Figure 3 the values are constant because MC-hardness is capped at 1.0 and none of the layers ever retrieve the full performance.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1efdYco3X" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xEtoRqtQ&amp;noteId=S1efdYco3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper436 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1lby2Fjhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea but needs better positioning, metrics, and analysis</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xEtoRqtQ&amp;noteId=B1lby2Fjhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper436 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper436 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes the interesting idea of analyzing how difficult
it is to re-initialize and re-train layers in neural networks.
They study these techniques in the context of ImageNet classification
and reinforcement learning in the Atari and DeepMind lab domains.
While these are interesting ideas and domains to study, I have
concerns with the positioning and execution of the paper.

[Positioning, execution and motivation]
On the positioning of the paper, a significant part of the introduction
and related work section is spent arguing that this approach can be used
for shared model governance in contexts where homomorphic encryption
or secure multi-party computation would instead be used.
Comparing the approaches studied in this paper to these
sophisticated cryptographically-motivated techniques seems
like too much of a stretch, as the methods serve very different
purposes and in most cases cannot even be directly compared.

The first and second paragraph discuss the vision of distributing
the training of models between multiple parties.
I agree that this is a useful area to study and direction
for the community to go, but as the introduction of this paper
states, this is the most interesting when the parties have
control over logically separate components of the modeling pipeline
and also when joint training of the components is being done,
potentially on disjoint and private datasets.
The empirical results of this paper do none of this,
as they only look at the case when a single layer is being
replaced.

Furthermore the motivation and positioning of the paper is
not carried through in the empirical setup, where they
investigate approaches that do training over all
of the parameters of the model, breaking the assumption
that the parties should be independent and should
not share information.

[Metrics for measuring model completeness]
Section 3.1 defines the metric of completion hardness that is
used throughout the rest of the paper. The metric looks at the
number of iterations that re-training the model takes to
reach the same performance as the original model.
It's not clear why this is an important metric and I am
not convinced it is the right one to use as it:
1) does not give a notion of how nicely the missing portion
was recovered, just that the accuracy reached the
same accuracy as the original network, and
2) methods with a very long per-iteration runtime such as
second-order and sampling-based methods could be used to
reach a good performance in a small number of iterations,
making these methods appear to be very "good" at
completing models. I don't think it is nice that this
metric relies on the same optimizer being used for the
original model and the completed model.

I think it's more interesting to study *how much* data is
required to recover missing portions of the model instead
of how many iterations are needed to recover the same performance.
The supervised learning experiments appear to be done
using the entire dataset while the RL experiments do
present a setting where the data is not the same.

[Empirical results]
I am also surprised by the empirical finding in Section 5.1
that T1 outperforms T2, since it seems like only optimizing
the parameters of the missing layer would be the best
approach. I think that if a similarity metric was used
instead, T2 would be significantly better at finding the
layer that is the most similar to the layer that was removed.

Some smaller comments:

1. In Section 3.1, the definition of C_T does not use T explicitly
   inside of it.
2. In the last paragraph of Section 3.1 and first paragraph of
   Section 3.2, N should be defined as an iteration that
   reaches the best loss.
3. The description of T3 does not say what method is used to
   optimize the over-parameterized layer, is it T1 or T2?
4. Why does T4 use T1 instead of T2?
5. In the experimental setup, why is T2 applied with a different
   learning rate schedule than the original training procedure?
6. Why is T2 not shown in the AlexNet results for Figure 2?
7. The dissimilar axes between the plots in Figure 2 and
   Figure 3 make them difficult to compare and interpret.
8. It's surprising that in Figure 3, the hardness of \alpha=1.0
   for T2 is 1.0 for everything.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1gdPwTdTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The model completion problem we study is the hardest realistic test for shared model governance via model splitting</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xEtoRqtQ&amp;noteId=B1gdPwTdTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper436 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper436 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for the long and thorough review!

&gt; Comparing the approaches studied in this paper to these sophisticated cryptographically-motivated techniques seems like too much of a stretch, as the methods serve very different purposes and in most cases cannot even be directly compared.

These are indeed very different approaches with different overhead/security trade-offs. To our knowledge, the approach we are investigating (model splitting) has not been discussed before and we made some changes in the phrasing to make this more clear in the paper. Both MPC and model splitting try to solve the same problem (shared model governance), but we argue that our approach is more scalable than MPC.

As you point out, a more realistic scenario for model splitting would be one in which the parties do not share the same dataset. However, since we are interested in the security guarantees of model splitting, we study the setting that is hardest to defend against, i.e., the setting that is easiest for the adversary. Since we do not know how much data the adversary has access to, we assume they have access to everything.

&gt; Furthermore the motivation and positioning of the paper is not carried through in the empirical setup, where they investigate approaches that do training over all of the parameters of the model, breaking the assumption that the parties should be independent and should not share information.

We think this could be a misunderstanding. While in retraining procedure T1 we optimize all parameters of the model, there is always at least one entire layer of parameters that has been removed and replaced with a freshly initialized one (See Figure 1 and Section 3.1). This reflects the assumption that there is at least one layer that the adversary would at no point in time have access to, and which is held by the other party. Do you have a sense how we could make this more clear in the paper?

Regarding your points about our metric for the hardness of model completion:

1. In our experiments we assumed that we only care about the final test accuracy. If there are auxiliary objectives (e.g. being able to fine-tune more easily), these could be added to loss (like a regularizer) and thus feed into the MC-hardness definition. What exactly do you mean by how ‘nicely’ the missing part has been recovered?

2. This is an excellent point! We wanted to compare computational costs because we are assuming this is what we care about. Computational costs are much harder to compare when you're using different optimizers. This is mentioned in the beginning of section 3.1, where we state our simplifying assumption: *"We assume that computational cost are constant for each step, which is approximately true in our experiments."* We have slightly adjusted the phrasing.

&gt; I think it's more interesting to study *how much* data is required to recover missing portions of the model [...]

This is a very interesting question for the setting in which different parties have different amounts of data. We didn’t study the problem from this angle because we wanted to assume the worst case (the adversary has access to all of the data) in order to stress-test model splitting as an approach for shared model governance. We have to leave the investigation of model completion under partial access to the dataset to future work.

&gt; the RL experiments do present a setting where the data is not the same.

Could you please elaborate how you mean this sentence? The RL experiments train and retrain on the same environment simulator. This simulator is stochastic, so the agent will not be trained on exactly the same data, but on data drawn from the same distribution.

&gt; I am also surprised by the empirical finding in Section 5.1 that T1 outperforms T2.

Your surprise is understandable. However, similar results have also been in the literature; e.g. Figures 1 and 2 in Yosinski et al. (2014) <a href="http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf." target="_blank" rel="nofollow">http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf.</a>

&gt;  I think that if a similarity metric was used instead, T2 would be significantly better at finding the layer that is the most similar to the layer that was removed.

Could you elaborate how you could use a similarity metric during retraining in our setup? What would you measure similarity to?</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkxYOwTu67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Regarding your smaller comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xEtoRqtQ&amp;noteId=SkxYOwTu67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper436 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper436 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1. We admit that this is an awkward property of the notation. To make it technically more rigorous, we could add T as a superscript to \theta_i everywhere, but we think this would decrease readability without adding clarity.
2. This is a somewhat arbitrary choice we made. You are right that defining N to be the best loss would also make sense, but the final loss is what we used in our experiments. Moreover, the retraining procedure always starts with the parameters in the final step, not the best parameters. We worry that it would make the formal setup confusing if we used the loss of the best parameters, but used the final parameters for retraining. In practice the difference is minor.
3. It is T1. We have clarified this in the paper.
4. Because T1 has been empirically shown to be a stronger retraining procedure.
5. We tried both and reported the one that performed better. We have clarified this in the paper.
6. Because the results were not very strong. The data is in Appendix C.1, but also added an additional figure (Figure 7) to the appendix.
7. This was a deliberate choice because we think that the comparison between different layers is more meaningful than the comparison between different values of \alpha.
8. In these runs, the model fails to recover the original performance for all layers over the course of the retraining run.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_HkgkrCXq2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting new nugget of a problem</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xEtoRqtQ&amp;noteId=HkgkrCXq2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper436 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper436 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors introduce the problem of Model Completion (MC) to the machine learning community.  They provide a thorough review or related works, and convincingly argue that existing solutions to this sort of task (i.e., homomorphic encryption and multi-party computation) are not fully satisfactory in the domain of neural network learning.

The authors also provide extensive numerical experiments attempting to quantify their proposed measure of hardness-of-model-completion, MC-hardness_T(\alpha) on a diverse set of Supervised and RL-related tasks, and they provide extensive analysis of those results.

I find the paper to raise more questions than it answers (in a good way!).  The authors note that their measure depends strongly on the peculiarities of the particular (re)training scheme used.  Do the authors worry that such a measure could end up being too loose--essentially always a function of whatever the fastest optimization scheme happens to be for any particular architecture?  

More broadly, there's an additional axis to the optimization problem which is "How much does the training scheme know about the particulars of the problem?", ranging from "Literally has oracle access to the weights of the trained model (i.e., trivial, MC-hardness = 0 always)" to "knows what the architecture of the held-out-layer is and has been designed to optimize that particular network (see, e.g., learned optimizers)" to "knows a little bit about the problem structure, and uses hyperparameter tuned ADAM" to "knows nothing about the problem and picks a random* architecture to use for the held out weights, training it with SGD".

Model completion seems, morally (or at least from a security stand-point) slightly under-specified without being more careful about what information each player in this game has access to.  As it stands, it's an excellent *empirical* measure, and captures a very interesting problem, but I'd like to know how to make it even more theoretically grounded.

An excellent contribution, and I'm excited to see follow-up work.



* We of course have tremendous inductive bias in how we go about designing architectures for neural networks, but hopefully you understand my point.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">9: Top 15% of accepted papers, strong accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rylzQd6_TX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Yes, there are numerous follow-up directions to explore</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xEtoRqtQ&amp;noteId=rylzQd6_TX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper436 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Nov 2018</span><span class="item">ICLR 2019 Conference Paper436 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for your thoughtful review!

&gt; Do the authors worry that such a measure could end up being too loose--essentially always a function of whatever the fastest optimization scheme happens to be for any particular architecture?

Yes, this is definitely a concern, and we mention this in the final paragraph. Since we tried a variety of retraining procedures (freezing, not freezing, overparameterization, different initialization schemes) with mostly similar results, we think it is reasonable to be optimistic that there are no radically faster retraining procedures. However, we need to improve our understanding with more experiments, which we have to leave to future work. But we should also stress that at this point the security guarantees this approach provides are only empirical rather than theoretical. Depending on the application that may or may not be a dealbreaker.

&gt; More broadly, there's an additional axis to the optimization problem which is "How much does the training scheme know about the particulars of the problem?"

This is an excellent way to phrase our setup! We decided to strike the balance between impractical retraining procedures like setting all weights to the former values in one step (which requires information that the retraining procedure does not have access to), and making the model completion problem as easy as possible to stress-test its viability as a technique for shared model governance. Since this is (to our knowledge) the first paper on the topic, we struck a balance with what experiments can be executed with reasonable effort; e.g. we did not try to learn a dedicated optimizer. Our goal was to gain some preliminary results on the viability of shared model governance.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>