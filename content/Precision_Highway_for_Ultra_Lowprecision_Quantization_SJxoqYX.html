<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Precision Highway for Ultra Low-precision Quantization | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Precision Highway for Ultra Low-precision Quantization" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SJx94o0qYX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Precision Highway for Ultra Low-precision Quantization" />
      <meta name="og:description" content="Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision. To resolve this..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SJx94o0qYX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Precision Highway for Ultra Low-precision Quantization</a> <a class="note_content_pdf" href="/pdf?id=SJx94o0qYX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019precision,    &#10;title={Precision Highway for Ultra Low-precision Quantization},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SJx94o0qYX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Quantization of a neural network has an inherent problem called accumulated quantization error, which is the key obstacle towards ultra-low precision, e.g., 2- or 3-bit precision. To resolve this problem, we propose precision highway, which forms an end-to-end high-precision information flow while performing the ultra-low-precision computation. First, we describe how the precision highway reduce the accumulated quantization error in both convolutional and recurrent neural networks. We also provide the quantitative analysis of the benefit of precision highway and evaluate the overhead on the state-of-the-art hardware accelerator. In the experiments, our proposed method outperforms the best existing quantization methods while offering 3-bit weight/activation quantization with no accuracy loss and 2-bit quantization with a 2.45 % top-1 accuracy loss in ResNet-50. We also report that the proposed method significantly outperforms the existing method in the 2-bit quantization of an LSTM for language modeling.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">neural network, quantization, optimization, low-precision, convolutional network, recurrent network</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">precision highway; a generalized concept of high-precision information flow for sub 4-bit quantization </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BJeYeCw-aQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An OK paper but need more evaluation.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJx94o0qYX&amp;noteId=BJeYeCw-aQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper23 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper23 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper investigates the problem of neural network quantization. The main idea is to employ an end-to-end precision highway to reduce the accumulated quantization error and meanwhile enable ultra-low precision in deep neural networks.  The experimental results on the 3- and 2-bit quantizations of ResNet-18/50 and 2-bit quantization of an LSTM model demonstrate the effectiveness of the proposed method. 

This paper is well written and organized. The idea of utilizing a high-precision information flow to reduce the accumulated quantization error is technically sound. The empirical studies on accumulated quantization error, loss surface analysis, model performance, and hardware cost are quite thorough and solid. 

The idea of precision highway, however, is quite similar to the skip connections used in Bi-Real Net. Therefore, it may be a good idea to provide a thorough discussion over these two different methods so as to make the distinction.

In Table 2, the results of Bi-Real Net is based upon 1 bit activation/weight quantization, while the proposed method uses 2 bit activation/weight quantization. To give a fair comparison, it may be better to provide 1 bit activation/weight quantization results of the proposed method.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkl1SxreR7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The response for Reviewer 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJx94o0qYX&amp;noteId=Bkl1SxreR7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper23 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper23 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
1. The difference between the proposed method and Bi-Real Net

We’d like to let you know that this study was conducted in parallel with Bi-Real Net. This work was submitted to another conference and re-submitted to ICLR2019 after adding additional extensive experiments including loss surface analysis and hardware cost estimation. We respect the research outcome of Bi-Real Net and refer to it in the original manuscript. 

As mentioned in the original manuscript, both Bi-Real Net and PACTv2 apply quantization on pre-activation style residual net, the basic module of which is composed of batch-normalization (BN) – ReLU – convolution. Meanwhile, the end-to-end precision highway is a generalized “network-level structural” method applicable to not only pre-activation style network but also post-activation style network, having the conv-BN-ReLU module as stated in the original manuscript as follows.

“in the case of feed-forward networks with identity path, our precision highway idea is applicable regardless of pre-activation or post-activation structure”

In addition, it is a general method also applicable to recurrent network including LSTM and GRU. We described how the precision highway can be applied to LSTM in Section 3.2 and showed it significantly outperforms the existing quantization method on a language model. 
Since it is a novel structural method, it raises new challenges for further improvements as mentioned in Section 3.3 of the original manuscript as follows. 

“In the case of networks with multiple candidates for the precision highway, e.g., DenseNet, which has multiple parallel skip connections (Huang et al., 2017), we need to address a new problem of selecting skip connections to form a precision highway, which is left for future work.”

We think the precision highway opened a new space of mixed precision neural network design where the precision of data representation, previously ignored, can now be jointly optimized with that of computation for further improvements of quantized networks.



2. 1-bit quantization result

We performed 1-bit activation/weight quantization for the post-act style ResNet-18. For a fair comparison, we didn’t apply the teacher-student and progressive quantization method and instead adopted BN-retraining proposed in Bi-Real Net. Our 1-bit activation/weight ResNet-18 gives 56.73 / 80.11 % of Top-1/Top-5 accuracy, which is by 0.33 / 0.61 % higher than the result of Bi-Real Net, respectively. According to our observation, the final accuracy is degraded when adopting a tight approximation of the derivative of the non-differentiable sign function proposed by Bi-Real Net. Instead, the conventional quantization method proposed by DoReFa-net can improve results. This difference seems to result from the difference of the network structure and activation quantization function. We will add this result and analysis to the revision.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BylJ0G-03X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A paper with good ideas and solid results, but some overlap with the literature</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJx94o0qYX&amp;noteId=BylJ0G-03X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper23 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper23 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies methods to improve the performance of quantized neural networks.  The paper is largely centered around the idea of "precision highways" (full-precision residual connections) that run in parallel to fully-quantized convolutions.  However, the paper also throws in a toolbox of other methods like distillation from a teacher network, a quantization method based on the Laplace distribution, and a fine tuning scheme.

The paper reports performance for the resulting networks that is impressive but still believable.   They also do very extensive experiments, including an ablation study in Table 1 that I really liked, and a study of how the precision of the skip connections impacts overall performance.   I also like the visualizations of how quantization impacts the loss surface.

My main concern about this paper is that is has conceptual overlap with other approaches.  The authors are not the first to quantize resnets, and other papers have looked at teacher training and distillation as a method of refinement.  The authors are fairly upfront about this though, and I think this paper is the first to do a really thorough investigation of the impacts of skip connections in their own right.    Realistically, fully binarizing neural nets without modification is unlikely to lead to good performance.  The idea of leaving the skip connections with higher precision is a good compromise that achieves hardware friendliness along with strong performance, so I think it's worth having a paper like this that takes a closer look at this approach.

A few questions I had:
1)  I can't tell exactly what methods are being used in Table 1.  When the "highway" box is unchecked, does this mean the skip connection is absent?  Or that it exists but with full precision?  Or maybe that the skip connection branched after the quantization instead of before?   Also, what fine-tuning methods is used when the "teacher" box is un-checked?

2) You implemented your own version of Zhuang's method.  However, I'd like to know how your numbers compare to the original reported numbers in Zhuang's paper.

One other minor criticism - When you fine-tune a modified network, the activations and weights will change.  It could be that the networks is modifying its parameters to account for (i.e., cancel out) the quantization errors.  For this reason I don't interpret Figure 4 as evidence for accumulation of error.  Perhaps this type of behavior would exists if you fine-tuned two full-precision networks using different random seeds, or different teacher networks.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HkeWxbrxR7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The response for Reviewer 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJx94o0qYX&amp;noteId=HkeWxbrxR7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper23 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper23 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
1. The answer to question 1

When the “highway” box is unchecked, the skip connection is branched after the quantization. This case corresponds to the conventional quantization where the quantization is combined with ReLU and, thus, skip connection is quantized before the branch. When the “teacher” box is unchecked, we use the conventional cross-entropy loss for training. We will clarify this in the revision.



2. The answer to question 2

We re-implemented Zhuang’s baseline, but final accuracy is different due to the minor difference of implementation details on input augmentation and teacher-student methods. According to Zhuang’s paper, their implementation shows 70.8 /88.3 % of Top-1/Top-5 accuracy for 2-bit ResNet-50, while our implementation shows 70.48 / 89.93 % of Top1-/Top-5 accuracy. 



3. The comments about Figure 4

We appreciate the comments. We’d like to first explain how we had obtained Figure 4 and how we performed again new experiments to clarify the phenomenon of accumulated quantization error in the revision. 
In order to obtain Figure 4, we first obtained a fully trained full-precision network. Then, we applied 4-bit weight/activation quantization to the network while having two cases of skip connection, 4-bit one (Zhuang’s in the figure) and 32-bit one (Proposed in the figure). Since they are from the same fully trained full-precision network, we think that the difference between the two graphs in Figure 4 represents the effect of high-precision skip connection.
In order to account for the reviewer’s comments and give a more direct comparison, we did new experiments where we trained, from the same initial condition, two activation-quantized networks (one with precision highway and the other with low precision skip connection) where weights are in full precision and activations are quantized to 4 bits. The new experiments give a similar result to Figure 4 while the difference in accumulated quantization errors gets increased, possibly, due to the removal of the quantization error of weights. In order to clarify the phenomenon of accumulated quantization error, we will use the new experimental results in the revision.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Syego60FhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper proposes to keep a high activation/gradient flow in two special kinds of networks structures, namely ResNet and LSTM. For ResNet, the skip connections are made high-precision by adding the skip connection before quantization. For LSTM, the cell and hidden state computations are of high precision.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJx94o0qYX&amp;noteId=Syego60FhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper23 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper23 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The proposed method is advantageous in that it only requires changes to some parts of the original ResNet or LSTM, without having to significantly change the network structure or training algorithm. It also reports empirical success of using high-precision skip connections in ResNet and cell/hidden state updates in LSTMs.

However, it is unclear why it is necessary to keep a high-precision activation/gradient flow. What is the problem with existing quantized networks that do not have these high-precision-flow? Also, how does the high-precision flow interact with the rest of the network (with low-precision operations)?

Moreover, the proposed method has limited novelty as the use of full-precision skip connections has been proposed in Bi-Real (Liu et al. 2018).

Minor:
- It is hard to tell that the weight histogram in Figure 3 is similar to a Laplacian distribution. It can also be approximated by other distributions (such as Gaussian or piecewise-linear distributions).
- What kind of activation quantization is used?
- In the experiments, when is the cosine similarity between the quantized and full-precision networks computed? after training or on an intermediate training step?
- What are the axes in Figure 5? Why is there only one local minimum in Figure 5(d)? Why the training with PH converges even slower than without PH at the early stage of training?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkeJ-QBl07" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The response for Reviewer 1 </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SJx94o0qYX&amp;noteId=rkeJ-QBl07"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper23 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper23 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
1. The  importance of precision highway

Precision highway helps reduce the accumulated quantization error. In ResNet, the difference between Equations (1) and (2) explains how the precision highway reduces quantization error. Without precision highway, the output of residual block has additional quantization error, ‘e’ in Equation (1) while the precision highway removes it as shown in Equation (2). 
Section 3.2 describes how the precision highway reduces the accumulation of quantization error in the LSTM as follows.
“Specifically, when calculating ct, the inputs are not quantized, which reduces the accumulation of quantization error on ct. The computation of ht can also reduce the accumulation of quantization error by utilizing high-precision inputs. The construction of such a precision highway allows us to propagate high-precision information, i.e., cell states ct and outputs ht, across time steps”
The result of low-precision computation is in high precision before quantization. We perform elementwise operations (additions in ResNet and multiplications in LSTM/GRU) between the precision highway and the high-precision result of low-precision computation. In other words, the elementwise computations in ResNet and LSTM/GRU are performed in high precision as mentioned in the original manuscript as follows.

“We keep high-precision activation only on the skip connection and utilize it only for the element-wise addition. ” in Section 3.1.
“In our proposed method, all of the element-wise multiplications in Equations 3e and 3f are performed in high precision.” in Section 3.2.



2. The novelty of the proposed method compared to Bi-Real Net

Please refer to our response to reviewer 3.



3. Laplace distribution approximation

Please note that the y-axis is in log-scale while x-axis in linear scale. The histogram decreases linearly in the plot, which is well modeled by Laplace distribution. The jitter at the ends is due to the fact that the number of samples is small, and the range is in log-scale. We performed the same quantization adopting other distributions including Gaussian and triangle distributions, and the Laplace distribution showed marginally better results than the others. 



4. What kind of activation quantization is used? 

We use the conventional quantization method used in DoReFa-net, and the method is also adopted in Zhuang’s work. After clipping the activation to a pre-defined value, typically 1, the linear quantization is applied to the activation. We will clarify this in the revision.



5. when is the cosine similarity between the quantized and full-precision networks computed? 

Please refer to our response to reviewer 2.



6.  What are the axes in Figure 5? Why is there only one local minimum in Figure 5(d)? Why the training with PH converges even slower than without PH at the early stage of training?

We appreciate the comments. It helped clarify the loss surface analysis in Figure 5. In order to obtain Figure 5, we applied Hao Li’s method as mentioned in the paper. In short, each figure represents loss surface seen from the local minimum we obtained from the training, i.e., the weight vector of the final trained model. In order to obtain two-dimensional view, we utilize two base vectors, u1 and u2, each of which corresponds to the axis of the figure. The base vector is a randomly generated vector having the same dimension of the weight vector. According to (Li et al., 2017), two randomly generated high-dimensional vectors tend to be orthogonal to each other. The origin of the figure at (0, 0) corresponds to the weight vector of the local minimum. The z-axis corresponds to the loss. In order to obtain a point, e.g., (0.25, 0.5) in the figure, we scale the two base vectors, i.e., 0.25*u1 and 0.5*u2, and add them to the local minimum weight vector corresponding to the origin. Then, we obtain the loss for the new weight vector, which is depicted at the point, (0.25, 0.5) on Figure 5. Since the figure is a loss surface near the local minimum, we tend to have a single local minimum in the figure unless we have another local minimum near the obtained one. The figure does not represent the relationship between loss and training epochs. We will clarify how we obtained Figure 5 in the revision.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>