<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Improving latent variable descriptiveness by modelling rather than ad-hoc factors | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Improving latent variable descriptiveness by modelling rather than ad-hoc factors" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BkMqUiA5KX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Improving latent variable descriptiveness by modelling rather than..." />
      <meta name="og:description" content="Powerful generative models, particularly in Natural Language Modelling, are commonly trained by maximizing a variational lower bound on the data log likelihood. These models often suffer from poor..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BkMqUiA5KX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Improving latent variable descriptiveness by modelling rather than ad-hoc factors</a> <a class="note_content_pdf" href="/pdf?id=BkMqUiA5KX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019improving,    &#10;title={Improving latent variable descriptiveness by modelling rather than ad-hoc factors},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BkMqUiA5KX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Powerful generative models, particularly in Natural Language Modelling, are commonly trained by maximizing a variational lower bound on the data log likelihood. These models often suffer from poor use of their latent variable, with ad-hoc annealing factors used to encourage retention of information in the latent variable. We discuss an alternative and general approach to latent variable modelling, based on an objective that encourages a perfect reconstruction by tying a stochastic autoencoder with a variational autoencoder (VAE). This ensures by design that the latent variable captures information about the observations, whilst retaining the ability to generate well. Interestingly, although our model is fundamentally different to a VAE, the lower bound attained is identical to the standard VAE bound but with the addition of a simple pre-factor; thus, providing a formal interpretation of the commonly used, ad-hoc pre-factors in training VAEs.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">generative modelling, latent variable modelling, variational autoencoders, variational inference, natural language processing</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">This paper introduces a novel generative modelling framework that avoids latent-variable collapse and clarifies the use of certain ad-hoc factors in training Variational Autoencoders.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BklgQ_jTnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>No Title</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkMqUiA5KX&amp;noteId=BklgQ_jTnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper200 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper200 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes to resolve the issue about a variational auto-encoder ignoring the latent variables. The paper presents a variational auto-encoder with repeated likelihoods, which results in a 1+m factor in front of the log-likelihood term. This paper justifies this model by defining a model as a combination of a variational auto-encoder and a stochastic auto-encoder.

This paper tries to re-interpret the VAE models with weighting factors such as beta-VAE as a model of repeated likelihoods/views. From the Bayesian modelling perspective, it is a bit problematic as the same observed variables appear multiple times in the model. The model assumes two independent observation of the sam latent variable, however, it is actually given the same observation twice. This introduces a bias in posterior. On the other hand, a weighting factor in front of likelihood is not new. This trick has been used in multi-view learning or imblanced classification as a practical solution to balance the views or the classes.

The derivations in Section 2 is hard to follow. It is unclear how to Equation 7 is derived from Equation 6.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1l4ZNK_2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reviews</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkMqUiA5KX&amp;noteId=S1l4ZNK_2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper200 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper200 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper aims to address the problem that the LSTM decoder of a language model might be too strong to ignore the information from the latent variable. This is a well-known problem and Bowman et al.(2016) proposed to use KL-annealing to help relieve this issue. The proposed solution in this work is to add a stochastic autoencoder to the original VAE model. The experimental results to some extent confirm the advantage of the model. 

My major concerns are 
1. The scope of the model is relative small. The problem to be solved is not the key problem in language modeling. And the KL-annealing trick is a good and cheap solution. Although the performance is relative better, the model is way to complicated than KL-annealing. 
2. The proposed method is somewhat contrived and not well motivated. What is the motivation behind Equation 2? And the assumption of the model, i.e. Eq. 5 and 6, seems too strong. 

Based on the above concerns, I will suggest a rejection for the paper.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkgwDLN6sX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting idea on better utilizing latent representations in generative models</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BkMqUiA5KX&amp;noteId=rkgwDLN6sX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper200 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">24 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper200 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents AutoGen, which combines a generative variational autoencoder (VAE) with a high-fidelity reconstruction model based on autoencoder. The motivation behind AutoGen is to address a common problem when training VAEs with powerful encoders (e.g., autoregressive models) that the model simply ignores the latent representation and does not associate latent representation with the generated data in a meaningful way. A common strategy to (partially) solve this problem is by introducing KL annealing, gradually turning up the KL term in the standard ELBO, which unfortunately means the model is no longer optimizing a valid lower bound on the log-likelihood of the data. AutoGen provides an alternative solution by adding a reconstructing term to the standard ELBO for VAE to enforce the latent representation striking a balance between generation and reconstruction -- the objective still remains a valid lower bound (albeit not on the data log-likelihood) and has close connection to the standard ELBO. It also provides alternative interpretations for some other similar techniques, e.g., beta-VAE and KL-annealing. Experimental results and survey studies demonstrate that AutoGen is able to leverage latent representation more effectively when comparing with VAE without annealing, has better reconstruction overall, but at the same time lose some ability to generate good samples from prior -- this is not too surprising considering the model objective balances between generation and reconstruction.   

Overall the paper is clearly written with some minor issues listed below. The paper presents a simple but reasonable adjustment to the standard VAE training and yields an objective that is intuitive and connects nicely to some other similar techniques that scale the KL term. I have a few concerns about the paper, however, that I hope the authors could better address:

1. My major concern is that VAE after all is a generative model and its ability to sample from prior is an important property. VAE with annealing, admittedly unprincipled, addresses this issue better than AutoGen, especially on shorter generated sentences. There might be cases where reconstruction is important, but the paper did not demonstrate that (this relates to the point 3 below). In the current state, even though the paper presents a simple and intuitive adjustment to the VAE training objective, it hasn't convinced me that if I want a generative model of language I would try to use AutoGen rather than VAE with annealing. 

2. As mentioned in the paper, VAE with annealing is an unprincipled approach. It would be interesting to see if AutoGen compares favorably over some other principled approaches along the line of better utilizing the latent representation, e.g., Krishnan et al. On the challenges of learning with inference networks on sparse, high-dimensional data, 2018.

3. I can understand that because of the objective of AutoGen, it does not make much sense comparing held-out likelihood or ELBO between VAE and AutoGen. However, currently the paper is lacking in terms of quantitative evaluation. An interesting experiment would be to use the latent representation z from both AutoGen and VAE (with/without annealing) for some downstream tasks and see if better reconstruction in this case helps. This would also demonstrate the importance of incorporating a reconstruction term in the objective. 

Minor: 

1. Above equation (5): What exactly do you mean by “symmetric”?

2. Above equation (9): instead of just 2 -&gt; instead of just 1? since m represents the number of reconstructions

3. Also above equation (9): it is worth more elaboration on how to generalize to m reconstructions: it is not L_AutoGen = L_VAE + m * L_SAE (which is what the text seems to suggest), rather it's L_SAE = \int_z p(x|z)^m p(z|x) dz. 

4. Section 3.2, since the model "finds different reconstructions every time from the same input sentence", how robust is the reconstruction to the sampling variations? </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>