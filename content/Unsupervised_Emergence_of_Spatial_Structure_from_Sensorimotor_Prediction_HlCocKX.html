<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Unsupervised Emergence of Spatial Structure from Sensorimotor Prediction | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Unsupervised Emergence of Spatial Structure from Sensorimotor Prediction" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1lC8o0cKX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Unsupervised Emergence of Spatial Structure from Sensorimotor..." />
      <meta name="og:description" content="Despite its omnipresence in robotics application, the nature of spatial knowledge and the mechanisms that underlie its emergence in autonomous agents are still poorly understood. Recent theoretical..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1lC8o0cKX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Unsupervised Emergence of Spatial Structure from Sensorimotor Prediction</a> <a class="note_content_pdf" href="/pdf?id=H1lC8o0cKX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019unsupervised,    &#10;title={Unsupervised Emergence of Spatial Structure from Sensorimotor Prediction},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=H1lC8o0cKX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Despite its omnipresence in robotics application, the nature of spatial knowledge and the mechanisms that underlie its emergence in autonomous agents are still poorly understood. Recent theoretical work suggests that the concept of space can be grounded by capturing invariants that space's structure induces in an agent's raw sensorimotor experience. Moreover, it is hypothesized that capturing these invariants is beneficial for a naive agent trying to predict its sensorimotor experience. Under certain exploratory conditions, spatial representations should thus emerge as a byproduct of learning to predict. We propose a simple sensorimotor predictive scheme, apply it to different agents and types of exploration, and evaluate the pertinence of this hypothesis. We show that a naive agent can capture the topology and metric regularity of its spatial configuration without any a priori knowledge, nor extraneous supervision.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">spatial perception, grounding, sensorimotor prediction, unsupervised learning, representation learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A practical evaluation of hypotheses previously laid out about the unsupervised emergence of spatial representations from sensorimotor prediction.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_r1xp--ns2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1lC8o0cKX&amp;noteId=r1xp--ns2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper223 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper223 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper investigates the exploratory conditions under which spatial representations will emerge as a byproduct of learning to predict the next sensory observation. In particular, the authors test three exploratory conditions:
(i) When the set of sensory-motor interactions (st, mt, s_{t+1}, m_{t+1}) are inconsistent. 
(ii) Environment is static and set of sensory-motor interactions are consistent. 
(iii)  Environment is dynamic and the set of sensory-motor interactions are consistent. 

Authors measure the quality of learned spatial representations by computing disparity between the set of agent positions and the corresponding embedding of motor commands learned by predicting the next sensory observation. They conclude that under condition (iii), the disparity is minimum -- i.e. the agent is best able to discover the spatial structure of it environment. 

Philosophically, I love the direction of this work -- understanding the origins of our spatial representations. But, I am concerned with the delivery. My concerns/questions are as following:

(a) Firstly, the writing is too verbose and vague without clarifying the details and there are too many references to Laflaquiere et al. I would recommend the authors to be more precise, i.e. define topological/metric invariants, clarify how in-consistent sensory/motor pairs are sampled in MTM condition and how environment is perturbed in MMT condition. These things are defined at a high-level and not precisely. 

(b) The whole premise of comparing the embedding of motor commands and the agent's spatial configuration only works under a special condition -- s = φε(m) (section 3 of the paper), which is not general. For e.g. if an arm is “torque controlled” it is not possible to predict the location of the arm (i.e. a potential sensory observation) just from the torques. Additional knowledge of the agent’s state such as current position and the velocity of the arm is required. In the examples mentioned in the paper, the arm is “position” controlled, i.e. given the orientation of each joint (i.e. the motor command) it is possible to predict the sensory observation.  This is a very special case. In biology for example, we control the flexing of muscle fibers using the motor system, we can’t directly output positions of the arm. The general, condition of operation should be: s_{t+1} = φε(m_t, s_t). 

(c) Authors argue that in order to learn metric invariants, the agent needs to observe the same sensory state under different motor commands. They further argue that in the MMT condition, where the environment also translates, this affect is achieved and therefore metric invariants are learned. My position is that this is simply an artifact of the restricted problem setup where  s = φε(m) holds. In a more general setup, s_{t+1} = φε(m_t, s_t) there is no requirement for the environment to move. An arm with different torques can be at the same position and hence the condition imposed by authors should be specified naturally even in MM environments. What do the authors think? 

(d) Under the condition of,  s = φε(m) difference between MM and MMT appears that in one case (MM), neural network is trained without translation perturbations, and in other case MMT is a form of data augmentation with translation perturbations. I am not sure if there is any other justification for why only topological invariants should be learned with MM and metric invariants with MMT. To me it seems like training with data augmentation leads to better metric learning. Do the authors have any other insights — I would love to know. 

(e) Finally the embeddings are useful, if they are useful for an end-task. I would love, if the authors evaluated the learnt embeddings in each of the three conditions for some end tasks such as reaching in case of an arm or something else that is more feasible. 

Despite it being a very interesting topic, due to theoretical concerns outlined above, I cannot recommend the paper for acceptance. With a strong rebuttal it is possible to convince me otherwise.  
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BkxVXXd5hQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting topic, contribution questionable, better write-up required</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1lC8o0cKX&amp;noteId=BkxVXXd5hQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper223 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper223 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes that agents can extract the spatial structure of the world if consistently explored solely from sensorimotor predictions.

The topic of the paper is relevant and fits the conference. However, I have doubts about the generality of the claims made.

Strong aspects:
Abstract makes the reader curious and excited about the content
Interesting and important topic for representation learning.

Weak aspects:
The paper is written in a way that it is not easy to follow. 
Important details are in the Appendix and some more formal description would be helpful. A clearer presentation of the three different exploration types for instance would help the reader. 
The paper is not using standard notation and deviates from the POMDP description for not clear reason. 
It is hard to compare and the results are in a very restricted setup.
No baseline or comparison to other methods are given.
Why does space emerge in the motor-state (h) not the sensor state?
Not enough details given to reproduce the paper. It is nothing mentioned about code to be made public.

Discussion:
The argument of the paper is that it is sufficient to do sensor prediction when the world is consistently explored and changing in a consistent manner. There is no need for additional losses or the like. However, I am not convinced about the generality of this statement. What if the world is much richer and the agent and the environment undergo the typical transformations. Rotation and movement in 3D space + non-linear distortions by sensors (e.g. a camera). I am not seeing why a representation of space should emerge without any pressure for minimality or the like. The paper also assumes that the sensor is moved by the motor command in space directly and not, for instance, as a double integrator equations where the motor command is a force that accelerates a body. 
In the current form, what it really says, is that the network figures out the actual effect of the motor command in moving the sensor around. The sensor is moved in a 2D space by the actions such that the network recovers this 2D manifold. And this can only happen if it actually observes movements and the dataset is rich enough. This distinguishes the 3 different exploration settings. 

Details:
Sec 2:  Prior work: Which priors in Jonschkowski and Brock 2015 do you mean that are specific to the emergence of space? Slowness?
Sec 3: what do you really mean by motor state? In MDPs there is the notion of action that effects the state of the system. There is not really a state of the motor system? Do you mean proprioceptive sensors or actually the motor command/action.
The definition s = \phi_e(m) is kind of nonstandard as one would expect a dependency on s_{t-1} which is in your notation hidden in \epsilon.
The logic is not so straight forward to me. Isn't the logic such that: Given a rigid metric space: when the agent moves around the same type of movements lead to the same kind of transformations independently of there the agent is located?
I believe this section could be streamlined and illustrated by some examples to drive the message home. Also, making a clear and potentially more explicit statement of these invariances (e.g. by an equation) and why they will be revealed by learning predictions.

Sec 4.2:
Improve the description of the settings. In the first setting (MTM) subsequent sensor/motor values are independent right? So it is the same as having randomly selected isolated data points from a normal interaction. 

MMT setting: you write ... which can translate randomly after each transition.  This is almost the same as in MTM where you write ...translates randomly between t and t+1
In the Appendix you write that in MMT that environment moves only every 100 steps? 

BTW:
Is the env-movement a smooth movement or a jump?
 
Fig 2: why does the green curve end so early? Is it because of your stopping criteria for training. I would like to see the same training time for all settings.

Fig 3 and text: Should one not expect a torus in (a)? The world is not a square but a torus, as you have cyclic boundary conditions. I am surprised to not see this in the plots. The current result somehow violates the smoothness because there is a big jump between the boundaries although in environment there is none.
 
Sec 6: par 2: ...these invariants represents for the predictive model?

Out of my curiosity:
you write that ... casts some doubts on purely passive and observational approaches....
In which sense did the actions help here? Do you mean that the agent needs to know its own actions right? 
So when it is to be done from a video (just sensor information) than the actions would need to be inferred first?

Typos:
p1 par2: approache
p8 last par: could be merge
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1er13fc37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Unconventional and interesting approach to unsupervised learning of sensorimotor perception</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1lC8o0cKX&amp;noteId=S1er13fc37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper223 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper223 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This very interesting paper is based on the sensorimotor contingency theory, which grounds the perception of the agent (motor perception and sensory perception) in the capacity to learn to predict future sensory experience and to build a compact internal representation of proprioception and motor states. They show that through active experience (exploration) of an environment, an agent can encode its motor state in a way that captures both the topology and the relative distances. The authors show that in the case of an environment consistent with sensorimotor transitions and with some changes in the environment that are not inconsistent with the sensorimotor transitions (resets?), the network can learn the representation h of motor that project to the actual position of the agent in the environment (albeit in very contrived 2D toy tasks).

The model does not rely on RL at all; rather, it uses the representation of motor states (proprioception) to make predictions about the sensory observations of the environment. If such an agent were to act, I presume that there would be a search procedure across motor states to make the agent reach a desired state. The paper merits publication at ICLR provided very extensive revisions are made, and I am listing here the improvements to be made.

It is cool to cite Kant, Poincaré and Nicod, whose philosophical work underlies subsequent work on representing space and sensory experience. When citing Kant, please cite the primary source, not the 1998 re-edition.

There are missing references to Wayne et al (2018) "Unsupervised Predictive Memory in a Goal-Directed Agent" arXiv:1803.10760 and Ha &amp; Schmidhuber "World Models" arXiv:1803.10122, where an agent is shown to build a representation of the world that can be decoded into spatial position and even a map of obstacles, on previously unseen environments, using only prediction of images, rewards and actions. Please include them in your work as they considerably change the narrative of section 2 (related work). Essentially, while the claims of the paper are interesting and relevant for the representation learning community, similar work has already been done, at much larger scale, from visual observations, using RL and self-supervised learning.

Section 2 is also somewhat overly critical of previous work: in (Banino et al, 2018; Cueva &amp; Wei, 2018) "rely[ing] on extraneous spatial supervision signals [do not] counter any claim of autonomy", first because these signals can come from sensory perception (e.g., smell) and also because the agent is still autonomous at test time. Similarly, the depth prediction task in (Mirowski et al, 2017) is rather intuitive (stereo-vision).

Part 3 is difficult to parse: it would help to use the word proprioception (or explain why it is inappropriate) when talking about motor states, and exteroception (sensing the environment). I understood the first assumption, which is local continuity in sensory and motor space as well as the ambiguity of redundant motor systems that can generate the same sensory states, but not the second assumption. From what I understand, there are two invariants in building the model of proprioception: invariance to the topology of the environment and to the distances between objects, but then this is hard to reconcile with the setup of (in)consistent transitions in a moving environment and consistent transitions in a static environment. Please rewrite this section in a way that is easier to parse for people who know state-space models and RL for navigation and grasping (who may be your audience). Moreover, all the references point to a single work, which suggests that it is a very peculiar way of approaching a much more general problem of sensorimotor prediction, and therefore begs for a clear and simple explanation.

The architecture of the model is interesting: typically deep RL papers encode the sensory observations s into a hidden representation h, to take actions and produce a motor state m. Here, the current and future motor states m_t and m_{t+1} are embedded into h_t and h_{t+1} using a siamese MLP and used, in combination with the current sensory observation s_t, to make a prediction of s_{t+1}. This is somewhat related to learning the dynamics in model-based RL; please look into and cite Pathak et al (2017) "Curiosity-driven exploration by self-supervised prediction", ICML and other work on intrinsic curiosity.

The experiments are in very simplistic 2D grid world environments, but it makes the analysis and understanding of the 3D representations h much more simpler to follow. On the other hand, the discrete world task is very contrived (especially the weird mapping from m to h and from p to s) and hard to relate to existing work. One difficult problem that is solved by (Banino et al, 2018) is that of path integration in 2D from egocentric velocity. Could the authors present results on such a nonlinear case?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>