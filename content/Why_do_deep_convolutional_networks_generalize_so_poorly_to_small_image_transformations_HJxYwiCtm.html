<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Why do deep convolutional networks generalize so poorly to small image transformations? | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Why do deep convolutional networks generalize so poorly to small image transformations?" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HJxYwiC5tm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Why do deep convolutional networks generalize so poorly to small..." />
      <meta name="og:description" content="Deep convolutional network architectures are often assumed to guarantee generalization for small image translations and deformations. In this paper we show that modern CNNs (VGG16, ResNet50, and..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HJxYwiC5tm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Why do deep convolutional networks generalize so poorly to small image transformations?</a> <a class="note_content_pdf" href="/pdf?id=HJxYwiC5tm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019why,    &#10;title={Why do deep convolutional networks generalize so poorly to small image transformations?},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HJxYwiC5tm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Deep convolutional network architectures are often assumed to guarantee generalization for small image translations and deformations. In this paper we show that modern CNNs (VGG16, ResNet50, and InceptionResNetV2) can drastically change their output when an image is translated in the image plane by a few pixels, and that this failure of generalization also happens with other realistic small image transformations. Furthermore,  we see these failures to generalize more frequently in more modern networks. We show that these failures are related to the fact that the architecture of modern CNNs ignores the classical sampling theorem so that generalization is not guaranteed. We also show that biases in the statistics of commonly used image datasets makes it unlikely that CNNs will learn to be invariant to these transformations. Taken together our results suggest that the performance of CNNs in object recognition falls far short of the generalization capabilities of humans.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Convolutional neural networks, The sampling theorem, Sensitivity to small image transformations, Dataset bias, Shiftability</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Modern deep CNNs are not invariant to translations, scalings and other realistic image transformations, and this lack of invariance is related to the subsampling operation and the biases contained in image datasets.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_B1lUFfAjpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Problems with reproducing the result</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxYwiC5tm&amp;noteId=B1lUFfAjpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper281 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We attempted to reproduce the jagged predictions seen in this work's Figure 1, using the polar bear video. The paper claims that the video, when naturally classified, rapidly changes in prediction confidence and often misclassifies frames, i.e., labels them as other than "polar bear."

We were unable to reconstruct these results. We found that the classification pipeline -- with the same classifier and video as in the paper -- correctly classifies the frames as polar bear in a smooth and reliable fashion. 

The resulting video can be found here: <a href="https://youtu.be/NF4hc-RiXqg" target="_blank" rel="nofollow">https://youtu.be/NF4hc-RiXqg</a>

Based on examining the frames presented in the paper and the subsequent correspondence with the authors, we are led to believe that the authors missed a crucial part of the image classification pipeline: square cropping (by the shortest edge length) the image before resizing. In fact, in Figure 1, we can see how the polar bear looks elongated vertically.

------------------------
Technical details:

In our reproduction we used the exact same video and network that the paper claims to use, the InceptionResNet-V2 model from Keras. To understand why our classification pipeline worked and this paper's authors' pipeline did not, we step back and examine the pipeline for image classification. For some image x we:

1. Take image x, then center square crop based on the shortest side of the image
2. Resize this square, cropped image to 299x299 pixels
3. Perform preprocessing on input
4. Feed preprocessed inputs to model for classification

We performed all these steps correctly and thereby achieved smooth, correct predictions. Based on the correspondence with the authors, we are led to believe that they did not perform the crucial cropping step in part 1 - they only scale the original image to 299x299 pixels, then proceed to step 3. </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Sye2D_EhaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>These problems have already been addressed in the text</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxYwiC5tm&amp;noteId=Sye2D_EhaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper281 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper281 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comment. Indeed we had a correspondence about this issue following a posting of a previous version of our paper on ArXiv. 

The reviewer is referring to one particular result shown at the bottom of figure 1. Contrary to the title of the reviewer’s comment, he/she acknowledged in our correspondence that given the input frames shown in the bottom of figure 1, the output of the network that we show is reproducible and it varies drastically even between almost identical frames. 

The issue that the reviewer is addressing is how to go from the YouTube video to the network input since the video has an aspect ratio very different from one. Indeed for this specific video, this “processing pipeline” makes a difference. However, and as we already mentioned in the correspondence, we easily found other YouTube videos of polar bears that give jagged predictions with the reviewer’s suggested pipeline.

As a result of the same correspondence, we added a footnote at the bottom of page 3 that explicitly says that different preprocessing pipelines give different results. 
Importantly, following the reviewer's comments, we verified that all the other results do not depend on the pipeline and all the graphs shown in the paper are with a pipeline that maintains the aspect ratio of the original image.

That being said, the question of "the right pipeline" is far from obvious. Using a center crop as suggested by the reviewer means that you will necessarily miss polar bears that do not appear in the center (an extreme case of taking advantage of photographer's bias). In fact, the default preprocessing suggested by Keras (<a href="https://keras.io/applications/)" target="_blank" rel="nofollow">https://keras.io/applications/)</a>  prefers to reshape the full image (as we did in the bottom of figure 1) rather than to perform a center crop.

Finally, we want to stress again that the three frames shown in the bottom of figure 1 are exactly the frames given to the network and given these frames anyone can reproduce our reported network output. The difference between these frames is a very small deformation that is imperceptible to humans and yet makes a huge difference to the output of the network. The goal of our paper is to explain why this happens.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJgzBmsyAm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxYwiC5tm&amp;noteId=HJgzBmsyAm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper281 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your response.

The input processing pipeline of Inception-ResNet-v2 (and indeed every Inception family classifier), as per the code release of the original paper, indicates that original Inception-ResNet-v2 model was trained such that it would be evaluated with a pipeline including a center crop (see *** below). Therefore since there is a shift in the distribution of input images due to preprocessing differences we cannot conclude that the observed 'jaggedness' is inherent to the model (in the authors results that use this pipeline (including Figure 1)).

We are not debating the fact that the input pipeline change can lead to incorrect predictions. We are also not debating that the frames in the figure are the frames given to the network. All we are saying is that these frames are distorted in a way that the network is not trained to expect. You can see a side by side comparison here: <a href="https://i.imgur.com/TxJeCsx.png." target="_blank" rel="nofollow">https://i.imgur.com/TxJeCsx.png.</a>

Therefore, it is unclear how the results presented in the paper support the claim that the standard classifier fails on natural images. It is hard to expect the classifier to work  when the input has (in effect) been transformed in an unexpected way. (After all, once the deformation introduced by misconfiguration of the pipeline are corrected, the model classifies the resulting images correctly.)

Regarding correspondence, we were not shown any videos that induced jagged predictions when classified with the correct input processing pipeline (although these may exist, and we would be happy to evaluate any source videos that the authors give -- we were unable to find the source videos for the other videos shown in Figure 1).

------------------------
***
During Inception-ResNet-v2 evaluation, as per the paper and its authors code release (https://github.com/tensorflow/models/tree/696b69a498b43f8e6a1ecb24bb82f7b9db87c570/research/slim), we can see that inference preprocessing uses a center crop. Any other input pipeline configuration is incorrect.

Evaluation calls the function `preprocess_image` here:
https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py#L285

Which ultimately uses a center crop, as seen here in the `preprocess_for_eval` function:
https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py#L244</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJxCFsEbA7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>misunderstanding?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxYwiC5tm&amp;noteId=SJxCFsEbA7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper281 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper281 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comment.
 

Perhaps it makes sense to take this particular discussion about videos offline since we do not seem to be converging despite many rounds of back and forth (maybe a phone call will be helpful).

The important thing to remember is that except for the bottom of Figure 1, all the other results use ImageNet images,  NOT YouTube videos. Thus the statement in the reviewer's comment "we were unable to find the source videos for the other videos shown in Figure 1" seems to suggest a misunderstanding by the reviewer. As the text clearly describes, we took an image from imageNet, resized it (while preserving aspect ratio) , embedded it in a square image that is the size expected by the network and synthetically moved it around or rescaled it. As we also explain in the text, we experimented with different ways to embed the image into the square image (black background, inpainting, cropping) and we found a lack of invariance using all these hundreds of images

The following anonymous dropbox folder contains YouTube videos that use exactly the same pipeline as suggested by the reviewer and the network outputs are highly jagged. 
<a href="https://www.dropbox.com/sh/wvis40zd3sltf31/AACn480D7DB14FtneBaIMWPoa?dl=0" target="_blank" rel="nofollow">https://www.dropbox.com/sh/wvis40zd3sltf31/AACn480D7DB14FtneBaIMWPoa?dl=0</a>  
 We are going to upload a code to github with the original frames to reproduce these results. We are also happy to send it to the reviewer.

 Again, we emphasize that figures 2 and 8 in the paper summarize experiments with thousands of presentations of images to the different networks, and all of these images are from ImageNet and do not require the reviewer's suggested pipeline nor the pipeline we describe in the appendix.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div><div class="note_with_children"><div id="note_B1lXL9e32Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Critical topic, but limited novelty and results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxYwiC5tm&amp;noteId=B1lXL9e32Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper281 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper281 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies the lack of shift invariance in state-of-the-art neural networks, namely, the paper introduces results that show that state-of-the-art deep neural networks are affected by 1-pixel shifts because the convolutional layers in the network poorly sample the feature maps. The topic addressed in the paper is critical for many computer vision systems, as lack of shift invariance is a catastrophic failure mode. The arguments of the paper help clarifying why the networks are so sensitive to small shifts of the objects (poor subsampling) but generalize well (there is a bias in the location of the objects in the dataset).  Both of these arguments and the sensitivity of the networks to small shifts are well known in the literature, but it is great to see a paper that puts them together and tests these arguments in state-of-the-art deep nets for ImageNet.

However, the paper could do a much better job providing evidence to support the arguments:

*Few quantiative results on the sensitivity to 1-pixel shift, most results are qualitative. This makes hard to assess whether the reported results are "accidents" found in certain images or are general. The results that support that 1-pixel shifts affect state-of-the-art neural networks are in Figure 2b. Yet, these results are unclear, eg. is "400 Jaggedness" a lot?, what is the size of the embedded image?, How are the 100 images selected? Is the network performing well in those images? How does the size of the embedded image change the "Jaggedness"? 

*Something that could help to strengthen the results would be to add networks with better sampling + larger pooling regions and see how this solves the lack of shift invariance. Now it has only been tested increasing the pooling regions, which misses the main point of the paper. Also, the results on these networks with larger pooling regions, are all qualitative.

*The mathematical proof is done for average pooling, which is rarely used nowadays. I would suggest using max pooling. Also, the aforementioned experiment in which the size of the pooling region is increased, is it max pooling or average pooling?

*Limited results on the ImageNet bias. These results are reported in one image category (Figure 5), how general are them?

*The paper assumes that shifting an image embedded an object in a black background is equivalent to shifting an object in a static background. A hypothesis would be that the embedding of the image in the black background creates artificial boundaries that make the network more fragile to 1-pixel shifts than for natural images.

In summary, I think this is a paper that may arise a lot of interest, although the different arguments are known and the experiments are poorly executed.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_H1g7rqLdnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice empirical study of invariants in modern CNNs, with quantitative support at all key points.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxYwiC5tm&amp;noteId=H1g7rqLdnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper281 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper281 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper describes an empirical study of translation and scale
invariance properties of modern CNN architectures. The authors conduct
a thorough study of translation invariance in VGG16, ResNet50, and
InceptionResNet with respect to the Nyquist frequency and shift-
versus translation-invariance properties of network layers as a
function of depth and subsampling rate. Empirical observations are
quantified using a variety of metric to measure the stability of
feature maps under geometric transformations of the input.

The paper has the following strong points:

 1. It tells an interesting (and engaging) story about a largely
    empirical study, and while doing this never pretends to be more
    than it is.
 2. Empirical observations are supported by quantitative measures that
    give compelling evidence for most observations in the paper. The
    discussion about shiftability versus translatability is
    particularly interesting with its link to nonlinearity, smoothing
    and Nyquist limits.

The paper has the following weak points:

 1. The reliance on inpainting for almost all experiments is somewhat
    worrying. It is not clear that this procedure isn't introducing
    its own biases affecting translation and scale invariance. The
    authors make reference to a separate protocol reported in the
    appendices, but it isn't clear which results in the appendices
    they are referring to. A more thorough control study seems in
    order to verify that inpainting is a reasonable simulation.
 2. Some figures are scaled down to the limits of legibility.

In summary: I like this paper a lot, and I think it adds useful
elements and analytical tools (both theoretical and empirical) to the
discussion on invariants in modeern CNNs.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJeBZeoN27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Although the general idea is interesting, experimental evaluation is not convincing. Similarly, some explanations like the photographer's bias as reason for susceptability to very small image transformation is not entirely convincing. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxYwiC5tm&amp;noteId=BJeBZeoN27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper281 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper281 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Paper summary: 

As is made clear in the title, this paper sets out to answer the following question: “Why do deep convolutional networks generalize so poorly to small image transformations?”. It focuses on natural image transformations on translation and scaling (rotation is missing though).

The paper proposes two main explanations: 
-	Strided convolution, called subsampling in the paper, ignores the classical sampling theorem,
-	CNNs will not learn invariance because of the (photographers') biases contained in the datasets.

On a general level, the paper is a good read, it is well written and the figures clearly convey the message they’re intended to. Adversarial attacks and robustness of CNNs in general is a very interesting and important topic in ML. The originality of this work is in the approach of the problem, the paper tries to explain the reasons why CNNs are vulnerable. Related works put more emphasis on coming up with novel attacks/defense strategies. Considering natural attacks as done in this submission is particularly interesting as it is probably a more surprising shortcoming of CNNs compared to optimally designed attacks or highly unnatural perturbations. The argument about subsampling (stride) being the reason of not having translational invariance is nice, especially the theoretical insight with the Shannon-Nyquist theorem and the more figurative example on part detectors. There are nevertheless a few major concerns about this work:

Major Concerns:

Theoretical arguments:
The theoretical argument made in this paper is interesting but to make the point stronger a more in-depth explanation would be needed.
-	The step from Eq (2) to Eq (3) is not entirely clear “K does not depend on x_i”, maybe one extra sentence to explain this step would be useful. 
-	Terms introduced such as the basis function B and the set of transformations T could be better defined.
-	For the extension to other types of transformations “While the claim focuses on global translation, it can also be extended to piecewise constant transformations.” it would be important to point out what type of natural transformations can be included in this set.

Empirical evidence:
Experiments are not fully convincing. Additional empirical evidence would be beneficial and necessary to support the claims of this:
-	“A natural criticism of these results is that they are somehow related to the image resizing and inpainting procedures that we used.” This is a very good point and the authors following arguments are not fully convincing. Results with different transformation procedures mentioned in the rest of the paragraph (and probably more) should be included to convince the reader.
-	The theoretical argument that translation invariance is not guaranteed because of the stride (subsampling) is not fully convincing and needs further explanation and experimental verification. In fact, feature maps of the CNNs that the authors consider do indeed contain many high frequencies.
-	The argument made in part 4 about the photographer’s bias seems valid for general natural transformations, but it does not apply to small transformations such as 1-pixel translations presented in the paper. Also, evidence that datasets without (or less) photographers' bias are less susceptible to natural attacks would make the argument in the paper a lot stronger. 
-	When using 6x6 avg pooling for the VGG16 architecture ”recognition performance decreases somewhat” . Results are only preliminary in the paper, but this statement needs a more thorough experimental backing. It should come with convincing quantitative evidence.
-	Please include some results or citation on other work about test time augmentation to support the statement “still only provides partial invariance”.

References and phrasing:
Generally previous work is well referenced in this paper, although there are some formulations that can be slightly modified to make a clear distinction between what is novel and what is previous work:
-	As is very well shown in the introduction, there is a lot of work on generating adversarial examples that drastically change the output of a CNN. This should be made clear in the abstract, in fact the sentence “In this paper we show that modern CNNs [...] also happens with other realistic small image transformations”  seems to indicate that this is the novel work in the paper. This is also why I believe the first sentence “Deep convolutional network architectures are often assumed to guarantee generalization for small image translations and deformations.” is somewhat contestable. 
-	“We find that modern deep CNNs are not invariant to translations, scalings and other realistic image transformations” as the paper points out earlier this is not a novel finding, so I would use a formulation that makes that clear and gives more emphasis to your own arguments as of why this happens.

Further Comments :
-	Part 5 "Implications for Practical Systems" could be moved to discussion as there is no new point and it seems more a reflection on what was already stated.
-	The final sentence of the abstract “Taken together our results suggest that the performance of CNNs in object recognition falls far short of the generalization capabilities of humans.” is not necessary, this is clearly true but it isn’t really contested in the ML community.
-	“despite the architecture being explicitly designed to provide such invariances” I agree that this has motivated the use and design of CNNs in the first place, but modern architectures are mostly designed to surpass the results on the common benchmarks rather than to provide such invariances.
-	”jaggedness is greater for the modern, deeper, networks compared to the less modern VGG16 network” might be worth interesting to consider if the residuals have anything to do with it.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>