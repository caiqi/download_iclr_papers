<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Graph U-Net | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Graph U-Net" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HJePRoAct7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Graph U-Net" />
      <meta name="og:description" content="We consider the problem of representation learning for graph data. Convolutional neural networks can naturally operate on images, but have significant challenges in dealing with graph data. Given..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HJePRoAct7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Graph U-Net</a> <a class="note_content_pdf" href="/pdf?id=HJePRoAct7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019graph,    &#10;title={Graph U-Net},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HJePRoAct7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We consider the problem of representation learning for graph data. Convolutional neural networks can naturally operate on images, but have significant challenges in dealing with graph data. Given images are special cases of graphs with nodes lie on 2D lattices, graph embedding tasks have a natural correspondence with image pixel-wise prediction tasks such as segmentation. While encoder-decoder architectures like U-Net have been successfully applied on many image pixel-wise prediction tasks, similar methods are lacking for graph data. This is due to the fact that pooling and up-sampling operations are not natural on graph data. To address these challenges, we propose novel graph pooling (gPool) and unpooling (gUnpool) operations in this work. The gPool layer adaptively selects some nodes to form a smaller graph based on their scalar projection values on a trainable projection vector. We further propose the gUnpool layer as the inverse operation of the gPool layer. The gUnpool layer restores the graph into its original structure using the position information of nodes selected in the corresponding gPool layer. Based on our proposed gPool and gUnpool layers, we develop an encoder-decoder model on graph, known as the graph U-Net. Our experimental results on node classification tasks demonstrate that our methods achieve consistently better performance than previous models.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">graph, pooling, unpooling, U-Net</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose the graph U-Net based on our novel graph pooling and unpooling layer for network embedding.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rkglzX79nX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting paper that could benefit from more empirical comparisons</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJePRoAct7&amp;noteId=rkglzX79nX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper905 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper905 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose a new technique to add “pooling” and “unpooling” layers to a graph neural network (GNN). To deal with the lack of spatial locality in graphs, the downsampling operation relies on a learned scalar projection vector (which gives the “scores” for selecting different nodes). During upsampling, the model simple relies on storing the un-sampled adjacency matrix. Thorough experimental results on Cora, Citeseer, and Pubmed highlight the utility of the approach, with ablation studies isolating the importance of the pool/unpool operations.

Overall, this is an interesting paper with the possibility of having a moderate impact within the area of GNNs/GCNs, and the method is clearly described. While there are a number of minor modifications made to the standard GCN model, which could potentially confound the results, the authors do provide a sensible ablation study to isolate the importance of their pool/unpool operations. The overall results on the three node classification datasets are also quite strong. 

The primary shortcoming of this paper is that it only evaluates the model on three citation network datasets (Cora, Citseer, and Pubmed). While these datasets are now standard in the GCN/GNN community, they are very small, have few labeled examples, and it would greatly strengthen the paper to use a different dataset or two, e.g., the Reddit or PPI datasets from Hamilton et al. 2017 or the BlogCatolog dataset used in Grover et al. 2016 could be used for node classification. Or the authors could apply the proposed technique to graph classification or link prediction. In this reviewers opinion, it is very hard to judge the general utility of a method when results are only provided on these three very-specific datasets, where the performance differences between methods are now very marginal. 

In a related point, while this work cites other approaches that apply pooling operations in graph neural networks (e.g., Ying et al. 2018, Simonovsky and Komodakis 2018), no comparisons are made against these approaches. One would suppose that these comparisons are not made because this paper only tests the graph U-net for node classification, but it would greatly strengthen this paper to add comparisons to these other pooling operations, e.g., for graph classification. Moreover, it is possible to define analogous unpooling operations for Ying et al. 2018 and Simonovsky and Komodakis 2018, similar to the unpooling operation used in this work (e.g., for Ying et al.’s DiffPool you can just “unpool” to the previous graph and assign each node a feature corresponding to the weighted sum of the features of the assigned clusters). Of course, it would require significant work (e.g., experiments on graph classification or some modifications of existing approaches) to actually test whether the pool approach proposed here is actually better than those in Ying et al. 2018 and Simonovsky and Komodakis 2018, but such comparisons are necessary to demonstrate whether the pooling operation proposed here is an improvement over existing works, or whether the primary novelty is the combined application of pooling and unpooling in a node classification setting. 

As another minor point, whereas unpooling operations can be used to define a generative model in the image setting, this is not the case here, as the unpooling operation relies on knowledge about the input graph (i.e., the model always unpools to the same connectivity structure). This is not necessarily a bad thing, but it could improve the paper to clarify this issue. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJxG0Xg9h7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting problem of pooling/upsampling graphs, experimental validation and literature review could be significantly improved</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJePRoAct7&amp;noteId=BJxG0Xg9h7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper905 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper905 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes pooling and upsampling operations for graph structured data, to be interleaved with graph convolutions, following the spirit of fully convolutional networks for image pixel-wise prediction. Experiments are performed on node classification benchmarks, showing an improvement w.r.t. architectures that do not perform any downsampling/upsampling operations.

Given that the main contribution of the paper is the introduction of a pooling operation for graph structured data, it might be a good idea to evaluate the operation in a task that does require some kind of downsampling, such as graph classification / regression. Moreover, authors should compare to other graph pooling methods.

Authors claim that one of the motivations to perform their pooling operation is to increase the receptive field. It would be worth comparing pooling/upsamping to dilated convolutions to see if they have the same effect on the performance when dealing with graphs. 

Some choices in the method seem rather arbitrary, such as the tanh non-linearity in \tilde y. Could the authors elaborate on that? How important is the gating?

It would be interesting to analyze which nodes where selected by the pooling operators. Are those nodes close together or spread out in the previous graph?

The proposed unpooling operation seems to be the same as unpooling performed to upsample images, that is using skip connections to track indices, by recovering the position where the max value comes from and setting the rest to 0. Have the authors tried other upsampling strategies analogous to the ones typically used for images (e.g. upsampling with nearest neighbors)?

When skipping information from the downsampling path to the upsampling path, is there a concatenation or a summation? How do both operations compare? (note that concatenation introduces many more parameters) How about only skipping only the indices (no summation nor concatenation)? This kind of analysis, as it has been done in the computer vision literature, would be interesting.

What is the influence of the first embedding layer to reduce the dimensionality of the features?

How do the models in Table 2 compare in terms of number of parameters?
 What's the influence of imposing larger weights on self loop in the graph?

What about experiments in inductive settings?

Please add references for the following claim "U-Net models with depth 3 or 4 are commonly used..."

Please double check your references, e.g. in the introduction, citations used for CNNs do not always correspond to CNN architectures.

The literature review could be significantly improved, missing relevant papers to discuss include:
- Gori et al. A new model for learning in graph domains, 2005.
- Scarselli et al. The graph neural network model, 2009.
- Bruna et al. Spectral networks and locally connected networks on graphs, 2014.
- Henaff et al. Deep convolutional networks on graph-structured data, 2015.
- Niepert et al. Learning convolutional neural networks for graphs, 2016.
- Atwood and Towsley. Diffusion-convolutional neural networks, 2016.
- Bronstein et al. Geometric deep learning: going beyond Euclidean data, 2016.
- Monti et al. Geometric deep learning on graphs and manifolds using mixture model cnns, 2017.
- Fey et al. SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels, 2017.
- Gama et al. Convolutional Neural Networks Architectures for Signals Supported on Graphs, 2018.
As well as other pixel-wise architecture for image-based tasks such as:
- Long et al. Fully Convolutional Networks for Semantic Segmentation, 2015.
- Jegou et al. The one hundred layers tiramisu: fully convolutional densenets for semantic segmentation, 2016.
- Isola et al. Image-to-image translation with conditional adversarial networks, 2016.
- Zhao et al. Stacked What-Where auto-encoders, 2015.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJxUcVxrhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good paper, clearly written and has some interesting ideas</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJePRoAct7&amp;noteId=HJxUcVxrhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper905 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper905 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
This paper introduces an encoder-decoder neural net architecture for arbitrary graphs. The core contribution is pooling and un-pooling operations for respectively graph down and up sampling.

Pros:
+ U-Net like architectures indeed are very successful in vision applications, and having a model that was similar properties on graphs would be very useful.
+ The paper is clearly written. 
+ I really liked the idea behind the pooling operation: it is simple, seems easy to implement efficiently, and generally makes sense (although see concerns below). 
+ The choice of the baselines is reasonable, and experimental results seem convincing. Ablation studies are also there.

Cons:
- It is not clear why the evaluation seem to only be done for the transductive learning settings. I understand that some of the previous work might have done that, but this application scenario is quite limited.
- One concern about the g-pool operation is that it is not local: unlike e.g. max pool on 2D which produces local maxima, here the selection is done globally, which could lead to situations where the entire parts of the graph are completely ignored. 
- Another concern, which has been partially addressed in section 3.4 is that the connectivity is not really taken into account when downsampling the adjacency matrix. The solution which introduces previously non-existing edges and thus kind of modifies the original graph is not very satisfying. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJxRweG0FQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>important baselines missing</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJePRoAct7&amp;noteId=HJxRweG0FQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures"><a href="/profile?id=~Michael_Bronstein1" class="profile-link">Michael Bronstein</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Sep 2018</span><span class="item">ICLR 2019 Conference Paper905 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">I believe that many important baseline algorithms for deep learning on graphs are missing, in particular CayleyNet [1] (a generalization of ChebNet using rational functions) MoNet [2] (a more general model of which GAT is a subsetting), and the recent Dual/Primal Graph CNN [3]. Please refer to a review paper [8] on geometric deep learning methods. 

1. CayleyNets: Graph convolutional neural networks with complex rational spectral filters, arXiv:1705.07664,

2. Geometric deep learning on graphs and manifolds using mixture model CNNs, CVPR 2017. 

3. Dual-Primal Graph Convolutional Networks, arXiv:1806.00770.

4. Geometric deep learning: going beyond Euclidean data, IEEE Signal Processing Magazine, 34(4):18-42, 2017
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJxCB1HUs7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Baselines</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJePRoAct7&amp;noteId=BJxCB1HUs7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper905 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Oct 2018</span><span class="item">ICLR 2019 Conference Paper905 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the interest in our work and references. We were aware of these work. But our work mainly focus on graph pooling and un-pooling operations, which are orthogonal to methods in these papers. We would like to add these references as needed in our final version.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rklHQEEpYX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>tanh gating?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJePRoAct7&amp;noteId=rklHQEEpYX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Sep 2018</span><span class="item">ICLR 2019 Conference Paper905 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Very interesting work!

I was wondering, why the hyperbolic tangent activation was used for the gating mechanism in your architecture? The choice doesn't seem to be motivated anywhere in the paper, and given that its output can be negative (and therefore inadvertently flip the activation), the logistic sigmoid should be more appropriate. 

Could you please comment on this decision?

Thanks!</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bygr-fsTYX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Why use tanh gating</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJePRoAct7&amp;noteId=Bygr-fsTYX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper905 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Sep 2018</span><span class="item">ICLR 2019 Conference Paper905 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, thank you for your appreciation and question. Actually, we have tried sigmoid, tanh and softmax. tanh performs the best. We have thought about reasons. There are some possible explanations. The values in y vector are the scalar projection values. The negative values do not mean they are negligible just because they are in the opposite direction of vector p. So if we do sigmoid, their corresponding node vectors will become trivial. And also tanh is zero centered, which facilitates the training of projection vectors. So we choose to use tanh for gate operation. Also the use of tanh can regularize node vectors such that they are in the same direction of projection vector p. We are not sure if this can help with the feature encoding. We may try to investigate this in the future. Hope these explanations can help you. Happy to have future discussion with you if any question. Thank you.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJgK6c3ptX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>possible further studies on gating</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJePRoAct7&amp;noteId=SJgK6c3ptX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Sep 2018</span><span class="item">ICLR 2019 Conference Paper905 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the prompt reply!

While I am surprised that the tanh performs the best, if this is indeed the case then of course you should use it. I would definitely recommend that you clarify (within the paper) how the tanh function was arrived at during the revision period.

Further, assuming your intuition about the benefits of tanh is correct, then the vectors substantially opposite of p could be useful too, right? This motivates another experiment, where you’d take the top-k from y^2, rather than y (to give the opposite direction equal footing). 

What do you think?

Once again, thanks for promptly responding to my query, and best of luck with the reviews.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJesYj2atX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>possible further studies on gating</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJePRoAct7&amp;noteId=rJesYj2atX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper905 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Sep 2018</span><span class="item">ICLR 2019 Conference Paper905 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Sure, I totally agree with you. We can do more experiments about this part. Very happy to have this great discussion with you.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>