<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Riemannian Adaptive Optimization Methods | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Riemannian Adaptive Optimization Methods" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=r1eiqi09K7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Riemannian Adaptive Optimization Methods" />
      <meta name="og:description" content="Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_r1eiqi09K7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Riemannian Adaptive Optimization Methods</a> <a class="note_content_pdf" href="/pdf?id=r1eiqi09K7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019riemannian,    &#10;title={Riemannian Adaptive Optimization Methods},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=r1eiqi09K7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=r1eiqi09K7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Several first order stochastic optimization methods commonly used in the Euclidean domain such as stochastic gradient descent (SGD), accelerated gradient descent or variance reduced methods have already been adapted to certain Riemannian settings. However, some of the most popular of these optimization tools - namely Adam , Adagrad and the more recent Amsgrad - remain to be generalized to Riemannian manifolds. We discuss the difficulty of generalizing such adaptive schemes to the most agnostic Riemannian setting, and then provide algorithms and convergence proofs for geodesically convex objectives in the particular case of a product of Riemannian manifolds, in which adaptivity is implemented across manifolds in the cartesian product. Our generalization is tight in the sense that choosing the Euclidean space as Riemannian manifold yields the same algorithms and regret bounds as those that were already known for the standard algorithms. Experimentally, we show faster convergence and to a lower train loss value for Riemannian adaptive methods over their corresponding baselines on the realistic task of embedding the WordNet taxonomy in the Poincare ball.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Riemannian optimization, adaptive, hyperbolic, curvature, manifold, adam, amsgrad, adagrad, rsgd, convergence</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Adapting Adam, Amsgrad, Adagrad to Riemannian manifolds. </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">12 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rkx1x9oeAm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Many thanks for reviewing our paper.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1eiqi09K7&amp;noteId=rkx1x9oeAm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper561 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper561 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank all three reviewers for their work and their interest in our work.

We have incorporated the modifications suggested by reviewers and are open to further comments.

Please find more detailed responses below each review.

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HkeQ6djp2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Riemannian Adam/Amsgrad on product manifolds with convergence guarantee but not supported well by experiments.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1eiqi09K7&amp;noteId=HkeQ6djp2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper561 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper561 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper extends Euclidean optimization methods, Adam/Amsgrad, to the Riemannian setting, and provides theoretical convergence analysis which includes the Euclidean versions as a special case. To avoid breaking the sparsity, coordinate-wise updates are performed on product manifolds. 

The empirical performance seems not very good, compared to RSGD which is easier to use.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJglV5jxCQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for reviewing our work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1eiqi09K7&amp;noteId=rJglV5jxCQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper561 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper561 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for reviewing our work. Even though RSGD is indeed slightly easier to use, we will make our code available to facilitate the use of our algorithms.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SylzCu-S3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Riemannian ADAM</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1eiqi09K7&amp;noteId=SylzCu-S3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper561 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper561 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">I have enjoyed reading this paper. The paper is accessible in most cases and provides a novel optimization technique. Having said this, I have a few concerns here,


- I am not sure why the notion of product manifolds is required in developing the technique. To me, all the arguments follow without that. Even if the authors are only interested in manifolds that can be constructed in a product manner (say R^n from R),  the development can be done without explicitly going along that path. Nevertheless I may have missed something so please elaborate why product manifolds. I have to add that in many cases, the underlying Riemannian geometry cannot be derived as a product  space. For example, the SPD manifold cannot be constructed as a product space of lower dimensional geometries. 

- I have a feeling that finding the operator \Pi in many interesting cases is not easy. Given the dependency of the developments on this operator, I am wondering if the method can be used to address problems on other manifolds such as SPD, Grassmannian or Stiefel. Please provide the form of this operator for the aforementioned manifolds and comment on how the method can be used if such an operator is not at our disposal.

- While I appreciate the experiments done in the paper,  common tests (e.g., Frechet means) are not presented in the paper (see my comment below as well).  

- last but not least, the authors missed the work of   Roy et. al., "Geometry Aware Constrained Optimization Techniques for Deep Learning", CVPR'18 where RSGC with momentum and Riemannian version of RMSProp are developed. This reference should be considered and compared.


Aside from the above, please

- define v and \hat{v} for Eq.(5) 

- provide a reference for the claim at l3-p4 (claim about the gradient and Hessian)

- maybe you want to mention that \alpha -&gt; 0 for |g_t^i| at the bottom of p4

- what does [.] mean in the last step of the algorithm presented in p7

- what is the dimensionality of the Hn in the experiments
 



</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1gnp5olCX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for reviewing our work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1eiqi09K7&amp;noteId=B1gnp5olCX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper561 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper561 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your interest and professionalism. We reply below to each of your concerns.

Product structure: 

(i) The product structure is natural for any optimization-based graph or word embedding method: if one wants to embed n nodes into a manifold M, then the parameter space is M^n. In particular, this would apply also if M is a PSD manifold.

(ii) We noticed recently that other very recent approaches propose to also embed each point into a product of spaces, arguing that it allows the embeddings to benefit from the metric properties of each space [1,2].

(iii) Our proof arguments would not hold without this product structure. This is easier to see from the convergence proof of Euclidean AMSgrad [3], appendix D, Eq.(18), where the last equality exploits the Euclidean coordinate system to expand the squared norms. This is not possible on a general Riemannian manifold. However, with a product structure, one can expand squared distances in the product manifold, as the sum of squared distances in each manifold of the product. 


Pi operator: 

-The presence of the projection operator is mostly useful for the convergence proof, to guarantee that the learning trajectory in parameter space is bounded, hence the presence of D_\infty in the bounds. 
-Note that this operator is also required to obtain theoretical bounds for Euclidean AMSgrad/Adam, even though it is often omitted in practice. 
-Note that in [4, section 3], it is assumed to be given, as a “projection oracle”. Also note that for many applications of interest, such as computing Karcher means on PSD manifolds (as done in [4, section 4]), a projection operator is not used nor needed for convergence, since the trajectory is trivially bounded (formally, this amounts to choosing a trivial projection into a ball containing the trajectory). 
-In the Poincaré ball, if X is a ball centered at the origin (as in our experiments), then the projection is naturally given by the parametrization in the Euclidean ambient space.


Fréchet means: 

-This is an interesting suggestion that we will keep in mind for future work. 


Related work: 

-Thank you for pointing us to this relevant reference. We have added it to the related work section.


Other remarks: 

In Eq.(5), \hat{v} is defined from v, which is defined as for Adam, defined just above. We have added a footnote explaining this.
Added a reference for the claim line 3, page 4, about gradient and Hessian.
At the bottom of page 4, \alpha is not required to go to 0: we consider here an (R)SGD update of fixed size \alpha. 
If by [.] you refer to gyr[. , .], this square bracket comes from the notation of the gyro-operator, for which we provide a reference. We use it in our experiments to efficiently compute parallel transport in the Poincaré ball.
The dimension we use in our experiments is 5, as suggested in [5]. Added to experiments section. 


[1] Learning mixed curvature representations in product spaces,
<a href="https://openreview.net/forum?id=HJxeWnCcF7" target="_blank" rel="nofollow">https://openreview.net/forum?id=HJxeWnCcF7</a>
[2] Poincaré Glove: hyperbolic word embeddings, 
https://openreview.net/forum?id=Ske5r3AqK7
[3] On the convergence of Adam and beyond, Reddi et al., ICLR 2018
https://openreview.net/forum?id=ryQu7f-RZ
[4] First order methods for geodesically convex optimization, Zhang &amp; Sra, JMLR 2016 
proceedings.mlr.press/v49/zhang16b.pdf
[5] Poincaré embeddings for learning hierarchical representations, Nickel &amp; Kiela, NIPS 2017
https://arxiv.org/abs/1705.08039
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HygZGC2Csm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper is well-writen except a few flaws (see below). The proposed methods are potentially important in some applications. Therefore, I suggest publish this paper after addressing the comments below.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1eiqi09K7&amp;noteId=HygZGC2Csm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper561 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper561 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents Riemannian versions of adaptive optimization methods, including ADAGRAD, ADAM, AMSGRAD and ADAMNC. There are no natural coordinates on a manifold. Therefore, the authors resort to product of manifolds and view each manifold component as a coordinate. Convergence analyses for those methods are given. The the theoretical results and their Euclidean versions coincide. An experiment of embedding a tree-like graph into a Poincare model is used to show the performance of the Riemannian versions of the four methods.

This paper is well-written except a few flaws (see below). I do not have time to read the proofs carefully. The proposed methods are potentially important in some applications. Therefore, I suggest publish this paper after addressing the comments below.

Remarks:
*) P1, line 2: it particular -&gt; in particular.
*) P3, line 9: Is R_x(v) = x + v most often chosen? A manifold is generally nonlinear. A simple addition would not give a point in the manifold.
*) P5, in Assumptions and notations paragraph: what are T and [T]? Is T the number of total iterations or the number of functions in the function family. The subscript of the function f_t seems to be an index of the functions. But its notation is also related to the number of iterations, see (8) and the algorithms in Figure 1.
*) P5, Figure 1: does a loop for the index $i$ missing?
*) Section 5: it would be clearer if the objective function is written as L:(D^n)^m \to R: \theta-&gt; , where m is the number of nodes. Otherwise, it is not obvious to see the domain. 
*) P7, last paragraph: Tables 2 and 3 -&gt; Figures 2 and 3.
*) Besides the application in the experiments, it would be nice if more applications, at least references, are added.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1x3Gsjl0m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for reviewing our work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1eiqi09K7&amp;noteId=r1x3Gsjl0m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper561 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper561 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your detailed feedback. We have updated our paper according to your suggestions.


We reply below to each of your remarks, more specifically.

*) Typo corrected.
*) Retraction: indeed, choosing the retraction R_x(v)=x+v requires having immersed the manifold into an ambient Euclidean space: note that we only say that the retraction is “most often chosen as” such, not that this choice is always a valid one. We mention it here because it is the one we used in our experiments. 
*) T is the number of iterations, [T] denotes the set of integers from 1 to T. We use same notations as in [1]: Each f_t is the objective function of the parameters to be optimized, evaluated at the batch taken at time t. For instance, when training a neural network, one could alternatively write f_t(x) = \sum_{y\in S_t} L(x,y), where x is the set of parameters of the model, L is the loss, each y is an input to the network, and S_t the (mini)batch taken at time t. 
*) yes, these are coordinate-wise operations. We did not write explicitly the loop over i to not influence the reader into implementing this algorithm with a loop over i. In most languages, such as python or C++, coordinate-wise operations such as adding vectors are highly optimized in the standard library. One could rewrite the algorithm without the “i”, with coordinate wise operations on vectors. 
*) We added a footnote clarifying the domain of the loss function in Section 5. 
*) Tables 2 &amp; 3 -&gt; figures 2 &amp; 3: Thank you, we corrected this typo. 
*) Other potential applications include any optimization-based graph or word embedding method on a manifold. Note that the product-structure assumption is natural, since if one needs to embed n nodes into a manifold, the parameter space is a product of n manifolds. 
Following your suggestions, we have added a few references [2,3,4] as suggestions for further experiments, at the beginning of the experiment section.


[1] On the convergence of Adam and beyond, Reddi et al., ICLR 2018
[2] Representation trade-offs for hyperbolic embeddings, De Sa et al., ICML 2018
[3] Hyperbolic entailment cones for learning hierarchical embeddings, Ganea et al., ICML 2018
[4] Learning continuous hierarchies in the Lorentz model of hyperbolic geometry, Nickel &amp; Kiela, ICML 2018
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1lynLJot7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Can you compute exponential map and parallel transport in general?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1eiqi09K7&amp;noteId=S1lynLJot7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 29 Sep 2018)</span><span class="item">ICLR 2019 Conference Paper561 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">A quick comment. Seems you use exponential map and parallel transport in your algorithm, and I guess it's okay to compute exponential map and parallel transport for some simple manifold. But is it usually possible for cases where people are interested? What I often see is retraction instead of exponential map, and sometimes parallel transport has no close form solution. It seems also unusual to assume cartesian product exists. 

As you said, you probably can replace exp map with retraction, so it's better to stick to it, say in algorithm figure 1 and your proof.

For example section, people have some manifolds in mind, like sphere, orthogonal group/stiefel, grassmann, etc. so maybe it helps to address one of them.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryxWLeM6F7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for you interest!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1eiqi09K7&amp;noteId=ryxWLeM6F7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Sep 2018 (modified: 30 Sep 2018)</span><span class="item">ICLR 2019 Conference Paper561 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Hi, thank you for your interest! 

1. Although the exponential map is not always known in closed-form, your remark also applies to any Riemannian optimization algorithm, even to Riemannian SGD. Note however that its formula is known for spherical and hyperbolic spaces, Stiefel and Grassmann manifolds, as well as for a variety of matrix Lie groups, which correspond nowadays to the main application cases of Riemannian optimization. In cases where it is not, a retraction mapping can be used instead, but the choice of your retraction will affect the trajectory. Although comparing the use of various retractions could constitute interesting future work, this was not our theoretical focus.

2. Could you please share with us why you think it would be better to stick with the retraction mapping in general? Besides the fact that using the exp map is more mathematically principled, note that [1] also obtained significantly better empirical results by using fully Riemannian methods in the Lorentz model of hyperbolic geometry. In practice, what is best seems to depend on the manifold, the model and the task.

3. As explained in the last paragraph of our introduction, the product structure is relevant to any method aiming to embed a set of points in a Riemannian manifold M. This concerns in particular all optimization-based graph and word embedding methods. If k is the size of the set of vertices or words to embed, then the parameter space is M^k. Moreover, note that one could also choose M itself as a product of manifolds, as was done for instance in [2,3,4].

4. Concerning the closed-form formula of parallel-transport, although it is also given in the above mentioned cases, it can be seen from our proofs that our convergence theorems still hold if it is replaced by any isometry between the corresponding tangent spaces. We will add this as an interesting remark, thank you for pointing it out! Also notice that in our experiments section, we provide the formula for parallel-transport in the Poincare ball, using gyro operations.


[1] Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry, Nickel &amp; Kiela, ICML 2018
[2] <a href="https://openreview.net/forum?id=HJxeWnCcF7" target="_blank" rel="nofollow">https://openreview.net/forum?id=HJxeWnCcF7</a>
[3] https://openreview.net/forum?id=Ske5r3AqK7
[4] https://openreview.net/forum?id=r1xRW3A9YX
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkx-VlBpFX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for your response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1eiqi09K7&amp;noteId=Bkx-VlBpFX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Sep 2018</span><span class="item">ICLR 2019 Conference Paper561 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your response. 

For 1. and 2., I just think if you claim that the convergence result with retraction is not too different, the statement is more general since exp map is also a retraction, see <a href="https://arxiv.org/abs/1802.09128." target="_blank" rel="nofollow">https://arxiv.org/abs/1802.09128.</a> I agree that for those manifolds exp map is easy to compute, retraction is an approximation of exp map and for general manifolds it is easier. If the convergence guarantee is different for retraction and exp map, it may be also worth pointing out and comparing.

For 3., the product structure is not used for sphere, etc., so it's just unusual to me. It helps reader if you stress it, and recall some basics or intuitions, such as Sec 3.2 (d(x,y))^2 = \sum d(x^i,y^i)^2. Maybe you can add a few line proof in appendix or just quote some literature.

For 4. is there a closed form solution for parallel translation for Stiefel manifold (see ex 8.1.2 Optimization Algorithms on Matrix Manifolds by Absil et al)? I'm not so sure about the isometry stuff, maybe you can explain more, and discuss how such "inexact" parallel translation interacts with geodesic convexity, Lipschitz constants and so on. But anyways, you have a concrete example where the algorithm works, so the statement are no doubt legit.

And I see your point for defining retraction in another reply.

Thanks again for your response! And my apology it's a quick comment so I did not go through every detail.

 </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HylTxaesFm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Retraction on page 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1eiqi09K7&amp;noteId=HylTxaesFm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 29 Sep 2018)</span><span class="item">ICLR 2019 Conference Paper561 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">And for retraction, you cannot do R_x(v) = x+v since it maps to manifold. Maybe a projection.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryxFFxGaYX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A matter of notations.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1eiqi09K7&amp;noteId=ryxFFxGaYX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Sep 2018</span><span class="item">ICLR 2019 Conference Paper561 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Concerning the use of a projection operation, note that we include it both in our algorithms and convergence proofs. As soon as you have an extrinsic representation of your manifold in an ambient Euclidean space (which we do in our experimental setup), the + operation is well defined, and whether you include the projection in the retraction or apply it on top is just a matter of notations. See [5] for similar definitions/notations as ours. 

[5] Poincare embeddings for learning hierarchical representations, Nickel &amp; Kiela, NIPS 2017

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>