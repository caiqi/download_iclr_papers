<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Set Transformer | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Set Transformer" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=Hkgnii09Ym" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Set Transformer" />
      <meta name="og:description" content="Many machine learning tasks such as multiple instance learning, 3D shape recognition and few-shot image classification are defined on sets of instances. Since solutions to such problems do not..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_Hkgnii09Ym" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Set Transformer</a> <a class="note_content_pdf" href="/pdf?id=Hkgnii09Ym" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019set,    &#10;title={Set Transformer},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=Hkgnii09Ym},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=Hkgnii09Ym" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Many machine learning tasks such as multiple instance learning, 3D shape recognition and few-shot image classification are defined on sets of instances. Since solutions to such problems do not depend on the permutation of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating increased performance compared to recent methods for set-structured data.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">attention, meta-learning, set-input neural networks, permutation invariant modeling</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Attention-based neural network to process set-structured data</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_ryl1nBJ0pQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Revision updated</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkgnii09Ym&amp;noteId=ryl1nBJ0pQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper656 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper656 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear reviewers, 

Thanks for your comments. According to your opinion, we added three baselines to all experiments (mean pooling based permutation equivariant deep set , max pooling based permutation equivariant deep set (Zaheer et al, 2017), dot product attention based pooling (Yang et al., 2018, Ilse et al., 2018)). We've also added some extra experiments to see the scalability of the set transformer on large scale clustering experiments. Right now we are running the point cloud experiments with 5,000 pts, and the results will be updated as soon as it is completed. 

There has been common concern about the novelty of our work. We want to emphasize again that our architecture is not a simple combination of existing works or naive adaptation of attention mechanism. Please refer to our comment to Reviewer 3 regarding the originality. Thanks.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJec0UPRhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A good paper but need some clarifications and improvements</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkgnii09Ym&amp;noteId=BJec0UPRhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper656 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper656 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presented an attention-based neural network, namely set transformer, a new neural model 
based on original transformer designed for set inputs. The basic idea is to introduce the attention
mechanism in both learning the feature embeddings of the set inputs during “encoding” and aggregating 
these embeddings during “decoding”. The paper is written clearly and well motivated. The extensive 
set of experiments were conducted to demonstrate the effectiveness of the proposed method. In general, 
I like reading this paper but there are some limitations or unclear parts I need authors to clarify
and explain. 

i) The proposed architecture is mainly adopted from the original transformer but it is highly related
to the baselines used in the experiments. For instance, it seems like that the current set 
transformer is a simple combination of Yang et al.(2018) and Mishra et al.(2018) (using Stack of
SABs) in encoder side and of Ilse et al.(2018) (using PMA and stack of SABs) in the decoder side. 
This simple combination makes the novelty of this paper unclear. I would like authors to clarify 
more on the originality w.r.t. these previous works. 

ii) Although authors proposed a variant of SABs - ISABs using landmark points to accelerate the 
computation, there are no any runtime comparisons between SABs and ISABs by fixing other components. 
It would be interesting to see that ISABs can approach the performance SABs and how it approaches it. 
For instance, shall we expect that ISABs approach the performance of SABs when increasing the number
of landmark points (inducing points)? Since in practice most of datasets are relatedly large, I think
understanding the behavior of ISABs is a more interesting problem. 

iii) After seeing the results in table 6, I have quite concerned about the practical performance of
set transformer on relatively large datasets (like 1000 points each class in the settings.) It looks
to me that not only set transformer may have computational issues to scale up, but more importantly
that when encoder learned really expressive embeddings with a relatively large number of the set 
inputs it might be little need to leverage attention in pooling anymore. I would like authors to 
conduct some other experiments on relatively large datasets to verify this hypothesis, which is 
important for the practical applications of the proposed model. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1lmlfyRa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clarification for the novelty and additional experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkgnii09Ym&amp;noteId=B1lmlfyRa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper656 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper656 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your constructive comments. 

i) Clarify originality
Our method is not a simple combination of [1,2,3]. [1,3] uses dot product attention, where the transformed features are fed into a FF layer to produce softmax weights to be used to pool the features via weighted average. Hence, these methods do not take into account pairwise/higher-order interactions between elements in sets. We added dot-product attention based pooling as another baseline for all experiments. As we reviewed in the related works section, there are works using transformer-type self attention mechanism in encoder part of the model [2,4], but none of them were presented in context of permutation invariant set-taking neural nets. We summarize the novelty of our model below.

- We adapted transformer based self-attention mechanism for *both* encoder and decoder part of permutation invariant set networks. 

- We introduce ISAB, which allows us to implement self-attention mechanism with reduced runtime complexity. This is an original contribution that was not present in previous works.

- We introduce PMA, which differs from the dot-product attention-based pooling schemes presented in previous works. Especially, having multiple seed vectors and applying self-attention among them is a novel idea that we found to be very effective, especially for clustering-like problems, where modeling of output interactions (such as explaining away) is important.

[1] Yang et al. 2018, Attentional aggregation of deep feature sets for multi-view 3d reconstruction.
[2] Mishra et al. 2018, A simple neural attentive meta-learner.
[3] Ilse et al. 2018, Attention-based deep multiple instance learning.
[4] Ma et al. 2018, Attend and interact: higher-order object interactions for video understanding.

ii) Runtime concerns; can Set Transformer scale up?
ISABs should be able to scale up since they require O(n) memory and time, where n is the number of points in a set. In fact this is precisely why we introduced ISAB. We have added additional experiments to demonstrate actual running time of ISAB and SAB, and the tradeoff between accuracy and running time with respect to the different number of inducing points: see Appendix C.1 and Figure 5 in the revised paper.

iii) Is attention useful when the set size is large and the embedding is expressive?
First of all, please note that ISAB + Pooling is also our contribution, which performed the best in Table 6. We presume that the reason why the set transformer was not as effective as ISAB + Pooling in Table 6 was due to the nature of the problem. In point-cloud classification, once we encode interactions between elements via the self-attention mechanism, decoding them into label vectors does not require complex architectures like PMA. To verify this, we conducted extra experiments on clustering, where we used up to 5,000 data points per set. See Appendix B.3.2 and Table 12. In this experiment, where the PMA plays an important role, set transformer works extremely well with as few as 32 inducing points.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_B1eH_an927" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting paper that uses attention for set inputs but needs more ablation study</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkgnii09Ym&amp;noteId=B1eH_an927"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper656 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper656 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes several variants of attention-based algorithms for set inputs. Compared with previous approach that processes each instance separately and then pooling, the proposed algorithm models the interactions among the instances within the set and performs better on tasks where such properties are important.

The experiments seem promising. The paper compares SAB and ISAB to rFF + pooling over multiple different tasks and SAB and ISAB outperform rFF + pooling in many tasks.

One drawback of the paper which limits its significance is that there are seemingly too many components and it is not clear which components are most important and which are not unnecessary. The authors can conduct some ablation study by removing some components and compare the performance to understand which parts are essential to the improvements.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkxjwGJRTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>About ablation studies</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkgnii09Ym&amp;noteId=rkxjwGJRTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper656 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper656 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your constructive comments.
In our experiments, we compare (rFF+Pooling, SAB+Pooling, ISAB+Pooling, rFF+PMA, SAB+PMA, ISAB+PMA).
Each of those variants are the Set Transformer with some (or no) components removed, so the experiments do report ablation results. We also added extra baselines (rFFp_mean + Pooling, rFFp_max + Pooling, rFF + Dotprod), and comparison to these methods supports our claim on the importance of having self-attention mechanism.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkx3ROgchX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Missing comparisons to permutation equivariant DeepSets</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkgnii09Ym&amp;noteId=rkx3ROgchX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper656 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper656 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper looks at stacking attention mechanism for learning over sets.

I think that the paper is well written overall. The architecture put forth is a fairly straightforward implementation of attention. Thus the methodological contribution is incremental. Still, it is nice to see some implementation of an attention model be considered for permutation invariant set embeddings.

However, there are some core misrepresentations and omissions that make publication difficult. The main problem is that the paper completely ignores the permutation equivariant mappings discussed in DeepSets (Zaheer 2017). See (4) and (23) of <a href="https://arxiv.org/pdf/1703.06114.pdf:" target="_blank" rel="nofollow">https://arxiv.org/pdf/1703.06114.pdf:</a> "Since composition of permutation equivariant functions is also permutation equivariant, we can build deep models by stacking layers."
In practice this is often done by mapping points x_i in a set as x_i -&gt; \phi(x_i) - max_j \phi(x_j). Stacking this layer works surprisingly well, typically better than just with a single pool. Thus, the permutation equivalent mappings of Zaheer 2017, which do have higher-order interactions and are linear in the number of points, are a glaring omission of table 1 and all of the experiments. Furthermore, the omission leads to a misrepresentation of the work.

Another unfortunate omission is previous work that considers set and distribution data through kernels and other nonparametric methods such as: 
Muandet, Krikamol, et al. "Learning from distributions via support measure machines." Advances in neural information processing systems. 2012.
Oliva, Junier, Barnabás Póczos, and Jeff Schneider. "Distribution to distribution regression." International Conference on Machine Learning. 2013.

It is also odd that the paper compared to DeepSets on modelnet with 100 and 1000 points but not with 5000 points. Will there be code available?

Without a better description of and comparison to permutation equivariant mappings I would feel hesitant to recommend publication.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SklNCMkRp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Added permutation equivariant baselines</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=Hkgnii09Ym&amp;noteId=SklNCMkRp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper656 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper656 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your constructive comments.
i) Consider permutation equivariant mappings (Zaheer et al).
Thanks for pointing this out. We added permutation equivariant architectures with both mean pooling and max pooling (rFFp-mean and rFFp-max) as baselines for all experiments, and have updated the paper. Our overall observation is that these permutation equivariant baselines do help, but the performance gain was not as significant as the gains achieved by SAB, ISAB and PMA.

ii) Cite and consider Muandet et al. and Oliva et al.
Thanks for mentioning the related works. Muandet et al. was cited and mentioned in the introduction in the submitted version of our paper. We have revised to include Oliva et al.

iii) Add modelnet w/5000; will code be available?
We had no time to conduct experiments with 5,000 pts during our first submission. 
Right now we are running experiments with 5,000 pts and they are going to be added to the appendix as soon as it is completed. The code will definitely be available open source.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>