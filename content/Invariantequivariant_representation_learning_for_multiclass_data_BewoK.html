<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Invariant-equivariant representation learning for multi-class data | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Invariant-equivariant representation learning for multi-class data" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1e4wo09K7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Invariant-equivariant representation learning for multi-class data" />
      <meta name="og:description" content="Representations learnt through deep neural networks tend to be highly informative, but opaque in terms of what information they learn to encode. We introduce an approach to probabilistic modelling..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1e4wo09K7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Invariant-equivariant representation learning for multi-class data</a> <a class="note_content_pdf" href="/pdf?id=B1e4wo09K7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019invariant-covariant,    &#10;title={Invariant-covariant representation learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=B1e4wo09K7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=B1e4wo09K7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Representations learnt through deep neural networks tend to be highly informative, but opaque in terms of what information they learn to encode. We introduce an approach to probabilistic modelling that learns to represent data with two separate deep representations: an invariant representation that encodes the information of the class from which the data belongs, and an equivariant representation that encodes the symmetry transformation defining the particular data point within the class manifold (equivariant in the sense that the representation varies naturally with symmetry transformations). This approach to representation learning is conceptually transparent, easy to implement, and in-principle generally applicable to any data comprised of discrete classes of continuous distributions (e.g. objects in images, topics in language, individuals in behavioural data). We demonstrate qualitatively compelling representation learning and competitive quantitative performance, in both supervised and semi-supervised settings, versus comparable modelling approaches in the literature with little fine tuning.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">representation learning, semantic representations, local vs global information, latent variable modelling, generative modelling, semi-supervised learning, variational autoencoders.</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">This paper presents a novel latent-variable generative modelling technique that enables the representation of global information into one latent variable and local information into another latent variable.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_B1e8bXi0h7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice, clean generative model, wonder how it would perform on a more challenging dataset.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e4wo09K7&amp;noteId=B1e8bXi0h7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper256 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper256 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a genarative model for images which explicitly separates the within class variation 
(the covariant part) from the across class variation (invariant part). Functionally, this achieves a similar 
result as various recent works on incorporating invariances in neural nets, but the fact that it is able 
to explicitly construct models for both parts of the distribution is nice. Results on MNIST are good, 
but of course this is a very simple dataset. It would be very interesting to see how the model 
performs on a more realistic problem. 

Admittedly, I am not an expert in generative models. This is a clean paper with a clear goal, it is hard 
for me to judge how original the idea is.

"Covariant" might not be the best word to use here because it has a very specific meaning in the context 
of some other neural networks related to how quantities transform according to representations of a symmetry 
group. This is a potential source of confusion. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1gyKAvgR7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Added experiments on new data set (SVHN)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e4wo09K7&amp;noteId=B1gyKAvgR7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper256 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper256 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We appreciate the reviewer's feedback, and have addressed both points made. 

Most importantly, we have included new results for all of the original experiments on the Street View House Numbers data set, which is a much more challenging data set than MNIST. These results are consistent with the MNIST results originally presented, but add robustness to the conclusions of the paper.

Secondly, the word "covariant" has been replaced throughout the paper with "equivariant" in order to avoid unnecessary confusion.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1gsi8U0hX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Technically Sound, Well Written, but The Key Idea is Not Very New</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e4wo09K7&amp;noteId=H1gsi8U0hX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper256 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper256 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper is well written, and the quality of the figures is good. In this paper, the authors propose an invariant-covariant idea, which should be dated back at least to the bilinear models. The general direction is important and should be pursued further. 

However, the literature is not well addressed. Eslami et al. 2018 have been cited, but some very important and related earlier works like: 
[1] Kulkarni et al. 2015, Deep Convolutional Inverse Graphics Network
[2] Cheung et al. 2015, Discovering Hidden Factors of Variation in Deep Networks
were not discussed at all. The authors should certainly make an effort to discuss the connections and new developments beyond these works. At the end of section 1, the authors argue that the covariant vector could be more general, but in fact, these earlier works can achieve further equivalence, which is much stronger than the proposed covariance.

There is also an effort to compare this work to Sabour et al. 2017 and the general capsule idea. I would like to point out, the capsule concept is a much more fine-grained what &amp; where separation rather than a coarse-grained class &amp; pose separation in one shot. In a hierarchical representation, what &amp; where can appear at any level as one class can consist of several parts each with a geometrical configuration space. So the comparison of this work to the generic capsule network is only superficial if the authors can not make the proposed architecture into a hierarchical separation. Besides different capsule network papers, I found another potentially useful reference on a fine-grained separation:
[3]Goroshin et al., Learning to Linearize Under Uncertainty

In the paper, it is argued several times that the latent vector r_y contains a rich set of global properties of class y, rather than just its label and the aim is that it can learn what the elements of the class manifold have in common. But this point is not supported well since we can always make a label and this latent vector r_y equivalent by a template. I think this point could be meaningful if we look at r_y's for different y, where each of the dimension may have some semantic meaning. Additional interpretation is certainly needed.

Under equation (3), "Note that v is inferred from r_y" should be "inferred from both r_y and x", which is pretty clear from the fig 5. Related to this, I could imagine some encoder can extract the 'style' directly from x, but here both r_y and x are used. I couldn't find any guarantee that v only contains the 'style' information based on the architecture with even this additional complication, could the authors comment on this?

Equation (5) is not really a marginalization and further equation (6) may not be a lower bound anymore. This is probably a relatively minor thing and a little extra care is probably enough.

The numbers in table 2 seems a little outdated.

To conclude, I like the general direction of separating the identity and configurations. The natural signals have hierarchical structures and the class manifold concept is not general enough to describe the regularities and provide a transparent representation. Rather, it's a good starting point. If the authors could carefully address the related prior works and help us understand the unique and original contributions of this work, this paper could be considered for publication.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkxqDIYe0Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Significant comparison to previous work added, along with all other points addressed (1 / 2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e4wo09K7&amp;noteId=SkxqDIYe0Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper256 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper256 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We appreciate deeply the reviewer's detailed feedback. The reviewer made a number of useful suggestions to improve this submission, of which we felt that the most prominent theme was improving our discussion and comparison to the literature. We discuss this first.

Overall, we have included 9 new references to the literature along with discussion in Section 1, which has been significantly modified. In particular, we discuss the two important references [1] and [2] provided by the reviewer. Indeed these two references are able to separate more structure into their latent variables (for data sets that contain structured labels for different variations of the data) using a 'clamping' technique during training to force particular latent components to be disentangled (as in [1]), or by predicting the labels and using a cross-covariance term in the objective function of their deterministic autoencoder (as in [2]). Similarly, we discuss InfoGAN (Chen et al, 2016), which achieves disentanglement by adding a mutual-information maximisation term between the meaningful latent and the model generations. We consider the simplicity of our probabilistic model, trained with log-likelihood maximisation (no additional objective function hyperparameters), augmented only with a complementary batch of same-class data for each training data point, to be attractive in comparison to these other approaches. 

However, there are other significant differences. One major difference is that our invariant representation takes as input multiple data points that come from the same class, but are different from the reconstructed data point. Thus, our invariant representation directly learns to encode the information common to the overall class, but not the individual data point. This is why there is no need for clamping or cross-covariance / mutual information penalty terms in our objective function: there is no way for the invariant representation to store data-point specific information due to the information flow through it. In further contrast to some of the other approaches in the literature, we deliberately use a deterministic latent for the class-invariant information, and a stochastic latent for the smooth 'style' information (an idea employed by Zhu et al. (2014), for their binary neurons). This modelling choice is why we do not need to force the equivariant latent to not contain any class-level information (which is clear from our results) because it is available and easier to access from the deterministic latent. This last point was a question raised by the reviewer which we have addressed with an added discussion after Equation 3 in Section 2.

...</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SkxPq8YlCX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Significant comparison to previous work added, along with all other points addressed (2 / 2)</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e4wo09K7&amp;noteId=SkxPq8YlCX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper256 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper256 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">...

We consider the usage of multiple data points (from the same class) in order to reconstruct a single data point to be novel as compared to previous work on the problem of learning disentangled or structured latents. To that end, we also included references to the recent Neural Processes work (Garnelo et al., 2018a;b) which also models a data set using multiple data points at once, though their goal of better modelling uncertainty in neural networks is very different from ours.

The reviewer also pointed out that the hierarchical nature of the capsule networks approach makes our comparison to Sabour et al. (2017) superficial. We concede this point, and have removed the direct comparisons to capsule networks from the paper in order to avoid misleading the reader. This is considered justified as, though this work is in the same vein at an abstract level, the capsule networks work is sufficiently unrelated to our work.

In order to improve the support of our claim that the invariant latent r_y contains more information than just the label y, we have updated the presentation of our results. Figure 3 now shows on the right the effect of varying r_y over each y for fixed equivariant latent v. Most importantly, Figure 4 (middle) shows the large-scale variations of r_y over the entire 2D space (recall these are results for 2D r_y). From this figure it can be seen that there is little intra-digit variation, but r_y is storing the full global relationships between the 10 MNIST digits, with 8 in the middle and the most similar digits to 8 nearby (e.g. 4 occupies a small region of r_y space immediately adjacent to 9 and 5 on either sides). We have updated the discussion around this figure to improve the justification for the claim that r_y stores more information than just its label value y.

On the final 3 specific points made by the reviewer, we have: 1) updated the text under equation (3) to address the point that v is inferred from both r_y and x, 2) we have included an additional appendix to derive the lower bound associated with equation (6), and 3) we have added a short discussion of why the numbers in table 2 were used.

In conclusion, we greatly appreciate the feedback, and have attempted to carefully address all the points provided. We have significantly elaborated the discussion of the literature, clarified the aspects of our approach that are novel as well as  those that are not, corrected / clarified each small suggestion made by the reviewer, and we have included results on a new, more challenging data set (Street View House Numbers). We hope these changes are sufficient to justify publication of this work in ICLR.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Hkesiu8R2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Equivariance</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e4wo09K7&amp;noteId=Hkesiu8R2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper256 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper256 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">"can achieve further equivalence" -&gt; "can achieve further equivariance"
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ByxGiJd5sX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>CoVAE</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e4wo09K7&amp;noteId=ByxGiJd5sX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper256 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">22 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper256 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper presents a VAE that uses labels to separate the learned representation into an invariant and a covariant part. The method is validated using experiments on the MNIST dataset.

The writing in this paper is somewhat problematic. Although it is hard to put the finger on a particularly severe instance, the paper is filled with vague and hyperbolic statements. Words like "efficiently", "meaningful", "natural", etc. are sprinkled throughout to confer a positive connotation, often without having a specific meaning in their context or adding any information. Where the meaning is somewhat clear, the claims are often not supported by evidence. Sometimes the claims are so broad that it is not clear what kind of evidence could support such a claim.

A relatively large amount of space is used to explain the general concept of invariant/covariant learning, which, as a general concept, is widely understood and not novel. There are other instances of overclaiming, such as "The goal of CoVAE is to provide an approach to probabilistic modelling that enables meaningful representations [...]". In fact, CoVAE is a rather specific model(class), rather than an approach to probabilistic modelling.

The paper is at times meandering. For instance, the benefits of and motivation for the proposed approach are not simply stated in the introduction and then demonstrated in the rest of the paper, but instead the paper states some benefits and motivations, explains some technical content, mentions some more benefits, repeats some motivations stated before, etc.

Many researchers working on representation learning hope to discover the underlying learning principles that lead to representations that seem natural to a human being. In this paper, labels are used to guide the representation into the "right" representation. It is in my opinion not very surprising that one can use labels to induce certain qualities deemed desirable in the representation.

To conclude, because of the writing, limited novelty, and limited experiments, I think this paper currently does not pass the bar for ICLR.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkxY3PFxRQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Rewrote paper to improve writing, addressed other points as well</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e4wo09K7&amp;noteId=rkxY3PFxRQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper256 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper256 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for their helpful feedback. The reviewer's main criticism was the writing: both the usage of vague, hyperbolic statements, as well as unnecessary discussion and meandering. 

On the vague, hyperbolic statements, the authors' intention was to provide clarity-by-repetition with these statements, but concede with regret in retrospect that they came across unhelpfully vague or unjustified. We have purged this language throughout the paper. There is now not a single usage of "efficient", "meaningful", or "natural" in describing our own work (we do still use such words in the Introduction when discussing the literature and representation learning in general). We have also tried to eradicate other examples of similar language (e.g. "general" in the original conclusions, EquiVAE is no longer described as a "framework"). We feel that the writing is now clearer and more objective.

To improve the conciseness and linearity of the paper, we have completely removed Section 2.1, which perhaps over repeated obvious attributes of our modelling choices. Section 2.1 also contained some of the problematic language referred to above. Similarly, the semi-supervised objective function and surrounding discussion was brought from Section 3.2 into Section 2 for linearity.

The reviewer also briefly mentioned limited novelty and limited experiments as weaknesses of the paper. We have addressed both of these points. Firstly, we have added 9 new references to relevant work in the literature along with discussions of how our approach is related, and indeed novel (e.g. our usage multiple same-class complementary data points for the invariant latent). Secondly, we have run all of our experiments on the Street View House Numbers (SVHN) data set in order to add robustness to our experimental results. Indeed the results on SVHN are consistent with our original results on MNIST (e.g. invariant representations are well separated, semi-supervised accuracies are competitive with Siddharth et al. (2017)) with minimal changes to our modelling setup (i.e. to accommodate different image sizes).

We regret the way the paper came out originally in its writing, but with the reviewers poignant criticism on this front, we feel that the updated version of our paper is much more appropriate and clear. We hope this, as well as the additional experimental results, now justify its publication.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>