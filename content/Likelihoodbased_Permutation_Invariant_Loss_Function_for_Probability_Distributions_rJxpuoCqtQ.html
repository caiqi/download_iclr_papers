<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Likelihood-based Permutation Invariant Loss Function for Probability Distributions | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Likelihood-based Permutation Invariant Loss Function for Probability Distributions" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rJxpuoCqtQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Likelihood-based Permutation Invariant Loss Function for..." />
      <meta name="og:description" content="We propose a permutation-invariant loss function designed for the neural networks reconstructing a set of elements without considering the order within its vector representation. Unlike popular..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rJxpuoCqtQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Likelihood-based Permutation Invariant Loss Function for Probability Distributions</a> <a class="note_content_pdf" href="/pdf?id=rJxpuoCqtQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 12 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019likelihood-based,    &#10;title={Likelihood-based Permutation Invariant Loss Function for Probability Distributions},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rJxpuoCqtQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=rJxpuoCqtQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We propose a permutation-invariant loss function designed for the neural networks reconstructing a set of elements without considering the order within its vector representation. Unlike popular approaches for encoding and decoding a set, our work does not rely on a carefully engineered network topology nor by any additional sequential algorithm. The proposed method, Set Cross Entropy, has a natural information-theoretic interpretation and is related to the metrics defined for sets. We evaluate the proposed approach in two object reconstruction tasks and a rule learning task.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Set reconstruction, maximum likelihood, permutation invariance</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">The proposed method, Set Cross Entropy, measures the information-theoretic similarity of sets in a permutation-invariant manner.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rkxJ7-a8a7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Overall reply to all reviewers</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxpuoCqtQ&amp;noteId=rkxJ7-a8a7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper396 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper396 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks very much for the thoughtful comments.  We first discuss the
general response to the reviewers, then in the threads we reply to the
specific concerns raised by each reviewer.

While ICLR may not mandate it, we would appreciate it if you confirm in
the reply that you read all of the rebuttals, including the replies to
the other reviewers.

To Reviewer #1 and #3:

The input is *not* assumed to be discrete.  [0,1] is meant to be a
closed set of reals between 0 and 1 (while an open set is denoted as
(0,1)).  We clarified this in the revision.  By "binary representation"
in page 3, we meant a binomial probability distribution (thus can be a
continuous value) in contrast to the multinomial
probability distribution.

In fact, each input feature vector in the Blocksworld experiment are
generated from the individual image patches by an additional autoencoder
with a sigmoid latent activation (Appendix 6.3, Figure 4), thus they are
in fact the continuous vectors.

To Reviewer #2 and #3:

&gt; #2 The question of comparing sets of different sample sizes would be a
&gt; valuable extension to the work.

&gt; #3 only sets of equal size are under consideration ...

(Sec 3, paragraph 3) When the size of the set varies, we can add an
arbitrary number of artificially generated distinct dummy elements to
the set in order to keep the size of the set equal throughout
the dataset.

For example, when there is a set of N objects of F features and we want
to normalize the size of the set to N' (&gt;N), one way is to add an
additional axis to the feature vector (F+1 features) where the
additional F+1-th feature is 0 for the real data and 1 for the dummy
data, and the distinct N'-N objects are generated in an arbitrary way
(e.g. as a binary sequence 100000, 100001, 100010, 100011, ... for F=5).
During the inference, the dummy vectors in the output can be removed.

We added this explanation in the revision.

The ability to learn from such an augmented dataset is only a matter of
the neural network topology and the hyperparameter tuning, which is not
the main topic of this paper.  Whether using an LSTM, a CNN or a fully
connected network does not affect the loss formulation presented in this
work.  Given enough capability for tuning and network engineering (which
includes the choise of NN), they should also be able to learn from such
an augmented dataset, as the augmented dataset has the
same characteristics.

However, to address this concern, we ran an additional experiment where
some of the tiles are randomly missing in 8 puzzles.  The feature
vectors are extended from 15 to 16 dimensions and the dummy elements are
inserted as explained above.  The system reconstructs the elements
including the dummy ones, which means it can identify how many and which
elements are missing for the given augmented output. Moreover, the
proposed Set Cross Entropy outperformed Set Average and other loss
functions.  See Section 4.1 for the additional details and the results.

Finally, we noticed that the evaluation metric for the
ILP task was not explained properly; Originally, we wrote:

&gt; We counted the ratio of the clauses across the test set where every
  output term matches against one of the body terms.

This is wrong: It is an improper way to measure the correct matching, as
there could be duplicates.  Our implementation did not measured the
success rate in this way.  The correct description that reflects our
implementation is:

&gt; We counted the ratio of the clauses across the test set where every
  *body* term matches against one of the *output* terms.

</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HJglfr-CiQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>new loss function for set autoencoders; experiments are not sufficient</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxpuoCqtQ&amp;noteId=HJglfr-CiQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper396 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">24 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper396 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes an objective function for sets autoencoders such that the loss is permutation invariant with respect to the order of reconstructed inputs. I think that the problem of autoencoding sets is important and designing custom loss functions is a good way to approach it. Thus, I quite like the idea of SCE  from that point of view. However, I find the experiments not convincing for me to accept the paper. 

While reading Section 3, I found it hard to keep in mind that x and y are discrete probability distributions and the notation like P(x=y) is not making things easier. Actually, I’ve never seen cross entropy written with P(x=y). Though is my personal opinion and I don’t have a suggestion on how to improve the explanations in Eq. 1-8. However, I’m glad there is an example at the end of Section 3.

I have some comments on the Experiments section. 

* Puzzles:
(1) Figure 1 could have been prettier. 
(2) The phrase “The purpose of this experiment is to reproduce the results from (Zaheer et al., 2017)” makes little sense to me.  In Deep Sets, there are many experiments and it’s not clear which experiment is meant here.
(3) Table 1 gives test error statistics for 10 runs. What is changed in every run? Does the test set stay the same in every run or is a kind of a cross-validation? Or is it just a different random seed for the initial weights? I could not find an explanation in the text, so there is no way I can interpret the results.

* Blocksworld: the reconstructions are nice, but the numbers in Table 2 are difficult to interpret. 
For example, I cannot estimate how important the difference of 10 points in SH scores is.

* Rule learning ILP tasks: I don’t know enough about learning logic rules tasks to comment on those experiments, but Table 3 seems overwhelming and the concept of 10 runs is still unclear.

--- General comment on the experiments ---

I think an important goal of any autoencoder is to learn a representation that can be useful in other tasks. There is even an example in the paper: “set representation of the environment is crucial in the robotic systems”. Thus, the experiments I would like to see are about evaluating the quality of a representation from an SCE-trained autoencoder compared to other training methods.  Without those experiments, I cannot estimate how valuable the SCE loss function is.  </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BygnoGpIpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Overall reply</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxpuoCqtQ&amp;noteId=BygnoGpIpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper396 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper396 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">  &gt; Actually, I’ve never seen cross entropy written with P(x=y).

  The usual notation of the probability distribution, such as P(x), is
  an abbreviation of a random variable X taking a particular value x,
  i.e. P(X=x), following the notation in Ian Goodfellow's Deep Learning
  textbook, chap.3
  ([<a href="http://www.deeplearningbook.org/contents/prob.html])." target="_blank" rel="nofollow">http://www.deeplearningbook.org/contents/prob.html]).</a> We just kept
  it expanded for denoting the cross entropy.

  &gt; The phrase “The purpose of this experiment is to reproduce the
  &gt; results from (Zaheer et al., 2017)” makes little sense to me.
  &gt; In Deep Sets, there are many experiments and it’s not clear which
  &gt; experiment is meant here.

  By "reproduce" we did not mean we run the same experiment in the prior
  work; What we did is that we confirmed their general claim (their
  particular network structure is able to encode an input in a
  permutation invariant manner) in *our* experimental setting.

  We rephrased it in the revision in order to avoid the confusion.

  &gt; Table 1 gives test error statistics for 10 runs. What is changed in
    every run?

  The purpose of running the experiments 10 times is to address the
  potential concern about the stability of the training.

  We kept the same set of training/testing data, the only difference is
  the random seed.  In one of the 10 runs, A1H (set average pseudo
  metric) did not converge, showing that the A1H (baseline) could be
  unstable, possibly due to the issue explained in the example at the
  end of section 3. # , though this is a speculation from the empirical
  result.  This shows another empirical evidence that our proposed
  method is superior.  We clarified these points in the revision.

  &gt; Blocksworld: the reconstructions are nice, but the numbers in Table
    2 are difficult to interpret.

  We agree with this point. To address it, we added the RMSE between the
  visualized pictures to give more insights.  Note that these pictures
  are not the direct output of the neural network; However, comparing
  the reconstructed pictures by RMSE should give some intuitive sense
  since the error directly translates to the pixel value.  A new table
  is added in the revision.

  Furthermore, we compared the visualized results between the networks
  trained with a different loss formulation.

  Since we believe the same issue applies to 8-puzzles, we also added a
  new evaluation metric for 8-puzzles: Since we know 8 puzzle feature
  vectors are discrete (a domain knowledge, not the assumption in our
  proposed method), we can directly compare the output reconstruction
  with the input by rounding the continuous output to 0/1 and comparing
  whether all elements are correctly reconstructed, and count the rate
  of the successful reconstructions across the dataset.  Another new
  table is added in the revision.

  &gt; Rule learning ILP tasks: the concept of 10 runs is still unclear.

  This is same as the previous experiments; The only difference is the
  random seed.

  &gt; I think an important goal of any autoencoder is to learn a
  &gt; representation that can be useful in other tasks. There is even an
  &gt; example in the paper: “set representation of the environment is
  &gt; crucial in the robotic systems”. Thus, the experiments I would like
  &gt; to see are about evaluating the quality of a representation from an
  &gt; SCE-trained autoencoder compared to other training methods.
  &gt; Without those experiments, I cannot estimate how valuable the SCE
  &gt; loss function is.

  We were surprised by this question.

  First, we do not focus only on the autoencoding task. In the ILP task,
  the neural network learns to predict a set from the single
  element. For example, in the last `neighbor5` experiment in Table 6,
  the task is to predict 5 elements from 1 element.  Our contribution is
  the method for training a NN to output a set, not limited
  to autoencoding.

  Regarding the value of reconstructing a set, existing work (Vinyal
  NIPS 2015, ICLR 2016) already showed that once we can train a NN to
  output a set (by whatever means), it allows a variety of tasks to be
  solved.  The problem is that existing methods relied on an ad-hoc
  preprocessing and/or a careful tuning that depends on the domain
  knowledge, or a sequential process such as Gale-Shapley.
  Our contribution is to completely remove these assumptions motivated
  by the theoretical formulation, NOT by an empirical success
  that may occur by chance in a particualr problem setting.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SklYVU2hs7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Extension to case of sets with different sample sizes?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxpuoCqtQ&amp;noteId=SklYVU2hs7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper396 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">23 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper396 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper is understandable and the question addressed is interesting. The use of log likelihoods to metrize distances between sets, although not new, is used quite effectively to address the issue of label switching in sets. Although the run time is O(N^2), the metric can be computed in a parallelized manner. The question of comparing sets of different sample sizes would be a valuable extension to the work. Although I think the proposed loss function addresses some important issues, would like to defer the question of acceptance/rejection to other reviewers due to lack of expertise in related areas.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1g0z4pU6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Overall comment</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxpuoCqtQ&amp;noteId=r1g0z4pU6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper396 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper396 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">  &gt; The use of log likelihoods to metrize distances between sets,
    although not new, ...

  Log likelihood itself is a general notion and is not new.  However,
  our contribution is formulating the log likelihood for sets, with a
  proof that it converges to the correct answer at the global minima.

  &gt; The question of comparing sets of different sample sizes would be a
  &gt; valuable extension to the work.

  See the overall reply. We added some explanations and also a
  new experiment that shows that it can model the sets with the different number of elements.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1lS16Ohsm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Lacks clarity</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxpuoCqtQ&amp;noteId=S1lS16Ohsm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper396 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">23 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper396 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In the manuscript entitled "Likelihood-based Permutation Invariant Loss Function for Probability Distributions" the authors propose a loss function for training against instances in which ordering within the data vector is unimportant.  I do not find the proposed loss function to be well motivated, find a number of confusing points (errors?) in the manuscript, and do not easily follow what was done in the examples.

First, it should be noted that this is a very restricted consideration of what it means to compare two sets since only sets of equal size are under consideration; this is fundamentally different to the ambitions of e.g. the Hausdorff measure as used in analysis.  The logsumexp formulation of the proposed measure is unsatisfactory to me as it directly averages over each of the independent probabilities that a given element is a member of the target set, rather than integrating over the combinatorial set of probabilities for each set of complete possible matches.  Moreover, the loss function H() is not necessarily representative of a generative distribution.

The definition of the Hausdorff distance given is directional and is therefore not a metric, contrary to what is stated on page 2.

I find the description of the problem domain confusing on page 3: the space [0,1]^NxF is described as binary, but then values of log y_i and log (1-y_i) are computed with y in [0,1] so we must imagine these are in fact elements in the open set of reals: (0,1).

Clarity of the examples could be greatly improved, in particular by explaining precisely what is the objective of each task and what are the 'ingredients' we begin with.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJlQWHaL67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>reply</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxpuoCqtQ&amp;noteId=BJlQWHaL67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper396 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper396 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
  &gt; only sets of equal size are under consideration ...

  See the overall reply. It is not only for sets of equal size.


  &gt; The logsumexp formulation of the proposed measure is unsatisfactory
  &gt; to me as it directly averages over each of the independent
  &gt; probabilities that a given element is a member of the target set,
  &gt; rather than integrating over the combinatorial set of probabilities
  &gt; for each set of complete possible matches.

  We have shown (the proof in page.3, sec.3, par.7) that our method
  guarantees that, at the global minima, every element x of the dataset
  X is matched by some element y of the output Y exactly once, just as
  in Hausdorff measure.

  We would like to hear more details about why it is unsatisfactory despite the guarantee.

  &gt; The definition of the Hausdorff distance given is directional and is
  &gt; therefore not a metric, contrary to what is stated on page 2.

  We already clarified this in page 2: "Note that, in this work, we use
  the informal usage of the terms “distance” or “metric” ..."  We
  also only stated that Hausdorff distance is a metric, and not that the
  directed Hausdorff distance is a metric.  We moved the clarification
  to the beginning of the section to avoid confusion.

  &gt; the space [0,1]^NxF is described as binary...

  See the overall reply. [0,1] is a closed set of reals, not discrete
  values.  The input is not assumed to be discrete. "binomial
  distribution" might be the more appropriate term. We rephrased them in the revision.

  &gt; what is the objective of each task and what are the 'ingredients' we
    begin with.

  The detailed descriptions are in the appendix.  The first two tasks (8
  puzzle, Blocksworld) are the autoencoding task, but without
  considering the ordering in the first axis of the data point (i.e. the order of the elements).
  In 8-puzzle, the object representation (an element of the set) is
  hand-crafted as in Fig.1.  In Blocksworld, a set of objects are
  extracted from the image, and the image patch and the bounding box
  information are compressed into 1224-D vector by a feature engineering
  using Conv-AE (Appendix, 6.3).

  The last task (rule learning task) is to predict a set of terms from a
  single term, where each term is a n-hot vector representing a
  first-order logic term (Section 4.2, Appendix 6.4).  We moved the
  description in the appendix to the main text.
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkgusNjaam" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>some improvements to clarity</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxpuoCqtQ&amp;noteId=BkgusNjaam"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper396 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper396 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The authors have clarified a number of the confusing points; however, I'm still not satisfied with the objective: the representation of the logsumexp that comes after the proof on page 3, namely the manipulations done in equation (4).</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1ezDoAkCQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>reply</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rJxpuoCqtQ&amp;noteId=H1ezDoAkCQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper396 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper396 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">  Thank you for the further comments.

  Getting back to the first comment,

  &gt; The logsumexp formulation of the proposed measure is unsatisfactory
  &gt; to me as it directly averages over each of the independent
  &gt; probabilities that a given element is a member of the target set,
  &gt; rather than integrating over the combinatorial set of probabilities
  &gt; for each set of complete possible matches.

  Since every sets have the equal number of permutations (assuming they
  are of the same size and every elements are distinct), the individual
  probability and such an integrated sum over every permutations of the
  probabilities will only differ by a constant factor scaling.

  For example, P([x,y,z]=[1,2,3]) vs sum of all permutations,
  P([x,y,z]=[1,2,3])+…+P([x,y,z]=[3,2,1]) differs by 6, the number of
  permutations for 3 elements.  Since other patterns, such as
  P([x,y,z]=[4,5,6]), also have 6 permutations, considering just one
  ordering and considering all ordering makes essentially no difference.

  Informally speaking, the key to understand our method is to notice
  that considering the exponential/combinatorial number of possible
  permutations (matches) is equivalent to treating every permutation as
  a distinct event, which is not necessary when the output order is
  ignored.  It is not something that is asked for in this
  problem setting.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>