<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Pix2Scene: Learning Implicit 3D Representations from Images | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Pix2Scene: Learning Implicit 3D Representations from Images" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJeem3C9F7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Pix2Scene: Learning Implicit 3D Representations from Images" />
      <meta name="og:description" content="Modelling 3D scenes from 2D images is a long-standing problem in computer vision with implications in, e.g., simulation and robotics. We propose pix2scene, a deep generative-based approach that..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJeem3C9F7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Pix2Scene: Learning Implicit 3D Representations from Images</a> <a class="note_content_pdf" href="/pdf?id=BJeem3C9F7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019pix2scene:,    &#10;title={Pix2Scene: Learning Implicit 3D Representations from Images},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=BJeem3C9F7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Modelling 3D scenes from 2D images is a long-standing problem in computer vision with implications in, e.g., simulation and robotics. We propose pix2scene, a deep generative-based approach that implicitly models the geometric properties of a scene from images. Our method learns the depth and orientation of scene points visible in images. Our model can then predict the structure of a scene from various, previously unseen view points. It relies on a bi-directional adversarial learning mechanism to generate scene representations from a latent code, inferring the 3D representation of the underlying scene geometry. We showcase a novel differentiable renderer to train the 3D model in an end-to-end fashion, using only images. We demonstrate the generative ability of our model qualitatively on both a custom dataset and on ShapeNet. Finally, we evaluate the effectiveness of the learned 3D scene representation in supporting a 3D spatial reasoning.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Representation learning, generative model, adversarial learning, implicit 3D generation, scene generation</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">pix2scene: a deep generative based approach for implicitly modelling the geometrical properties of a 3D scene from images</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">8 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_ByxFX5Yi3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting inverse graphics model. Motivation and experiments are lacking.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJeem3C9F7&amp;noteId=ByxFX5Yi3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1327 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1327 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper explored explaining scenes with surfels in a neural recognition model. The authors demonstrated results on image reconstruction, synthesis, and mental shape rotation. 

The paper has many strengths. The model is clearly presented, the implementation is neat, the results on synthetic images are good. In particular, the results on the mental rotation task are interesting and new; I feel we should include more studies like these for scene and object representation learning. 

A few concerns remain. First, the motivation of the paper is unclear. The main advantage of the proposed representation, according to the intro, is its `implicitness’, which enables viewpoint extrapolation. I’d like to see more explanation on why ‘explicit’ representations don’t support that. A lot of the intro is currently talking about related work, which can be moved to later sections or to the supp material.

The paper then moves on to discuss surfels. While it’s new combine surfels with deep nets, I’m not sure how much benefits it brings over voxels, point clouds, or primitives. It’d be good to compare with these scene representations. 

My second concern is the results are all on synthetic data, and most shapes are very simple. While the paper is called ‘pix2scene’, it’s really about ‘pix2object’ or ‘pix2shape’. I’d like to see results on more realistic scenes, where the number of objects as well as their shape and material varies.

For the mental rotation task, the authors should cite and discuss the classic work from Shepard and Metzler and include human performance for calibration.

I’m on the border for this paper. Happy to adjust my rating based on the discussion and revision.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkl_mZ_iTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Quick question regarding comparison to voxel/point clouds/primitives</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJeem3C9F7&amp;noteId=Bkl_mZ_iTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1327 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1327 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear AnonReviewer3,

Thank you so much for your review. We appreciate the time you put into this and your feedback.

Before we respond in full to all of your points, we have a quick question:
You mentioned, "It’d be good to compare with these [voxel/point clouds/primitives] scene representations". We'd love to implement this but we're having some issues finding suitable code.  Do you know of any code for methods that implicitly (i.e. without supervised training) learns object reconstruction using meshes/voxels/point clouds?

Thanks,
the Pix2Scene team
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1gAUXuopQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJeem3C9F7&amp;noteId=S1gAUXuopQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1327 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1327 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for asking. I realize 'implicit' means without supervision.  In this case, how would the system compare with Rezende et al. [2016]?  Their code might not be available. An alternative is perspective transformer net [Yan et al, 2016] and its follow-ups.

A common problem with these unsupervised/self-supervised (or 'implicit') approach is that the learned scene representation is often incomplete and looks bad from a different view. How would the reconstructed scenes (objects) in Fig 4 look like from a different view? These results are important for a '3D' representation.

Rezende et al. Unsupervised Learning of 3D Structure from Images. NIPS'16.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_HJejoMW9hX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice model but some details missing</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJeem3C9F7&amp;noteId=HJejoMW9hX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1327 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1327 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper introduces a method to create a 3D scene model given a 2D image and a camera pose. The method is: (1) an "encoder" network maps the image to some latent code vector, (2) a "decoder" network uses the code and the camera pose to create a depthmap, (3) surface normals are computed from the depthmap, and (4) these outputs are fed to a differentiable renderer which reconstructs the input image. At training time, a discriminator provides feedback to (and simultaneously trains on) the latent code and the reconstructions. The model is self-supervised by the reconstruction error and the GAN setup. Experiments show compelling results in 3D scene generation for simple monochromatic synthetic scenes composed of an empty room corner and floating ShapeNet shapes. 

This is a nice problem, and if the approach ever works in the real world, it will be useful. On synthetic environments, the results are impressive.

The paper seems to claim more ground than it actually covers. The abstract says "Our method learns the depth and orientation of scene points visible in images", but really only the depth is learned, and the "orientation" is an automatically-computed surface normal, which is a free byproduct of any depth estimate. The "surfel" description includes a reflectance vector, but this is never estimated or further described in the paper, so my guess is that it is simply treated as a scalar (which equals 1). Taking this reflectance issue together with the orientation issue, the model is not really estimating surfels at all, but rather just a depthmap, which makes the method seem considerably less novel. Furthermore, the differentiable rendering (eq. 1) appears to assume that all light sources are known exactly -- this is not a trivial assumption, and yet it is never mentioned in the paper. The text suggests that only an image is required to run the model, but Figure 3 shows that the networks are conditioned on the camera pose -- exact knowledge of the camera pose is difficult to obtain precisely in real settings, so this again is not an assumption to ignore. 

To rewrite the paper more plainly, one might say that it receives a monochrome image as input, estimates a depthmap, and then shades this depthmap using perfect knowledge of lighting and camera pose, which reconstructs the input. This may sound less appealing, but it also seems more accurate.

The paper is also missing some details of the method and evaluation, which I hope can be cleared up easily.
- What is happening with the light source? This is critical in the shading equation (eq. 1), and yet no information is given on it -- we need the color and the position of every light in the scene. 
- How is the camera pose represented? Section 3.3.3 says conditional normalization is used, but what exactly is fed to the network that estimates these conditional normalization parameters? 
- What is the exact form of the reconstruction error? An equation would be great.
- How is the class-conditioning done in 4.2?
- In Eq. 4, the first usage of D_\theta should use only the object part of the vectors, and the second usage should use only the geometric part, right? Maybe this can be cleared up with a second D_subscript.
- I do not understand the "interleaved" training setup in 4.4.1. Please explain that more. 
- It is not clear to me why the task in 4.4.2 needs any supervised training at all, if the classification is just done by computing L2 distances in the latent space. What happens with "0 sampled labels"?

Overall, I like the paper, and I can imagine others in my group liking it. I hope it gets in, assuming the technical details get cleaned up and the language gets softer.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJgja6-aTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Reviewer 1's "Nice model but some details missing" (part 2 of 2).</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJeem3C9F7&amp;noteId=SJgja6-aTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1327 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018 (modified: 18 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1327 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
== RE: How camera and class conditioning is done.

For the view conditioning, we used conditional batch-normalization which transforms a 3-dimensional vector (representing the camera coordinates) into affine batch-normalization parameters. For the class conditioning, we used the standard conditional GAN technique: We encoded the class labels as one-hot vectors and concatenated this vector to the inputs of the decoder, encoder, and discriminator. We added all of this to the paper.


== RE: Reconstruction error.

We added the exact formula for the reconstruction loss to the paper. 
It is a weighted sum of two terms: (a) the L2 loss of the input image given to the encoder and the rendered output on the decoder, and (b) the L2 loss of the noise given to the decoder and the inferred latent code of the rendered decoder output. This loss is similar to the bi-directional L2-loss used in [Li et al. 2017] and [Huang et al. 2018].

[Huang et al. 2018] “Multimodal Unsupervised Image-to-Image Translation”, 2018
[Li et al. 2017] “ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching”, 2017


== RE: 3D-IQTT loss function and training algorithm.

In these experiments, we removed the assumption of knowing the camera position. Our model had to learn to represent the scene geometry in a part of the latent vector (z_scene) and the camera position in another part (z_view).

The loss term for the supervised part of the training enforced this: it rewards the z_scene of the correct answer to be close to the z_scene of the reference and it pushes z_scene of the reference and z_scene of wrong answers apart. We also minimized mutual information between z_scene and z_view in order to enforce distinct source of information captured by the latent dimensions. We also improved the explanation of this method in the paper.

We also added experiments where we did not add any supervised samples. In this case, z_scene and z_view get entangled and the task becomes significantly harder. Both CNN baselines were trained on the supervised data; therefore when comparing them in the unsupervised condition they perform according to the random initialization while our model was able to at least leverage the unsupervised data. Please see our updated Table-1 (<a href="https://ibb.co/nhHUS0" target="_blank" rel="nofollow">https://ibb.co/nhHUS0</a> )

We also added the interleaved training algorithm for the semi-supervised task to the paper. It's similar to algorithm 2 from [Kingma et al. 2014], except instead of a randomized minibatch, we train a few iterations of unsupervised data followed by a few iterations of supervised data.

[Kingma et al. 2014] "Semi-supervised Learning with Deep Generative Models", 2014.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1epOpZ6TQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Reviewer 1's "Nice model but some details missing" (part 1 of 2).</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJeem3C9F7&amp;noteId=B1epOpZ6TQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1327 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018 (modified: 18 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1327 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks so much for your time.
TL;DR: We’re completely rewriting the intro to focus on our actual contribution and not the long-term plan. We’re also working towards removing all the assumptions like camera position and light position knowledge. And we’ve added a lot more details about the 3D-IQTT methods.

== RE: Providing clarification on the claims.

We agree that the extent of the contributions claimed in the introduction was unclear. We set out to present our long-term goal, to “learn the 3D structure of the real world just from single images”; however, in this paper, we have just made the first steps towards the goal and that was not apparent from the original introduction. We have reworked our claims accordingly in our new paper version (will be uploaded in the next few days) to reflect that our method “receives a monochrome image as input, estimates a depth map, and then shades this depth map using perfect knowledge of lighting and camera pose, which reconstructs the input” - as you suggested.

Our model makes several assumptions: (a) the camera pose is known, (b) the material properties are constant, (c) the light positions are known, and (d) the world is piece-wise smooth. In order to achieve our long-term goal, we have to eventually get rid of these. Therefore we have made some first steps to address each one:

- (Camera pose is known): In the 3D-IQTT experiments, we estimated the camera position in our latent representation while we kept the camera looking at the center of the object. We used the estimated camera parameters in the generator for the rendering process.
- (Material properties are constant): We used diffuse materials with uniform reflectance for all our experiments. The reflectance values were chosen arbitrarily but kept fixed for input-output pairs. In other words, we use fixed material which can be chromatic (reflects different wavelengths by different amount) or monochromatic (reflects all wavelengths the same amount). This is not the same as using "monochromatic image", it is just that material is constant and doesn't need to be inferred.  We've added the details to the appendix. 
Learning the reflectance and color/texture properties (in addition to the surface depth and orientation) is significantly more challenging, but we are currently working towards that.
- (Lighting assumptions):  In our work presented in the last version of the paper, we used multiple point light sources that were placed randomly on the surface of a spherical sector around the scene and colored randomly. For each pair of rendered input image and model-reconstructed output image, these light conditions were identical. We added more details about how we handled the lighting to the appendix. 
- (The world is piece-wise smooth): This might not be perfectly accurate, but it’s a common assumption in 3D reconstruction. In an extreme case like when capturing cactus spikes, this might not work, but for example, when we were reconstructing a chair with a thin stretcher (see our video, <a href="https://bit.ly/2zADuqG)," target="_blank" rel="nofollow">https://bit.ly/2zADuqG),</a> the reconstruction worked well.


We agree that we are currently mainly recovering a depth map, but the surfel representation was picked with our long-term goal in mind, since gives us several advantages: (a) surfel representation allow us to represent only the visible surface of a complicated scene instead of explicitly representing the complete scene. Given an image we can infer its implicit 3D representation and then recreate novel surfel representations of the underlying scene from unobserved viewpoints. Moreover this representation fits well with current convolutional architectures (b) with our existing normal estimation and additional material estimation this allows for realistic shading.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rygmHzvtnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>learning 3D or depth images from 2D images</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJeem3C9F7&amp;noteId=rygmHzvtnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1327 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1327 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper deals with creating 3D representations or depth maps from 2D image data using adversarial training methods. 
The flow makes the paper readable.

One main concern is that most of the experiments seem to have results as visual inspections of figures provided. It is really hard to judge the correctness or how well the algorithms do.

It would be useful to provide references of equation 1 if used from previous text.

In the experiments, it is usually not clear how many training images were used, how many test. How different were the objects used in the training data vs test? Were all the test objects novel? How useful were the GAN techniques? Which part of the GAN did the most work i.e. the usefulness and accuracy of the different parts of the net? Even in 4.2, though it mentions use of 6 object types for both training and testing, using the figures is hard to estimate how well the model does compared to a reference baseline.

In 4.4.1, the discussion on how much improvement there is due to use of unlabeled images is missing? Do they even help? It is not quite clear from table 1. How many unlabeled images were used? How many iterations in total are used of the unlabeled ones (given there is 1 in 100 update of labeled ones). 

Missing reference: <a href="http://www.cs.cornell.edu/~asaxena/reconstruction3d/saxena_make3d_learning3dstructure.pdf" target="_blank" rel="nofollow">http://www.cs.cornell.edu/~asaxena/reconstruction3d/saxena_make3d_learning3dstructure.pdf</a>
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">1: The reviewer's evaluation is an educated guess</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJxwzGBJ0X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Reviewer 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJeem3C9F7&amp;noteId=BJxwzGBJ0X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1327 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1327 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear reviewer, thank you for your time and effort. 
TL;DR: We added more quantitative results. Our paper already includes examples generalizing to viewpoints that weren’t part of the training data, but we included additional samples. And we added significantly more details about the methods.

Here are our responses in more detail:


== RE: Most evaluations are qualitative.

There is no standard protocol to evaluate 3D reconstruction and generation. Most of the state-of-the-art methods (fully unsupervised methods learned on single images) just show qualitative results in their papers. 
We did quantitatively evaluate the surfel reconstruction against the ground truth via Hausdorff distance (HD) as described in Appendix B and the reconstructions of our model via mean squared error (MSE) on the depth map. We achieved near-zero MSE and reasonably lower HD (when reconstructed from the same view). We included a table showing the difference in these values for different, unseen camera views: <a href="https://ibb.co/nj73qL." target="_blank" rel="nofollow">https://ibb.co/nj73qL.</a> On top of these metrics, we also created the 3D IQ test task (3D-IQTT) which is exclusively quantitative. We compared our method with two CNN baselines and we now also included human evaluation. The CNN baselines demonstrate that the task can only be solved with an understanding of the 3D geometry. A preview of the updated comparison table can be found here: ( https://ibb.co/nhHUS0) .


== RE: Add reference for the rendering equation.

Sorry for the oversight. We have added the reference for the rendering equation: it is an approximation of Kajiya’s rendering equation [Kajiya 1986].

[Kajiya 1986] “The Rendering Equation”


== RE: More details on experimental setup.

We have added more details on the experimental setup (camera, lights, and material properties used) in the appendix. All our images are of resolutions 128x128. 
Except for the 3D-IQTT, we didn’t store a fixed dataset but rather created the dataset on the fly. For example in the existing Figure 4, during the data generation process the rotation, translation, and object categories were randomized. The probability of seeing the same configuration from two different views is near zero.


== RE: Which parts of the GAN are more important.

Our Pix2Scene architecture is a bidirectional adversarial model. It consists of an encoder, decoder, renderer, and discriminator. The encoder translates the input image into a latent representation. The decoder transforms a similar latent vector, sampled from noise, into our surfel representation, which is converted into a 2D image by the renderer. The discriminator’s purpose is to make sure the output images become the same distribution as the input images, and ascertain that the encoded latent representation corresponds to the latent input to the decoder. See our existing Figure 3 for an overview. The decoder-rendering part is important for generating new viewpoints for a given latent code and the encoder-decoder pipeline allows us to infer the 3D structure of a 2D image. Without the encoder, the model would be purely generative.


== RE: Novelty of the generated images.

GAN-based models usually suffer from mode-collapse. We demonstrated in Figure 8 that our model overcame this issue and was able to interpolate between two given scenes. We’ve added another figure to further emphasize the interpolation capabilities of our model.


== RE: 3D-IQTT semisupervised learning.

Thanks for this feedback. We agree that this section wasn’t sufficiently clear. We’ve rewritten a part of this section and added the details on the interleaved training. It's similar to algorithm 2 from [Kingma et al. 2014], except instead of a randomized minibatch, we train a few iterations of unsupervised data followed by a few iterations of supervised data. We also extended Table 1 to include an entirely unsupervised case as well as human performance on the same task. A preview of the table can be found here: https://ibb.co/nhHUS0. In all cases, the model was trained with an unsupervised dataset of 100,000 lines of data, where each line contained the reference image, the 3 possible answers, but no information on which one was the correct answer.

[Kingma et al. 2014] "Semi-supervised Learning with Deep Generative Models", 2014. 


== RE: Missing reference Make-3D.

Sorry for the oversight. We will add the reference in our introduction.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>