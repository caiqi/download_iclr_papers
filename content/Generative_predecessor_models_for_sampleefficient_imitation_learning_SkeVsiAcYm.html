<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Generative predecessor models for sample-efficient imitation learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Generative predecessor models for sample-efficient imitation learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SkeVsiAcYm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Generative predecessor models for sample-efficient imitation learning" />
      <meta name="og:description" content="We propose Generative Predecessor Models for Imitation Learning (GPRIL), a novel imitation learning algorithm that matches the state-action distribution to the distribution observed in expert..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SkeVsiAcYm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Generative predecessor models for sample-efficient imitation learning</a> <a class="note_content_pdf" href="/pdf?id=SkeVsiAcYm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 10 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019generative,    &#10;title={Generative predecessor models for sample-efficient imitation learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SkeVsiAcYm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=SkeVsiAcYm" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We propose Generative Predecessor Models for Imitation Learning (GPRIL), a novel imitation learning algorithm that matches the state-action distribution to the distribution observed in expert demonstrations, using generative models to reason probabilistically about alternative histories of demonstrated states. We show that this approach allows an agent to learn robust policies using only a small number of expert demonstrations and self-supervised interactions with the environment. We derive this approach from first principles and compare it empirically to a state-of-the-art imitation learning method, showing that it outperforms or matches its performance on two simulated robot manipulation tasks and demonstrate significantly higher sample efficiency by applying the algorithm on a real robot.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Imitation Learning, Generative Models, Deep Learning</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Skl8Fy36hm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting idea, but some issues need to be clarified.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkeVsiAcYm&amp;noteId=Skl8Fy36hm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper616 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper616 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies the problem of matching the state-action distributions of agent and expert demonstrations. In order to address this problem, the authors consider a likelihood treatment comprising a conditional probability (which is estimated from demonstrations) and a state distribution (which is estimated from sampling approximations). 

The authors provide a descent result (i.e., equ. (7)) to estimate the gradient of the logarithmic state distribution. One problem is that it is unclear how the discount factor $\gamma$ influence this result?

In addition, in (12), two scaling factors are used, so how to balance these weights?

Specifically, in (11), it seems the authors are considering the stationary joint state-action distribution, which is different from the state-action distribution generated by the agent on-line, it is suggested to clarify this issue.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1lGouImTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Additional discussion added concerning the effect of approximations made in the paper.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkeVsiAcYm&amp;noteId=H1lGouImTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper616 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper616 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review. The questions you raise about the effect of the approximations made in our derivation are valid and we have added additional discussion to the paper that we hope will answer these questions satisfactorily:

+ The discount factor γ is not only similar to the discount factor used in reinforcement learning but can be seen as identical. This was not immediately apparent in the original submission and we have added Appendix C to the manuscript to explore the connection between our gradient estimate and policy gradients. As a result, we can draw on the understanding of the discount factor in reinforcement learning to gain insight into the behavior of γ and conclude that first, the discount factor introduces a trade-off where an agent with lower γ prefers to reach demonstrated states more quickly while agents with higher γ will aim to reproduce the state-action distribution more closely in the long-term. Second, as γ approaches 1, the variance grows. However, in reinforcement learning, it has been empirically shown that lower discount factors can be an accurate, low-variance approximation even when the true objective is more accurately described by the average reward objective (γ -&gt; 1). The alternative derivation introduced in Appendix C thus indicates that lower values of γ are likely to be reasonable approximations in the imitation learning setting as well.
+ With regards to stationarity, under the usual ergodicity assumptions the expected distribution of state-action pairs the agent will observe and the stationary distribution should be identical in the infinite horizon case (using the modified MDP with terminal states being treated as transitions to initial states as discussed in section 2.1). In general, matching the joint stationary distribution to the empirical distribution of the expert implies a form of loop in the agents behavior which may be as simple as restarting after reaching a terminal state. This is the case in many practical scenarios as well as our experiments. While handling the finite horizon case explicitly might also be interesting, we are not considering it for the purposes of this work.
+ The scaling factors β were added to provide more freedom to tune the behavior of the learning algorithm but we agree that additional discussion would be useful and have added it to section 3.3. In particular, the factors are the result of dropping the factor of 1/(1-γ) in equation 7, this indicates that a sensible starting point would be β_π=(1-γ)β_d. However, we did not find this to be optimal in all cases. In particular, if behavioral cloning is likely to overfit strongly, lower values of β_π may be adequate while in cases where exploring to learn the generative models is more difficult, higher values of β_π may provide more guidance. 


We hope that we answered your questions to your satisfaction, please let us know if you have further questions or concerns that you would like us to address.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BkeXEPYKnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good Results But Relevant Literature is missing.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkeVsiAcYm&amp;noteId=BkeXEPYKnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper616 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper616 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes to use predecessor models for imitation learning to overcome the issue of only observing expert samples during imitation from expert trajectories.

The paper is very well written. But the proposed method is really not novel. The idea of using predecessor models have already been explored in multiple places [1], [2] (but not in imitation learning scenario!). Hence, the novelty comes from using the predecessor models for imitation learning. The introduction of the paper should mention this  to reflect the contribution. 

[1] Recall Traces: Efficient Backtracking models for efficient RL <a href="https://arxiv.org/abs/1804.00379" target="_blank" rel="nofollow">https://arxiv.org/abs/1804.00379</a>
[2] Organizing Experience: A Deeper Look at Replay Mechanisms for Sample-based Planning in Continuous State Domains
https://arxiv.org/abs/1806.04624

Both of these papers should be cited and discussed.

Results: The proposed method outperforms GAIL and behaviour cloning in terms of sample efficiency   on simulation-based manipulation tasks.

Regarding experiments, I would like to see certain baselines.

- What happens when you predict sequentially using predecessor models ? I understand that the sequential generation is prone to accumulating errors, but as [1] points out, using predecessor models you can sample from many states on the expert trajectory. And Hence possible to get good learning signal even while sampling shorter trajectories using predecessor models.

- Comparison with Dyna based methods. For this baseline, authors would learn a forward model. And then sample from the forward model, and use the samples from the forward model for imitation learning.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJlWIFUQpX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Specifying the scope of our work and additional citations.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkeVsiAcYm&amp;noteId=rJlWIFUQpX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper616 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper616 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review. Predecessor models do indeed have a long history in reinforcement learning and recent work explores the use of deep networks in this context. While our algorithm followed directly from the derivation of the state-distribution gradient in section 3.1, we can see that a comparison to the aforementioned works might be useful to the reader and have added this in section 1. We hope that this addition will adequately specify the scope of our work. In particular, we claim the following two contributions:

1. Derivation of the state-distribution gradient based on samples from a predecessor model. While such models have been used in the past, to the best of our knowledge this connection has not been pointed out before. Instead, most work focuses on the use of predecessor models as a more efficient order of bellman backups while the recent Recall Traces uses a justification based on a variational lower bound. We believe that our derivation provides further justification to the approach used in Recall Traces and may furthermore help to guide design decisions when developing such algorithms in the field of reinforcement learning.
2. Development of a novel, state-of-the-art imitation learning algorithm. To the best of our knowledge, the use of predecessor models to achieve state-action distribution matching in imitation learning is novel. We believe that predecessor models are a natural fit for imitation learning as, unlike in reinforcement learning, future observations and their accordance with demonstrations are very difficult to evaluate. We demonstrate the effectiveness of such models on traditionally difficult real world imitation learning problems in our evaluation.

Regarding our choice of multi-step models and comparison to one-step models of either direction, we note that in the general case, the error in naive one-step models grows exponentially (Venkatraman et al., 2015) thus requiring careful design of such models. Recent work such as Ha and Schmidhuber, 2018 and Gregor and Besse, 2018 achieves impressive predictions on sequential rollouts indicating that it is very likely that a one-step model can be applied in our setting as well. However, these works require a significant effort on the modelling side and we thus decided to side-step the issue by modelling the desired distribution directly. As the contact dynamics in our domain can be complex, we believe that the effort required in our domain would have been significant as well. We note that the main contribution of this paper is the use of samples of a predecessor model to match state-action distributions in a principled way while the choice of model is a design choice that was made to avoid increasing complexity.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1xOiCceCQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkeVsiAcYm&amp;noteId=S1xOiCceCQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper616 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper616 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Sorry for late reply. 

"the error in naive one-step models grows exponentially " 

Learning any kind of model (whether forward model or predecessor model) for more than (lets say 10 steps) is challenging task. This paper needs to argue (by showing experiments empirically) that using predecessor models is better as compared to these 2 baselines.

- When the agent learns a jumpy forward model. (Also used in learning to query paper [1]) as compared to jumpy predecessor model.
- Unrolling the predecessor model step by step.
- Lets say you have a trajectory of length 1000, then one can take every 5th state on this expert trajectory, and generate traces (according to the learned forward model) and use these generated traces for imitation learning. As the authors pointed out that unrolling the forward model (or predecessor model ) is prone to compounding errors, but one can easily make predictions for 5/10 steps. And since you have an expert trajectory, you can use every kth state as an input to forward model for generating traces (here k==5/10). This wont be prone to "compounding errors".

Unless until authors compare to these baselines, I dont think the contribution is justified. 

[1] <a href="https://arxiv.org/abs/1802.03006" target="_blank" rel="nofollow">https://arxiv.org/abs/1802.03006</a></span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_B1x9M-TO37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Compelling, sample efficient approach to imitation learning using learned dynamics models. Experiments could be extended.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkeVsiAcYm&amp;noteId=B1x9M-TO37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper616 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper616 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The submission builds up on recent advances in neural density estimation to develop a new algorithm for imitation learning based on a probabilistic model for predecessor state dynamics. In particular, the method trains masked autoregressive flows as a probabilistic model for state action pairs conditioned on future states. This model is used to estimate the gradient of the stationary distribution of a policies visited states. Finally, the proposed objective uses this estimate and the gradient of the log likelihood of expert actions under the policy to maximise the similarity of the expert’s and agent’s stationary state-action distributions. 

The proposed method outperforms existing imitation learning approach (GAIL &amp; BC) on 2 simulation-based manipulation tasks. It performs particularly well in terms of sample efficiency. 
The magnitude of difference between the sample efficiencies of GAIL and the proposed approach seems quite surprising and it would be beneficial if the authors could explicitly state if the measured number of samples include the ones used for training of the probabilistic model as well as the policy (apologies if I have missed a section fulfilling this purpose).

While the improvements on the presented experiments are clear, the experimental section represents a small shortcoming of the submitted paper. The 2 experiments (clip and peg insertion) are quite similar in type and to not take into account other common domains e.g. locomotion tasks from the original GAIL paper. Furthermore, an additional comparison to SAIL would be recommended since the approaches are closely related as the authors acknowledge. The provided comparison with different types of available expert data is quite interesting and could possibly be extended to test other state-of-the-art methods (action-free versions of GAIL, AIRL,etc.).

Nonetheless, the paper overall presents a strong submission based on novelty &amp; relevance of the proposed method and is recommended for publication. 

Minor issues:
- Related work: improve transitions between the section about trajectory tracking and BC.
- Ablation studies with less flexible probabilistic models would strengthen the experiment section further. 
- Add derivation from Eq. 3 to 4 and 5 to appendix to render the paper more self-contained and easier to access.
- A release of the code base would further strengthen the contributions of the submission.

General recommendation:
- The authors are encouraged to further investigate off-policy corrections for improved convergence.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Hyg1Q9I7pQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Changes made to the manuscript &amp; addressing questions raised.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkeVsiAcYm&amp;noteId=Hyg1Q9I7pQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper616 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper616 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review. We would like to address some of the questions and points raised in your review:

+ Regarding sample efficiency, our algorithm only uses artificial samples and expert samples when updating the policy. The reported number of environment samples are the samples used to train the generative model. Note that while it is efficient compared to other algorithms of this type, the data efficiency is not extraordinary to train a network of this size on a problem of these dimensions. This indicates that the samples generated from the model are likely to be useful even as the policy changes. 
+ Regarding the chosen domains, we believe that they highlight a type of problem that is difficult for approaches based on adversarial updates (due to sample efficiency as well as the ability to control the state in its entirety) while also being representative for a class of problems one might encounter in practice. While we believe our approach to be widely applicable and would like to see it applied in other domains, existing approaches are already able to achieve very high scores on domains such as the mujoco walkers.
+ In comparison to SAIL, we aim to address scalability w.r.t. the number of parameters of the policy. We found that larger policies are able to achieve more accurate results on our domains, yet policies of this size are out of reach for SAIL which has to predict the gradient for each parameter of the network.
+ We added additional steps to Appendix A to make the derivation more self-contained and changed the wording in the related works section based on your suggestion. Unfortunately, we are currently not able to release the code.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>