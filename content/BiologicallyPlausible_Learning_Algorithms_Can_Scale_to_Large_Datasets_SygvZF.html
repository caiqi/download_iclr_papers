<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Biologically-Plausible Learning Algorithms Can Scale to Large Datasets | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Biologically-Plausible Learning Algorithms Can Scale to Large Datasets" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SygvZ209F7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Biologically-Plausible Learning Algorithms Can Scale to Large Datasets" />
      <meta name="og:description" content="The backpropagation (BP) algorithm is often thought to be biologically implausible in the brain. One of the main reasons is that BP requires symmetric weight matrices in the feedforward and..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SygvZ209F7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Biologically-Plausible Learning Algorithms Can Scale to Large Datasets</a> <a class="note_content_pdf" href="/pdf?id=SygvZ209F7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019biologically-plausible,    &#10;title={Biologically-Plausible Learning Algorithms Can Scale to Large Datasets},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SygvZ209F7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">The backpropagation (BP) algorithm is often thought to be biologically implausible in the brain. One of the main reasons is that BP requires symmetric weight matrices in the feedforward and feedback pathways. To address this “weight transport problem” (Grossberg, 1987), two more biologically plausible algorithms, proposed by Liao et al. (2016) and Lillicrap et al. (2016), relax BP’s weight symmetry requirements and demonstrate comparable learning capabilities to that of BP on small datasets. However, a recent study by Bartunov et al. (2018) finds that although feedback alignment (FA) and some variants of target-propagation (TP) perform well on MNIST and CIFAR, they perform significantly worse than BP on ImageNet. Here, we additionally evaluate the sign-symmetry algorithm (Liao et al., 2016), which differs from both BP and FA in that the feedback and feedforward weights do not share magnitudes but share signs. We examine the performance of sign-symmetry and feedback alignment on ImageNet and MS COCO datasets using different network architectures (ResNet-18 and AlexNet for ImageNet, RetinaNet for MS COCO). Surprisingly, networks trained with sign-symmetry can attain classification performance approaching that of BP-trained networks. These results complement the study by Bartunov et al. (2018), and establish a new benchmark for future biologically plausible learning algorithms on more difficult datasets and more complex architectures.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">biologically plausible learning algorithm, ImageNet, sign-symmetry, feedback alignment</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Biologically plausible learning algorithms, particularly sign-symmetry, works well on ImageNet</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">11 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HJels3U-67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>misleading title, misleading claims, main result not novel</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SygvZ209F7&amp;noteId=HJels3U-67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1180 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">The sign-symmetry method does not solve the weight transport problem. It just shows that a coarser kind of transport may be sufficient for practical purposes. The biological mechanism concocted in Figure 4 to show how it *may* be implemented is completely ad hoc and without any empirical support (one may as well concoct a similar scheme for standard backprop). Also, however that scheme is supposed to work (which is not explained clearly in the text, by the way), it has to show why the feedback weights have to be *exactly* +1 or -1, which is what the sign-symmetry algorithm assumes (again biologically completely unrealistically). The scheme only appears to show how sign consistency can be achieved, not why the weights have to be exactly +1 or -1.

As another commenter pointed out, the success of the sign-symmetry method in practical applications is also not surprising, given the success of the signSGD method: <a href="https://arxiv.org/abs/1802.04434" target="_blank" rel="nofollow">https://arxiv.org/abs/1802.04434</a> (a paper the authors unfortunately do not discuss or cite) and especially the success of the very similar (and even more restrictive) binary weight architectures (such as the XNOR-Net: https://arxiv.org/abs/1603.05279 and a whole slew of other work that followed it), again an entire literature not even mentioned in this paper.

In conclusion, the main claim of this paper (that "biologically plausible learning algorithms can scale to large datasets") is misleading and the main result is not novel.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkxHwKZA6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Weight-transport, unitary weight, and XNOR-Net</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SygvZ209F7&amp;noteId=BkxHwKZA6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1180 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1180 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for your thoughtful comments. Here is a more detailed reply after we've run additional experiments to address your concerns.

1) Weight-transport
We do not claim to completely solve the problem of "transport." However, we do address weight-transport by eliminating the need to synchronize magnitude (many bits of information) and only asking forward and backward weights to share signs (1 bit of information). This requires a much looser connection between the two, and indeed makes it much easier to devise an implementation for SS in the brain. Although one could perhaps concoct a scheme to achieve precise weight symmetry, it will likely be much more difficult and complex because more information need to be shared. Related, although the implementation in Figure 4 is ad hoc, its purpose is only to show that it's relatively *simple* to implement sign-symmetry in the brain (especially with consistent neurons), not that it is *likely* implemented in this way in the brain. Hence, we chose to not grasp for speculative neuroscientific evidence and overstretch our claims for Figure 4. 

2) Unitary weights
We have run additional experiments where feedback weight B = sign(W) * R, where W is the feedforward weight, R is a random weight matrix (as in Feedback Alignment), and * is elementwise multiplication. This setting achieves similar performance to SS and BP in Figure 1, consistent with our interpretation that sign-symmetry works because of sign symmetry, not because of any special property of the weight magnitudes.

3) XNOR-Net
Thank you for bringing up binary weight networks and signSGD for discussion. We omitted discussing them because they are not motivated by biological plausibility of the learning algorithm, and hence although they are superficially similar to SS, they are fundamentally different. SignSGD has been discussed in the previous comment; it still computes exact gradients, only using gradient signs during update. In XNOR-Net, although its feedforward and feedback weights are binary, they are still exactly symmetrical, making gradient computation exact in form.

In contrast, consider this simple case in sign-symmetry, where input h_0 is connected by {w_0, w_1} to output {h_10, h_11}. If {w_0, w_1} = {1, -0.5} and grad_output = {1, 1.5}, grad_input = 0.25 in BP but -0.5 in SS. Not only is the SS gradient imprecise, it is in a different direction than the BP gradient! Hence SS is fundamentally different from both XNOR-Net and signSGD.

We are grateful for all three comments above, and we will add them to the discussion in the paper to make our contribution clearer.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1ekcnvWp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>signSGD</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SygvZ209F7&amp;noteId=S1ekcnvWp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1180 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018 (modified: 09 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1180 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Thanks for the feedback!

signSGD seems to be very similar (if not the same as) the "Batch Manhattan" (BM) approach first used in [1], which is discussed in this paper and [1].

One central question in biologically-plausible training of neural network is how much different (from SGD) can the weight updates be while maintaining good performance. How much noise can SGD tolerate if evolution wants to implement an approximated SGD in the brain.

With signSGD/BM, we can see that as long as the direction of the weight update is the same as standard SGD, the performance is quite good. As your comment said, this might be only mildly surprising.

With sign-symmetry feedback, however, the gradients are propagated imprecisely *every* layer, leading to drastically different update directions in many early layers of the network.  Without the results of this paper and [1], it is much unclearer whether this drastic level of divergence from SGD can still lead to good performance. 

Although not completely eliminating the problem of weight transport, the results of this paper constitute an important step towards that direction, showing that this non-trivial level of discrepancy from SGD can be tolerated to achieve good performance on large-scale tasks like ImageNet. It is a good news for evolution --- it has more flexibility in implementing approximated SGD in the brain.

[1] Liao, Q., Leibo, J. Z., &amp; Poggio, T. (2015). How Important is Weight Symmetry in Backpropagation?. arXiv 2015, AAAI 2016

Footnote: This is just a quick reply by one author. We are working on replies to all other comments. Thank you all very much for constructive comments!</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BkeAYwE03X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Assuming weight transport?</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SygvZ209F7&amp;noteId=BkeAYwE03X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1180 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">It's nice to see more interest around the question of biologically-motivated deep learning! I've been wondering a couple of things about the central claim of the manuscript.  As I understand it, the manuscript is aimed at examining the question of whether biologically motivated algorithms can scale to large and difficult datasets.  The abstract frames this question in particular around the problem of 'weight transport' and biologically motivated algorithms that do away with this issue.  I may be missing something, but it seems to me that the approach suggested in the manuscript still makes liberal use of weight transport.  That is, the proposed approach uses backward matrices that are constructed dynamically in terms of the forward weights via: 

B = sign(W^{T})

Is this true? Even though this throws away sign information, this operation still transports lots of weight information from the forward path to backward synapses.  Thus, the approach appears to assume weight transport.

It might still be an interesting datapoint to know that backward passes constructed in this way are effective.  Though I would have said that this wasn't particularly surprising, since sign information is well known to be the crucial information for learning:  for example, aggressive gradient clipping works well in many instances and as early as the 1990s Rprop (robust prop [1]) was shown to work very effectively by discarding the magnitudes of gradients (and keeping just the sign information).  

Several of the other biologically-motivated algorithms that are referenced in the manuscript aim to get rid of weight transport, e.g by learning useful backward weights (Difference Target Prop).  So, is it reasonable to compare the approach in this manuscript to other algorithms that don't use weight transport?  'Sign-symmetry' seems to exist in a very different category, in that it takes weight transport for granted.  If I understand correctly, the wiring diagram in Figure 4 is meant to suggest how why it would be ok to take weight transport for granted in the brain.  But, I would have said that existing empirical evidence speaks against this outlined implementation.  At the very least, I found myself wanting citations that would strengthen the claim.

In sum, it seems like what could be said given the evidence presented in the manuscript is that: if there were an algorithm that could successfully construct backward B matrices with the correct signs (i.e. matching sign(W^{T})) without weight transport, then this hypothetical algorithm would be successful on large scale datasets.  This is interesting in its own right, but at first blush this statement seems far from the central claim of the manuscript that existing biologically-plausible algorithms already scale to large data sets?  But I may have missed something in my reading of the work, and would be happy to be corrected on details.

[1] Martin Riedmiller und Heinrich Braun: Rprop - A Fast Adaptive Learning Algorithm. Proceedings of the International Symposium on Computer and Information Science VII, 1992
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Bkxf7H_W6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks for the comment!</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SygvZ209F7&amp;noteId=Bkxf7H_W6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1180 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1180 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Thanks a lot for such a detailed and constructive comment.

Many of the concerns are similar to:
<a href="https://openreview.net/forum?id=SygvZ209F7&amp;noteId=HJels3U-67&amp;noteId=HJels3U-67" target="_blank" rel="nofollow">https://openreview.net/forum?id=SygvZ209F7&amp;noteId=HJels3U-67&amp;noteId=HJels3U-67</a>

And we answered some of them. We are going to provide more replies soon.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SkguetB9h7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An important step in our understanding of biologically plausible learning.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SygvZ209F7&amp;noteId=SkguetB9h7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1180 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1180 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: The authors are interested in whether particular biologically plausible learning algorithms scale to large problems (object recognition and detection using ImageNet and MS COCO, respectively). In particular, they examine two methods for breaking the weight symmetry required in backpropagation: feedback alignment and sign-symmetry. They extend results of Bartunov et al 2018 (which found that feedback alignment fails on particular architectures on ImageNet), demonstrating that sign-symmetry performs much better, and that preserving error signal in the final layer (but using FA or SS for the rest) also improves performance.

The paper is clear, well motivated, and significant in that it advances our understanding of how recently proposed biologically plausible methods for getting around the weight symmetry problem work on large datasets.

In particular, I appreciated: the clear introduction and explanation of the weight symmetry problem and how it arises in the context of backprop, the thorough experiments on two large scale problems, the clarity of the presented results, and the discussion about future directions of study.

Minor comments:
- s/there/therefore in the first paragraph on page 2
- The authors claim that their conclusions "largely disagree with results from Bartunov et al 2018". I would suggest a slight rewording here: the authors' results *extend* our understanding of Bartunov et al 2018. They do not disagree in the sense that this paper also finds that feedback alignment alone is insufficient to train large models on ImageNet.
- Figure 1: I was expecting to see a curve for performance of feedback alignment on AlexNet
- Figure 1: The colors are hard to follow. For example, the two shades of purple represent the two FA models, which makes sense, but then there are two separate hues (black and blue) for the sign-symmetry models. Instead, I would suggest keeping black (or gray) for backpropagation (the baseline), and then using two hues of one color (e.g. light blue and dark blue) for the two sign-symmetry models. This would make it easier to group the related models.
- Figure 2: Would be nice if these colors (for backprop/FA/SS) matched the colors in Figure 1.
- Figure 3: Why is there such a small change in the average alignment angle (2 degrees?) I found that surprising.
- Figure 3: The right two panels would be clearer on the same panel. That is, instead of showing the std. dev. separately, show it as the spread (using error bars) on the plot with the mean. This makes it easier to get a sense if the distributions overlap or not.
- Figure 3 (b/c): Could also use the same colors for BP/SS as Figs 1 and 2.
- Figure 3 (caption): I think the blue/red labels in the caption are mixed up for panel (a).</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">9: Top 15% of accepted papers, strong accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJx1pgdbTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you very much for the review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SygvZ209F7&amp;noteId=HJx1pgdbTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1180 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1180 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
We are very excited to see your encouraging review! We really appreciate your super detailed comments. 

This is a quick reply and we are working on detailed replies to all comments!</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJg34EAt27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice alternative to backprop</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SygvZ209F7&amp;noteId=BJg34EAt27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1180 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1180 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In the submitted manuscript, the authors compare the performance of sign-symmetry and feedback alignment on ImageNet and MS COCO datasets using different network architectures, with the aim of testing biologically-plausible learning algorithms alternative to the more artificial backpropagation.
The obtained results are promising and quite different to those in (Bartunov , 2018) and lead to the conclusion that biologically plausible learning algorithms in general and sign- symmetry in particular are effective alternatives for ANN training.

Although all the included ideas are not fully novel, the manuscript shows a relevant originality, paving the way for what can be a major breakthrough in deep learning theory and practice in the next few years. The paper is well written and organised, with the tackled problem well framed into the context. The suite of experiments is broad and diverse and overall convincing, even if the performances are not striking. Very interesting the biological interpretation and the proposal for the construction in the brain.
A couple of remarks: I would be interested in understanding the robustness of the sign-symmetry algorithm w.r.t. for instance dropout and (mini)batch size, and to see the behaviour of the algorithm on datasets with small sample size; second, there is probably too much stress on comparing w/ (Bartunov , 2018), while the manuscript is robust enough not to need such motivation.

Minor: refs are not homogeneous, first names citations are not consistent.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">9: Top 15% of accepted papers, strong accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJg3Sz_WTm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you very much for the review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SygvZ209F7&amp;noteId=HJg3Sz_WTm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1180 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1180 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you very much for your encouraging review.

Regarding datasets with small sample size, many such experiments can be found in [1]. We did not formally repeat them but observe similar conclusions.

This is a quick reply and we are working on a more detailed version!

[1] Liao, Q., Leibo, J. Z., &amp; Poggio, T. (2015). How Important is Weight Symmetry in Backpropagation?. arXiv 2015, AAAI 2016</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_S1eEJYfCsX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>the claims, conclusion, and general writing need to be better situated in the context of the concerns in the field</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SygvZ209F7&amp;noteId=S1eEJYfCsX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1180 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">25 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1180 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This work adds to a growing literature on biologically plausible (BP) learning algorithms. Building off a study by Bartunov et al. that shows the deficiencies of some BP algorithms when scaled to difficult datasets, the authors evaluate a different algorithm, sign-symmetry, and conclude that there are indeed situations in which BP algorithms can scale. This seemingly runs counter to the conclusions of Bartunov et al.; while the authors state that their results are "complementary", they also state that the findings “directly conflict” with the results of Bartunov, concluding that BP algorithms remain viable options for both learning in artificial networks and the brain.

To reach these conclusions the authors report results on a number of experiments. First, they show successful training of a ResNet-18 architecture on ImageNet using sign-symmetry, with their model performing nearly as well as one trained with backpropagation. Next, they demonstrate decent performance on MS COCO object detection using RetinaNet. Finally, they end with a discussion that seeks to explain the differences in their approach and the approach of Batunov et al, and with a potential biological implementation of sign symmetry.

Overall the clarity of the writing is sufficient. The algorithm is properly explained, and there are sufficient citations to reference prior work. The results are generally clear (though there is an incomplete experiment, I agree with the authors that it is unlikely for the preliminary results to change). I believe that there is enough detail for this work to be reproducible. The work is also sufficiently novel in that experiments using sign-symmetry on difficult datasets have not been undertaken, to my knowledge.

Unfortunately, the clarity and rigor of the *scientific argument* is insufficient for a number of reasons. These will be enumerated below.

First, the explicit writing and underlying tone of the paper reveal a misrepresentation of the scientific argument in Bartunov et al. The scientific question in Bartunov et al. is not a matter of whether BP algorithms can be useful in purely artificial settings, but rather whether they can say anything about the way in which the brain learns. In this work, on the other hand, there seems to be two scientific questions: first, to assess whether BP algorithms can be useful in artificial settings, and second, to determine whether they can say anything about how the brain learns, as in Bartunov (indeed, the author’s conclusions highlight precisely these two points). Unfortunately, the experiments and underlying experimental logic push towards addressing the first question, and use this as evidence towards a conclusion to the second question. More concretely, experiments are run on biologically problematic architectures such as ResNet-18, often with backpropagation in the final layer (though admittedly this doesn’t seem to be an important detail with sign-symmetry, for reasons explained below). This is fine under the pretense of answering the first question, but to seriously engage with the results of Bartunov et al. and assess sign-symmetry’s merit as a BP algorithm for learning in the brain, the work requires the authors the algorithms to be tested under similar conditions before claiming that there is a “direct conflict”. To this end, though the authors claim that the conditions on which Bartunov et al tested are “somewhat restrictive”, this logic can equally be flipped on its head: the conditions under which this paper tests sign-symmetry are not restrictive enough to productively move in the direction of assessing sign-symmetry’s usefulness as a description of learning in the brain, and so the conclusion that the algorithm remains a viable option for describing learning in the brain is not sufficiently supported. On the other hand, I think the conclusions regarding the first question -- whether sign-symmetry can be useful in artificial settings -- are fine given the experiments.

Second, the work does not sufficiently weigh the “degree” of implausibility of sign-symmetry compared to the other algorithms, and implicitly speaks of feedback alignment, target propagation, and sign-symmetry as equally realistic members of a class of BP algorithms. Of course, one doesn’t want to go down the road of declaring that “algorithm A is more plausible than algorithm B!”, but the nuances should at least be seriously discussed if the algorithms are to be properly compared. In backpropagation the feedback connections must be similar in sign and magnitude. Sign-symmetry eliminates the requirement that the connections be similar in magnitude. However, this factor is arguably the least important of the two (the direction of the gradient is more important than the magnitudes), and we are still left with feedback weights that somehow have to tie their sign to their feedforward counterparts, which is not an issue in target propagation or feedback alignment. The authors try to explain away this difficulty with an appeal to molecular biology, which leads into my third point.

Third, the appeal to molecular mechanisms to explain how sign-symmetry can arise is not rigorous. There is a plethora of molecular mechanisms at play in our cells; indeed, there are enough mechanisms to hand-craft *any* sort of circuit one likes. Thus, it is somewhat vacuous to conclude that a particular circuit can be “easily implemented” in the brain simply by appealing to a hand-crafted circuit. For this argument to hold one needs to appeal to biological data to demonstrate that such a circuit either a) exists already, b) most probably exists because of reasons X, Y, Z. Unfortunately there is no biological backing, rendering this argument a possibly fun thinking exercise, but not a serious scientific proposal. But perhaps most problematic, the argument leaves the problem of sign-switching in the feedforward network to “future work”. This is perhaps *the most* important problem at play here, and until it is answered, these arguments don’t have sufficient impact.

Altogether the scientific argument of this work needs tightening. The tone, the title, and the overall writing should be modified to better tackle the nuances underlying the arguments of biologically plausible learning algorithms. The claims and conclusions need to be more explicit, and the work needs to better seated in the context of both the previous literature, and the important questions at play for assessing biologically plausible learning algorithms.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1lbbE_W67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you very much for the review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SygvZ209F7&amp;noteId=r1lbbE_W67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1180 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1180 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">As a quick comment, we really appreciate your very detailed feedback! We are working heavily on a hopefully equally detailed reply! :D</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>