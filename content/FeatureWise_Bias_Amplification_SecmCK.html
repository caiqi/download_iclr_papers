<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Feature-Wise Bias Amplification | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Feature-Wise Bias Amplification" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=S1ecm2C9K7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Feature-Wise Bias Amplification" />
      <meta name="og:description" content="We study the phenomenon of bias amplification in classifiers, wherein a machine learning model learns to predict classes with a greater disparity than the underlying ground truth. We demonstrate..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_S1ecm2C9K7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Feature-Wise Bias Amplification</a> <a class="note_content_pdf" href="/pdf?id=S1ecm2C9K7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 13 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019feature-wise,    &#10;title={Feature-Wise Bias Amplification},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=S1ecm2C9K7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=S1ecm2C9K7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">We study the phenomenon of bias amplification in classifiers, wherein a machine learning model learns to predict classes with a greater disparity than the underlying ground truth. We demonstrate that bias amplification can arise via inductive bias in gradient descent methods resulting in overestimation of importance of moderately-predictive ``weak'' features if insufficient training data is available. This overestimation gives rise to feature-wise bias amplification -- a previously unreported form of bias that can be traced back to the features of a trained model. Through analysis and experiments, we show that the while some bias cannot be mitigated without sacrificing accuracy, feature-wise bias amplification can be mitigated through targeted feature selection. We present two new feature selection algorithms for mitigating bias amplification in linear models, and show how they can be adapted to convolutional neural networks efficiently. Our experiments on synthetic and real data demonstrate that these algorithms consistently lead to reduced bias without harming accuracy, in some cases eliminating predictive bias altogether while providing modest gains in accuracy.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">bias, bias amplification, classification</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_H1lSw16ch7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting result, need more comparison in the experiment section, need more explaining of related work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1ecm2C9K7&amp;noteId=H1lSw16ch7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1382 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1382 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this paper, the authors studied bias amplification. They showed in some situations bias is unavoidable; however, there exist some situations in which bias is a consequence of weak features (features with low influence to the classifier and high variance). Therefore, they used some feature selection methods to remove weak features; by removing weak features, they reduced the bias substantially while maintaining accuracy (In many cases they even improved accuracy).  Showing that weak features cause bias is very interesting, especially in their real-world dataset in which they improved bias and accuracy simultaneously.  


My main concerns about this paper are its related work and its writing.
Authors did a great job in reviewing related work for bias amplification in NLP or vision. 
However, they studied bias amplification in binary classification, in particular, they looked at GNB; and they did not review the related work about bias in GNB.  I think it is clear that using MAP causes bias amplification. Therefore, I think changing theorem 1 to a proposition and shifting the focus of the paper to section 2.2 would be better. Right now, I found feature orientation and feature asymmetry section confusing and hard to understand. In the paper, the authors claimed bias is a consequence of gradient descent’s inductive bias, but they did not expound on the reasoning behind this claim. Although the authors ran their model on many datasets, there is no comparison with previous work. So it is hard to understand the significance of their work. It is also not clear why they don’t compare their model with \ell_1 regularization in CIFAR.


Minor:

Paper has some typos that can be resolved.
Citations have some errors, for example, Some of the references in the text does not have the year, One paper has been cited twice in two different ways, For more than two authors you should use et al., sometimes \citet and \citep are used instead of each other.
Authors sometimes refer to the real-world experiment without first explaining the data which I found confusing.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1g-livup7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your thoughtful feedback</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1ecm2C9K7&amp;noteId=S1g-livup7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1382 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1382 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comments regarding the previous work section. We have included a more in-depth comparison to other work around bias in GNB in our update to the paper. 

We have updated Section 2.2 (now Section 3.2) with a more precise description of the data used in that section, which was constructed to exemplify the feature asymmetry we describe. We hope that it clears up some of the confusion in that part of the paper, and are willing to revise with additional clarifications if needed.

Regarding the claim that bias follows from an inductive bias of SGD, the argument is that because we see bias when we train SGD-LR in a setting where the Bayes-optimal classifier would have no bias, the bias cannot be explained by Theorem 1 (i.e., as bias that is inevitable when optimizing accuracy), hence we conclude the bias must have been caused by the learning rule (SGD-LR). While the inductive bias may not be uniquely attributable to SGD, and instead may be a consequence of using LR regardless of how the coefficients were obtained, we found that LR models trained on the same data using other methods, such as L-BFGS, did not result as much consistent bias as LR trained with SGD. Moreover, training with SGD using other loss functions, such as hinge, modified-Huber, and perceptron, resulted in the same bias characteristics as shown in Figure 2. Thus, linear classifiers trained with SGD consistently show the inductive bias we describe, whereas comparable classifiers trained using other methods may not. We have included an additional figure (Fig. 3 in the appendix) that details these results.

In our experiments we compare our feature selection method targeted at feature-wise bias to L1 regularization. We are not aware of other feature selection methods intended to mitigate the bias we target in the paper, but are willing to include additional comparisons if there are comparable approaches that we missed.

We additionally added results for L1 regularization on CIFAR. In general, L1 is harder to apply to the deep network scenarios because training takes a long time, making the hyperparameters hard to tune.

Thank you also for your formatting comments; we have addressed them in the updated version of the paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BkldIeZq27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Well-motivated paper with a good balance of novel insight and practical methods</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1ecm2C9K7&amp;noteId=BkldIeZq27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1382 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1382 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:

In this paper the authors identify a specific source of marginal class probability bias that occurs when using logistic regression models. Using synthetic and real datasets they demonstrate this bias and explore characteristics of the data that exacerbate the issue. Finally, they propose two methods for correcting this bias in logistic regression models and neural network models with logistic output layers and evaluate these methods on several benchmark datasets.

Review:

Overall, I found the paper well-written, the problem well-motivated, and the proposed methods clear and reasonable. While I have a few concerns about presentation and experimentation, these are issues that can easily be remedied and I recommend acceptance.

Major comments:

- The authors repeatedly say that gradient descent is the cause of the bias amplification (e.g. Section 2.2 title, "...features that are systematically overestimated by gradient descent.", "... i.e., a consequence of gradient descent's inductive bias.", "... gradient descent may lead to systematic bias..."). The inductive bias they describe is coming from the use of logistic regression, not the use of gradient descent. Specifically, a logistic regression model has a convex likelihood, which means that regardless of what algorithm is used to maximize the likelihood, it should converge to the same point. In fact, most off-the-shelf implementations of logistic regression do not use vanilla gradient descent. Further, gradient descent may be used to estimate the parameters of any number of models which may or may not have the same inductive bias the authors describe.

- I thought the related work section was well-written and would strongly recommend moving it to the beginning of the paper as it motivates the entire problem. I also think it could be helpful to ground the technical definitions of bias amplification in a meaningful example.

- I think that the experimental setup for comparing \ell_1 regularization to the proposed feature selection methods is not quite fair. In particular, the hyperparameters of the "expert" method are selected to minimize bias subject to the constraint that loss not increase. In contrast, the \ell_1 regularization hyperparameter is selected purely to minimize bias. Instead, I would select the \ell_1 regularization hyperparameter in the same way as the expert method, that is, to minimize bias subject to a constraint on loss. In general, I think hyperparameters should be selected using the same criterion for all methods.

- The authors make a point of highlighting results on the "prostate" which showed a large increase in accuracy along with a large decrease in bias. I think the paper would benefit from some exploration of why this happened. Specifically, it would be valuable to answer the question: what are the properties of the "prostate" dataset that make this method so effective and are these properties general and identifiable a priori?

- Section 2, paragraph 2, line 5: The stated goal in this paragraph is "minimizing 0-1 loss on unknown future i.i.d. samples". As stated in the introduction, this is, in fact, not the goal. The goal is to minimize loss while also minimizing bias. A larger criticism that I would have of this work is: if minimizing bias is a first order goal, then why are we using empirical risk minimization in the first place? Put another way, why use post-hoc correction for an objective function that does not match our actual stated goals rather than using an objective function that does?

Minor comments:

- Section 1, paragraph 4, line 2: "Weak" is not clearly defined here. Is it different than "moderately-predictive"?

- Section 2.1, last paragraph, line 1: I understand what the authors are saying when they say "Bias amplification is unavoidable", but it is avoidable by changing our objective function. I would consider rewording this statement to something like "Using an ERM objective will lead to bias amplification when the learning rule..."

- Equation 4: I believe h should be changed to f in this equation.

- Equation 6: L is not defined anywhere.

- Table 1: As defined in equation 1, B_D(h_s) should be between 0 and 1. Also, the accuracy results for the glioma dataset have the wrong result in bold.

- Section 4, methodology paragraph, line 5: forthe --&gt; for the

- Section 5, paragraph 5, lines 5-6: Feature selection is not used "only to improve accuracy". For example, Kim, Shah, and Doshi-Valez (2015) use feature selection to improve interpretability (<a href="https://beenkim.github.io/papers/BKim2015NIPS.pdf)." target="_blank" rel="nofollow">https://beenkim.github.io/papers/BKim2015NIPS.pdf).</a></span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJxsEivOaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your thoughtful feedback</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1ecm2C9K7&amp;noteId=SJxsEivOaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1382 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1382 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We agree that the results we have presented do not indicate that SGD is the exclusive cause of the bias-inducing behavior examined in the paper. We note that LR will, given enough data, converge to the Bayes-optimal classifier, and because the data used in Figure 2 has an unbiased prior, we would expect no bias in the predictions according to Thm. 1. However, we posit that feature-wise bias occurs when the learner has not seen enough data to converge. While we observed this consistently with models trained using SGD, it may indeed happen when other methods are used to learn the coefficients from insufficient data. On the other hand, different methods may yield different models when training ends prior to convergence.

We have updated the paper with additional results that shed more light on the sources of bias in linear models. Figure 3 in the appendix depicts the bias of classifiers trained using the same data as in Figure 2, including LR trained with either L-BFGS or SGD, linear SVM trained with either SMO or SGD, and SGD using modified Huber and squared hinge losses. In short, while LR trained with L-BFGS does exhibit some bias, it is not as pronounced or consistent as it is in models trained with SGD, whereas all the models trained with SGD exhibited nearly identical bias trends. In slightly more detail, LR trained without SGD was less sensitive to the number of weak features, i.e., there was less bias than LR trained with SGD until there was a sufficiently high number of weak features, and even then, the effect was not as strong. Furthermore, SVM trained without SGD exhibited essentially no such bias, while SVM trained with SGD exhibited the same bias as LR with SGD. These results suggest that while the bias-inducing behavior may occur when other methods are used, they consistently follow from the use of SGD.

Thank you for your feedback on the related work section, we have moved it to the front of the paper as suggested.

Thank you for your comment about L1 versus experts method parameters--upon review, the wording in the experiments section is not clear. We did use the same procedure for finding the hyperparameter for L1 regularization as for the experts technique, i.e. we optimized for minimizing bias subject to the constraint that accuracy should not decrease from the original model. You may have noticed that on the glioma dataset, the accuracy goes down for L1. We conjecture that this is caused by the hyperparameter not generalizing well to the test data, as we evaluated hyperparameters on the training data. We have updated the writing in Section 4 to clarify this.

It’s not immediately clear what distinguishes the prostate data from the others, but upon inspection, prostate has a rather high Mahalanobis distance between classes compared to many of the other datasets. This might suggest there was a lot of room for improvement on this dataset (i.e., the bias was largely preventable because the classes are well-separated). Like most of the other datasets, prostate had a huge disparity in the number of data points (small) to features (large), so it is perhaps unsurprising that despite having the classes fairly well-separated in its feature space, a model with no regularization was unable to generalize well on it. Furthermore, prostate was the only dataset for which the feature disparity opposed the prior bias (and moreover the bias went in the direction of the features rather than the prior), so perhaps the feature-wise bias was the most significant source of bias in this example. It may be an interesting avenue for future work to investigate whether, e.g., Mahalanobis distance between classes, is a good predictor for the effectiveness of our techniques on real data.

In Section 3 (previously Section 2), paragraph 2, we state the goal (minimizing 0-1 loss) of the “standard binary classification problem,” not the overall goal of our paper. In fact, our goal is not exactly to generally minimize bias along with loss; we note that there are multiple possible sources of bias, only some of which are avoidable when optimizing accuracy. Namely, as stated in Theorem 1, an optimal classifier may necessarily be biased in some cases. Our goal is to remove bias that is not “necessary” in this way, which is not easily captured by additional terms in the training objective. Our work identifies feature-wise bias as one type of preventable or “unnecessary” bias, and attempts to remove it in a targeted fashion with post-hoc feature selection. In other words, we want our model to be no more biased than the most accurate predictor, which may still have some bias according to Theorem 1 (but we consider this bias unavoidable because it can be considered equally problematic to sabotage accuracy in order to reduce bias).

Thank you for your minor comments as well, we have addressed them in the updated paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_r1xgFKumnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>some insights on predictive bias and weak features</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1ecm2C9K7&amp;noteId=r1xgFKumnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1382 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1382 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper provides some new insights into classification bias. On top of the well known unbalanced group size, it shows that a large number of weak but asymmetry weak features also leads to bias. This paper also provides a method to reduces bias and remain the prediction accuracy.

In general, the paper is well written, but some description can be clearer. Some notation seems inconsistent. For example, D in equation (1) denotes the joint distribution (x,y), but it also refers to the marginal distribution of x somewhere else. 

In the high level, I am not totally convinced of how significant the result is.  In particular, the bias this paper defines is on the probability (softmax) scale, but logistic regression is on logit scale--   not even aimed at the unbiasedness in the original scale. So the result in section 2 seems to be expected.  Given the fact that unbiasedness is not invariant under transformation, I am wondering why it should be the main target in the first place.  

In the bias reduction methods in equation 5 and 6, both the objective function and the constraint are empirical estimations. Will it be too noisy to adapt to the high dimensional setting? On the other hand, adding some sparsity regularization improves prediction seems well known in practice.

I would also encourage the authors to have extended work both theoretically and experimentally.  The asymmetry feature is only illustrated by a single logistic regression. Is it a problem of weak features, or indeed a problem of logistic regression? What will happen in a more general case beyond mean-field Gaussian?  I would imagine in this simple case the authors may even derive the closed form expression to verify their heuristics.  

Based on the evaluations above, I would recommend a weak reject. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1xctjvOaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for your thoughtful feedback</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=S1ecm2C9K7&amp;noteId=S1xctjvOaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1382 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1382 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">While logistic regression is often on the logit scale, we tried to consistently use the probability scale in our analysis and experiments. If the paper contains any inconsistencies on this matter, we would appreciate knowing where they appeared so that we can address them. However, we would like to better understand the reviewer’s concern about unbiasedness failing to be invariant under transformation, and how we could have otherwise targeted our approach to better address the problem. With additional details, we hope to be able to address your concern.

In (7) (formerly 6), we are minimizing the bias of the model over the choices of alpha and beta subject to not harming accuracy. It is true that when optimizing, the bias and accuracy of the model are necessarily obtained via an empirical estimation, so it is possible that the alpha and beta chosen wouldn’t generalize well to the test data. We treated these as normal hyperparameters in our experiments. The numbers reported in Table 1 report the bias and accuracy on the test data, while the optimization problem from (7) was solved on the training data, so we are reasonably confident that in practice the optimal alpha and beta generalize well, even in high-dimensional settings.

Our aim was to identify the phenomenon of feature-wise bias on a class of problems that are sufficiently controlled so that we can make reasonable conclusions about the source of the bias. In the general case, beyond mean-field Gaussian, it may be harder to identify the source of the bias, as many sources may be interacting at once (e.g., feature-wise, class-imbalance, correlated features, etc.). We believe the results in Table 1 shed some light on the general case, namely, the bias is typically in the direction of the feature imbalance, even when this is at odds with the prior bias (as is the case in prostate). Furthermore, on some of the datasets (arcene in particular), balancing the number of features was quite effective at removing bias while improving accuracy, suggesting that a reasonable portion of the bias was caused by feature asymmetry.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>