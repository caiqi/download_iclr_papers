<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Robust Text Classifier on Test-Time Budgets | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Robust Text Classifier on Test-Time Budgets" />
        <meta name="citation_author" content="Md Rizwan Parvez" />
        <meta name="citation_author" content="Tolga Bolukbasi" />
        <meta name="citation_author" content="Kai-Wei Chang" />
        <meta name="citation_author" content="Venkatesh Saligrama" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1xLsjAqtX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Robust Text Classifier on Test-Time Budgets" />
      <meta name="og:description" content="In this paper, we design a generic framework for learning a robust text classification model that achieves accuracy comparable to standard full models under test-time&#10;  budget constraints. We take a..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1xLsjAqtX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Robust Text Classifier on Test-Time Budgets</a> <a class="note_content_pdf" href="/pdf?id=H1xLsjAqtX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=rizwan%40cs.ucla.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="rizwan@cs.ucla.edu">Md Rizwan Parvez</a>, <a href="/profile?email=tolgab%40bu.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="tolgab@bu.edu">Tolga Bolukbasi</a>, <a href="/profile?email=kwchang%40cs.ucla.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="kwchang@cs.ucla.edu">Kai-Wei Chang</a>, <a href="/profile?email=srv%40bu.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="srv@bu.edu">Venkatesh Saligrama</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=H1xLsjAqtX" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">In this paper, we design a generic framework for learning a robust text classification model that achieves accuracy comparable to standard full models under test-time
budget constraints. We take a different approach from existing methods and learn to dynamically delete a large fraction of unimportant words by a low-complexity selector such that the high-complexity classifier only needs to process a small fraction of important words. In addition, we propose a new data aggregation method to train the classifier, allowing it to make accurate predictions even on fragmented sequence of words. Our end-to-end method achieves state-of-the-art performance while its computational complexity scales linearly with the small fraction of important words in the whole corpus. Besides, a single deep neural network classifier trained by our framework can be dynamically tuned to different budget levels at inference time.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Data Aggregation, Budget Learning, Speed  Up, Faster Inference, Robust Classifier</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Modular framework for document classification and data aggregation technique for making the framework robust to various distortion, and noise and focus only on the important words. </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Hye2Gd_Npm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xLsjAqtX&amp;noteId=Hye2Gd_Npm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper623 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper623 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1xKKvdT2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The idea of jointly training a word/phrase/sentence mask with the classifier at training time (with some added complexity) to get faster predictions at test time is interesting; however, current results are largely preliminary and requires refinement of performance and understanding</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xLsjAqtX&amp;noteId=B1xKKvdT2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper623 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper623 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors consider the setting of training a RNN-based text classification where there is a resource restriction on test-time prediction (e.g., time). Conceptually, the approach is essentially a masking mechanism to reduce the number of words/phrases/sentences used in the prediction at test time followed by a classifier trained to handle these â€˜missingâ€™ components. This robustness is achieved by a data aggregation approach that trains on several masked variants determined by the budget level to generalize when encountered new masked instances. This work is most similar to finding â€˜rationalesâ€™ in text (e.g., [Lei, Barzilay &amp; Jakkola; EMNLP16]) except the selector is much simpler (specifically, using â€˜shallowâ€™ learners) and is compensated for with the robust classifier component. Experiments are conducted on four datasets, demonstrating varying levels of performance improvements over baseline and competing methods in terms of accuracy and test-time speedup â€” followed by more in-depth experiments on multi-aspect sentiment and IMDB datasets, resulting in several more detailed observations.

From a high-level perspective, reducing resource requirements specifically at test-time is ostensibly useful in practice as many applications would likely be willing to trade-off some accuracy for {faster performance, lower power requirements, reduced communication bandwidth}, etc. Furthermore, the approach described is sensible and results in some promising empirical findings. My primary concerns with this specific paper are two-fold: (1) the experiments seem mixed and somewhat difficult to interpret in terns of what I would expect to work on a new dataset (2) the tradeoffs arenâ€™t well calibrated in the sense that I donâ€™t know how to set them easily (as everything is performed retrospectively, even if tuned on a dev set, it doesnâ€™t seem trivial to set tradeoffs).  Some of this was considered with additional empirical results, but these also didnâ€™t have sufficient clarity nor discussion for me to really understand the recommendation. Accordingly, as is, this seems like a promising idea with solid preliminary evidence of proposed configurations ostensibly working on multiple datasets, but neither a strong theory nor empirical understanding of the dynamics of the proposed methods.

Evaluating the paper along the requested dimensions:

= Quality: The number of datasets are sufficient and the results are convincing for some configurations. Thus, the authors have established that the method is practical and is promising. However, there isnâ€™t an in-depth study of when the proposed method works, under what configurations, and discussion on how to push this forward. One way to make this more transparent would be to include some curves wrt Table 1 such that I can observe the performance/speedup tradeoffs (is it monotonic?). A good start, but seems incomplete to be really convincing. (4/10)

= Clarity: The writing is fairly clear and well-contextualized wrt existing work. I do think that the discussion could be improved overall in terns of better telling the reader what they should be focusing on in the empirical results. My one specific issue was I still donâ€™t understand what is in the parentheses of {LSTM-jump, skim-RNN} in Table 1. Maybe it is in the text, but I really couldnâ€™t decode (and I can understand the semantics of parentheses in other parts of the table). (4/10)

= Originality: I havenâ€™t seen anything specifically like this, even if it does draw on very closely related methods. However, I think putting them together required good knowledge from ML and NLP communities, and thus wasnâ€™t trivial and the basic idea is clever. (6/10)

= Significance: The underlying idea is interesting and could be significant if further developed, both in terms of better understanding the dynamics and providing better guidance regarding how to do this well in practice. However, just based on the content of the paper, I might play with this, but donâ€™t see it as a solution to problems in low-resource settings, mixed-computation networks (like the cloud example in 4.2). Interesting, but needs more development to be widely adopted. (5/10)

=== Pros ===
+ an interesting idea that pragmatically builds on existing work to provide a practical solution
+ experiments conducted on several datasets with some interesting results

=== Cons ===
- empirical results mixed and little clarity provide on why
- other than speedup, doesnâ€™t consider other settings in experiments (e.g., power, compression)
- little theoretical development
- nothing said about training time difference (even if not the point)

Overall, I like the direction as reducing amortizing computation by focusing on adding resources to training time to reduce during testing time (when most of the volume of predictions should occur). However, I think the idea needs to be further developed before being published at a top-tier venue.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BylvXdrThm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A co-trained logistic regression can discover task-dependent stop words</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xLsjAqtX&amp;noteId=BylvXdrThm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper623 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper623 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper is unacceptable to me in current form.  I would find it acceptable if it were rewritten to be "a much more straightforward description and demonstration of how a simple strategy is competitive with (unnecessarily) complex approaches on an important problem."

I found this paper's layout and exposition frustrating.  There is a space for papers that demonstrate simple approaches to complicated problems are competitive (e.g., Hellinger PCA for word embeddings).  Such papers, when done well, 1) clearly indicate the importance of the problem, 2) propose a simple solution, and 3) show it is competitive.   Instead, this paper 1) defers important motivation (e.g., federated mobile/cloud computation) to the end of the experiment section, 2) disguises a simple solution as complex via excessively ornate discussion, and 3) mostly fails to show it is competitive.

Bag-of-words uses a one-hot vocabulary encoding of the input but trains directly on the task rather than indirectly via the classifier.  Whereas, "word embedding" co-trains with the classifier but uses word embeddings.  I'm really curious to see whether directly training on the class label using the word embedding representation with a sparsity-enforcing regularizer would do just as well as "word embedding" eliminating the need for co-training.  In any event, it would be more convincing baseline.

I don't understand how the data aggregation step works when the selector and classifier are co-trained, since it appears to assume a collection of selectors is already present before training the classifier.

None of the experimental results are compelling to me.  The differences seem so slight that I would prefer the simplest approach, which is the L1 regularized logistic regression for feature selection.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1x_fuK53m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting idea, more experiments needed</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xLsjAqtX&amp;noteId=S1x_fuK53m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper623 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper623 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a framework for fast text classification that relies on a text selector and classifier that are jointly optimized using an aggregation scheme for the output of the text selector. This has an interesting application in cases where the text selector has to be implemented in a low performance device and the classification can be done on the cloud. The approach the authors take is interesting and different from what has done before, however I believe some further details / tests are needed to make this paper more convincing.

It doesnâ€™t seem like the model has a clear advantage over existing methods. The advantage of the method could be brought up by comparing other complexity metrics such as how many words need to be transmitted (this is shown in figure 2 but not in relation to other approaches), what is a possible other advantage over the skim-RNNs etc. The interpretability of the results is encouraging and perhaps this could be explored more and leveraged as an advantage.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>