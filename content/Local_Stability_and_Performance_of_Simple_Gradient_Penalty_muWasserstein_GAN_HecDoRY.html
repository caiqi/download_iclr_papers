<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Local Stability and Performance of Simple Gradient Penalty $\mu$-Wasserstein GAN | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Local Stability and Performance of Simple Gradient Penalty $\mu$-Wasserstein GAN" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1ecDoR5Y7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Local Stability and Performance of Simple Gradient Penalty..." />
      <meta name="og:description" content="Wasserstein GAN(WGAN) is a model that minimizes the Wasserstein distance between a data distribution and sample distribution. Recent studies have proposed stabilizing the training process for the..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1ecDoR5Y7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Local Stability and Performance of Simple Gradient Penalty $\mu$-Wasserstein GAN</a> <a class="note_content_pdf" href="/pdf?id=H1ecDoR5Y7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019local,    &#10;title={Local Stability and Performance of Simple Gradient Penalty $\mu$-Wasserstein GAN},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=H1ecDoR5Y7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=H1ecDoR5Y7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Wasserstein GAN(WGAN) is a model that minimizes the Wasserstein distance between a data distribution and sample distribution. Recent studies have proposed stabilizing the training process for the WGAN and implementing the Lipschitz constraint. In this study, we prove the local stability of optimizing the simple gradient penalty $\mu$-WGAN(SGP $\mu$-WGAN) under suitable assumptions regarding the equilibrium and penalty measure $\mu$. The measure valued differentiation concept is employed to deal with the derivative of the penalty terms, which is helpful for handling abstract singular measures with lower dimensional support. Based on this analysis, we claim that penalizing the data manifold or sample manifold is the key to regularizing the original WGAN with a gradient penalty. Experimental results obtained with unintuitive penalty measures that satisfy our assumptions are also provided to support our theoretical results.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">WGAN, gradient penalty, stability, measure valued differentiation</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">This paper deals with stability of simple gradient penalty $\mu$-WGAN optimization by introducing a concept of measure valued differentiation.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SylIqtVQp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>rigorous math with heavy machinery but not well motivated</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ecDoR5Y7&amp;noteId=SylIqtVQp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper291 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">10 Nov 2018</span><span class="item">ICLR 2019 Conference Paper291 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Based on a dynamic system perspective, this paper characterizes the convergence of gradient penalized Wasserstein GAN. The analytic framework is similar to the one used in Nagarajan &amp; Kolter but requires very heavy machinery to handle measure valued differentiation. Overall the math seems solid but I have a few questions about the motivation and assumption. 

1. To my limited knowledge, it seems that the two-time-scale framework [1] handles both batch and stochastic settings well also from a dynamic system perspective. I am wondering why not follow their path since under their framework adding a gradient penalty does not introduce all the technical difficulty in this paper. 

2. The main theorem characterizes the stability or convergence but does not characterize the advantage of gradient penalty. Does it make the system more stable? At least more technical discussion around the theorem is needed. 

3. Besides the technicality of handling the support of the measure, what is new beyond the analysis of Nagarajan &amp; Kolter?

[1] GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium
by Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter

I may be missing something and would like to see the author's response. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1xXUvGn6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ecDoR5Y7&amp;noteId=B1xXUvGn6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper291 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper291 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We sincerely thank for reviewerâ€™s kind and thoughtful comments to our paper. The comments will be helpful to improve our work.

1. Our work was motivated from the analysis of [1] and [2]. We mainly focused on general methods (general penalty measures) for giving gradient penalty; therefore, our work chose the simplest dynamic system setting of [1], which is based on simultaneous gradient descent (SGD) algorithm. We believe that extensions based on [3] with our simple gradient penalty term are also possible extensions and future works of ours. For the technical difficulties, we believe that TTUR scheme might have the same problem as in our case, since the differentiability of penalty term(E_\mu [||\nabla_x D||^2]) in dynamic system must be guaranteed.

2. Compared with WGAN, we showed in experiment that the system with gradient penalty term becomes stable. For a theoretic sense, penalty term also helps to make dynamic system much more stable. This can be analyzed by observing the real part of eigenvalues of Jacobian of the dynamic system. Roughly speaking with no technical details, without penalty term (rho = 0 case), the upper-left block of Jacobian matrix becomes zero, which makes Jacobian matrix fail to obtain eigenvalues with negative real part as mentioned in [1]. Due to the penalty term in our dynamic system, the upper-left block in the Jacobian matrix is negative-semidefinite, which makes the real part of eigenvalues of Jacobian negative. (Zero eigenvalue of our dynamic system has no effect on the stability of the system by assumption 2 in our work)

3. Comparing with previous work of [1], our new point is suggesting a mathematical tool for handling abstract singular cases in theoretical analysis, such as singular penalty measure with lower-dimensional support. 

We want to extend results of [2] for even singular penalty measures, and one crucial tool is a concept of 'Measure-Valued differentiation', which defines the parametric measure's derivative with respect to their parameter(\psi) by weak-form and makes it possible to analyze the 1st and 2nd derivative of integral term with singular measure in a closed form

While computing Jacobian of the dynamic system, the difficulty in our analysis lies in handling the 1st and 2nd derivatives of integral term(E_\mu [||\nabla_x D||^2]) with respect to discriminator's parameter, which cannot be analyzed in a closed form with previous existing analysis. Previous works assumed absolutely continuous case which has differentiable probability density function [1], or penalty measure is constant with respect to discriminatorâ€™s parameter such as p_d and p_g(p_\theta) [2].
. 
[1] Vaishnavh Nagarajan and J. Zico Kolter. Gradient Descent GAN optimization is locally stable. In Advances in Neural Information Processing Systems, pp. 5591-5600, 2017.
[2] Lars M. Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do actually converge? In Proceedings of the 35th International conference on Machine Learning, pp 3478-3487, 2018.
[3] Martin Heusel, Hubert Ramsauer, Thomas Unterhiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances in Neural Information Processing Systems, pp. 6629-6640, 2017.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BylhohL6hQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>assumptions need better justification</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ecDoR5Y7&amp;noteId=BylhohL6hQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper291 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper291 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper shows that an ideal equilibrium point of a SGP-WGAN is stable. It makes several assumptions that, while clear why they are needed in the proof, is unjustified in practice. The authors should elaborate on these assumptions and comment on why they are reasonable. 

Assumptions 1 and 3 essentially say that there is a tube (both in sample space and in parameter space) around the true data generating distribution in which the discriminator cannot distinguish. This seems a strong restriction to the effect of the discriminator is weak. For example, Assumption 1 says if given a sample slightly off the data manifold, it still cannot distinguish at all. A more reasonable assumption is the ability of the discriminator decays gracefully as samples approach the data manifold.

Assumption 2 is also unjustified. Its main effect seems to be to eliminate a few terms in the projected Jacobian in the proof, but its relevance and whether it is reasonable in practice is entirely unmentioned.

Finally, it is unclear why this notion of ``measure valued differentiation'' is needed. First, differentiation in measure spaces is no different from differentiation in other infinite dimensional functions spaces: the usual notions of Gateaux and Frechet differentiability apply. Second, the derivatives in questions are not true ``measure-derivatives'' in the sense that the argument to the function being differentiated is not a measure, it is a finite dimensional parameter. In the end, this seems essentially a derivative of a multi-variate function.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1e2yLG2am" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ecDoR5Y7&amp;noteId=H1e2yLG2am"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper291 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper291 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We sincerely thank for reviewerâ€™s kind and thoughtful comments to our paper. The comments will be helpful to improve our work.

The main concerns about our work are summarized into two issues: 
1) Assumptions of the main theorem and its real-world interpretations
2) The concept of Measure-Valued Differentiation (MVD)
 
â—† We have intended to omit the interpretation of the assumption 1 ~ 4 since most of them are already studied deeply in previous works of [1] and [2]. We worried the repetition of such interpretation would distract readability of our paper. However, as you kindly mentioned above, providing â€˜real-world meaningâ€™ of the mathematical assumptions will possibly improve the quality of our work. We are willing to do so if it is needed.  

â—† The contribution of our work is providing general conditions of penalty measure area and parameters which ensuring the local stability of simple gradient penalty GAN system. The detailed explanations on assumption 6 are: 

- The penalty measure area approaches to data manifold and its weight changes smoothly with respect to both discriminator's and generator's parameter
- Ideal discriminator will remain flat on the penalty area.

We revised them and added the above interpretation to assumption 6.

â—† As you thoughtfully pointed out that the assumption 1 and 3 seem strong, our analysis mainly focuses on "realizable cases" with various gradient penalty area. Actually "non-realizable cases" would be very important issue, but in our paper we mainly focused on realizable case with gradient penalty for mathematical analysis, as mentioned in [1] and [2].

â—† For the Measure-Valued Differentiation, we employed this concept to deal with the differentiation of integral. Therefore, your comment

&gt;&gt; In the end, this seems essentially a derivative of a multi-variate function.

is reasonable to raise, since in fact we differentiate the real-valued integral with respect to finite-dimensional parameter.

However, the reason we brought measure-valued differentiation is to deal with the 1st and 2nd derivative of integral term (E_\mu [||\nabla_x D||^2]) with respect to discriminator's parameter. The existing analysis does not possess any mathematical concept to treat this term in a closed form. Previous works assumed absolutely continuous case, so that integral under the penalty measure can be represented into either Lebesgue integral and related probability density function or some simple cases (p_d or p_\theta, which are constant with respect to discriminator's parameter). 

Measure-valued differentiation enables us to define a derivative of the parametric measure with respect to its parameter in a â€˜weak formâ€™. This, in turn, makes it possible to deal with the 1st and 2nd derivative of integral term (penalty term) in a closed form.

Therefore, this is different from the concept of "Gateaux/Frechet Derivative". in general measure space. Two concepts deal with general derivatives in Banach Space (specifically in a space of finite signed measures on R^n), while our measure-valued differentiation concept only concerns about derivative of a parametric probability measure with respect to finite dimensional parameter.

Ref
[1] Vaishnavh Nagarajan and J. Zico Kolter. Gradient Descent GAN optimization is locally stable. In Advances in Neural Information Processing Systems, pp. 5591-5600, 2017.
[2] Lars M. Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do actually converge? In Proceedings of the 35th International conference on Machine Learning, pp 3478-3487, 2018.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkg1ktuu3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Implications of local stability of dynamical system to "real world" setting not clear</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ecDoR5Y7&amp;noteId=rkg1ktuu3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper291 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper291 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In the paper, WGAN with a squared zero centered gradient penalty term w.r.t. to a general measure is studied. Under strong assumptions, local stability of a time-continuous gradient ascent/descent dynamical system near an equilibrium point are proven for the new GP term. Experiments show comparable results to the original WGAN-GP formulation w.r.t. FID and inception score.

Overall, I vote for rejecting the paper due to the following reasons:
- The proven convergence theorem is for a time-continuous "full-batch" dynamical system, which is very far from what happens in practice (stochastic + time discrete optimization with momentum etc). I don't believe that one can make any conclusions about what is actually happening for GANs from such an idealized setting. Overall, I don't understand why I should care about local stability of that dynamical system.
- Given the previous point I feel the authors draw too strong conclusions from their results. I don't think Theorem 1 gives too many insights about the success of gradient penalty terms.
- There are only marginal improvements in practice over WGAN-GP when using other penalty measures. 

Further remarks:
- In the introduction it is claimed that mode collapse is due to JS divergence and "low-dimensionality of the data manifold". This is just a conjecture and the statement should be made more weak.

- The preliminaries on measure theory are unnecessarily complicated (e.g. partly developed in general metric spaces). I suggest that the authors try to simplify the presentation for the considered case of R^n and avoid unnecessarily complicated ("mathy") definitions as they distract from the actual results. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BylHr_fna7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to AnonReviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1ecDoR5Y7&amp;noteId=BylHr_fna7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper291 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper291 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We sincerely thank for reviewerâ€™s kind and thoughtful comments to our paper. The comments will be helpful to improve our work.

â—† As the reviewer kindly pointed out, our work mainly focuses on 'theoretical analysis' of simple gradient penalty WGAN system. Our work on local-stability seem to treat a restricted topic since majority of practitioners will care convergence on their numerous experimental setting.

However, although local-stability does not necessarily implicate the global stability, we still argue that there should be the least (or the most general) boundary for local stability. Study of local stability is a foundation on understanding the global stability of a model. If one algorithm is not even locally stable, then it may potentially fail in practice, regardless of practical factors such as learning rate, optimizers and update times. In this sense, we believe that such mathematical analysis of our work could make some contribution to GAN study.

â—† The reviewer pointed out continuous-time dynamic system is an asymptotic tool to analyze discrete update system. By asymptotic property we mean as learning rate close to zero, a discrete update system can be modeled by continuous-time dynamic system. 
In GAN study, typical choice of learning rate is a small enough value (1e-2 ~ 1e-5) which does not harm convergence of a system. Furthermore, there are references which argue there is a marginal discrepancy between continuous- and discrete-time analysis. To the best of our knowledge, as mentioned in [2], Simultaneous Gradient Descent (SGD) in [1] and Alternating Gradient Descent (AGD) in [2] showed both theoretical stability and similar results in experiment. Therefore, we chose SGD method to model our simple gradient penalty WGAN system as in [1].
In the same manner, we believe a mini-batch setting is an asymptotic analysis of a full-batch setting. Ideally, statistics of mini-batch and full-batch are almost the same. In practice, we argue there is a marginal stochasticity between mini-batch and full-batch, which yet does not harm the results of our work. If our simple gradient penalty WGAN system is locally stable with full-batch setting, then our work can be extended to more complex cases, the work that is already studied in [4].
â—† For the comment on the introduction, which says that the remark on mode-collapse is too strong, we agree and revise it in a weaker form.

â—† We also fixed the mathematical definitions about measure-theoretic things into R^n cases. As the reviewer pointed out, some of them were written in unnecessary 'mathy' words. 

â—† Even in practice, it is not guaranteed that every expectation is represented with probability density or mass function. As a result, expressing optimization procedure of such expectations through gradient descent algorithm (or its variants) is not trivial. Namely, gradient of the expectations cannot be represented in a closed form.
  
To extend our understanding of GAN convergence on these singular cases, we expect measure-theoretic approaches will facilitate the study of GANâ€™s stability with singular penalty measures.

â—† The main point of our work is to provide theoretical reasons and basis for the success of previously proposed GAN regularization methods ([2],[3]) with gradient penalty term. We understand our experiments can be reflected as a marginal improvement from the existing studies. However, we appeal to the reviewer that our work is a theoretical work and the experiments confirm our main theorem even on unintuitive cases. Furthermore, these results are theoretically guaranteed for even singular penalty measures. 

In this sense, Theorem 1 still has meaning that
 &gt; Simple gradient penalty GAN (WGAN) system for ideal (realizable) case and suitable assumptions on penalty measure(penalty area should cover data manifold at the equilibrium and the ideal discriminator remains flat on the penalty area) is at least locally stable.
 &gt; Penalizing method was generalized in abstract form, which can explain good results of recently proposed gradient-penalty based methods.

Ref
[1] Vaishnavh Nagarajan and J. Zico Kolter. Gradient Descent GAN optimization is locally stable. In Advances in Neural Information Processing Systems, pp. 5591-5600, 2017.
[2] Lars M. Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do actually converge? In Proceedings of the 35th International conference on Machine Learning, pp 3478-3487, 2018.
[3] Ishaan Gulrajani, Faruk ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C. Courville. Improved training of Wasserstein GANs. In Advances in Neural Information Processing Systems, pp 5769-5779, 2017.
[4] Martin Heusel, Hubert Ramsauer, Thomas Unterhiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances in Neural Information Processing Systems, pp. 6629-6640, 2017.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>