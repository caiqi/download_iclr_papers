<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Towards a better understanding of Vector Quantized Autoencoders | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Towards a better understanding of Vector Quantized Autoencoders" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HkGGfhC5Y7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Towards a better understanding of Vector Quantized Autoencoders" />
      <meta name="og:description" content="Deep neural networks with discrete latent variables offer the promise of better symbolic reasoning, and learning  abstractions that are more useful to new tasks. There has been a surge in interest..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HkGGfhC5Y7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Towards a better understanding of Vector Quantized Autoencoders</a> <a class="note_content_pdf" href="/pdf?id=HkGGfhC5Y7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019towards,    &#10;title={Towards a better understanding of Vector Quantized Autoencoders},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HkGGfhC5Y7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=HkGGfhC5Y7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Deep neural networks with discrete latent variables offer the promise of better symbolic reasoning, and learning  abstractions that are more useful to new tasks. There has been a surge in interest in discrete latent variable models,  however, despite several recent improvements, the training of discrete latent variable models has remained  challenging and their performance has mostly failed to match their continuous counterparts. Recent work on vector quantized autoencoders (VQ-VAE) has made substantial progress in this direction, with its perplexity almost matching that of a VAE on datasets such as CIFAR-10. In this work, we investigate an alternate training technique for VQ-VAE, inspired by its connection to the Expectation Maximization (EM) algorithm. Training the discrete autoencoder with EM and combining it with sequence  level knowledge distillation alows us to develop a non-autoregressive machine translation model whose accuracy almost matches a strong greedy autoregressive baseline Transformer, while being 3.3 times faster at inference.
</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">machine translation, vector quantized autoencoders, non-autoregressive, NMT</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Understand the VQ-VAE discrete autoencoder systematically using EM and use it to design non-autogressive translation model matching a strong autoregressive baseline.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">10 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HkeSbl7j6X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting parts, but needs more rigour</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkGGfhC5Y7&amp;noteId=HkeSbl7j6X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1247 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1247 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper discusses VQ-VAE for learning discrete latent variables, and its application to NMT with a non-autoregressive decoder to reduce latency (obtained by producing a number of latent variables that is much smaller than the number of target words, and then producing all target words in parallel conditioned on the latent variables and the source text). The authors show the connection between the existing EMA technique for learning the discrete latent states and hard EM, and introduce a Monte-Carlo EM algorithm as a new learning technique. They show strong empirical results on EN-DE NMT with a latent Transformer (Kaiser et al. (2018)).

The paper is clearly written (excepting the overloaded appendix), and the individual parts of the paper are interesting, including the link between VQ-VAE training and hard EM, the Monte-Carlo EM, and strong empirical results. I'm less convinced that the paper as a whole delivers on what it promises/claims.

The first contribution of the paper is that it shows a simple VQ-VAE to work well on the EN-DE NMT task, in contrast to the results by Kaiser et al. (2018). The paper attributes this to tuning of the code-book, but the results (table 3) seem to contradict this, with a code-book size of 2^16 even slightly better than the 2^12 that is used subsequently. The reason for the performance difference to Kaiser et al. (2018) remains opaque. While interesting, the empirical effectiveness of Monte-Carlo EM is a bit disappointing, achieving +0.3 BLEU over the best configuration for EN-DE (after extensive hyperparameter tuning, seen in table 4), and -0.1 BLEU on EN-FR. Monte-Carlo EM also seems very sensitive to hyperparameters, namely the sample size (tables 4,5), contradicting the later claim that EM is robust to hyperparameters. The last claimed contribution (using denoising techniques) is hidden in the appendix, an application of an existing technique, and not compared to knowledge distillation (another existing technique).

I'd like to see some of the results in the paper published eventually. However, the claims need to better match the empirical evidence, and for a paper that has "better understanding" in the title, I'd like to gain a better understanding of the differences to Kaiser et al. (2018) that make VQ-VAE fail for them, but not in the present case.

+ clearly written paper
+ interesting, novel EM algorithm for VQ-VAE
+ strong empirical results on non-autoregressive NMT

- the strong performance of the VQ-VAE baseline remains unexplained, and the claimed explanation contradicts empirical results.
- the new EM algorithm gives relatively small improvements, with hyperparameters that were likely selected based on test set scores .
- most of the empirical gain is attributable to knowledge distillation, which is not a novel contribution</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkeQ2ZXspm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>reading other reviews/comments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkGGfhC5Y7&amp;noteId=rkeQ2ZXspm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1247 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1247 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">This was an extra review requested after the end of the official review period; now looking at the other reviews and replies, I can see that the question as to whether hyperparameters were optimized on the test set was already addressed. I stand by the comment that obtaining this small improvement required extensive hyperparameter tuning, which devalues it slightly.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BygRDlrs6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Reviewer 4</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkGGfhC5Y7&amp;noteId=BygRDlrs6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1247 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1247 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for a careful reading of our paper and for their thoughtful review. Below we address the specific points raised by the reviewer:

&gt;&gt;&gt;
The first contribution of the paper is that it shows a simple VQ-VAE to work well on the EN-DE NMT task, in contrast to the results by Kaiser et al. (2018)...
&lt;&lt;&lt;

The main difference between the setup of Kaiser et al (2018) and the current work is the point "Attention to Source Sentence Encoder" in the Analysis section. The discrete latents in Kaiser et al (2018) are a function ae(x, y) where x is the input sequence and y is the target sequence. The dependence on x is in the form of attention layers. This makes it a much more complex function to learn and the authors of that work report that VQ indeed did not work for them, and so they had to resort to Product Quantization (referred to as DVQ in their work) with multiple codebooks to get a good result. We found that the attention to source sequence x to be an unnecessary complication, and so our latents are just a function ae(y), where y is the target sequence.

We do not attribute this to tuning the code-book size, we apologize for the misunderstanding. The robustness of EM is in the case when the latents ae(x, y) are also a function of x, see Figure 5 in the Appendix (" Comparison of VQ-VAE (green curve) vs EM with different number of samples (yellow and blue curves) on the WMT’14 English-German translation dataset with a codebook size of 2 14, with the encoder of the discrete autoencoder attending to the output of the encoder of the source sentence as in Kaiser et al. (2018).") The optimization problem is much harder in this case and we see that the VQ runs collapse while various versions of EM (with different number of samples) still give a good result.  The EM version does depend on the number of samples, but is much more stable compared to VQ even when the latents are a function of x. 

We apologize if the "Attention to source sentence encoder" was not adequately clear: we had a statement to the effect of "Also, removing this attention step results in more stable training particularly for large code-book sizes, see e.g., Figure 3.", but it unfortunately seems to have got lost in a revision..

&gt;&gt;&gt;
The last claimed contribution (using denoising techniques) is hidden in the appendix...
&lt;&lt;&lt;

Denoising autoencoders as used by Lample et al., were used in the context of learning better initial representations for unsupervised MT. We found that applying it to the context of discrete autoencoders like VQ-VAE can give some improvements
 in larger datasets like En-Fr. For En-De denoising VQ-VAE did not give us any improvement over VQ-VAE. We do not claim we invented denoising autoencoders, we write: 

"On the larger English-French dataset, we show that denoising discrete autoencoders gives us a significant improvement (1.0 BLEU) on top of our non-autoregressive baseline (see Section D)"

&gt;&gt;&gt;
I'd like to see some of the results in the paper published eventually...
&lt;&lt;&lt;

We hope our first paragraph addresses the question of why VQ-VAE did not work in Kaiser et al. without product quantization, but worked in our case. We have made this more explicit in the latest version. Also note that all our VQ-VAE runs for MT do not have the attention to source sequence x, except Figure 5 where we explicitly mention this.

&gt;&gt;&gt;
- the strong performance of the VQ-VAE baseline remains unexplained, and the claimed explanation contradicts empirical results.
&lt;&lt;&lt;

We hope that the previous paragraphs and the new draft addresses this concern. 

&gt;&gt;&gt;
- the new EM algorithm gives relatively small improvements, with hyperparameters that were likely selected based on test set scores .
&lt;&lt;&lt;

The hyperparameters were selected on WMT'13 and the results are reported on WMT'14. EM gives small improvements with knowledge distillation, because the optimization problem is much easier in this case. When the optimization problem is harder we see more gains from EM: 

1) In the setting when the latents are informed by the source sequence x, EM is much more stable than VQ-VAE (Figure 5) 
2) In the case when knowledge distillation is not used it gives a gain of +1.0 BLEU 
3) When the hidden dimension is smaller (256 or 384) instead of 512, we see gains of +1.3 BLEU and +0.6 BLEU respectively.

&gt;&gt;&gt;
- most of the empirical gain is attributable to knowledge distillation, which is not a novel contribution
&lt;&lt;&lt;

That is a valid point, and we did indeed find knowledge distillation to be very important for good performance for NMT in addition to removing the attention to source sequence x.  


</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_H1xD-S0dh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a> Training procedure for VQ-VAE is equivalent to the EM algorithm</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkGGfhC5Y7&amp;noteId=H1xD-S0dh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1247 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1247 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">General:
The paper presents an alternative view on the training procedure for the VQ-VAE. The authors have noticed that there is a close connection between the original training algorithm and the well-known EM algorithm. Then, they proposed to use the soft EM algorithm. In the experiments the authors showed that the soft EM allows to obtain significantly better results than the standard learning procedure on both image and text datasets.

In general, the paper shows a neat link between the well-known EM algorithm and the learning method for the VQ-VAE. I like the manner the idea is presented. Additionally, the results are convincing. I believe that the paper will be interesting for the ICLR audience.

Pros:
+ The connection between the EM algorithms and the training procedure for the VQ-VAE is neat.
+ The paper is very well written, all concepts are clear and properly outlined.
+ The experiments are properly performed and all results are convincing.

Cons:
- The paper is rather incremental, however, still interesting.
- The quality of Figure 1, 2 and 3 (especially Figure 3) is unacceptable.
- There is a typo in Table 6 (row 5: V-VAE → VQ-VAE).
- I miss two references in the related work on training with discrete variables: REBAR (Tucker et al., 2017) and RELAX (Grathwohl et al., 2018).
- The paper style is not compliant with the ICLR style.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Byg4DS-Zp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to reviewer 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkGGfhC5Y7&amp;noteId=Byg4DS-Zp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1247 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1247 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for taking the time to read our paper and for the useful comments to help improve our presentation! We have increased the resolution of the images by moving some of them to the appendix, and hope that fixes the visibility issue for the figures. We have also fixed the typo - thanks for pointing it out! We have added the two references pointed out and have also fixed the bibliography style to be the ICLR style. Please let us know if we can improve anything else.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJeUvaXU2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A soft-EM training algorithm for vector-quantized autoencoders</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkGGfhC5Y7&amp;noteId=BJeUvaXU2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1247 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1247 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary: 

This paper presents a new training algorithm for vector-quantized autoencoders (VQVAE), a discrete latent variable model akin to continuous variational autoencoders.
The authors propose a soft-EM training algorithm for this model, that replaces hard assignment of latent codes to datapoints with a weighted soft-assignment.

Overall the technical writing in the paper is sloppy, and the presentation of the generative model takes the form of an algorithmic description of the training algorithm, rather than being a clear definition of the generative model itself.

The technical presentation of the work by the authors starts only at page 5 (taking less than a full page), after several pages of imprecise presentation of previous and related work. The paper could be significantly improved by making this preceding material more concise and rigorous. 

Quantitative experimental evaluation is limited to a machine translation task, which is rather uncommon in the literature on generative latent variable models. I would expect evaluation in terms of held-out data log-likelihood (ie bits-per-dimension) used in probabilistic generative models, and possibly also using measures from the GAN literature such as inception scores. Datasets that are common include CIFAR-10 and resized variants of the imagenet dataset. 	 


Specific comments:

- Please adhere to the ICLR template bibliography style, which is far more readable than the style that you used. 

- Figure 1 does not seem to be referenced in the text. 

- The last paragraph of section 2.1 is unclear. It mentions a sampling a sequence of latent codes. The notion of sequentiality has not been mentioned before, and it is not clear what it refers to in the context of the model defined so far up to that point. 

- The technical notation is very sloppy. 
* In numerous places the paper refers to the joint distribution P(x1,…,x_n, z1, …, zn) without defining that the distribution factorizes across the samples (xi,zi), and without specifying the forms of p(zi) and p(xi|zi). 
* This makes that claims such as “computing the expectation in the M step (Equation 11) is computationally infeasible” are not verifiable. 

- Please be clear about how much is gained by replacing the exact M-step with a the one based on the samples from the posterior computed in the E-step. 

- What is the reason to decode the weighted average of the embedding vectors, rather than decoding all of them, and updating the decoder in a weighted manner?

- reference 14 for Variational autoencoders is incorrect, please use the following citation instead: 
@InProceedings{kingma14iclr,
  Title                    = {Auto-Encoding Variational {B}ayes},
  Author                   = {D. Kingma and M. Welling},
  Booktitle                = {{ICLR}},
  Year                     = {2014}
}

- The related work section (4) provides a rather limited overview of relevant related work. 
Half of it is dedicated to recent advances in machine translation, which does not bear a direct connection to the technical material presented in section 3.

- There is no justification of using *causal* self-attention on the source embedding, is this a typo?

- As for the experimental evaluation results: it seems that distillation is a much more critical factor to achieve good performance than the proposed EM training of the VQ-VAE model. Unfortunately, this fact goes unmentioned when discussing the experimental results. 

- What is the significance of the observed differences in BLEU scores? Please report average performance and standard deviations over several runs with randomized parameter initialization and batch scheduling. 

- It seems that the tuning of the number of discrete latent codes (table 2 in appendix) and other hyper-parameters (table 3 in appendix) was done on the test set, which is also used to compare to related work. A separate validation set should be used for hyper parameter tuning in machine learning experiments.

- It seems that all curves in figure 3 collapse from about 45 BLEU to values around 17 BLEU, why is this? The figure is hard to read since poor quality, and curves that are superposed. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SylKiwqf6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Reviewer 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkGGfhC5Y7&amp;noteId=SylKiwqf6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1247 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1247 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for taking the time to read our paper. Below we address the specific points raised by the reviewer:

&gt;&gt;&gt;
Overall the technical writing in the paper is sloppy....
&lt;&lt;&lt;

In this work, we improve upon VQ-VAE to learn shorter latent representations of a target sentence in order to speed up MT, rather than to train a generative model. We achieve considerable speedup in decoding state of the art NMT models without much loss in BLEU (a universally accepted metric for translation quality), which has powerful implications for real world, production level MT systems. While evaluating the improvements of our training for generative modeling is interesting, our focus is on using VQ-VAE for a practical task. 

Moreover, we have now added a paragraph on the generative process (Page 3). We hope that this will clarify some of the content. We welcome the reviewer to share what they think is "sloppy" and "imprecise", and what would help us further improve the content of the paper.

&gt;&gt;&gt;
The technical presentation of the work by the authors starts only at page 5...
&lt;&lt;&lt;

Our goal is to use the autoencoder from VQ-VAE as a tool to compress the target sentence for fast decoding. We therefore chose to focus on the part of the algorithm, describing it's connection to hard-EM and our improvements on it using EM. We would appreciate concrete suggestions to improve the content.

&gt;&gt;&gt;
Quantitative experimental evaluation is limited to a machine translation task...
&lt;&lt;&lt;

The main focus of our work is to design a better non-autoregressive machine translation model and which is an area of active research (see for e.g., [1, 2, 3, 4]). None of those works evaluate their proposed method on datasets other than machine translation because the goal of their work is non-autoregressive MT. We do not care about generative modeling of images with VQ-VAE because plenty of other models do it much better (for e.g., a GAN/VAE/PixelCNN++). 

The keywords of our paper states: "machine translation, vector quantized autoencoders, non-autoregressive, NMT", while the TL;DR of our submission is "Understand the VQ-VAE discrete autoencoder systematically using EM and use it to design non-autogressive translation model matching a strong autoregressive baseline." 

&gt;&gt;&gt;
- The related work section (4) provides a rather limited overview of relevant related work...
&lt;&lt;&lt;

Again, the main aim of our work is to speed up the decoding for real world Neural Machine Translation (NMT) systems, which is an active area of research (see e.g., [1, 2, 3, 4]). We have focussed on generative models that are practically relevant to non-autoregressive NMT and because of page limitations we have not been able to include every paper on generative modeling. If we have missed relevant references we would appreciate if the reviewer would let us know what they are.
 
[1] <a href="https://openreview.net/forum?id=B1l8BtlCb" target="_blank" rel="nofollow">https://openreview.net/forum?id=B1l8BtlCb</a>
[2] http://proceedings.mlr.press/v80/kaiser18a/kaiser18a.pdf
[3] https://openreview.net/forum?id=r1gGpjActQ
[4] https://arxiv.org/abs/1802.06901</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryxRaw5zpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Reviewer 1 continued</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkGGfhC5Y7&amp;noteId=ryxRaw5zpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1247 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1247 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Continued from above:

&gt;&gt;&gt;
- There is no justification of using *causal* self-attention...
&lt;&lt;&lt;

Attention to the source embeddings is a natural and justified way to inform the discrete latents (see e.g., [2]). Also, the attention to source sequences for generating the discrete latents from the targets is not causal. The only causal attention layers are for encoding the inputs and in the autoregressive decoder from the latents. 

&gt;&gt;&gt;
- As for the experimental evaluation results: it seems that distillation...
&lt;&lt;&lt;

In page 7 of the current draft (and page 6 of the original submission), we say "Additionally, we see a large improvement in the performance of the model by using sequence-level distillation (12), as has been observed previously in non-autoregressive models (6; 16)." We have also added a sentence to this effect in the conclusion in the updated draft.

&gt;&gt;&gt;
- What is the significance of the observed differences in BLEU scores? ...
&lt;&lt;&lt;

We point the reviewer to [1, 2, 3, 4] which are the current state-of-the-art literature on non-autoregressive machine translation. None of these works report average or std devs on several runs, instead they select the best hyperparameter from a validation set and report the result of this model on a held out test set (which is a perfectly valid thing to do).

&gt;&gt;&gt;
- It seems that the tuning of the number of discrete latent codes...
&lt;&lt;&lt;

The optimal hyperparameters are selected on the validation set (WMT'13) while the reported results are on the held out WMT'14 test set. This is standard practice in the NMT literature. We have made this more explicit in the latest draft.

&gt;&gt;&gt;
- It seems that all curves in figure 3 collapse from about 45 BLEU...
&lt;&lt;&lt;

We have made this figure larger so that it is easier to read. The figure is intended to show the robustness of the EM runs vs the VQ-VAE runs: the collapsed curve is  a VQ-VAE run with bad initialization, while the other superimposed curves are different EM runs of the same configuration with various values of the number of samples. 

[1] <a href="https://openreview.net/forum?id=B1l8BtlCb" target="_blank" rel="nofollow">https://openreview.net/forum?id=B1l8BtlCb</a>
[2] http://proceedings.mlr.press/v80/kaiser18a/kaiser18a.pdf
[3] https://openreview.net/forum?id=r1gGpjActQ
[4] https://arxiv.org/abs/1802.06901
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_r1lCtgTx27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Experimental section</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkGGfhC5Y7&amp;noteId=r1lCtgTx27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1247 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">27 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1247 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper introduces a new way of interpreting the VQ-VAE, 
and proposes a new training algorithm based on the soft EM clustering. 

I think the technical aspect of this paper is written concisely. 
Introducing the interpretation as hard EM seems natural for me, and the extension
to the soft EM training is sound reasonable. 
Mathematical complication is limited, this is also a plus for many non-expert readers. 

I'm feeling difficulties in understanding the experimental part.
To be honest, I think the experimental section is highly unorganized, not a quality for ICLR submission. 
I'm just wondering why this happens, given clean and organized technical sections...

First, I'm confusing what is the main competent in the Table 1. 
In the last paragraph of the page 6, it reads; 
"Our implementation of VQ-VAE achieves a significantly better BLEU score and faster decoding speed compared to (10)."
However, Ref. (10) is not mentioned in the Table 1. Which BLEU is the score of Ref. (10)? 

Second, terms "VQ-VAE", (soft?)"EM" and "our {model, approach}" are used in a confusing manner. 
For example, in Table 1, below the row "Our Results", there are:
- VQ-VAE
- VQ-VAE with EM
- VQ-VAE + distillation
- VQ-VAE with EM + distillation

The "VQ-VAE" is not the proposed model, correct? 
My understanding is that the proposal is a VQ-VAE solved via soft EM, which corresponds to "VQ-VAE with EM". 

Third, a paragraph "Robustness of EM to Hyperparameters" is mis-leading. 
The figure 3 does not show the robustness against a hyperparameter. 
It shows the BLEU against the number of "samples" (in fact, there is no explanation about what the "samples" means). 
I think hyperparameters are model constants such as the learning rate of the SGD, alpha-beta params for Adam, dimension of hidden units, number of layers, etc. The number of samples are not considered as a model hyperparameter; it's a dataset property. 
The figure 5 shows the reconstructed images of the original VQ-VAE and the proposed VQ-VAE with EM. 
However, there is no explanation which hyperparameter is tested to assess "the robustness to hyperparameters". 

Fourth, there is no experimental report on the image reconstructions (with CIFAR and SVHN) in the main manuscript. 
In fact, there is a short paragraph that mentions about the SVHN results, 
but it only refers to the appendix. 
I think appendix is basically used for additional results or proofs, that are not essential for the main message of the paper. 
However, performance in the image reconstruction is one of the main claims written in the abstract, the intro, etc. 
So, the authors should include the image reconstruction results in the main body of the paper. 
Otherwise, claims about the image reconstructions should be removed from the abstract, etc. 


+ Insightful understanding of the VQ-VAE as hard EM clustering
+ Natural and reasonable extension to soft-EM based training of the VQ-VAE
-- Unorganized experiment section. This simply ruins the quality of the technical part. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Byearq9zpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Reviewer 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HkGGfhC5Y7&amp;noteId=Byearq9zpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1247 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">09 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1247 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for reading our paper. Below we address specific points raised by the reviewer:

&gt;&gt;&gt;
I'm feeling difficulties in understanding the experimental part.
To be honest, I think the experimental section is highly unorganized, not a quality for ICLR submission. 
I'm just wondering why this happens, given clean and organized technical sections...
&gt;&gt;&gt;

We have made an effort to clean up the experimental section part in the updated draft. We would appreciate specific comments to help us make the experimental section more readable and organized.

&gt;&gt;&gt;
First, I'm confusing what is the main competent in the Table 1. 
In the last paragraph of the page 6, it reads; 
"Our implementation of VQ-VAE achieves a significantly better BLEU score and faster decoding speed compared to (10)."
However, Ref. (10) is not mentioned in the Table 1. Which BLEU is the score of Ref. (10)? 
&gt;&gt;&gt;

This should be fixed in the updated version.

&gt;&gt;&gt;
Second, terms "VQ-VAE", (soft?)"EM" and "our {model, approach}" are used in a confusing manner. 
For example, in Table 1, below the row "Our Results", there are:
- VQ-VAE
- VQ-VAE with EM
- VQ-VAE + distillation
- VQ-VAE with EM + distillation

The "VQ-VAE" is not the proposed model, correct? 
My understanding is that the proposal is a VQ-VAE solved via soft EM, which corresponds to "VQ-VAE with EM". 
&lt;&lt;&lt;

Yes VQ-VAE is not the proposed model, although we report it in "Our Results" because the implementation is different from Kaiser et al in two crucial aspects 1) No attention to source sequences for the discrete latents 2) Product Quantization (PQ) which the authors of Kaiser et al call DVQ is not being used. Hence we also report it in "Our Results".

&gt;&gt;&gt;
Third, a paragraph "Robustness of EM to Hyperparameters" is mis-leading. 
The figure 3 does not show the robustness against a hyperparameter. 
It shows the BLEU against the number of "samples" (in fact, there is no explanation about what the "samples" means). 
I think hyperparameters are model constants such as the learning rate of the SGD, alpha-beta params for Adam, dimension of hidden units, number of layers, etc. The number of samples are not considered as a model hyperparameter; it's a dataset property. 
&gt;&gt;&gt;

The number of samples used for EM training of VQ-VAE is a hyperparameter, how is it a property of the dataset? You are free to choose any number of samples regardless of the dataset.

&gt;&gt;&gt;
The figure 5 shows the reconstructed images of the original VQ-VAE and the proposed VQ-VAE with EM. 
However, there is no explanation which hyperparameter is tested to assess "the robustness to hyperparameters". 
&lt;&lt;&lt;

Our apologies, this should be robustness to initialization of the codebook. VQ-VAE/K-means is much more sensitive to a good initialization as compared to EM.

&gt;&gt;&gt;
Fourth, there is no experimental report on the image reconstructions (with CIFAR and SVHN) in the main manuscript. 
In fact, there is a short paragraph that mentions about the SVHN results, 
but it only refers to the appendix. 
I think appendix is basically used for additional results or proofs, that are not essential for the main message of the paper. 

However, performance in the image reconstruction is one of the main claims written in the abstract, the intro, etc. 
So, the authors should include the image reconstruction results in the main body of the paper. 
Otherwise, claims about the image reconstructions should be removed from the abstract, etc. 
&gt;&gt;&gt;

We have removed all image references from the main section and now only report it in the Appendix. We hope this helps improving the quality and clarity of the main paper.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>