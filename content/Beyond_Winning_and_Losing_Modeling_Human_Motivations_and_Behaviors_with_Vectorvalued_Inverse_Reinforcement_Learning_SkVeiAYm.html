<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Beyond Winning and Losing: Modeling Human Motivations and Behaviors with Vector-valued Inverse Reinforcement Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Beyond Winning and Losing: Modeling Human Motivations and Behaviors with Vector-valued Inverse Reinforcement Learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SkVe3iA9Ym" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Beyond Winning and Losing: Modeling Human Motivations and Behaviors..." />
      <meta name="og:description" content="In recent years, reinforcement learning methods have been applied to model gameplay with great success, achieving super-human performance in various environments, such as Atari, Go and..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SkVe3iA9Ym" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Beyond Winning and Losing: Modeling Human Motivations and Behaviors with Vector-valued Inverse Reinforcement Learning</a> <a class="note_content_pdf" href="/pdf?id=SkVe3iA9Ym" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019beyond,    &#10;title={Beyond Winning and Losing: Modeling Human Motivations and Behaviors with Vector-valued Inverse Reinforcement Learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SkVe3iA9Ym},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=SkVe3iA9Ym" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">In recent years, reinforcement learning methods have been applied to model gameplay with great success, achieving super-human performance in various environments, such as Atari, Go and Poker.
However, those studies mostly focus on winning the game and have largely ignored the rich and complex human motivations, which are essential for understanding the agents' diverse behavior.
In this paper, we present a multi-motivation behavior modeling which investigates the multifaceted human motivations and models the underlying value structure of the agents.
Our approach extends inverse RL to the vectored-valued setting which imposes a much weaker assumption than previous studies.
The vectorized rewards incorporate Pareto optimality, which is a powerful tool to explain a wide range of behavior by its optimality.
For practical assessment, our algorithm is tested on the World of Warcraft Avatar History dataset spanning three years of the gameplay.
Our experiments demonstrate the improvement over the scalarization-based methods on real-world problem settings.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SkeDO_x1am" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting paper with some issues</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkVe3iA9Ym&amp;noteId=SkeDO_x1am"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper681 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">06 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper681 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies inverse reinforcement learning with a vector-valued setting. A key motivation of the paper, as suggested by its title, is to incorporate and analyze the complex human motivations.

The proposed setting seems new to me, although vectored-valued rewards and Pareto optimality have been studied in the context of RL. The biggest issue of this paper, in my opinion, is it doesn't properly support its claim that it improves the understanding of the agents' motivations and the reward functions. Details comments / questions are listed below.

- Pareto dominance is a rather weak relation. When the number of criteria increases, it is less likely one alternative dominates another. In this case, the binary comparisons defined in Sec. 2.1 becomes less discriminative. Is this a problem to the proposed method?

- Pareto dominance and vector-valued rewards have been studied in preference-based reinforcement learning, such as Fürnkranz et al. 2012 @ MLJ and Cheng et al. 2011 @ ECML. 

- Please fix the citation style in the paper and use \citep and \citet properly. 

- The empirical study in this paper doesn't properly support the authors' claim. (1) It's questionable to assume the actions of a player in an online game are optimal or even rational. (2) The results presented in Figure 2 is hard to read and the differences look minor. (3) Maybe I miss it, but has Table 2 been referenced and explained in the paper?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BkxdGEkwaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to the review </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkVe3iA9Ym&amp;noteId=BkxdGEkwaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper681 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper681 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for the review! The most important clarification that we would like to make, is that "Pareto dominance is a rather weak relation" makes the model rather strong. That is because the dominance relation is the assumption of the IRL models, and weak assumptions are desired. We believe that justifies our motivation of studying the Pareto dominance in the IRL regime.

On the empirical study, we agree that presenting the algorithm on only the real-world environments may depend on the rational assumption. In fact, we are aware of that the diversity in action originates from both the diversity of the agents' objective and their optimality (or even rationality). There is not too much one can resolve in the real-world dataset, but we can test the algorithm on the well-known RL environment and show the performance. We are also working on improving the writing quality and thanks for pointing those out.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_r1xcwnpq3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting work, but need further improvement</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkVe3iA9Ym&amp;noteId=r1xcwnpq3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper681 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper681 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents NMBM, a general inverse reinforcement learning (IRL) model that considers multifaceted human motivations. The authors have motivated and proposed the algorithm (Section 2 and 3), and demonstrated some experiment results based on a real-world dataset (WoWAH, Section 4).

-- Originality and Quality --

To the best of my knowledge, the proposed NMBM algorithm is new. However, I feel that the derivation of this algorithm is relatively straightforward based on existing literature. Specifically, this algorithm is based on (1) Theorem 3 and (2) the linear program defined in equation 9. My understanding is that both Theorem 3 and the derivation of the linear program in equation 9 are relatively straightforward based on existing literature.

On the other hand, the experiment results in Section 4 are very strong and interesting. It is the main strength of this paper.

-- Clarity --

My understanding is that the writing of Section 3 and 4 can be (and should be) further polished.

Some key notations in the paper seem to be wrong:

(1) In Theorem 3, how can the value function v^\pi(s) be in the convex hull of policies? Also, e_i is not a set.

(2) In equation 9, the linear program, \eta should be another decision variable. 

-- Pros and Cons --

Pros:

1) Strong experiments.

Cons:

1) Insufficient novelty for algorithm design.

2) No performance analysis for the proposed algorithm.

3) Clarity needs to be further improved.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJeG4mqH6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to the review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkVe3iA9Ym&amp;noteId=rJeG4mqH6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper681 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Nov 2018</span><span class="item">ICLR 2019 Conference Paper681 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks very much for the review! For the algorithm, we agree that it is indeed straightforward. We would like to note, though, that dealing with the scalarized reward function has long been an open problem in inverse reinforcement learning. We have tried other (more complex) approaches but finally found out that the lower bound introduced in Theorem 3.1 is the most appropriate one. We believe that the estimation in Theorem 3.1 is a reasonable solution to the problem. On the other hand, there are rooms for other subtle methods related to distance measurement, in Section 3.2. We are working on employing that into the algorithm for better performance.

We thank the reviewer for the comments on the strong experiments. In fact, involving some real-world problem gives the implication beyond the typical simulator-based environments. It is important that the reviewer mentions "No performance analysis for the proposed algorithm". Keeping that in mind, we are working on adding some results on openAI gym, which includes benchmarked tasks and quantitative evaluations.

We have corrected the notation typos in the updated draft and updated some other writings for its clarity.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SkgHFfw52X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review of "Beyond Winning and Losing..."</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkVe3iA9Ym&amp;noteId=SkgHFfw52X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper681 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper681 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">======== Summary ============

The authors consider a setup where there is a set of trajectories (s_t, a_t, r_t) where r_t is a *vector* of rewards. They assume that each agent is trying to maximize \sum_t \gamma^t (\phi . r_t) where \phi is a preference vector that lives on the simplex. Their goal is to calculate \phi (and maybe also an optimal policy under \phi?). The 

The authors first prove that this problem can be decomposed into finding Q functions for optimal policies for each component of r_t individually, and then solving for \phi that rationalizes the trajectory of actions in terms of these Q functions. Given the entire collection of trajectories, they perform off-policy Q-learning on each component of r_t in order to learn the Q function for that component, and then use linear programming to solve for \phi based on these Q function.

========== Comments =============

I think it's a worthwhile direction to combine IRL with modeling a diversity of preferences among agents. I can imagine several reasons you might want to do this, but the authors are not clear what their goal is besides "to propose methods that can help to understand the intricacy and complexity of human motivations and their behaviors". Is the goal to do better policy prediction? To do better policy prediction conditional on \phi? To infer \phi to understand people's preferences from a social science perspective? These all seems reasonable but not sufficiently teased out in the work. (For comparison, IRL is typically - although not always - interested in learning the reward function in order to construct robust policies that maximize it). The authors also don't seem to solve a particular task of importance on the WoW dataset.

The theoretical approach seems sound, and I liked the way their algorithm was motivated and the way the problem was decomposed into off-policy Q-learning and then solving for \phi.

However, I found myself quite confused in the experimental section (4.3). The authors evaluate their approach by action prediction. Given the trajectories, is \phi computed for each player and then compute actions based on that value of \phi? Is \phi computed on the same trajectory data used for evaluation or a different subset? Or is action prediction performed in aggregate across the entire population? The experimental setup was never clarified for this (main) experiment.

I was also confused about the motivation for Figure 2 and Appendix D. The authors are showing that their predictions about which reward is motivating the players is consistent with external factors. But wouldn't you see the same thing if you just plotted the observed *rewards* themselves? E.g. players in a guild will achieve more Relationship reward. 
The proposed approach takes the vector of reward, learns which actions are consistent with achieving each reward, then infers from the actions which reward is trying to be achieved. What advantages does this have vs. just looking at the empirical trajectory of rewards for each player/group?
I can certainly imagine that the IRL approach has certain advantages over looking at the empirical reward stream, but the authors have not talked about this nor compared against it experimentally.

The writing could also use some improvement for a future iteration, I've listed a few points below:

pg.1, Neither Brown &amp; Sandholm nor Moravcik et al use "RL algorithms"
pg.1, Finn et al unmatched )
pg.1, "a scalar reward despite observed or not" -&gt; "a scalar reward whether observed or not"
pg.2, "Either the range of" -&gt; "Both the range of" (and this sentence needs further cleanup)
pg.2, "which records the pathing of players" ??
Theorem 3: "each of the set e_i has an unique element..." This isn't clear. I think you mean "For each e_i there is a unique vector v^\pi(s) for all \pi \in \Pi_{e_i} . The equality holds if these vectors are distinct for each e_i".
pg. 5 "If otherwise all elements in \phi are generative" how can they be negative if they are on the simplex?
pg.5 "we do not perform any scalarization on the reward...the model assumption is easier to be satisfied" I think this is a strange comparison to IRL because in IRL you're trying to find a (possibly parametric) function (s,a) -&gt; R, whereas here you're *given* the vector R and are trying to find \phi. So while you have more degrees of freedom by adding \phi, you lose the original degrees of freedom in the reward function.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_Byl9CJ1Da7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to the review </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkVe3iA9Ym&amp;noteId=Byl9CJ1Da7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper681 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper681 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks a lot for your detailed review! A quick note is that pg. 5 "If otherwise all elements in \phi are generative" the \phi is the existence specification by the separation theorem. It is not what mentioned in Section 3.2 and is not restricted to the simplex. We are updating Section 3.2 to avoid the possible ambiguity. We also note that the model is trained and tested on disjoint subsets of WoWAH. The spider map was calculated by the entire population though.

For the contribution of the paper, we believe it is indeed worth investigating combining IRL with the diversity of preferences among agents. In fact, the problem of IRL with scalar-valued reward has been long open. The assumption (that is used by almost every IRL algorithm) is too strong to complex agents (such as humans). We developed Theorem 3.1 which significantly weaker the assumption. We agree that armed with the theorem, Section 3.2 and Section 4 was not aiming at a clear objective as you may expect. To make the objective more clear, it is more reasonable to reproduce the policy (or the set of policy) that was used to generate the trajectory dataset. That is because we already have the reward vector and also the Pareto optimal relationship assumption, and the policy is the only unknown element. Some updates on the algorithm will be necessary then, which is currently undergoing.

For experiments, we agree that it can be confusing to demonstrate the real-world problem. There are several constraints to run the algorithm on the real-world dataset, such as the query of the state transition function. That makes the experiments itself more dependent on its context. As a solution, we find it better to add some well-known experiments such as openAI gym or gridworld with vector-reward, which provide a more intuitive understanding of the empirical performance of the algorithm. We are currently working on that.

We have updated the writing in the revised version of the paper. Thanks a lot for pointing them out!</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>