<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>A Generalized Active Learning Approach for Unsupervised Anomaly Detection | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="A Generalized Active Learning Approach for Unsupervised Anomaly Detection" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HJex0o05F7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="A Generalized Active Learning Approach for Unsupervised Anomaly..." />
      <meta name="og:description" content="This work presents a new approach to active anomaly detection. We show that a prior needs to be assumed on what the anomalies are, in order to have performance guarantees in unsupervised anomaly..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HJex0o05F7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A Generalized Active Learning Approach for Unsupervised Anomaly Detection</a> <a class="note_content_pdf" href="/pdf?id=HJex0o05F7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019a,    &#10;title={A Generalized Active Learning Approach for Unsupervised Anomaly Detection},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HJex0o05F7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">This work presents a new approach to active anomaly detection. We show that a prior needs to be assumed on what the anomalies are, in order to have performance guarantees in unsupervised anomaly detection. We argue that active anomaly detection has, in practice, the same cost of unsupervised anomaly detection but with the possibility of much better results. To solve this problem, we present a new layer that can be attached to any deep learning model designed for unsupervised anomaly detection to transform it into an active anomaly detection method, presenting results on both synthetic and real anomaly detection datasets.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Anomaly Detection, Active  Learning, Unsupervised Learning</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A new approach to active anomaly detection. We present a new layer that can be attached to any deep learning model designed for unsupervised anomaly detection to transform it into an active anomaly detection method.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_B1gpbIRl6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>An interesting problem : active learning for anomaly detection; method suffering from a lack of novelty; questions about experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJex0o05F7&amp;noteId=B1gpbIRl6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper864 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">08 Nov 2018</span><span class="item">ICLR 2019 Conference Paper864 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper provided a convincing and intuitive motivation regarding the need for active learning in unsupervised anomaly detection. 
However the proposed approach of requesting expert feedback for the top ranked anomalies is straightforward and unsurprising, given past work on active learning. 
The experiments on synthetic data are also unsurprising. Moreover these are based on a questionable premise: the instances that are "hard" to classify are treated as anomalies. This is not very realistic.
Regarding the real data experiments: In Table 1 the results for DAE_uai are based on which budget b?  How does the result vary with b? 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1ghJ5FqnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting paper that can be significantly improved by a better organization.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJex0o05F7&amp;noteId=S1ghJ5FqnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper864 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper864 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This is an interesting paper on a topic with real-world application: anomaly detection.

The paper's organization is, at times quite confusing:
- the introduction is unusually short, with a 1st paragraph virtually unreadable due to the abuse of citations. Two additional paragraphs, covering in an intuitive manner both the proposed approach &amp; the main results, would dramatically improve the paper's readability
- section 2.1 starts quite abruptly with he two Lemmas 7 and Theorem 3 (which, in fact, is Theorem 1). This section would probably read a lot better without the two Lemmas, as the authors only refer to the main result in the Theorem. The second, intuitive part of 2.1 is extremely helpful.
- it is unclear why the authors have applied the approach in "4.3" only to a single dataset, rather than all the 11 datasets

Other comments:
- please change the color schemes for Figures 3 &amp; 4, where the red/orange (Fig 3) and various blues (Fig 4) are difficult to distinguish 
- bottom of page 3: "are rare as expected" --&gt; "are as rare as expected"</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ryg9Strt2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Active anomaly detection technique employing existing approaches and lacking appropriate literature review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJex0o05F7&amp;noteId=ryg9Strt2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper864 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper864 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">(Since the reviewer was unclear about the OpenReview process, this review was earlier posted as public comment)

Most claims of novelty can be clearly refuted such as the first sentence of the abstract "...This work presents a new approach to active anomaly detection..." and the paper does not give due credit to existing work. Current research such as Das et al. which is the most relevant has been deliberately not introduced upfront with other works (because it shows lack of the present paper's novelty) and instead deferred to later sections. The onus of a thorough literature review and laying down a proper context is on the authors, not the reviewers. Detailed comments are below.
      
      1. Related Works: "...active anomaly detection remains an under-explored approach to this problem..."
          - Active learning in anomaly detection is well-researched (AI2, etc.). See related works section in Das et al. 2016 and:
            - K. Veeramachaneni, I. Arnaldo, A. Cuesta-Infante, V. Korrapati, C. Bassias, and K. Li, "Ai2: Training a big data machine to defend," International Conference on Big Data Security, 2016.
        
      2. "To deal with the cold start problem, for the first 10 calls of select_top...":
          - No principled approach to deal with cold start and one-sided labels (i.e., the ability to use labels when instances from only one class are labeled.)
        
      3. Many arbitrary hyper parameters as compared to simpler techniques:
          - The number of layers, nodes in hidden layers.
            - The number of instances (k) per iteration
            - The number of pretraining iterations
            - The number of times the network is retrained (100) after each labeling call
            - Dealing with cold start (10 labeling iterations of 10 labels each, i.e. 100 labels).
        
      4. The paper mentions that s(x) might not be differentiable. However, the sigmoid form of s(x) is differentiable.
      
      5. Does not acknowledge the well-known result that mixture models are unidentifiable. The math in the paper is mostly redundant. Some references:
          - Identifiability  Of  Nonparametric  Mixture  Models And  Bayes  Optimal  Clustering (pradeepr/arxiv npmix v.pdf)" target="_blank" rel="nofollow"&gt;<a href="https://www.cs.cmu.edu/" target="_blank" rel="nofollow">https://www.cs.cmu.edu/</a> pradeepr/arxiv npmix v.pdf)
          - Semiparametric estimation of a two-component mixture model by Bordes, L., Kojadinovic, I., and Vandekerkhove, P., Annals of Statistics, 2006 (https://arxiv.org/pdf/math/0607812.pdf)
          - Inference for mixtures of symmetric distributions by David R. Hunter, Shaoli Wang, Thomas P. Hettmansperger, Annals of Statistics, 2007 (https://arxiv.org/pdf/0708.0499.pdf)
          - Inference on Mixtures Under Tail Restrictions by K. Jochmans, M. Henry, and B. Salanie, Econometric Theory, 2017 (http://econ.sciences-po.fr/sites/default/files/file/Inference.pdf)
          
      6. Does not acknowledge existing work that adds classifier over unsupervised detectors (such as AI2). This is very common.
        - This is another linear model (logistic) on top of transformed features. The difference is that the transformed features are from a neural network and optimization can be performed in a joint fashion. The novelty is marginal.
        
      7. While the paper argues that a prior needs to be assumed, it does not use any in the algorithm. There seems to be a disconnect. It also does not acknowledge that AAD (LODA/Tree) does use a prior. Priors for anomaly proportions in unsupervised algorithms are well-known (most AD algos support that such as OC-SVM, Isolation Forest, LOF, etc.).
        
      8. Does not compare against current state-of-the-art Tree-based AAD
          - Incorporating Expert Feedback into Tree-based Anomaly Detection by Das et al., KDD, 2017.
        
      9. The 'Generalized' in the title is incorrect and misleading. This is specific to deep-networks. Stacking supervised classifiers on unsupervised detectors is very common. See comments on related works.
      
      10. Does not propose any other query strategies than greedily selecting top.
      
      11. Question: Does this support streaming?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkx-wWBacQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Ignores existing work in statistics and semi-supervised anomaly detection, and does not make principled effort to overcome practical challenges like cold start and one-sided labels.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJex0o05F7&amp;noteId=rkx-wWBacQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Oct 2018</span><span class="item">ICLR 2019 Conference Paper864 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">This work starts making clearly refuted claims of novelty right from the first sentence of the abstract "...This work presents a new approach to active anomaly detection..." and does not give due credit to existing work. Current research such as Das et al. which is the most relevant has been deliberately not introduced upfront with other works (because it shows lack of the present paper's novelty) and instead deferred to later sections. The onus of a thorough literature review and laying down a proper context is on the authors, not the reviewers. Detailed comments are below.

1. Related Works: "...active anomaly detection remains an under-explored approach to this problem..."
    - Active learning in anomaly detection is well-researched (AI2, etc.). See related works section in Das et al. 2016 and:
      - K. Veeramachaneni, I. Arnaldo, A. Cuesta-Infante, V. Korrapati, C. Bassias, and K. Li, "Ai2: Training a big data machine to defend," International Conference on Big Data Security, 2016.
  
2. "To deal with the cold start problem, for the first 10 calls of select_top...":
    - No principled approach to deal with cold start and one-sided labels (i.e., the ability to use labels when instances from only one class are labeled.)
  
3. Many arbitrary hyper parameters as compared to simpler techniques:
    - The number of layers, nodes in hidden layers.
      - The number of instances (k) per iteration
      - The number of pretraining iterations
      - The number of times the network is retrained (100) after each labeling call
      - Dealing with cold start (10 labeling iterations of 10 labels each, i.e. 100 labels).
  
4. The paper mentions that s(x) might not be differentiable. However, the sigmoid form of s(x) is differentiable.

5. Does not acknowledge the well-known result that mixture models are unidentifiable. The math in the paper is mostly redundant. Some references:
    - Identifiability  Of  Nonparametric  Mixture  Models And  Bayes  Optimal  Clustering (<a href="https://www.cs.cmu.edu/ &lt;a href=" profile?id="~pradeepr/arxiv_npmix_v2&quot;" target="_blank">pradeepr/arxiv npmix v</a>.pdf)" target="_blank" rel="nofollow"&gt;https://www.cs.cmu.edu/ <a href="/profile?id=~pradeepr/arxiv_npmix_v2" target="_blank">pradeepr/arxiv npmix v</a>.pdf)
    - Semiparametric estimation of a two-component mixture model by Bordes, L., Kojadinovic, I., and Vandekerkhove, P., Annals of Statistics, 2006 (https://arxiv.org/pdf/math/0607812.pdf)
    - Inference for mixtures of symmetric distributions by David R. Hunter, Shaoli Wang, Thomas P. Hettmansperger, Annals of Statistics, 2007 (https://arxiv.org/pdf/0708.0499.pdf)
    - Inference on Mixtures Under Tail Restrictions by K. Jochmans, M. Henry, and B. Salanie, Econometric Theory, 2017 (http://econ.sciences-po.fr/sites/default/files/file/Inference.pdf)
    
6. Does not acknowledge existing work that adds classifier over unsupervised detectors (such as AI2). This is very common.
  - This is another linear model (logistic) on top of transformed features. The difference is that the transformed features are from a neural network and optimization can be performed in a joint fashion. The novelty is marginal.
  
7. While the paper argues that a prior needs to be assumed, it does not use any in the algorithm. There seems to be a disconnect. It also does not acknowledge that AAD (LODA/Tree) does use a prior. Priors for anomaly proportions in unsupervised algorithms are well-known (most AD algos support that such as OC-SVM, Isolation Forest, LOF, etc.).
  
8. Does not compare against current state-of-the-art Tree-based AAD
    - Incorporating Expert Feedback into Tree-based Anomaly Detection by Das et al., KDD, 2017.
  
9. The 'Generalized' in the title is incorrect and misleading. This is specific to deep-networks. Stacking supervised classifiers on unsupervised detectors is very common. See comments on related works.

10. Does not propose any other query strategies than greedily selecting top.

11. Question: Does this support streaming?
</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkl6GQj_3X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for the feedback.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJex0o05F7&amp;noteId=rkl6GQj_3X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper864 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018</span><span class="item">ICLR 2019 Conference Paper864 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your comment and we appreciate the feedback, we will incorporate suggestions in our manuscript. In this work we present new methods based on the proposed new architectures (UaiNets), which we see as a new approach to active anomaly detection. This might be better phrased as "This work presents new active anomaly detection methods". And we do give credit to Das et al. stating " The most similar prior work to ours in this setting is (Das et al., 2016), which proposed an algorithm that can be employed on top of any ensemble methods based on random projections.", but we should have mentioned it in Section 3.1 when we describe our approach and we will fix this during the rebuttal period. Nonetheless this was not ill intended or deliberate.
We address each of your detailed comments bellow:

1. We still believe it is an under-explored approach to this problem. In the well known (Chandola et al. 2009) survey, they don't mention active learning at all. Only citing (Abe et al. 2006) as supervised anomaly detection. (Das et al. 2016) only has 12 citations and (Das et al. 2017) has only one self-citation. These are some really interesting works in this area, but we believe if this was a well-researched topic they would have more recognition (assuming citations can be used as a measure for recognition).
   Nonetheless, we should indeed have cited (Veeramachaneni et al. 2016) and (Das et al. 2017). We will add it during the rebuttal phase.
- Chandola, V., Banerjee, A. and Kumar, V., 2009. Anomaly detection: A survey. ACM computing surveys (CSUR), 41(3), p.15.
- Abe, N., Zadrozny, B. and Langford, J., 2006, August. Outlier detection by active learning. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 504-509). ACM.
- Das, S., Wong, W.K., Dietterich, T., Fern, A. and Emmott, A., 2016, December. Incorporating expert feedback into active anomaly discovery. In Data Mining (ICDM), 2016 IEEE 16th International Conference on (pp. 853-858). IEEE.
- Das, S., Wong, W.K., Fern, A., Dietterich, T.G. and Siddiqui, M.A., 2017. Incorporating Feedback into Tree-based Anomaly Detection. arXiv preprint arXiv:1708.09441.
- Veeramachaneni, K., Arnaldo, I., Korrapati, V., Bassias, C. and Li, K., 2016, April. AI^ 2: training a big data machine to defend. In Big Data Security on Cloud (BigDataSecurity), IEEE International Conference on High Performance and Smart Computing (HPSC), and IEEE International Conference on Intelligent Data and Security (IDS), 2016 IEEE 2nd International Conference on (pp. 49-54). IEEE.

2. Our architecture can be built on top of state-of-the-art unsupervised anomaly detection models, so using them during our models cold start is a good option. it gives state of the art anomaly detection in these first steps.
   We state in the paper though that an interesting future work would be "using the UAI layers confidence in its output to dynamically choose between either directly using its scores, or using the underlying unsupervised modelâ€™s anomaly score to choose which instances to audit next".
   This is not straight forward though, since confidence scores from deep learning architectures are usually unregulated.

3. Our model has several hyper parameters, but we show through our experiments that the network can produce good results to all analyzed datasets with the same choice of hyper parameters.
   We only change k when dealing with datasets with few anomalies to give the model the chance to further interact with labels.
   Our algorithm is robust to k. For k âˆˆ {5; 10; 20; 30; 40; 50; 100}, using KDDCUP-rev, we get F1 scores of {0.90; 0.91; 0.90; 0.90; 0.91; 0.91; 0.91}, respectively, with no statistical difference between them (p &lt; 0.1).
   The choice of k is left to the user, since it might depend on their business model.
   A large company with several experts might want to parallelize the models feedback and get more instances per iteration with the model.

4. Both base models used have differentiable s(x) (squared error in DAE and sigmoid in classifier), but we wanted to build an architecture which could (potentially) be applied to different deep learning models in the future. Since this models might have non differentiable s(x) we didn't allow gradients to flow through it in our experiments.
   we didn't test this, but we believe we might actually see better results if we allowed gradients through s(x).

5. We believe our results expand on the unidentifiability of mixture models, showing that in this case *all* possible options are equally unidentifiable.
   Nonetheless we should have cited these results and will mention and compare to them during rebuttal phase.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BklLwmi_n7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thank you for the feedback. - Continuation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJex0o05F7&amp;noteId=BklLwmi_n7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper864 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018</span><span class="item">ICLR 2019 Conference Paper864 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">6. We do not think the novelty is marginal.
   Deep Learning architectures excel exactly as feature extractors, being great for learning representations.
   Besides, we show through the experiments in Appendix C.1 that end to end learning helps the base architecture learn better feature representations for anomaly detection.

7. We do not argue that a prior needs to be assumed for all cases (although the no free lunch theorem does). We only argue that unsupervised anomaly detection needs one.
   Supervised algorithms have, in general, presented good priors for most supervised learning problems, and the UAI layer learns in a supervised (active) way.
   We also show in Section 4.1 that although unsupervised active learning have to trade off accuracy in a setting for another, active algorithms are robust to their choice and can give good results in all analyzed settings.

8. We should have compared to them. Here are the results:
Tree-AAD      0.89* 0.29 0.86 0.50 0.32 0.53 0.69 0.76 0.94 0.59 0.92
DAE_uai        0.94   0.47 0.57 0.91 0.33 0.55 0.66 0.64 0.86 0.60 0.93
In order: KDDCUP, Arrhythmia, Thyroid, KDDCUP-Rev, Yeast, Abalone, CTG, Credit Card, Covtype, MMG, Shuttle
* to to run Tree-AAD on KDDCUP we needed to limit its memory about the anomalies it had already learned, forgetting the oldest ones. This reduced its runtime complexity from O(b^2) to O(b) in our tests, where b is the budget limit for the anomaly detection task.

9. We can see how it might be misleading and will consider changing the title.

10. Greedily selecting top is a good strategy in practical settings and (Das et al. 2016) and (Das et al. 2017) also use it.
    In practical scenarios we want to have the most anomalies for a given budget, so selecting the most anomalous instance at a time is a useful strategy.
    Also, if we select a non anomalous instance, we will use it to correct our probability distribution, improving our results in the next iteration.
    Finally, since anomaly detection is already a highly imbalanced setting, we might not get anomalous instances even when picking top anomalous results, so actively searching them might be a good choice.

11. What do you mean by streaming?
    Section 4.3 shows a setting when we have new anomalous instances arriving and we want to detect anomalies in it.
    If you wanted to run it on streaming data you would need to revisit the previously labeled instances every one in a while to keep training on them, while continuing training the base model on the streamed data.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>