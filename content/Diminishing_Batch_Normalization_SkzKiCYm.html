<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Diminishing Batch Normalization | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Diminishing Batch Normalization" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=SkzK4iC5Ym" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Diminishing Batch Normalization" />
      <meta name="og:description" content="In this paper, we propose a generalization of the BN algorithm, diminishing batch normalization (DBN), where we update the BN parameters in a diminishing moving average way. Batch normalization..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_SkzK4iC5Ym" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Diminishing Batch Normalization</a> <a class="note_content_pdf" href="/pdf?id=SkzK4iC5Ym" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019diminishing,    &#10;title={Diminishing Batch Normalization},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=SkzK4iC5Ym},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">In this paper, we propose a generalization of the BN algorithm, diminishing batch normalization (DBN), where we update the BN parameters in a diminishing moving average way. Batch normalization (BN) is very effective in accelerating the convergence of a neural network training phase that it has become a common practice. 
Our proposed DBN algorithm remains the overall structure of the original BN algorithm while introduces a weighted averaging update to some trainable parameters. 
We provide an analysis of the convergence of the DBN algorithm that converges to a stationary point with respect to trainable parameters. Our analysis can be easily generalized for original BN algorithm by setting some parameters to constant. To the best knowledge of authors, this analysis is the first of its kind for convergence with Batch Normalization introduced. We analyze a two-layer model with arbitrary activation function. 
The primary challenge of the analysis is the fact that some parameters are updated by gradient while others are not. 
The convergence analysis applies to any activation function that satisfies our common assumptions.
For the analysis, we also show the sufficient and necessary conditions for the stepsizes and diminishing weights to ensure the convergence. 
In the numerical experiments, we use more complex models with more layers and ReLU activation. We observe that DBN outperforms the original BN algorithm on Imagenet, MNIST, NI and CIFAR-10 datasets with reasonable complex FNN and CNN models.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">deep learning, learning theory, convergence analysis, batch normalization</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We propose a extension of the batch normalization, show a first-of-its-kind convergence analysis for this extension and show in numerical experiments that it has better performance than the original batch normalizatin.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Bkg4xgD62m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting idea, empirical results are not convincing</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkzK4iC5Ym&amp;noteId=Bkg4xgD62m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper19 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper19 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper introduces a modification of batch normalization technique. Original
batch normalization normalizes minibatch examples using their mean and standard deviation. 
The proposed version of batch normalization, called diminishing batch normalization, normalized 
examples in the current minibatch using  mean and standard deviations that are weighted average 
of mean and standard deviation from the current and all previous minibatches. The authors prove convergence of 
batch gradient descent with diminished batch normalization. Also, the authors show empirically
that Adagrad optimization with diminishing batch normalization can find a better local minimum than
Adagrad optimization with original batch normalization.

The idea of diminishing normalization is very sound. However I was not convinced that it gives empirical advantage.
The paper says that Table 1 shows "the best result obtained from each choice of \alpha^m ". Probably the numbers in 
this table were obtained using some particular choice of the number of epochs. Unfortunately I didn't find in the 
paper any details about the choice of the number of epochs. If we choose the number of epochs that minimize validation
loss, then according to Figures 4(a) and 3(a), if \alpha^m=1 then the validation loss is minimized around epoch 55
and corresponding test error should be less than 2.2%. But the corresponding top left entry in Table 1 has error 2.7%. 

Additional technical remarks:
1. The abstract says "we also show the sufficient and necessary conditions for the step sizes and diminishing weights to ensure the convergence". I didn't find necessary conditions in the paper.

2. The authors claim that they are not aware of any prior analysis of batch normalization. The papers at <a href="https://arxiv.org/abs/1805.11604" target="_blank" rel="nofollow">https://arxiv.org/abs/1805.11604</a> and 
https://arxiv.org/abs/1806.02375 , published initially in 5-6/2018, provide interesting theoretical insights on batch normalization.

3. Sentence after equation (2): change from D_1 to D.

4. Usually batch normalization is applied before non-linear activation. According to equations 3-6, the paper applies 
batch normalization after the nonlinear activation. My understanding is that convergence proof relies on the former architecture. Does section 5 use the former or the latter architecture? 

5. I am not sure that fully connected neural network is an efficient architecture for MNIST dataset. I would like to 
to see experiments with CNN and MNIST. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BJx32um9TX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Rebuttal </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkzK4iC5Ym&amp;noteId=BJx32um9TX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper19 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper19 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your delightful review. Please allow us to try to address your remarks below:

We present the values in the table based on the best value on the validation dataset during the training period of 100 epochs. The training loss is standard and is minimized during training. This procedure is very standard in deep learning. 

1) We have included the analysis for sufficient and necessary conditions in the appendix, which is also submitted. We can move the main result for these conditions from the appendix to the main paper.

2) These two works do not cover convergence analyses, and thus they are different from this work. Besides, these two works came out later than our initial release on arxiv (<a href="https://arxiv.org/abs/1705.08011)" target="_blank" rel="nofollow">https://arxiv.org/abs/1705.08011)</a> and therefore have been conducted after our work. We will definitely add these two citations to our paper.  

3) We will change this typo.

4) There is a misunderstanding here. We always apply batch normalization before nonlinear activation. In equation (3) and everything that follows, we denote the nonlinear activation with α(⋅). The computation for batch normalized value is always after this α(⋅).

5) In this work, we are also showing the results with CNN on the Cifar-10 dataset, which is even a more complex dataset than MNIST. We can use CNN on MNIST, but the fact that the algorithm works on Cifar-10 with CNN is a great indication that the algorithm is suitable also for CNN.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1gyh8vq2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A momentum based approach for batch normalization with asymptotic convergence analysis</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkzK4iC5Ym&amp;noteId=H1gyh8vq2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper19 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper19 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose a momentum based approach for batch normalization and provide an asymptotic convergence analysis of the objective in terms of the first order criterion. To my understanding, the main effort in the analysis is to show that the sequences of interest are Cauchy. Some numerical results are reported to demonstrate that the proposed variant of BN slightly outperforms BN with careful adjustment of some hyper parameter. The proposed approach is incremental, and the theoretical results are somewhat weak.

The most important issue is that the zero gradient of the objective function does not imply that it attains an (even local) minimum point. As for the 2-layer case, the objective function can be nonconvex in terms of the weight parameters with stationary points being saddle points, it is crucial to understand whether an iterative algorithm (GD or SGD) converges to a minimum point rather a saddle point. Thus, the first order criterion alone is not enough for this purpose, which is why extensive studies are carried out for nonconvex optimization (e.g., using both first and second order criteria for convergence [1]) and considering the specific structure of neural nets [2].

The analysis is somewhat confusing. The authors assume that the objective of interest have stationary points (\theta*, \lambda*), and also show that the sequence of the norm of gradient convergence to zero, with the \lambda^(m) converges to \bar{\lambda}. What is the relationship between \lambda* and \bar{\lambda}? It is not clear whether they are the same point or not. Moreover, since there is no converge of the parameter, it is not clear what the convergence for the \lambda imply here, as we also discussed above that the zero gradient itself may mean nothing.

In addition, the writing need improvements. Some statements are not accurate. For example, on page 3, after equation (2), the authors state “The deep network …”, though they mentioned it is for a 2-layer net. Also, more explicit explanation and definitions are necessary for notations. For example, it is clearer to define explicitly the parameters with \bar (e.g., for \lambda) as the limit point. 

[1] Ge et al. Escaping from saddle points—online stochastic gradient for tensor decomposition.
[2] Li and Yuan. Convergence Analysis of Two-layer Neural Networks with ReLU Activation.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJerXYm967" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Rebuttal</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkzK4iC5Ym&amp;noteId=rJerXYm967"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper19 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper19 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your interesting review. Please allow us to try to address your remarks below:

We agree that our current analysis relies on the assumptions of bounded parameter space and a gradient zero point. The major contribution of our work is that we are the first to provide a convergence analysis when considering transformation with BN layers. We believe we never claim that we show convergence to a local minimum. 

We should have made this notation clearer in the paper. We have noted the difference in the analysis part. \lambda* stands for the lambda value at the stationary point, where the parameters are (\theta*, \lambda*). \bar{\lambda} is the value that our algorithm converges to in Theorem 7. We show in lambda 10 and Theorem 11 that this \bar{\lambda} eventually converges to \lambda*, where the loss function reaches zero gradients.

We agree that having more layers makes the problem more complex. However, the analysis would essentially be the same except that the notation would be substantially more complex (but the proof ideas remain the same). 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Sylf5fDPhm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review: Diminishing Batch normalization. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkzK4iC5Ym&amp;noteId=Sylf5fDPhm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper19 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper19 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
In this work, the authors propose a generalization of the batch normalization (BN) technique often used in training neural networks, and analyzed this convergence. In particular, a one hidden layer and one BN hidden layer fully connected network is considered, and a deterministic gradient descent algorithm with certain kind of BN has been considered in this work. The proposed “generalized” BN strategy is devised on the deterministic setting, but it is a slight generalization of the original BN by introducing a moving average operation. Classical results of Bertsekas is leveraged to show the asymptotic convergence of the algorithm. 

I have the following three main comments about the paper. 
1)	Only deterministic setting is considered, but in this case every time the entire data set will be used to perform the averaging, it appears to be much easier to analyze than the stochastic setting. Further the reviewer has doubt on whether the resulting deterministic algorithm has any practice value. 
2)	Because the authors have used the Bertsekas/Tsitsiklis (B/T) argument, only asymptotic convergence is shown. It is not clear, even in the deterministic case, whether some kind of sublinear convergence rate can be obtained. 
3)	Only one hidden layer of neural network with one BN operation is considered. It is not clear whether the analysis can be extended to multiple layers, despite the statement of the author saying that “the technique presented can be extended to more layers with additional notation”. In particular, when there are multiple layers, the BN layers will be further composite together across multiple nonlinear operations. 
4)	The authors have mentioned that the derivative is always taking w.r.t. theta. However, in (9) is appears that the derivative is taken with respect to lambda, in order to get the Lipschitz condition on \lambda. This is a bit confusing. Also it is not clear how the gradient in Assumption 5 is defined. 
5)	Assumption 5 does not make sense. Problem (1) is a constrained problem with both variables being confined in compact feasible sets. And this condition is important in Assumption2. Now the authors say that at stationary solution the gradient is zero? Please specify functions when this will happen. I will suggest that the authors use a proper definition of stationarity solution for constrained problems. 
6)	Follow up on the previous point. The analysis builds upon B/T argument for unconstrained optimization. However it is not suitable for the constrained problems that the authors started out at the beginning of the paper. The authors may consider develop new analysis tools to understand the problem at hand, rather than assuming away the difficulties. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rJgohYm5pm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Rebuttal</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=SkzK4iC5Ym&amp;noteId=rJgohYm5pm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper19 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper19 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your delightful review. Please allow us to try to address your remarks below:

1) We believe it is possible to extend the current analysis to a mini-batch setting by reusing most of the present analysis. We point out that in numerical experiments we use mini-batches.

2) Our analysis does not draw any conclusion regarding the convergence rate. This is an exciting future direction. 

3) We agree that having more layers makes the problem more complicated. However, the analysis would essentially be the same except that the notation would be substantially more complex (but the proof ideas remain the same). 

4) Although the function in (9) takes lambda as part of the input, the derivative notation we use here is only with respect to theta (and not including lambda). We actually note in the line before (2) that all of the derivatives in this work are taken with respect to theta not lambda. We should claim this again at (9) as a clarification.  

5) We are not showing that the algorithm is converging to a local minimum but a point with the norm of gradient being zero. This is by definition a stationary point. It is unclear what happens with functions that do not satisfy Assumption 2 (bounded parameters) and 5 (stationary point existence).

6) Our analysis requires the parameters to be bounded within a compact set instead of any arbitrary constraints on parameters. Bounded parameters are required in the analysis to bound their norms. In practice, clipping is performed which is equivalent. Even in much simpler cases the proofs in an unbounded case are very complicated. The vast majority of convergence proofs in optimization assume bounded parameters. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>