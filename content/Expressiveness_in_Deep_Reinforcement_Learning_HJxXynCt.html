<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Expressiveness in Deep Reinforcement Learning | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Expressiveness in Deep Reinforcement Learning" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HJxXynC9t7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Expressiveness in Deep Reinforcement Learning" />
      <meta name="og:description" content="Representation learning in reinforcement learning (RL) algorithms focuses on extracting useful features for choosing good actions. Expressive representations are essential for learning..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HJxXynC9t7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Expressiveness in Deep Reinforcement Learning</a> <a class="note_content_pdf" href="/pdf?id=HJxXynC9t7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019expressiveness,    &#10;title={Expressiveness in Deep Reinforcement Learning},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HJxXynC9t7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Representation learning in reinforcement learning (RL) algorithms focuses on extracting useful features for choosing good actions. Expressive representations are essential for learning well-performed policies. In this paper, we study the relationship between the state representation assigned by the state extractor and the performance of the RL agent. We observe that representations assigned by the better state extractor are more scattered than which assigned by the worse one. Moreover, RL agents achieving high performances always have high rank matrices which are composed by their representations. Based on our observations, we formally define expressiveness of the state extractor as the rank of the matrix composed by representations. Therefore, we propose to promote expressiveness so as to improve algorithm performances, and we call it Expressiveness Promoted DRL. We apply our method on both policy gradient and value-based algorithms, and experimental results on 55 Atari games show the superiority of our proposed method.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HyeaeHQihX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxXynC9t7&amp;noteId=HyeaeHQihX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper971 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper971 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value"># Summary
This paper proposes a simple regularizer for RL which encourages the state representations learned by neural networks to be more discriminative across different observations. The main idea is to (implicitly) measure the rank of the matrix which is constructed from a sequence of observations and state feature vectors and encourage the rank to be high. This paper introduces three different objectives to implement the same idea (increasing the rank of the matrix). The experimental results on Atari games show that this regularizer improves A3C on most of the games and show that the learned representations with the proposed regularizer has a high rank compared to the baseline. 

[Pros]
- Makes an interesting point about the correlation between RL performance and state representations. 
- Proposes a simple objective function that gives strong empirical results.

[Cons]
- Needs more empirical evidences to better support the main hypothesis of the paper.

# Novelty and Significance
- This paper makes an interesting observation about the correlation between the RL performance and the how discriminative the learned features.
- To verify it, this paper proposes a new and simple regularizer which improves the performance across many Atari games. 

# Quality and Experiment
- The main hypothesis of this paper is that the expressiveness of the features (specifically the rank of the matrix that consists of a sequence of features) and its RL performance is highly correlated. Although this paper showed some plots (Figure 2, 6) to verify this, a more extensive statistical test or experiments would be more convincing to show the hypothesis. Examples would be:
1) Measuring the correlation between the two across all Atari games.
2) An ablation study on the hyperparameter (alpha). 
3) Learning state representations just from the reconstruction task (without RL) with/without the proposed regularizer and separately learning policies on top of that (with fixed representations). It would be much more convincing if the regularizer helps even in this setup, because this would show the general effect of the expressiveness term (by removing the effect of RL algorithm on the representation).
- This paper only reports "relative" performances to the baseline. Though it looks strong, it is also important to report the absolute performances in Atari games (e.g., median human-normalized score, etc) to show how significant the performance gap is. 
- The results with DQN are not convincing because the agents are trained only for 20M frames (compared to 200M frames in many other papers). It is not much meaningful to compare performances on such a short training regime. I would suggest running longer or removing this result from the paper and focusing on more analysis.

# Clarity and Presentation
- Figure 1a is not much insightful. It is not surprising that the representations that led to a poor policy (which achieves 0 reward) are much less discriminative given five situations with five distinct optimal actions, because the the policy has no idea which action is better than the other in such situations. It would be more informative to pick N consecutive frames and show how scattered they are in the embedding space. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJlPHDfq2Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting but many elements are lacking or unclear</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxXynC9t7&amp;noteId=rJlPHDfq2Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper971 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper971 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper aims at characterizing and discussing the impact of the expressivity of a state representation on the performance. It then discusses three possible ways of improving performance with a specific regularization on the state representation. Overall the idea 
of studying the impact of expressiveness and comparing different ways of ensuring it is interesting. However, some parts in the paper are not well-supported and many important elements to understand the experiments are lacking.

Some elements are not well supported and probably not true as stated.
"For fixed h, the expressiveness is related to the MDP M: 1) if the transition p(xt+1|xt, at)Ï€(at|xt) of the MDP is not ergodic, i.e., it can only visit a subset of observations, then the matrix {h(X1 ), Â· Â· Â· , h(Xt ), Â· Â· Â· } will more possibly be low rank; 2) if the reward is very sparse, the representation matrix will be low rank."
If you compare two MDPs, where the first one has two states and is ergodic, while the second one has many more states but is not ergodic, do you think your consideration still applies? For a given h, why would the sparsity of the reward function plays any role?

The paper states that the "method can be widely applied to current DRL algorithms, with little extra computational cost." However, it is not explained clearly how the regularization is enforced. If a supplementary loss is used and minimized at each step, the computational cost is not negligeable.

In the experiment section, the protocol is not clear. For instance the coefficient \alpha is apparently set differently for some of the games? How are they chosen? And why are the hyper-parameters chosen not given?
"However, as the observation type and the intrinsic reward mechanism vary considerably for each game, we also use some other regularization term and coefficient for some of games."

Why are there no details related to the NN architecture?
In Table 1, what does it mean "Times Better" and "Times Worse"?


Other comments:
- In Definition 1, what does it mean X_t \sim \mathcal M? \mathcal M is an MDP, not a distribution. Are the X_t taken following a given policy? are they taken on a given trajectory sequentially or i.i.d.?
- (minor) "From the above description, it is easy to see that the performance of an RL agent depends on two parts. First, it depends on whether the state extractor is good. With a good state extractor, the representation which is a depiction of the observation will retain necessary information for taking actions. Second, it depends on the accuracy of the policy: whether the feed-forward model can correctly take the optimal action given the state." Do the two elements in that paragraph mean the same: "A good state extractor provides an abstract representation from which a performant policy can be obtained?"
- Figure 2: The name of the ATARI game is not mentioned.
- In section 2.2, the MDP framework is introduced (with a state space \mathcal S) but there is no mention of the concept of observation x that is used throughout afterwards.
- In the conclusion, it is stated that "Experiments of A3C and DQN on 55 Atari games demonstrate that ExP DRL can promote their performances significantly." However, the algorithm is not tested on 55 games with DQN.
- The related work discusses papers about state representation but even more directly related to this paper, other papers have also discussed the importance of disentangled representation or entropy maximization for deep RL: <a href="https://arxiv.org/abs/1707.08475," target="_blank" rel="nofollow">https://arxiv.org/abs/1707.08475,</a> https://arxiv.org/abs/1809.04506, ... And papers that discuss expressiveness in deep learning such as https://arxiv.org/pdf/1606.05336.pdf should also be discussed.
- There are many typos/english errors.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Bylzc0V_n7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting results, but "insights" seem misleading</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJxXynC9t7&amp;noteId=Bylzc0V_n7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper971 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper971 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose the notion of "expressiveness" of state representation by simply checking the effective rank of the state vectors formed by a sampled trajectory. The authors then propose a regularizer that promotes this expressiveness. Experiments on a large set of Atari games show that this regularizer can improve the performance of reinforcement learning algorithms.

1. The authors motivate this notion through the example in section 2 where the model with higher capacity performs better. The authors note that the learned state matrix of the higher-capacity model has higher rank. But this is completely expected and well-known in machine learning in the regime where the models have insufficient capacity. Imagine the case where the true state representation consists of only a two-dimensional vector (say, the location vector), and an image is produced through a linear map. Now suppose we try to learn a K-dimensional state representation for K&gt;&gt;2 and promote a high-rank matrix. What do we expect to get from this?

2. It seems that promoting a small gap between the singular values gives a performance improvement in the case of Atari games. One can easily interpret this as a regularizer that simply prevents overfitting by equalizing the magnitudes in most parameter values. In this sense, I do not see a fundamental difference between this regularizer and say, the L2 norm regularizer. Did the authors try comparing this with an L2 norm regularizer?


The authors present very interesting empirical results, but I am not convinced that the proposed notion of "expressiveness" properly explains the performance improvement in this set of tasks. I am also not convinced that this is the right notion to promote in general. In this sense, I am afraid I cannot recommend acceptance.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">Ã—</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>