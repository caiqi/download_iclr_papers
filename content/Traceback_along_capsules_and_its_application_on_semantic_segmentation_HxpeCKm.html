<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Trace-back along capsules and its application on semantic segmentation  		 | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Trace-back along capsules and its application on semantic segmentation  &#9;&#9;" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1xpe2C5Km" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Trace-back along capsules and its application on semantic..." />
      <meta name="og:description" content="In this paper, we propose a capsule-based neural network model to solve the semantic segmentation problem. By taking advantage of the extractable part-whole dependencies available in capsule layers..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1xpe2C5Km" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Trace-back along capsules and its application on semantic segmentation  		</a> <a class="note_content_pdf" href="/pdf?id=H1xpe2C5Km" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019trace-back,    &#10;title={Trace-back along capsules and its application on semantic segmentation  &#9;&#9;},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=H1xpe2C5Km},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">In this paper, we propose a capsule-based neural network model to solve the semantic segmentation problem. By taking advantage of the extractable part-whole dependencies available in capsule layers, we derive the probabilities of the class labels for individual capsules through a layer-by-layer recursive procedure. We model this procedure as a traceback layer, and take it as a central piece to build an end-to-end segmentation network. In addition to object boundaries, image-level class labels are also explicitly sought in our model, which poses a significant advantage over the state-of-the-art fully convolutional network (FCN) solutions. Experiments conducted on modified MNIST and neuroimages demonstrate that our model considerably enhance the segmentation performance compared to the leading FCN variant.
</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">capsule, capsule network, semantic segmentation, FCN</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A capsule-based semantic segmentation, which the probabilities of the class labels are traced back through capsule layers. </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_r1lglVjsnX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Original and interesting, requires further explanation of the architecture and experiment on multi-class segmentation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xpe2C5Km&amp;noteId=r1lglVjsnX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1123 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1123 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Authors present a trace-back mechanism to associate lowest level of Capsules with their respective classes. Their method effectively gets better segmentation results on the two (relatively small) datasets. 

Authors explore an original idea with good quality of experiments (relatively strong baseline, proper experimental setup). They also back up their claim on advantage of classification with the horizontal redaction experiment. 
The manuscript can benefit from a more clear description of the architecture used for each set of experiments. Specially how the upsampling is connected to the traceback layer.
This is an interesting idea that can probably generalize to CNNs with attention and tracing back the attention in a typical CNN as well.

Pros:
The idea behind tracing the part-whole assignments back to primary capsule layer is interesting and original. It increases the resolution significantly in compare to disregarding the connections in the encoder (up to class capsules). 

The comparisons on MNIST &amp; the Hippocampus dataset w.r.t the U-Net baseline are compelling and indicate a significant performance boost. 

Cons:
Although the classification signal is counted as the advantage of this system, it is not clear how it will adopt to multi-class scenarios which is one of the major applications of segmentation (such as SUN dataset).

The assumption that convolutional capsules can have multiple parents is incorrect. In Hinton 2018, where they use convolutional Capsule layers, the normalization for each position of a capsule in layer below is done separately and each position of each capsule type has the one-parent assumption. However, since in this work only primary capsules and class capsules are used this does not concern the current experiment results in this paper.

The related work section should expand more on the SOTA segmentation techniques and the significance of this work including [2].

Question: 
How is the traceback layer converted to image mask? After one gets p(c_k | i) for all primary capsules, are primary capsule pose parameters multiplied by their p(c_k |i ) and passed all to a deconv layer? Authors should specify in the manuscript the details of the upsampling layer (s) used in their architecture. It is only mentioned that deconv, dilated, bilinear interpolation are options. Which one is used in the end and how many is not clear. 


Comments:
For the Hippocampus dataset, the ensemble U-Net approach used in [1] is close to your baseline and should be mentioned cited as the related work, SOTA on the dataset. Also since they use all 9 views have you considered accessing all the 9 views as well?


[1]: Hippocampus segmentation through multi-view ensemble ConvNets
Yani Chen ; Bibo Shi ; Zhewei Wang ; Pin Zhang ; Charles D. Smith ; Jundong Liu
[2]: RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic Segmentation
Guosheng Lin, Anton Milan, Chunhua Shen, Ian Reid</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1et6sL9hX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A neat idea</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xpe2C5Km&amp;noteId=B1et6sL9hX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1123 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1123 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a traceback layer for capsule networks to do semantic segmentation. Comparing to previous works that use capsule networks for semantic segmentation, this paper makes explicit use of part-whole relationship in the capsule layers. Experiments are done on modified MNIST and Hippocampus dataset. Results demonstrate encouraging improvements over U-Net. The writing could be tremendously improved if some background of the capsule networks is included. 

I have a question about the traceback layer. It seems to me that the traceback layer re-uses the learned weights c_{ij} between the primary capsules and the class capsules as guidance when “distributing” class probabilities to a spatial class probabilistic heatmap. One piece of information I feel missing is the affine transformation that happens between the primary capsule and the class capsule. The traceback layer doesn’t seem to invert such a transformation. Should it do so? 

Since there have been works that use capsule networks for semantic segmentation, does it make sense to compare to them (e.g. LaLonde &amp; Bagci, 2018) ?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hker6v3V37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good paper which seems technically correct. Not sure if the method will generalize well beyond MNIST.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1xpe2C5Km&amp;noteId=Hker6v3V37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1123 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1123 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Based on the CapsNet concept of Sabour the authors proposed a trace-back method to perform a semantic segmentation in parallel to classification. The method is evaluate on MNIST and the Hippocampus dataset.

The paper is well-written and well-explained. Nevertheless, I think it would be useful to have some illustrations about the network architecture. Some stuff which is explained in text could be easily visualized in a flow chart. For example, the baseline architecture and your Tr-CapsNet could be easily explained via a flow chart. With the text only, it is hard to follow. Please think about some plots in the final version or in the appendix. One question which is aligned to that: How many convolutional filters are used in the baseline model?

Additionally, think about a pseudo-code for improved understandability. 

Some minor concerns/ notes to the authors:
1.	At page 5: You mentioned that the parameters lambda1 and lambda 2 are important hyper-parameters to tune. But in the results you are not explaining how the parameters were tuned. So my question is: How do you tuned the parameters? In which range do you varied the parameters?
2.	Page 6; baseline model: Why do you removed the pooling layers?
3.	I’m curious about the number of parameters in each model. To have a valid discussion about your model is better than the U-Net-6 architecture, I would take into account the number of parameters. In case that your model is noticeably greater, it could be that your increased performance is just due to more parameters. As long as your discussion is without the number of parameters I’m not convinced that your model is better. A comparison between models should be always fair if two models are architectural similar.
4.	Why is the magnitude of lambda1 so different between the two dataset that you used?
5.	Could you add the inference times to your tables and discuss that in addition?
6.	What kind of noise is added to MNIST?
7.	What is the state-of-the-art performance on the Hippocampus dataset?
8.	What would be the performance in your experiments with a MaskRCNN segmentation network?
9.	I’m not familiar with the Hippocampus dataset. I missed a reference where the data is available or some explaining illustrations. 
10.	For both datasets, more illustrations about the segmentation performance would be fine to evaluate your method. At least in the appendix…
	
My major concern is that both datasets are not dealing with real background noise. I’m concerned that the results are not transferable to other datasets and that the method shines promising just because of the simple datasets only. For example, due to the black background MNIST digits are well separated (if we skip that you added some kind of noise). So, from that point of view your results are not convincing and the discussion of your results appearing sparse and not complete.
To make your results transparent you could think about to publish the code somewhere.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>