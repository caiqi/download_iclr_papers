<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>UNSUPERVISED CONVOLUTIONAL NEURAL NETWORKS FOR ACCURATE VIDEO FRAME INTERPOLATION WITH INTEGRATION OF MOTION COMPONENTS | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="UNSUPERVISED CONVOLUTIONAL NEURAL NETWORKS FOR ACCURATE VIDEO FRAME INTERPOLATION WITH INTEGRATION OF MOTION COMPONENTS" />
        <meta name="citation_author" content="Thang Van Nguyen" />
        <meta name="citation_author" content="Kyu-Joong Lee" />
        <meta name="citation_author" content="Hyuk-Jae Lee" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HklmIsC9Y7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="UNSUPERVISED CONVOLUTIONAL NEURAL NETWORKS FOR ACCURATE VIDEO FRAME..." />
      <meta name="og:description" content="Optical flow and video frame interpolation are considered as a chicken-egg problem such that one problem affects the other and vice versa. This paper presents a deep neural network that integrates..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HklmIsC9Y7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>UNSUPERVISED CONVOLUTIONAL NEURAL NETWORKS FOR ACCURATE VIDEO FRAME INTERPOLATION WITH INTEGRATION OF MOTION COMPONENTS</a> <a class="note_content_pdf" href="/pdf?id=HklmIsC9Y7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=itmanhieu%40snu.ac.kr" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="itmanhieu@snu.ac.kr">Thang Van Nguyen</a>, <a href="/profile?email=kyujoonglee%40sunmoon.ac.kr" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="kyujoonglee@sunmoon.ac.kr">Kyu-Joong Lee</a>, <a href="/profile?email=hjlee%40capp.snu.ac.kr" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="hjlee@capp.snu.ac.kr">Hyuk-Jae Lee</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Optical flow and video frame interpolation are considered as a chicken-egg problem such that one problem affects the other and vice versa. This paper presents a deep neural network that integrates the flow network into the frame interpolation problem, with end-to-end learning. The proposed approach exploits the relationship between the two problems for quality enhancement of interpolation frames. Unlike recent convolutional neural networks, the proposed approach learns motions from natural video frames without graphical ground truth flows for training. This makes the network learn from extensive data and improve the performance. The motion information from the flow network guides interpolator networks to be trained to synthesize the interpolated frame accurately from motion scenarios. In addition, diverse datasets to cover various challenging cases that previous interpolations usually fail in is used for comparison. In all experimental datasets, the proposed network achieves better performance than state-of-art CNN based interpolations. With Middebury benchmark, compared with the top-ranked algorithm, the proposed network reduces an average interpolation error by about 9.3%. The proposed interpolation is ranked the 1st in Standard Deviation (SD) interpolation error, the 2nd in Average Interpolation Error among over 150 algorithms listed in the Middlebury interpolation benchmark.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Frame Interpolation, Frame Rate Up Conversion, Convolutional Neural Networks, CNN, Unsupervised learning</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">4 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SJlZjzmTTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HklmIsC9Y7&amp;noteId=SJlZjzmTTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper159 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper159 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJe2mx2dhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>not well-written, confusing with limited novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HklmIsC9Y7&amp;noteId=rJe2mx2dhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper159 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper159 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">the paper proposes a neural architecture for the task of frame interpolation. It consists of two branches where the first branch interpolates the middle frame directly, while the second branch first extracts motion vectors which are refined using a flownet network and then goes to another network to interpolate the middle frame. In general, the paper is not easily understandable. descriptions are not clear and it's hard to understand the paper. terms are stated before they are defined (e.g. loss function in the second line of section 3). motivation is not clear. there are related works in both section 1 and 2. 
technical novelty is limited: the proposed method consists of encoder-decoder modules that are already introduced. the complexity of architecture is not justified in the experiments. 
the performance improvement is marginal (table 2) and extra flownet, and interpolator network (table 3, 4) is not justified in term of contribution to performance.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ryeU_UIL27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good writing and results, with limited novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HklmIsC9Y7&amp;noteId=ryeU_UIL27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper159 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper159 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper is well written and achieved good performance on Middlebury benchmark. However, I don't see much novelty. The authors claim two contributions:

(1) An end-to-end unsupervised learning with the smoothness constraint for video frame interpolation.
However, it is quite similar to approaches that learn optical flow in unsupervised manner, with image reconstruction loss and local smoothness loss. 

(2) Highly accurate frame interpolation network that integrates motion information estimated by a flow estimation network as a driver for the motion-guided frame interpolator.
I believe this step is crucial to the final good performance, but it is similar to synctx [cvpr18]. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SkeApQzYiQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A paper that lacks original ideas and needs significant rewriting to better present this work and results. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HklmIsC9Y7&amp;noteId=SkeApQzYiQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper159 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">21 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper159 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper is not well organized and contains many typos or mistakes. The claimed contributions are ambiguous, especially the “unsupervised” learning, and “smoothness constraint”. The proposed method should be evaluated on larger benchmark datasets, e.g., the UCF101 and Vimeo90K datasets, which are commonly used in the state-of-the-art algorithms. The ablation studies are insufficient by only evaluation on the proposed HCD dataset. 

Clarity: 
Moderately clear. Authors need to polish the presentation and term (e.g., SD), and fix typos (e.g., state-of-the-art) throughout the paper.

Originality:
Minor novelty. The network takes the SepConv architecture as backbone for both the Flow Network and Interpolator Networks. The authorsThis paper aims to find important regions to classify an image.
The main algorithm, FIDO, is trained to find a saliency map based on SSR or SDR objective functions. The main novelty of this work is that it uses generative models to in-fill masked out regions by SSR or SDR. As such, compared to existing algorithms, FIDO can synthesize more realistic samples to evaluate.

I like the motivation of this paper since existing algorithms have clear limitations, i.e., using out-of-distribution samples. This issue can be addressed by using a generative network as described in this paper.

However, I think this approach yields another limitation: the performance of the algorithm is bound by the generative network. For example, let’s assume that a head region is important to classify birds. Also assume that the proposed algorithm somehow predicts a mask for the head region during training. If the generative network synthesizes a realistic bird from the mask, then the proposed algorithm will learn that the head region is a supporting region of SSR.
In the other case, however, the rendered bird is often not realistic and classified incorrectly.
Then, the algorithm will seek for other regions. As a result, the proposed method interprets a classifier network conditioned on the generative network parameters. Authors did not discuss these issues importantly in the paper.

Although the approach has its own limitation, I still believe that the overall direction of the paper is reasonable. It is because I agree that using a generative network to in-fill images to address the motivation of this paper is the best option we have at this current moment. In addition, authors report satisfactory amount of experimental results to support their claim.
 propose a flow derivation layer to transform convolution kernels to flows, which is the novel part.

Significance:
Moderately significant. The authors present good video frame interpolation results by inferring flow from estimated convolution kernels. 

Pros:
1. The interpolation performance on the Middlebury dataset is good. Especially on the 4 of the 8 compared sequences, the performance is the best with significant margins than the second best approaches. However, larger datasets, such as Vimeo90k and UCF101, should also be compared to understand the generalization of the proposed method.

2. This paper proposes an interesting combination of interpolator network and flow network.  Since it is claimed that ”The interpolator networks and flow network help each other to learn efficiently”(Page2,Line9) and the two networks as a basic block can be repeatedly concatenated in the model (e.g., {interpolator, flow}-&gt;{interpolator, flow}-&gt; ... -&gt;{ interpolator }), it is interesting to see with how many repeated blocks the  best performance can be obtained.

Cons:
1. The authors claim that the proposed method is an unsupervised learning approach. However, the term “unsupervised” might be misleading. Within the context of frame interpolation, the ground truth second frame is used as a supervision signal for training. Therefore, it is definitely not an “unsupervised” frame interpolation approach. On the other hand, if the “unsupervised learning” refers to estimating optical flow without using the ground truth flow, this paper should compare the learned optical flow with existing unsupervised learning based optical flow methods (such as Ahmadi et al in ICIP2016; Lai et al in NIPS2017; UnFlow in AAAI2018) on the MPI Sintel, KITTI, and Middlebury datasets. 

2. I do not see any relationship of the proposed network with the claimed contribution “smoothness constraint” since there is no flow field’s total variation loss adopted in loss function. The paper claims that “The flow network works as a refinement process that blends the smoothness constraint into the raw initial optical flow of pixels through a convolution–deconvolution neural network” (Page5, Line4-6), which has no support from the experimental results to show the difference between before and after refinement. 

3. This paper provides a new dataset for video frame interpolation. However, there are only 6 sequences in this dataset, which are not sufficient enough to evaluate various scenes. The challenging cases only include 3 “subtitle”, 2 “occlusion”, and 1 “complex motion” (or say sports scene, halo) sequence, which do not cover the cases of “large motion” and “small object”. The term “halo” used in Table 2/3/4/5 is not consistent with the descriptions in the body text (Page6, Line23-24, “The last sequence captures a soccer match where the movement of players is fast and complex and the ball is a small object.”) and thus should be clarified clearly.

4. Although this paper obtains good results on the Middlebury dataset, it is noticed that this dataset is small with only 8 sequence for frame interpolation. There are larger benchmark datasets used in previous papers, such as the UCF101 in Liu et al in ICCV17 (379 sequences), and Vimeo90K in Xue et al in arxiv17 (3783 sequences). 

5. The proposed method consists of two SepConv and one Flow network, which result in approximately 3 times more parameters than the SepConv (21M parameters) model. To validate the proposed network architecture, an ablation study might be conducted by reducing the network parameters to a comparable capacity with SepConv, so that the contributions of the proposed network architecture can be understood well. Also note that CtxSyn by Niklaus et al in CVPR18 has about 13M parameters which is the second best method in Table 1.

6. Since the first interpolator network can generate intermediate frames, it is better to evaluate the intermediate results as well. In that way, we can observe the improvements made from the Flow Net and Interpolator Net 2 for a better understanding of the proposed model.

7. The Standard Deviation (SD) interpolation error is not defined in the paper. More importantly, since Middlebury benchmark only evaluates the Interpolation Error (IE) and Normalized Interpolation Error, I think the authors might mistakenly use SD to represent Normalized Interpolation Error (NIE). Please check the definition of NIE in Baker et al.’s paper in IJCV 2011(“A Database and Evaluation Methodology for Optical Flow”). It’s suggested to use the common evaluation metrics for a fair comparison.

8. The notations for flow and interpolator network in Section 3 and the Figure 1 is messy. There are “Flow net”, “Flow Network”, “Flow Net”, “Flow network”, and “Interpolator network”, “Interpolator Net”, “interpolator network”. 

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>