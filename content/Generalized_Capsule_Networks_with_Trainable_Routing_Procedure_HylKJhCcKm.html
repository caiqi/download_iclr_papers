<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Generalized Capsule Networks with Trainable Routing Procedure | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Generalized Capsule Networks with Trainable Routing Procedure" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HylKJhCcKm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Generalized Capsule Networks with Trainable Routing Procedure" />
      <meta name="og:description" content="CapsNet (Capsule Network) was first proposed by Sabour et al. (2017) and lateranother version of CapsNet was proposed by Hinton et al. (2018).  CapsNet hasbeen proved effective in modeling spatial..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HylKJhCcKm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Generalized Capsule Networks with Trainable Routing Procedure</a> <a class="note_content_pdf" href="/pdf?id=HylKJhCcKm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019generalized,    &#10;title={Generalized Capsule Networks with Trainable Routing Procedure},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HylKJhCcKm},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">CapsNet (Capsule Network) was first proposed by Sabour et al. (2017) and lateranother version of CapsNet was proposed by Hinton et al. (2018).  CapsNet hasbeen proved effective in modeling spatial features with much fewer parameters.However, the routing procedures (dynamic routing and EM routing) in both pa-pers are not well incorporated into the whole training process,  and the optimalnumber for the routing procedure has to be found manually.  We propose Gen-eralized GapsNet (G-CapsNet) to overcome this disadvantages by incorporatingthe routing procedure into the optimization.  We implement two versions of G-CapsNet (fully-connected and convolutional) on CAFFE (Jia et al. (2014)) andevaluate them by testing the accuracy on MNIST &amp; CIFAR10, the robustness towhite-box &amp; black-box attack, and the generalization ability on GAN-generatedsynthetic images.  We also explore the scalability of G-CapsNet by constructinga relatively deep G-CapsNet.   The experiment shows that G-CapsNet has goodgeneralization ability and scalability. </span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Capsule networks, generalization, scalability, adversarial robustness</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A scalable capsule network</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HJgjucrkp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Code is available now</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HylKJhCcKm&amp;noteId=HJgjucrkp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1004 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1004 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value"><a href="https://github.com/chenzhenhua986/CAFFE-CapsNet" target="_blank" rel="nofollow">https://github.com/chenzhenhua986/CAFFE-CapsNet</a></span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SklaDkLp3Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Replacing dynamic routing with trainable layers is interesting, but the contributions of the paper are not very clear.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HylKJhCcKm&amp;noteId=SklaDkLp3Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1004 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1004 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes to replace the dynamic routing layer of the original capsule networks with a trainable neural network layer. The idea is interesting. However, the following problems concern me.

The model is named as “generalized capsule networks” but it is not very clear what “generalized” means here. Does making the dynamic routing layer trainable generalize capsule?

The contributions of the paper will be clearer if further comparison is provided. The model is proposed based on capsule nets but it lacks comparison between the proposed model and the original capsule nets. For example, the experiments did not include the original CapsNet models (Sabour et al., 2017 or Hinton et al., 2018), which, if performed, would help understand the differences/advantages of the proposed models.

The major modification made on capsule nets is on the dynamic routing layer. In order to incorporate routing into the whole trainable process, this paper incorporate the coefficients c_{ij} to the model parameters. It is not clear if the proposed model constrains c_{ij} are further constrained, e.g., (as in the original capsule nets) to sum up to 1 along the dimension i?  

In general, the paper is well structured and easy to follow. 
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyeVlGjkTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our primary contribution is providing a scalable version of CapsNet that can guarantee convergence</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HylKJhCcKm&amp;noteId=SyeVlGjkTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1004 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1004 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Q: The model is named as “generalized capsule networks” but it is not very clear what “generalized” means here. Does making the dynamic routing layer trainable generalize capsule?
A: By "generalized", we mean we can train a CapsNet just like training a standard neural network. The routing coefficients in the original CapsNet (Sabour et al., 2017) have to be acquired by applying for the routing number (e.g., routing number equals 3) of iterations for each layer and these routing coefficients cannot be guaranteed to be optimal.  For example, in the paper (Sabour et al., 2017), the routing number has to be set as 3. A routing number that is smaller than 3 or larger than 3 would cause degradation of performance.  For a 10-layer CapsNet, assume we have to try 3 routing numbers for each layer, then totally we have to try 3^10 times to find the best routing number assignment. That is why the scalability and efficiency of CapsNets are questioned. For G-CapsNets, we can compute the routing coefficients just like computing the normal parameters of a neural network, and the (local) optimality can be guaranteed. Thus we can build a deep CapsNet (e.g., 100-layer CapsNet) without worrying about how to choose the best routing numbers for each layer. 


Q: The contributions of the paper will be clearer if further comparison is provided. The model is proposed based on capsule nets but it lacks comparison between the proposed model and the original capsule nets. For example, the experiments did not include the original CapsNet models (Sabour et al., 2017 or Hinton et al., 2018), which, if performed, would help understand the differences/advantages of the proposed models.
A: That is a good suggestion. We will add the comparison with the original CapsNet. 

Q: The major modification made on capsule nets is on the dynamic routing layer. In order to incorporate routing into the whole trainable process, this paper incorporate the coefficients c_{ij} to the model parameters. It is not clear if the proposed model constrains c_{ij} are further constrained, e.g., (as in the original capsule nets) to sum up to 1 along the dimension i?  

A: A short answer to your question is the routing coefficients are not necessarily, to sum up to 1. As Equation 2 shows, the routing coefficients are constrained to two terms. One is the loss term, and the other one is the regularizer.  In other words, G-CapsNets choose whatever routing coefficients that can best fit the loss function. Let me specify it.  The routing procedure is for combing the capsules in the lower layer. Dynamic or EM routing is based on the similarity between two capsules while our method is training the coefficients for each capsule. The advantage of G-CapsNet is that all the routing coefficients are guaranteed to be (locally) optimal (as Equation 2 shows, the routing procedure is part of the optimization) while the CapsNets (Sabour et al., 2017) can not. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HklkHqSypm" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HylKJhCcKm&amp;noteId=HklkHqSypm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper1004 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rJl5Vrn7nQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reinvention of what is already proposed by Sabour et al.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HylKJhCcKm&amp;noteId=rJl5Vrn7nQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1004 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">29 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1004 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper deals with the idea to generalize the CapsNet architecture from Sabour. Under generalization the authors mean, to define a routing procedure without an iteration parameter.

In general, your paper has a good length, is well explained and good organized. To be honest, I don’t like your writing style. It seems to be a bit too casual and not formal enough for a scientific work. Additionally, note that:
-	You are not staring with Fig. 1 in the introduction…the counting should start by one.
-	AC-GAN is the abbreviation for? 
-	Eq. 2 is outside the page space.

I have several concerns about the contribution. My major concern is that it isn’t new in general. If I break down your method, it is just the basic Dynamic Routing procedure with:
-	the number of iterations defined to be one;
-	trainable initial routing coefficients;
-	no softmax normalization over routing coefficients.
The usage of trainable initial routing coefficients was already mentioned by Sabour. Thus, the only thing which is new in your method is that you skip the normalization and I’m not sure that this has a positive effect on the process. 

Minor concerns/minor mistakes:
1.	You mentioned that the code is public available. Where is the link to a respective repository?
2.	Page 1: “[…] so it makes sense to believe that CapsNet has a better generalization ability.” Compared to what?
3.	Page 2: “The routing iterations is a meta-parameter that needs to be set manually which limits the scalability of CapsNet […].” Why it should limit the scalability? It has no effect on the model size, etc. It’s just a parameter which has to be defined.
4.	Page 3: Are you sure that a linear transformation of a hyper-cube is defined in that way in general?
5.	Page 4: What is T_k?
6.	Page 7: “Hinton et al. (2018) claimed that CapsNets […]” Are you aware that Hinton worked on Matrix Capsules and not on the CapsNet architecture of Sabour?
7.	Page 8: How you can guarantee the convergence of your method? Moreover, the convergence to what?
8.	Could you add some histograms plots of your c_ij values after the training?
9.	Why are your performance values so bad compared to CapsNet and Matrix Capsules?
10.	Could you add to your tables the inference, training times? If you remove the iteration parameter I would assume that your method should be faster, or?
11.	Is the parameter lambda in Eq. 2 the same as in Eq. 6? How you tune that parameter?
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJgBIcFyp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Explain the routing procedure in G-CapsNet</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HylKJhCcKm&amp;noteId=HJgBIcFyp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1004 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1004 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Q: The routing procedure
A: It seems like you misunderstood the routing procedure of G-CapsNet. We do not use dynamic routing at all. The routing procedure is for combing the capsules in the lower layer. Dynamic routing is based on the similarity between two capsules while our way is training the coefficients for each capsule. The advantage of G-CapsNet is that all the routing coefficients are guaranteed to be (locally) optimal (as Equation 2 shows, the routing procedure is part of the optimization) while the CapsNets (Sabour et al., 2017) can not. We can get the routing coefficients just like we acquire the normal parameters of a neural network. 

Q: Code
A: <a href="https://github.com/chenzhenhua986/CAFFE-CapsNet" target="_blank" rel="nofollow">https://github.com/chenzhenhua986/CAFFE-CapsNet</a>

Q: Page 1: “[…] so it makes sense to believe that CapsNet has a better generalization ability.” Compared to what?
This statement is an assumption, that is why we apply an experiment in section 4.1.2. We use a GAN to generate artificial images to test whether G-CapsNets generalize better.

Q: Page 2: “The routing iterations is a meta-parameter that needs to be set manually which limits the scalability of CapsNet […].” Why it should limit the scalability? It has no effect on the model size, etc. It’s just a parameter which has to be defined
A: CapsNets (Sabour et al., 2017) can not guarantee the optimal routing coefficients are optimal and finding the best routing numbers for a deep CapsNet is computationally expensive. For example, in the paper (Sabour et al., 2017), the routing number has to be set as 3. According to our experiment, a routing number that is smaller than 3 or larger than 3 would cause degradation of performance.  For a 10-layer CapsNet, assume we have to try 3 routing numbers for each layer, then totally we have to try 3^10 times to find the best routing number assignment. That is why the scalability and efficiency of CapsNets are questioned. For G-CapsNets, we can compute the routing coefficients just like computing the normal parameters of a neural network, and the (local) optimality can be guaranteed.

Q:  Page 3: Are you sure that a linear transformation of a hyper-cube is defined in that way in general?
A: Thanks for pointing this out. I am not sure if this is a general way, we will modify our paper and explain that this is one possible way. 

Q: Page 4: What is T_k?
A: It comes from (Sabour et al., 2017), signifies positive samples and negative samples. We will add an explanation in our paper.

Q: Page 7: “Hinton et al. (2018) claimed that CapsNets […]” Are you aware that Hinton worked on Matrix Capsules and not on the CapsNet architecture of Sabour?
A: Yes, we are aware of that. Could you please illustrate your question? I am not sure what you were asking. 

Q: Page 8: How you can guarantee the convergence of your method? Moreover, the convergence to what?
A: We can guarantee the convergence because we incorporate the routing procedure into the whole optimization process, as Equation 2 shows. We treat the routing coefficients the same as other parameters in a neural network. Thus, these routing coefficients are guaranteed to converge to (locally) the optimal points.

Q: Could you add some histograms plots of your c_ij values after the training?
Thanks for the suggestion, we will add the histograms of our routing coefficients.


Q: Why are your performance values so bad compared to CapsNet and Matrix Capsules?
A: There are two possible reasons that our accuracy is lower than the original one. 1). We do not use any data-augmentation technique while the original paper (Sabour et al., 2017) adopts 2-pixel shifting, "Training is performed on 28 × 28 MNIST (LeCun et al. [1998]) images that have been shifted by up to 2 pixels in each direction with zero padding." 2). We use different frameworks. We use Caffe (https://github.com/chenzhenhua986/CAFFE-CapsNet) while the original paper uses TensorFlow (https://github.com/Sarasra/models/tree/master/research/capsules). 
We are not trying to get better performance than CapsNet (Sabour et al., 2017), our primary contribution is providing a scalable version of CapsNet, at the same time, all other advantages (for example, achieving better performance with fewer parameters) of CapsNets are preserved. 

Q: Could you add to your tables the inference, training times? If you remove the iteration parameter I would assume that your method should be faster, or? 
A: Sorry we did not mention that in our paper. We train all our models for 10k iterations. As for the speed, assume that calculating the routing coefficients with dynamic routing or EM routing procedure cost the same time as G-CapsNet, G-CapsNets should be faster since we need only one step. 

Q: Is the parameter lambda in Eq. 2 the same as in Eq. 6? How you tune that parameter?
A: Thanks for pointing that out. They are different. The lambda in Eq. 2 is a weight that balances the loss and the regularizer while the lambda in Eq. 6 balances the loss between positive samples and negative samples.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1xGRV3XhQ" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HylKJhCcKm&amp;noteId=H1xGRV3XhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper1004 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJe2vOjb3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The paper is poorly written, not clear about the contributions, evaluation is questionable</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HylKJhCcKm&amp;noteId=rJe2vOjb3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1004 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">27 Oct 2018 (modified: 13 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1004 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=rJe2vOjb3m" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Pros:

The paper claims to make CapsuleNet's routing trainable. The proposed G-CapsNet have two variants (within feature map and across feature map). 

It presents evaluation of G-CapsNet in terms of robustness and generalization. It is interesting that Capsule networks are as bad as traditional CNNs for strong white box attacks.

Cons:

The idea is not clearly explained. It seems that the main idea is to relax routing from a discrete problem to a continuous problem. Making the routing assignments as a regularization term in the loss function. The assignment fraction can be trained end-to-end. However, the paper discusses two loss functions in Equation 2 and 6. It is not clear how the two loss functions are related. 

It is not clear why fractional routing is more efficient than the original non-trainable CapsuleNet routing. There is not clear explanation or evaluation.

The authors did not reproduce the baseline CNN model used in the original CapsuleNet routing. The original one is 0.39% error rate and the authors' implementation is 0.83%. So this makes G-CapsNet result much worse than the original CapsuleNet. So it is not clear what the benefit of G-CapsNet over the original one.

There is a related paper on approximate routing, see:
Neural Network Encapsulation, ECCV 2018
<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Hongyang_Li_Neural_Network_Encapsulation_ECCV_2018_paper.pdf" target="_blank" rel="nofollow">http://openaccess.thecvf.com/content_ECCV_2018/papers/Hongyang_Li_Neural_Network_Encapsulation_ECCV_2018_paper.pdf</a>

Overall, the paper does not have enough contributions both in terms of new methods and evaluations. It does not meet the expectation of ICLR acceptance.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByxZPSHJTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Our contribution is providing a salable CapsNet.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HylKJhCcKm&amp;noteId=ByxZPSHJTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1004 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1004 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
Q: The relations between Equation 2 and 6. 
A: The loss function in Equation 2 shows that the convergence of G-CapsNet can be guaranteed mathematically, just like standard neural networks. In contrast, the CapsNet in (Sabour et al., 2017) can not ensure convergence mathematically since the computation of coupling (routing) coefficients is not part of the optimization. For example, the best routing number for MNIST is 3, as suggested in (Sabour et al., 2017). We found that if the routing number is 4 or larger, the performance degraded. The loss function in Equation 6 gives details. You are right that the relation between Equation 2 and 6 is not clear, we will add an explanation. 

Q: Explain why efficient?
A: Thanks for pointing this out. We will add a detailed explanation as well as evaluation. The efficiency of G-CapsNet is that it does not need to set the routing number manually, and the optimal routing coefficients can be acquired, as Equation 2 shows. For example, in the paper (Sabour et al., 2017), the routing number has to be set as 3. A routing number that is smaller than 3 or larger than 3 would cause degradation of performance.  For a 10-layer CapsNet, assume we have to try 3 routing numbers for each layer, then totally we have to try 3^10 times to find the best routing number assignment. That is why the scalability and efficiency of CapsNets are questioned. For G-CapsNets, we can compute the routing coefficients just like computing the normal parameters of a neural network, and the (local) optimality can be guaranteed.

Q: Why accuracy is lower? 
A: There are two reasons that our accuracy is lower than the original one. 1). We do not use any data-augmentation technique while the original paper (Sabour et al., 2017) adopts 2-pixel shifting, "Training is performed on 28 × 28 MNIST (LeCun et al. [1998]) images that have been shifted by up to 2 pixels in each direction with zero padding." 2). We use different frameworks. We use Caffe (<a href="https://github.com/chenzhenhua986/CAFFE-CapsNet)" target="_blank" rel="nofollow">https://github.com/chenzhenhua986/CAFFE-CapsNet)</a> while the original paper uses TensorFlow (https://github.com/Sarasra/models/tree/master/research/capsules/). 

The benefit of G-CapsNet is that we can build a deep CapsNet easily without worrying about divergence. For CapsNets (Sabour et al., 2017), finding the best routing number for a deep network would be computationally expensive. Our contribution is providing a scalable version of CapsNet while all other advantages of CaspNets are preserved. 


Q: The ECCV paper.
A: Thanks for pointing this out. This paper is definitely related to our paper. We will compare it with our paper. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>