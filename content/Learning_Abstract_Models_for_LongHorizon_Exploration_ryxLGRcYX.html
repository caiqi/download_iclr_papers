<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Learning Abstract Models for Long-Horizon Exploration | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Learning Abstract Models for Long-Horizon Exploration" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ryxLG2RcYX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Learning Abstract Models for Long-Horizon Exploration" />
      <meta name="og:description" content="Despite recent progress in reinforcement learning (RL), state-of-the-art RL algorithms&#10;  continue to struggle with high-dimensional, long-horizon, sparse-reward&#10;  tasks. Even with a perfect model..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ryxLG2RcYX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Learning Abstract Models for Long-Horizon Exploration</a> <a class="note_content_pdf" href="/pdf?id=ryxLG2RcYX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019learning,    &#10;title={Learning Abstract Models for Long-Horizon Exploration},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=ryxLG2RcYX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Despite recent progress in reinforcement learning (RL), state-of-the-art RL algorithms
continue to struggle with high-dimensional, long-horizon, sparse-reward
tasks. Even with a perfect model, model-based RL can be intractable because the state
space is often high-dimensional (e.g. over 10^100 states). We address this by automatically
constructing an abstract Markov Decision Process (MDP) with an exponentially
smaller number of states (e.g. 10^5), where the actions are skills learned
by a worker policy. We learn a near-optimal policy on the resulting abstract MDP,
which maps to a near-optimal policy on the original MDP. Our approach provably
makes monotonic progress and is guaranteed to learn a near-optimal policy.
We empirically evaluate our approach on three of the hardest games from the
Arcade Learning Environment: Montezuma’s Revenge, Pitfall!, and Private Eye,
and outperform the previous state-of-the-art by over a factor of 2 in
each game. In Pitfall!, our approach is the first to achieve superhuman performance
without demonstrations</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Reinforcement Learning, Hierarchical Reinforcement Learning, Model-based Reinforcement Learning, Exploration</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We automatically construct and explore a small abstract Markov Decision Process, enabling us to achieve state-of-the-art results on Montezuma's Revenge, Pitfall!, and Private Eye by a significant margin.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_rylm4WhLTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Relevant topic, poor evaluation, unclear related work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxLG2RcYX&amp;noteId=rylm4WhLTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1269 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1269 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper deal with learning abstract MDPs for planning in tasks that require long-horizon due to sparse rewards.
This is an extremely important and timely topic in the RL community.

The paper is generally clear and well written.

The proposed algorithm seems reasonable and it is conceptually simple to understand. In the current experimental results presented it also seems to outperform the alternative baselines.

Nonetheless, the paper has few flaws that significantly impact the stated contributions and reduced my rating.
1) a stated contribution are theoretical guarantees about the performance of the algorithm. this analysis is not currently included in the main body of the manuscript, but rather in the appendix, which I find rather annoying. Moreover, said the analysis is in my opinion not sufficiently rigorous, with hand-wavy arguments, no formal proof and unclear terms (e.g. how do you define near-optimal?). Moreover, as observed by the authors this analysis currently rely on strong assumptions that might make it rather unrealistic. Overall, if you want to claim theoretical guarantees you will have to significantly improve the manuscript.
2) Related work, although extensive in terms of the number of references, do not help to place this work in the literature. Listing related work is no the same as describing similarities and differences compared to previous methods. For example, a paper that obviously comes to mind is "FeUdal Networks for Hierarchical Reinforcement Learning". What are the differences to your approach? Also, please place the related work earlier on in the paper. Otherwise, it is impossible for a reader to correctly and objectively relate your proposed approach to previous literature.
3) In its current form, the experimental results are extremely cherry-picked, with a very small number of tasks evaluated, and for each task a single selected baseline used. This needs to be changed: a) you should run all the baselines for each of the current tasks b) you should also expand the experiments evaluated to include tasks where it is not obvious that a hierarchy would help/is necessary c) you should include more baselines. feudal RL should be one, Roderick et al 2017 should be another one (especially considering your discussion in Sec 8)

Additional feedback:
- The paper is currently oriented towards discrete states. What can you say about continuous spaces?
- The use of random exploration for the discoverer is underwhelming. Have you tried different approaches? Would more advanced exploration techniques work or improve the performance?
- Using only 4 seeds seems too little to provide accurate standard deviations. Please run at least 10 experiments.
- The use of RAM is a fairly serious limitation of your experimental setting in my view. You should include results also for the pixel space, even if negative. Otherwise, this choice is incomprehensible.
 </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1ehFbba37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Effective but complex method which achieves good exploration performance conditioned on substantial prior knowledge</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxLG2RcYX&amp;noteId=B1ehFbba37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1269 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1269 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper considers how to effectively perform exploration in the setting where a difficult, high-dimensional MDP can be mapped to a simpler, lower-dimensional MDP. They propose a hierarchical approach where a model of the abstract MDP is incrementally learned, and then used to train sub-policies to transition between abstract states. These sub-policies are trained using intrinsic rewards for transitioning to the correct state, and the transition probabilities in the abstract MDP reflect how well a sub-policy can perform the transition. 

The approach is evaluated on three difficult Atari games, which all require difficult exploration: Montezuma's Revenge, Pitfall and Private Eye, and is shown to achieve good performance in all of them. Furthermore, the model can be used to generalize to new tasks by changing the rewards associated with different transitions. 

The main downside with this paper is that the mapping from original state (i.e. pixels) to the abstract state is assumed to be known beforehand, which requires prior knowledge. The authors hardcode this mapping for each of the games by fetching the relevant bits of information from RAM. This prevents fair comparison to many other methods which only use pixels, and makes this paper borderline rather than strong accept. 


Quality: the method is evaluated on difficult problems and shown to perform well. The experiments are thorough and explore a variety of dimensions such as robustness to stochasticity, granularity of the abstract state and generalization to new tasks. The approach does strike me as rather complicated though - it requires 19 (!) different hyperparameters as shown in table 2. The authors do mention that many of these did not require much tuning and they intend on making their code public. Still, this suggests that re-implementation or extensions by others may be challenging. Are all of these moving parts necessary?

Clarity: the paper is well-written, for the most part clear, and the details are thoroughly described in the appendix. 

Originality: this approach in the context of modern deep learning is to my knowledge novel.

Significance: This paper provides a general approach for hierarchical model-based planning when the mapping from the hard MDP to the easy one is known, and in this sense is significant. It is limited by the assumption that the mapping to abstract states is known. I suspect the complexity of the approach may also be a limiting factor. 

Pros:
+ good results on 3 challenging problems
+ effective demonstration of hierachical model-based planning

Cons:
- requires significant prior knowledge for state encoding
- complicated method

Minor:
- in the intro, last paragraph: "Our approach significantly outperforms previous non-demonstration SOTA approaches in all 3 domains". Please specify that you use extra knowledge extracted from RAM, otherwise this is misleading. 
- Algorithm 1: nagivate -&gt; navigate
- Section 4, last sentence: broken appendix link. 
- Bottom of page 6: "Recent work on contextual MDPs...as we do here" is not a sentence. 
- In related work, it would be nice to mention some relevant early work by Schmidhuber on subgoal generation: <a href="http://people.idsia.ch/~juergen/subgoals.html" target="_blank" rel="nofollow">http://people.idsia.ch/~juergen/subgoals.html</a>
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryl096Y5pm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Reviewer 2 [1 / 2]</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxLG2RcYX&amp;noteId=ryl096Y5pm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1269 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1269 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We would like to thank Reviewer 2 for their detailed and thoughtful feedback! Reviewer 2 raises two main concerns: 1) that our approach requires prior knowledge and 2) that our approach is complicated, which we address in the two sections below:

----------------------------------------

Prior Knowledge

In this work, we assume access to prior knowledge (i.e., RAM state information) in the form of the state abstraction function. However, in our experiments, we compare with state-of-the-art approaches that use a comparable amount of prior knowledge (these approaches use more prior knowledge in 1 game, the same prior knowledge in 1 game, and less prior knowledge in 1 game). In each game, we compare with the highest scoring non-demonstration approach and we achieve new state-of-the-art results in each game, by over 2x:

- In Montezuma’s Revenge, we compare with SmartHash, which requires RAM state information equivalent to the prior 
knowledge used by our approach. Our approach achieves over 2x as much reward as SmartHash on average.
- In Pitfall!, we compare with SOORL, which requires parsing out all the relevant objects on the screen, prior knowledge much stronger than that used by our approach. Our approach achieves over 10x as much reward on average. In addition, we also compare with Apex DQfD, which uses expert demonstrations, even stronger prior knowledge. Our approach achieves about 2.5x the reward of Apex DQfD on average. We note that no prior approach has ever achieved &gt;0 reward on Pitfall! with only RAM state information (our approach achieves ~10K reward).
- In Private Eye, we compare with DQN-CTS, which encodes the prior knowledge that semantically different states tend to have very different pixels. DQN-CTS uses weaker prior knowledge than our approach, but we compare with DQN-CTS because it achieves the best performance out of all non-demonstration prior approaches. Our approach achieves over 2x as much reward as DQN-CTS on average.

To further understand what portion of the performance of our method is due to just prior knowledge, we’ve run additional experiments with AbstractStateHash, an approach (described in greater detail in the paper) which uses the same prior knowledge as our approach and uses this prior knowledge to do count-based exploration (count-based exploration methods have achieved the prior state-of-the-art results in the hardest exploration games). In the initial submission, we already reported results of AbstractStateHash on Montezuma’s Revenge, which achieves results competitive with the prior state-of-the-art; our approach achieves &gt;2x the reward of AbstractStateHash. We will soon submit an updated draft with results of AbstractStateHash on Pitfall! and Private Eye and we provide a summary below. 

- On Pitfall!, AbstractStateHash achieves 0 reward (comparable with many strong approaches, e.g., DQN-PixelCNN and Rainbow), whereas our approach achieves ~10K reward.
- On Private Eye, our approach achieves &gt;100x the reward of AbstractStateHash.

These results suggest that while the RAM state prior knowledge does provide our approach valuable signal, prior state-of-the-art methods do not effectively leverage this prior knowledge.

In addition, in Section 7.5, we analyze the effect of varying the state abstraction function to answer the question of: how hard is it to find a state abstraction function that works well with our method? We find that our approach significantly outperforms the prior state-of-the-art under many abstract state representations. This alleviates the burden of selecting the perfect state abstraction function for new tasks (in our case, for each game, we selected an abstraction function and never changed or tuned it), and suggests that future work could find different state abstraction functions requiring less prior knowledge. In other domains, it may also be possible to easily extract abstract states from the state. For example, many robotics tasks have fully observable states (e.g. consisting of joint angles of a robotic arm and positions of objects). In these tasks, a good state abstraction function might just extract the dimensions corresponding to the position of the gripper and the positions of the objects.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SygvApF5TQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Reviewer 2 [2 / 2]</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxLG2RcYX&amp;noteId=SygvApF5TQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1269 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1269 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Complexity

While our approach has many pieces, it consists of three highly modularized components with simple interfaces: the manager, worker, and discoverer. These components can be (and in our case were) developed and improved separately, significantly limiting the effective complexity of working with the system. For example, the worker can use any state-of-the-art RL algorithm to learn its goal-conditioned policy. In addition, in contrast to most end-to-end deep RL methods whose metrics (e.g., Q-values, loss functions) are hard to interpret, the metrics in the framework are interpretable and make debugging and improving the system easier. For example, the growth of the safe set indicates good exploration, and the number of episodes required for the worker to learn each transition indicates how well the worker’s RL algorithms are learning. We plan to release our code to further aid reproducibility efforts.

Reviewer 2 notes that our approach has many (19) hyperparameters. We used the same hyperparameters to achieve state-of-the-art performance on all games and only tuned (exclusively on Montezuma’s Revenge) 4 hyperparameters total, suggesting that applying our approach to new tasks may not require heavy hyperparameter tuning. In addition, while our approach does have many hyperparameters, the total number of hyperparameters is comparable to other approaches, e.g. DQN-CTS has 14 hyperparameters. 

----------------------------------

Minor

We thank Reviewer 2 for pointing out these minor issues and will address them in newer drafts, which we will post shortly.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div><div class="note_with_children"><div id="note_SJl2_N-q2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The proposed algorithm outperforms the state of the art algorithms on three very hard games</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxLG2RcYX&amp;noteId=SJl2_N-q2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1269 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1269 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper considers reinforcement learning tasks that have high-dimensional space, long-horizon time, sparse-rewards. In this setting, current reinforcement learning algorithms struggle to train agents so that they can achieve high rewards. To address this problem, the authors propose an abstract MDP algorithm. The algorithm consists of three parts: manager, worker, and discoverer. The manager controls the exploration scheduling, the worker updates the policy, and the discoverer purely explores the abstract states. Since there are too many state, the abstract MDP utilize the RAM state as the corresponding abstract state for each situation. 

The main strong point of this paper is the experiment section. The proposed algorithm outperforms all previous state of the art algorithms for Montezuma’s revenge, Pitfall!, and Private eye over a factor of 2. 

It is a minor weak point that the algorithm can work only when the abstract state is obtained by the RAM state. In some RL tasks, it is not allowed to access the RAM state. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">8: Top 50% of accepted papers, clear accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BklWPRFqpQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Reviewer 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxLG2RcYX&amp;noteId=BklWPRFqpQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1269 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">15 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1269 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank Reviewer 3 for their comments. Reviewer 3 points out the strong state-of-the-art performance of our approach as a strength and mentions prior knowledge (our use of RAM state information) as a minor weakness. To clarify, in our experiments, we outperform previous non-demonstration state-of-the-art approaches that use a comparable amount of prior knowledge. We discuss our usage of prior knowledge in greater detail in the section titled “Prior Knowledge” in our response to Reviewer 2.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>