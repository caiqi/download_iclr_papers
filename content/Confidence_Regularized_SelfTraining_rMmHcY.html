<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Confidence Regularized Self-Training | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Confidence Regularized Self-Training" />
        <meta name="citation_author" content="Yang Zou" />
        <meta name="citation_author" content="Zhiding Yu" />
        <meta name="citation_author" content="Xiaofeng Liu" />
        <meta name="citation_author" content="Vijayakumar Bhagavatula" />
        <meta name="citation_author" content="Jinsong Wang" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=r1MmH30cY7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Confidence Regularized Self-Training" />
      <meta name="og:description" content="Recent advances in domain adaptation show that self-training with deep networks presents a powerful means for unsupervised domain adaptation. Specifically, these methods often involve an iterative..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_r1MmH30cY7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Confidence Regularized Self-Training</a> <a class="note_content_pdf" href="/pdf?id=r1MmH30cY7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=yzou2%40andrew.cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="yzou2@andrew.cmu.edu">Yang Zou</a>, <a href="/profile?email=zhidingy%40nvidia.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="zhidingy@nvidia.com">Zhiding Yu</a>, <a href="/profile?email=liuxiaofeng%40cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="liuxiaofeng@cmu.edu">Xiaofeng Liu</a>, <a href="/profile?email=kumar%40ece.cmu.edu" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="kumar@ece.cmu.edu">Vijayakumar Bhagavatula</a>, <a href="/profile?email=jinsong.wang%40gm.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="jinsong.wang@gm.com">Jinsong Wang</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Recent advances in domain adaptation show that self-training with deep networks presents a powerful means for unsupervised domain adaptation. Specifically, these methods often involve an iterative process of predicting on target domain and then taking the confident predictions as pseudo-labels for retraining. Basic self-training treats selected pseudo-labels equally as one-hot vectors, and the selection process is often modeled to be progressive via self-paced learning. While one-hot vector is a natural choice to model multiclass targets, such encoding scheme does not consider the difference between selected samples. As a result, the approach sometimes generates overconfident false pseudo-labels that lead to the convergence to deviated solutions with propagated errors, especially when there is a large domain gap. To address this problem, we generalize self-training as an expectation maximization (EM) problem which treats pseudo-labels as latent variables, and solves maximum marginal likelihood estimation (MMLE) by maximizing its lower bound. We then propose confidence regularized self-training, where we introduce multiple confidence regularizers along with their solutions. These regularizers mainly consider the control of pseudo-label confidence from two aspects: model regularization and label refinement. Experiments on both semantic segmentation and image classification show that self-training with different confidence regularizers comprehensively outperform their non-regularized counterparts.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Neural network, domain adaptation, self-training, maximum marginal likelihood estimation, Expectation-Maximization, regularization</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">A self-training optimization framework with pseudo-label confidence regularization </span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_H1xNUcrTaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1MmH30cY7&amp;noteId=H1xNUcrTaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1529 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1529 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_B1gYOWac27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Confidence Regularized Self-Training</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1MmH30cY7&amp;noteId=B1gYOWac27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1529 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1529 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes the regularized framework of self-training, for the unsupervised domain adaptation. Authors reformulate basic self training problem with some relaxation and regularization encouraging not to use unconfident pseudo-labels. They also different instantiations of such regularizers under the proposed framework based on L2, entropy and KL and evaluate them on semantic segmentation and image classification tasks.  Overall, while the idea is potentially interesting and worthwhile to further explore, the current versions does not seem to be ready to be published:  the structure of paper needs to be improved, details about the experiments are missing and the connection to the EM should be clarified.  

Detailed question/comments:
-The structure of the paper is very kind in providing details on their derivations. However, given the space constraint, it seems the balance between the model derivation and experiments should considered.  The experimental verification is quite weak as it is while there are lots of white space and redundancies in equations.
- After Eq. (1), authors argue that the constraint of (1) has the problem of over-confidence on wrong pseudo labels and derive Eq (2) with simplex relaxation. But it turns out after Eq (4) that the optimal solution for y_hat is still one hot binary vector. Can we say (2) is better in terms of pseudo label overconfidence problem (not just ease of optimization)?
- In section 3, it is not clear which part is the contribution and which is just review. It would be great to add some references and comparisons on existing literatures.
- In section 3, it is not clear to me how one p rules all lambda_t; it seems we need additional parameters to adaptively increase (5% for instance as authors mentioned) for every iteration.
- Algorithm 1 and 2 are not given in the main part. This should be explicitly mentioned.
- The connection to EM seems wrong. In E-step, we compute the posterior distribution given the parameter computed in previous M step. It's not computing "argmax", but should be computing the 
distribution", which will not probably be one-hot in eq (e).


 </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SJe3ovraTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks and Response to AnonReviewer1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1MmH30cY7&amp;noteId=SJe3ovraTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1529 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1529 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for appreciating the idea and kindly pointing out the weakness of this paper, which helped us very much in comprehensively rewriting the paper. We agree with the reviewer that the paper is not well-balanced with enough details. We also apologize for the hurry submission that leads to bad paper writing. Given the significant difference between new version and previous one submission, we decide to withdraw our paper and resubmit to a new venue.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_BJg7ETFq2m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper proposes an regularizer to encourage soft label assignment during self-training. This is a cute idea and leads to notable improvement. The EM interpretation, however, needs more discussions.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1MmH30cY7&amp;noteId=BJg7ETFq2m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1529 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1529 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Pros:
This paper points out an intrinsic issue in conventional self-training formulation and proposes a simple yet effective reformulation with regularization to resolve the problem. This kind of work should be appreciated.

Cons:
1) The organization and clarity could be improved. 
1-1) When the authors first mentioned Eq. (2), it still leads to sparse (over-confidence) solutions---indeed, the same solutions from Eq. (1). Would it be better to move this modification to Section 4? Or at least clearly mention that this modification alone is not enough.
1-2) Section 3.3 needs completely rewriting. The p=15% comes out nowhere. What is the meaning of U in UCBST?
1-3) In Section 4, the authors should clearly state that the regularizer can be included in either step of learning, and then briefly overview the Section instead of directly jumping into Sec. 4.1 and 4.2. Besides, please make R_c(YX_T, Y_T, W) consistent. Some part in Eq. (8) misses X_T. 
1-4) In Eq. (11), does &gt;= \lambda_t lead to 0?
 
2) The EM formulation needs more discussion. Specifically, isn't the very right term in the 3rd and 4th row a constant term (i.e. log 1 = 0)? If so, why bother using the variational method and optimize the lower bound? The current formulation seems artificial to me. 

3) As the paper mentioned the over-confidence problem, I would hope to see some qualitative studies instead of just qualitative ones---more specifically, the motivations of the paper need to be justified. Besides, I would suggest the authors expanding the experimental section with more analysis and sufficient descriptions about the tasks/datasets. In order to do so, the authors can actually move some contents of Section 4 to Appendix.

4) What are the pros/cons of the 5 different forms of regularizations?

Other comments:
1) In Section 2, domain adaptation, the author mentioned: "there exists an interesting ...". Could the authors give more details as it is not clear to me?

2) Could the authors provide more discussions about how noisy label learning relates to the proposed method? I saw some discussions in Section 4.1, but I would suggest having a section in Appendix about this.

3) Could the authors provide an intuition about the meaning of \lambda on page 4? Could the authors include the KKT derivation in the Appendix?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1e-KOrp67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks and Response to AnonReviewer2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1MmH30cY7&amp;noteId=r1e-KOrp67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1529 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1529 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for appreciating the idea and kindly pointing out the weakness of this paper, which helped us very much in comprehensively rewriting the paper. We agree with the reviewer that the paper needs significant improvement on organization and clarity with more details. We also apologize for the hurry submission that leads to bad paper writing. Given the significant difference between new version and previous one submission, we decide to withdraw our paper and resubmit to a new venue.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ryxJvRrc27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The idea presented in the paper is good but its elaboration in the current version of the paper seems preliminary.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1MmH30cY7&amp;noteId=ryxJvRrc27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1529 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1529 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">pros:

- Introducing soft-labels to self-training has a good common sense motivation.
- The experiments (if valid) suggest the method works.

cons:

- No theoretical justification. 
- One of the two contributions (connection between EM and self-training) is known.
- Messy experiments. 


- Overall assessment: 

The idea of using soft labels instead of hard labels makes sense as it allows to impose prior on the label distribution of the un-labeled data. The authors impose the label prior by introducing a regularization term into the objective. Besides the latent labels the authors also proposed to regularize the posterior distribution of the learned model. Motivation for the latter is much less clear. The authors argue that regularizing the posterior prevents over-fitting, however, then it is unclear why the regularization is NOT used also for the labeled examples. 

The proposed approach is common a sense heuristic, however no theoretical justification of the idea is presented in the paper. As a result it is unclear which regularization is suitable for given type of data and mainly if there is any guarantee that the approach will work without just trying. The authors provide an empirical study in which several different regularizes are tested. The problem is that the experiments are a bit unorganized and there is no systematic evaluation/interpretation of the results. 

- Section 5: The connection between the self-training and regularized MMLE

The connection is claimed to be one of the two main contributions of the paper. However, the connection between self-training and the "classification" (=using hard labels) EM algorithm is known, e.g. see:

Massih-Reza Amini and Patrick Gallinari. Semi-supervised logistic regression.
In Proceedings of the 15th European Conference on Artificial Intelligence, 2002.

In addition, the presented derivation has several flaws:

a) The initial likelihood should be defined for p(y|x) instead of p(x,y) because when using the NN we model the former as modeling p(x) is usually too difficult.

b) The last step is probably not correct because: the omitted term $\sum_{x_s} log p(x_s) + \sum_{x_t}p(x_t)$ can be negative in general (if p(x) &lt; 0).

c) At the end it is assumed that the auxiliary distribution $q(k)$ is one-hot vector. However, in order to derive the EM lower bound it is crucial to assume that all components of $q(k)$ are non-zero, because of $q(k)/q(k)$ term, i.e you cannot divide by zero.

- Section 6: Experiments 

Reading this section is though. The selection of the regularization constants is not discussed. The results (test accuracy in tab 3 and 4) are given without any confidence measure. In turn, it is unclear if the differences are statistically significant. It is not 100% clear how the classification setting dealt with in the paper applies to image segmentation. </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1g8XcSTa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Thanks and Response to AnonReviewer3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=r1MmH30cY7&amp;noteId=S1g8XcSTa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1529 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1529 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for appreciating the idea and kindly pointing out the weakness of this paper, which helped us very much in comprehensively rewriting the paper. We have comprehensively improved the experiments with more details, and specifically added discussions on the properties of the regularizations. We also found some interesting connection to previous works such as label smoothing. All these will be included in the newer version. 

We also apologize for the hurry submission that leads to bad paper writing. Given the significant difference between new version and previous one submission, we decide to withdraw our paper and resubmit to a new venue.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>