<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Multi-task Learning with Gradient Communication | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Multi-task Learning with Gradient Communication" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=B1e9W3AqFX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Multi-task Learning with Gradient Communication" />
      <meta name="og:description" content="  In this paper, we describe a general framework to systematically analyze current neural models for multi-task learning, in which we find that existing models expect to disentangle features into..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_B1e9W3AqFX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Multi-task Learning with Gradient Communication</a> <a class="note_content_pdf" href="/pdf?id=B1e9W3AqFX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019multi-task,    &#10;title={Multi-task Learning with Gradient Communication},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=B1e9W3AqFX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">  In this paper, we describe a general framework to systematically analyze current neural models for multi-task learning, in which we find that existing models expect to disentangle features into different spaces while features learned in practice are still entangled in shared space,  leaving potential hazards for other training or unseen tasks. We propose to alleviate this problem by incorporating a new inductive bias into the process of multi-task learning, that different tasks can communicate with each other not only by passing hidden variables but gradients explicitly. Experimentally, we evaluate proposed methods on three groups of tasks and two types of settings (\textsc{in-task} and \textsc{out-of-task}). Quantitative and qualitative results show their effectiveness.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Pretend to share, Gradient Communication</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We introduce an inductive bias for multi-task learning, allowing different tasks to communicate by gradient passing.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">3 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Hkeyx04527" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>interesting paper on improving MTL using gradient communication</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e9W3AqFX&amp;noteId=Hkeyx04527"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1197 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1197 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Paper summary: 
In this paper, the authors propose a general framework for multi-task learning (MTL) in neural models. The framework is general for including some of the current neural models for MTL. Under the framework, the author propose a new method that could allow tasks to communicate each other with explicit gradients. Based on the gradients being communicated, the system could adjust the updates of one task based on the gradient information of the other task. Also, prior task relatedness information could be incorporated to the system. 

The idea of incorporating passing gradients among tasks seems very interesting, which is new as far as I am aware of. Although the idea is simple, but it seems intuitive since purely aggregating gradient updates might have undesired cancelling effects on each other.  

There are some questions I have about this method. 
1.	I’m curious about how the sequential update in pairwise task communication affects the performance. 
2.	Also, how does sequential update nature of the method affect the training speed, as for now, the parameter update consists of two sequential steps which also involve changes to the traditional update rule. 
3.	What is fast weight for and how it is used in (9)? It would be better if there are more details on how the update is carried out during the gradient communication.
4.	Regarding the relatedness for List-wise communication, is it possible to update the relatedness dynamically? Since the pre-computed relatedness might not always make sense. During the learning of the representations, the task relatedness could change in the process.
The system framework for MTL introduced by the authors seem to be kind of isolated to the method proposed. I feel that the framework is not quite easy to understand from the way it is presented.  From my perspective, the effectiveness of analyzing MTL methods using the framework seems a bit limited to me, as it serves more like a way of abstracting MTL models instead of analyzing it. Therefore, I feel the content devoted to that part might be too much.

Overall, I think the paper is interesting although the method itself is relatively simple. And the direction of utilizing gradient communication among tasks seem interesting and could be further explored. But I do feel the organization of the paper is a bit too heavy on the framework instead of the methodology proposed. And more details of the algorithm proposed could be provided.

On a side note, I think the paper exceeds the required length limit of 10 pages if appendices are counted towards it.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rkgfxIgOh7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>just packing exsiting algorithms</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e9W3AqFX&amp;noteId=rkgfxIgOh7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1197 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">01 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1197 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper tries to address the"pretend-to-share" problem by designing the gradient passing schemes in which the gradient updates to specific parameters of tasks are passed to the shared parameters. Besides, the authors summarize existing multitask learning algorithms in a framework called Parameters Read-Write Networks (PRAWN). 

Pros:
- The view of putting existing multi-task learning algorithms in a read-write framework is quite intriguing and inspiring.

Cons:
- Motivation: The whole paper is assumed to address the "pretend-to-share" problem, while the authors never provide any evidence that such problem really exists for any other algorithm. It seems to be an assumption without any support.
- Method:  
   - Though the read-write framework is very interesting, the authors do not clearly present it, so that the readers can be totally get lost. For example, what do you mean by writing {\Theta^{*r}_k - \theta^{swr}_k}? In the line of structural read-op, where are \theta_3 and \theta_4  in the column of the constituent para. ? What do you mean by writing the equation (4)? How do you define g() in equation (8)? This is a research paper which should be as clear as possible for the readers to reproduce the results, rather than a proposal only with such abstract and general functions defined. 
   - In the list-wise communication scheme, you define the task relationship in equation (11). The problem is how do you evaluate the effectiveness of such definition, since massive works in multitask learning pursue to learn the task relationship automatically to guarantee the effectiveness instead of such heuristic definition. 
- Related works: The authors do not clearly and correctly illustrate the connections between this work and meta-learning/domain adaptation. To my best knowledge, meta-learning, including MAML (Finn et al. 2017), can obviously solve both in-task setting and out-task setting. In some sense, I think this work is almost equivalent to MAML. 
- Experiments: 
   - First, several state-of-the-art baselines including MAML and cross-stitch networks should be compared. Specifically, for the text classification dataset, there have been a lot of domain adaptation works discovering the transferable pivots (shared features) and non-pivots (specific features), which the authors should be aware of and compare in Table 3.  
   - The Figure 5 is not clear to me, and so is the discussion. The authors try to explain that the updating direction of shared parameters for PGP-SR is an integration of two private updating directions. I tried hard to understand, but still think that Figure 5(a) is even better than Figure 5(b). The updating direction of the shared parameters is almost the same as the cyan line.
- Presentation: there are so many grammatical errors and typos. For example,
   - In the introduction, "...datasets, range from natural" -&gt; "...datasets, ranging from natural"
  - In the related work, "and they propose address it with adversarial" -&gt; "and they propose to address it with adversarial"
 - In the beginning of Section 4, " an general" -&gt; "a general"</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJxg0PiSnQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Review</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=B1e9W3AqFX&amp;noteId=rJxg0PiSnQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1197 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1197 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes that models for different tasks in multi-task learning cannot only share hidden variables but also gradients.

Pros:
- The overall framework is theoretically motivated and intuitive. The idea of passing gradients for multi-task learning is interesting and the execution using fast weights is plausible.
- The experiments are extensive and cover three different task combinations in different domains.
- The results are convincing and the additional analyses are compelling.

Cons:
- I would have liked to see a toy example or at least a bit more justification for the "pretend-to-share" problem that models "collect all the features together into a common space, instead of learning shared rules across different tasks". As it is, evidence for this seems to be mostly anecdotal, even though this forms the central thesis of the paper.
- I found the use of Read and Write ops confusing, as similar terminology is widely used in memory-based networks (e.g. [1]). I would have preferred something that makes it clearer that updates are constrained in some way as "writing" implies that the location is constrained, rather than the update minimizing a loss.

Questions:
- How is the weight list of task similarities \beta learned when the tasks don't share the same output space? How useful is the \beta?
- Could you elaborate on what is the difference between pair-wise gradient passing (PGP) and list-wise gradient passing (LGP)

[1] Graves, A., Wayne, G., &amp; Danihelka, I. (2014). Neural turing machines. arXiv preprint arXiv:1410.5401.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>