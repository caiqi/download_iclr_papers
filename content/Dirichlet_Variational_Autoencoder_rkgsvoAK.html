<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Dirichlet Variational Autoencoder | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Dirichlet Variational Autoencoder" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rkgsvoA9K7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Dirichlet Variational Autoencoder" />
      <meta name="og:description" content="This paper proposes Dirichlet Variational Autoencoder (DirVAE) using a Dirichlet prior for a continuous latent variable that exhibits the characteristic of the categorical probabilities. To infer..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rkgsvoA9K7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Dirichlet Variational Autoencoder</a> <a class="note_content_pdf" href="/pdf?id=rkgsvoA9K7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 16 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019dirichlet,    &#10;title={Dirichlet Variational Autoencoder},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rkgsvoA9K7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=rkgsvoA9K7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">This paper proposes Dirichlet Variational Autoencoder (DirVAE) using a Dirichlet prior for a continuous latent variable that exhibits the characteristic of the categorical probabilities. To infer the parameters of DirVAE, we utilize the stochastic gradient method by approximating the Gamma distribution, which is a component of the Dirichlet distribution, with the inverse Gamma CDF approximation. Additionally, we reshape the component collapsing issue by investigating two problem sources, which are decoder weight collapsing and latent value collapsing, and we show that DirVAE has no component collapsing; while Gaussian VAE exhibits the decoder weight collapsing and Stick-Breaking VAE shows the latent value collapsing. The experimental results show that 1) DirVAE models the latent representation result with the best log-likelihood compared to the baselines; and 2) DirVAE produces more interpretable latent values with no collapsing issues which the baseline models suffer from. Also, we show that the learned latent representation from the DirVAE achieves the best classification accuracy in the semi-supervised and the supervised classification tasks on MNIST, OMNIGLOT, and SVHN compared to the baseline VAEs. Finally, we demonstrated that the DirVAE augmented topic models show better performances in most cases.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Variational autoencoder, Unsupervised learning, (Semi-)Supervised learning, Topic modeling</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_r1ergtDq37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>well written paper with novelty concerns </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgsvoA9K7&amp;noteId=r1ergtDq37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper293 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper293 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">In this paper, authors proposes an algorithm to use Dirichlet prior on the variational auto-encoder (VAE). They used this prior as natural conjugate to likelihood distributtion of multinomial (categorical). The paper proposes a way to use scalability power of VAE for data distributed by categorical distribution. In order to apply reparametrization trick, authors have used iid Gamma random variable to construct draw from Dirichlet distribution and have used approximation with inverse gamma CDF,  it is discussed how this method has better performance than other approximations method for gamma distribution such as Weibull and logistic Gaussian.

Authors pointed out, one of the weak points in competing models such as  Guassian softmax prior or Griffith -Engen-McCloskey prior which has been used for Stick breaking VAE is to not encouraging of having multi-modal posteriori, while this prior empower having multi-modal posteriori distribution which give them advantage over previous papers. 

 In experimental results, paper has used different datasets of MNIST, MNIST+rotation , OMNIGLOT , 20newsgroup and RCVI and used different measures to compare the existing method with the baselines. 

To summarize the contribution of this paper, following three points can be named as main contribution of this paper:
- proposed a Dirichlet prior, for categorical likelihood which encourages having multi-modal posteriori. paper demonstrates couple of techniques  to apply the reparametrization trick on Dirichlet distribution, by using sum of iid Gamma random variables.  

- used method of moments estimator to update the hyper parameter of the Dirichlet distribution which helps to have closer approximation of log likelihood. They update hyper-parameters after every few updates of VAE parameters.

-discussed how to overcome  Stick-breaking VAE “component collapse” issue. Experiments show superior results on supervised and semi supervised, and authors claimed the main reason of this superiority being due to not having disadvantage of component collapse which happens in SBVAE.


Quality and Novelty:
claims in paper are supported by proofs and/or experimental results and there does not exist significant technical issues with the details of claims made in this paper and proofs provided. There are following issues with novelty and quality of paper that I would like discuss them under following three points:

- Authors need to be clear about the motivation of the paper, if the motivation of the paper is to encourage the multi-modality in posteriori distribution, using Gaussian prior and methods like normalizing flow Rezende, Danilo Jimenez, and Shakir Mohamed. "Variational inference with normalizing flows." arXiv preprint arXiv:1505.05770 (2015) or similar may be able to do the same work in which case paper should compare its results to those ideas which has not been done in this paper.

- second appealing point that this paper can make is to use Dirichlet prior for the purposes like community detection, topic modeling and LDA  etc etc. In this case, I did not find significant difference between the proposed method and what is found in Srivastava, Akash, and Charles Sutton. "Autoencoding variational inference for topic models." arXiv preprint arXiv:1703.01488 (2017), but due to the encourages of multi-modality authors show in average DirVAE performs better in measures like perplexity and NPMI. Under this condition, my main concern is interpretablity of posteriori. That will be discussed under next point

- Main motivation behind using Dirichlet prior, is to have posteriori with a few significant related topic and many unrelated topic for every word. By changing the concentration parameter in stick-breaking, it is possible that performance of stick-breaking method increase in perplexity and NPMI scores in cost of loosing interpretability of the model. So having higher concentration parameter can show better performance in the cost of interpretablity that put second point of the paper at risk


Clarity: 
The paper is well written and previous relevant methods have been reviewed well. The organization of paper is good, experiments well explained and proofs and mathematical reasoning are clear.



Significance of experiments:  
As discussed,in previous sections, the results show superior performance and compared to other methods on semi-supervised and supervised classification on different datasets. Also it has shown in average better perplexity and NPMI score for topic modeling, the only issue can be these scores come as cost of interpretablity of the model. Also it is possible that other competing models can be matching to this results if they do not aim for sparse posteriori.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ByluKKS3aQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Responses to Reviewer3 with additional experiments. </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgsvoA9K7&amp;noteId=ByluKKS3aQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper293 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">16 Nov 2018 (modified: 18 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper293 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">To respond to your comments on quality &amp; novelty part, we did additional experiments, and some figures and tables are added or modified.

1. Regarding the paper "Variational inference with normalizing flows" (Rezende et al.), we add the followings: Table 5, Figure 5, and Table 6. In our experimental setting, the performances on pure VAE of GVAE and GVAE-NF20 were barely different, and DirVAE shows discriminative results compared to the both GVAEs. Not only the DirVAE gives better quantitative results as in Table 5 and 6, but it also has better learned latent representations which can be supported by t-SNE visualization such as Figure 3(b) and 5(b), and the decoder weight collapsing of GVAE-NF20 is one reason for such worse quality.

2. We add SBVAE augmentation on topic models as a baseline in the topic modeling experiments. The related results can be found in Table 4, Figure 7, 8, and 9. As you can see, SBVAE augmentation is better than the original model in some sense, and for a certain case, it is quite comparable. However, still, the DirVAE augmentation shows better results in most of cases in terms of datasets and the performance measures.

3. To show the interpretability of DirVAE augmentation on topic models, we add Table 8 which lists top probability words per topic. We manually re-ordered and put the topics together if there are similar semantic meanings. Also, Figure 7, 8, and 9 shows that DirVAE augmentation on topic models brought better learned latent representation than others, as in the case of pure VAE experiments.

Thanks again for your comments.

Sincerely.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Hyg1EakUTX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Responses to Reviewer3.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgsvoA9K7&amp;noteId=Hyg1EakUTX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper293 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper293 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review. 

Currently, we are doing additional experiments to respond to your constructive comments. Sorry for the delay, but we will give you proper responds to your review with results as soon as possible. Up to the current status of experiments, the VAE with the normalizing flow still suffers from the decoder weight collapsing problem. Hence, the performance would not match to our approach, but we are going to make a certain on this premature result with the experiments, next few days.

Sincerely.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_Hke6jd7w27" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Limited novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgsvoA9K7&amp;noteId=Hke6jd7w27"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper293 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper293 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes DirVAE, a variational autoencoder with Dirichlet prior on latent variables. The advantage of using Dirichlet distribution is that due the nature of Dirichlet distribution the model does not suffer from decoder weight collapsing and latent value collapsing. Stochastic gradient variational Bayes with inverse CDF reparametrization of gamma distribution is presented.

The motivation behind using Dirichlet instead of GEM makes sense, but other than that I fail to find any novelty in the paper. The authors should tone down the statement "to our knowledge, combining the two statistical results is the first finding in the machine learning field". Even though left unpublished, I've been using this combination of inverse CDF gamma reparametrization and transformation to Dirichlet all the time for my own problems. It's just trivial once we have both techniques. See also [2], where an improved way of reparametrizing gamma and Dirichlet distribution is presented. The observation that DirVAE does not suffer from latent value collapsing is interesting, but not really surprising. 

Minor question
- What is the difference between negative LL's and reconstruction losses in experiments?
- The approximation for inverse CDF of gamma works well only when alpha &lt;&lt; 1. How did you treat the regime alpha &gt; 1?


References
[1] Diederik P Kingma, Max Welling, Auto-encoding variational Bayes, ICLR, 2014.
[2] Michael Figurnov, Shakir Mohamed, Andriy Mnih, Implicit reparametrization gradients, arXiv, 2018.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1lXXtoI6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Responses to Reviewer2.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgsvoA9K7&amp;noteId=r1lXXtoI6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper293 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper293 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review. 

Firstly, we would like to say thank you for introducing the paper [2]. Even though the paper [2] and our paper lie on the same path in terms of reparametrizing Gamma distribution, paper [2] deals with a general reparametrization trick on various probabilistic distributions while our paper focuses on the advantages and the applicabilities of Dirichlet prior in VAE. We believe that our contribution is not based on reparametrizing Gamma or Dirichlet distiribution, but introducing Dirichlet prior, which is a conjugate multi-modal prior of categorical distribution, on VAE which has better learned latent representation due to no component collapsing. To support this, we did extensive experiments including topic modeling experiments and experimentally showed that DirVAE with the Dirichlet prior does not have component collapsing for the first time in this field. This component collapsing was not experimented and discussed in the prior work of [2]. Moreover, our experiments on the topic modeling shows the consistent performance increases when we apply the DirVAE, which was not discusses in [2].

The below is the response to your questions.
The log likelihood (LL) is the log-probability that a learner optimizes to train a model given an observed dataset. However, since the likelihood or log-likelihood function is intractable given a latent variable in VAE, so we use the evidence lower bound (ELBO) to optimize the LL. ELBO is a tractable alternative of LL, so the optimization on ELBO is feasible. ELBO term consists of two parts: Reconstruction Error (or Reconstruction Loss, which you asked) and KL divergence terms. Here, Reconstruction Error measures the error between the input and the output, which is an auto-encoder reconstructed input. Additionally, for your information, equation (1) in our paper, can be re-written as follows: Negative Log-likelihood &lt;= Negative ELBO = Reconstruction Loss + KL Divergence.

The author of paper [3] on the inverse Gamma recommends a finite difference approximation method when alpha&gt;1. We only encountered such alpha&gt;1 cases when we updated alphas, and the topic modeling often sets the alpha to be in the range of [0,1]. In the cases of alpha&gt;1, we approached this problem via approximating the inverse function of the Gamma CDF with a Newton method, but the learning performance was not satisfactory. Thus, we left the updated alpha parameters with values greater than one in the appendix. 

Sincerely.

References
[1] Diederik P Kingma, Max Welling, Auto-encoding variational Bayes, ICLR, 2014.
[2] Michael Figurnov, Shakir Mohamed, Andriy Mnih, Implicit reparametrization gradients, arXiv, 2018.
[3] David. A. Knowles. Stochastic gradient variational bayes for gamma approximating distributions. arXiv, 2015.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ByxO1XWLsm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A simple method giving improved results</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgsvoA9K7&amp;noteId=ByxO1XWLsm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper293 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">18 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper293 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Review:

This paper proposes to change the typical Gaussian posterior distribution (and prior) for the latent features z associated to an image x that is used in Variational Autoencoders by a Dirichlet distribution. The work improves over previous attempts based on a soft-max + Gaussian distribution and the soft-max + Weibull distribution. The trick proposed to make feasible training the model includes approximating the inverse CDF of the gamma distribution and using the fact that the Dirichlet distribution can also be obtained as a normalized sum of gamma random variables. The method is compared in several problems. Some analysis of the reasons why it performs better is also carried out.

Quality: 

	I think the quality of the paper is high. It is a well written paper in which the choices made are well supported. It also has a strong experimental section.

Clarity: 

	The paper is well written and reads very smoothly. I have missed however a more clear statement in the introduction supporting the use of the Dirichlet for the prior and posterior of the latent variables, simply because it seems to give better results and the typical Gaussian choice.

Originality: 
	
	The paper is based on ideas already known. E.g., Dirichlet a normalized sum of gamma random variables and approximation of the inverse CDF of the gamma random variable. The combination of these two techniques is however novel. 

Significance:

	The results obtained indicate that the proposed approach improves over previous work on the Dirichlet VAE and on the Gaussian VAE. So I believe the significance of the paper is high.

pros:

	- Good results.

	- Simple method proposed.

	- Extensive experiments.

	- Well written paper.

cons:
	
	- The idea is a combination of already known techniques put in practice for the VAE.

	- A better motivation that the Dirichlet VAE gives good results should be given at the introduction.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_ryx1y01Lpm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Responses to Reviewer1.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkgsvoA9K7&amp;noteId=ryx1y01Lpm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper293 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper293 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your review. 

Firstly, as a motivation for the better result, we can state as the following, and if you are okay with the below sentences, we would like to add it to the introduction part.
"Due to the component collapsing issues, the existing VAEs have less meaningful latent values or could not effectively use its latent representation. Meanwhile, DirVAE does not have component collapsing due to the multi-modal prior which possibly leads to superior qualitative and quantitative performances. We experimentally showed that the DirVAE has more meaningful or disentangled latent representation by image generation and latent value visualizations."

Secondly, although the techniques are already known, we've rather wanted to focus our paper on the characteristic of Dirichlet prior on VAE such as better latent representation due to no component collapsing, or its applicability like topic modeling.

Thanks again for your valuable comments.

Sincerely.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>