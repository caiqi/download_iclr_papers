<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>ANYTIME MINIBATCH: EXPLOITING STRAGGLERS IN ONLINE DISTRIBUTED OPTIMIZATION | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="ANYTIME MINIBATCH: EXPLOITING STRAGGLERS IN ONLINE DISTRIBUTED OPTIMIZATION" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rkzDIiA5YQ" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="ANYTIME MINIBATCH: EXPLOITING STRAGGLERS IN ONLINE DISTRIBUTED..." />
      <meta name="og:description" content="Distributed optimization is vital in solving large-scale machine learning problems. A widely-shared feature of distributed optimization techniques is the requirement that all nodes complete their..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rkzDIiA5YQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>ANYTIME MINIBATCH: EXPLOITING STRAGGLERS IN ONLINE DISTRIBUTED OPTIMIZATION</a> <a class="note_content_pdf" href="/pdf?id=rkzDIiA5YQ" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 19 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019anytime,    &#10;title={ANYTIME MINIBATCH: EXPLOITING STRAGGLERS IN ONLINE DISTRIBUTED OPTIMIZATION},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rkzDIiA5YQ},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=rkzDIiA5YQ" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Distributed optimization is vital in solving large-scale machine learning problems. A widely-shared feature of distributed optimization techniques is the requirement that all nodes complete their assigned tasks in each computational epoch before
the system can proceed to the next epoch. In such settings, slow nodes, called stragglers, can greatly slow progress. To mitigate the impact of stragglers, we propose an online distributed optimization method called Anytime Minibatch. In this approach, all nodes are given a fixed time to compute the gradients of as many data samples as possible. The result is a variable per-node minibatch size. Workers then get a fixed communication time to average their minibatch gradients via several rounds of consensus, which are then used to update primal variables via dual averaging. Anytime Minibatch prevents stragglers from holding up the system without wasting the work that stragglers can complete. We present a convergence analysis and analyze the wall time performance. We evaluate the method empirically using Amazon Elastic Compute Cloud (EC2) and show our
approach is twice as fast as fixed minibatch approaches.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">distributed optimization, gradient descent, minibatch, stragglers</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Accelerate distributed optimization by exploiting stragglers.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BygJelUjnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The contribution is not clear and significant for accelerating distributed algorithms</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkzDIiA5YQ&amp;noteId=BygJelUjnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper184 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper184 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Overall, this paper is well written with clearly presentation.
However, the contribution is not good enough to research the ICLR requirement.
Although the authors propose some method to balance the computation between distributed workers, which should be important for distributed optimization algorithm design, but not enough numerical experiments are proposed to prove the efficiency.
Even though some convergence analysis is given.
The main concern of this paper is to significantly increase the algorithm efficiency, but both theoretical and numerical results are lack of strength.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1gL2Y_y0m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We thank the reviewer for the comments. In the following we address them.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkzDIiA5YQ&amp;noteId=r1gL2Y_y0m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper184 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper184 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We briefly summarize the impact of the work as we see it. We have developed a method for distributed optimization that accounts for the real-world non-idealities of cloud computing systems, including variability in compute node throughput and load, network congestion, and hardware failure. This approach allows straggler nodes to make full use of their computations, and in numerical experiments is twice as fast as fixed minibatching. We provide a rigorous theoretical analysis of our approach applied to convex problems, showing that it can achieve the optimum regret bound and that it offers a speedup factor of up to \sqrt{n-1} over fixed minibatching. These performance improvements substantially accelerate the learning of representations from large-scale datasets, and therefore we believe our contribution will be of benefit to the ICLR community.

Although the main paper has only a few simulations, we have more numerical results in the appendix (cf., App. H, Figure 3-5). Moreover, in the revised appendix, we have included additional simulations beyond those in the original submission. For example, in response to Reviewers 1’s comments we performed additional simulations that are found in the appendix (cf., Figure 7). We performed these new experiments on EC2 inducing the wider variety of straggler types requested by Reviewer 1.  In this environment, AMB is twice as fast as FMB, taking about half the time it takes FMB to achieve the same error rate. 

We welcome and will be happy to address in detail any further critiques of this work during the rebuttal period.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_ByeDZyv5n7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Useful approach to mitigate stragglers in distributed computing setups</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkzDIiA5YQ&amp;noteId=ByeDZyv5n7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper184 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper184 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper studies distributed optimization in the presence of straggling computing nodes. In a synchronous distributed optimization approach, the stragglers delay the entire computation as the synchronization operation cannot be performed till every computing nod has completed its task. This paper aims to mitigate the effect of stragglers by proposing Anytime MiniBatch (AMB) approach, where each computing node is allowed to process the different number of samples between two synchronization steps. In particular, each node is given $T$ unit time to process as many samples as it can. After that, the nodes are allowed to aggregate the information among themselves through a consensus mechanism for another $T_c$ unit time. In contrast with this, the usual Fixed MiniBatch (FMB) approach requires each node to process a fixed number of samples before invoking aggregating step. The presence of stragglers can significantly increase the time between two synchronization step and slow down the overall optimization process. 

This paper combines their AMB approach with the dual averaging method. The paper presents sample-path wise regret bounds for convex optimization under additional standard assumptions (e.g., Lipschitz continuousness, smoothness). The paper then compares analytically and experimentally compare the speed-ups obtained by their AMB approach as compared to the FMB approach. The paper studies an interesting problem and proposes a simple and practical solution. The paper is well written and makes novel contributions with sound analysis. The experimental evaluation on the real system also corroborates the theoretical findings.

Comments/questions: 

The reviewer did not find the justification of using $c_i(t)$ in the definition of regret (cf. (17)) very clear. Is it because we also want compare with a centralized setting which does not have the communication overhead? As far as evaluation between distributed schemes (e.g., AMB, FMB etc) is concerned, shouldn't one define the regret with respect to $b_i(t)$s itself?

Can the authors comment on the setup where the communication links are also unpredictable and may experience congestion? In this case, one would encounter variable communication overhead to achieve the consensus error up to $epsilon$.

In Sec. 4, could authors comment on the settings where $O(sqrt(n-1))$ speed up is achievable?

Minor typos: In eq. (127)  E[S] -&gt; S_F, S_T -&gt; S_A. In eq. (128)  S_T -&gt; S_A.





</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BklhOquy0m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We thank the reviewer for the comments. In the following we address them.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkzDIiA5YQ&amp;noteId=BklhOquy0m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper184 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper184 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">++There are two ways to think about whether to include the c_i(t) in the definition of regret.  The first is when comparing AMB to FMB, the second is when comparing AMB to the performance attained by a single machine (i.e., a non-distributed context in which there are no communication delays.  In the first case, both AMB and FMB suffer the same communication delays and one can consider either only b_i(t) – i.e., ignoring the communication delays since both schemes suffer the same delays – or, as we do, can consider c_i(t). Either approach will lead to the same design insight.  The second case is quite different.  In the single-processor setting, we should consider c_i(t), rather than b_i(t), as the loss in potential computation during communication times that is suffered by a distributed scheme is not suffered by a single-processor approach. Thus, by deriving our results with respect to c_i(t), rather than b_i(t), one gains the flexibility to compare our scheme to either FMB or to single-processor systems.  We note that a similar approach was taken by other researchers in various papers including [Tsianos, Rabbat (2016)][ Dekel et al. (2012)].

++In AMB, there is a fixed communication time T_c. This means that each node, in general, performs several rounds of consensus until the communication period of length T_c concludes. A node that suffers from network congestion will not hold up the entire process. Our choice of fixing T_c does make the number of rounds of consensus that occur in each consensus epoch variable.  This naturally leads to variability in the amount of consensus error in each round. One can choose T_c to be large enough to meet a target average level of consensus. 

++There are distributions that achieve this (maximal) bound on the speed up. Such a distribution is given in [Bertsimas et al. (2006)] (after equation 10). Of course, not all distributions achieve that speed up. A standard distributed used to model computation time is, as already mentioned, the shifted exponential distribution.  In our paper, we show that for the shifted exponential distribution, the acceleration is log(n), which is lower than O(sqrt(n-1)). We have revised the paper to clarify this point further. 

++We thank the reviewer for pointing out the typo. We corrected the typo in our revision.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_H1eu6b-q2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good paper with a nice idea</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkzDIiA5YQ&amp;noteId=H1eu6b-q2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper184 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper184 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Summary:
The paper considers the problem of online stochastic convex optimization in a fully distributed topology. In particular, the authors focus on the synchronous setting and to avoid the slow progress that can be obtained by slow nodes, called stragglers, they propose an online distributed optimization method called Anytime Minibatch (AMB). In the update of AMB rather than fixing the minibatch size, they fix the computation time in each epoch. This characteristic prevents the stragglers from holding up the entire network, while allowing nodes to benefit from the partial work carried out by the slower nodes. 

A convergence analysis of AMB is provided showing that the online regret achieves the optimum performance. Numerical evaluations where a comparison of AMB and the "Fixed MiniBatch" method (FMB)are also presented.

Comments:
I believe that the idea of the paper is interesting and the convergence analysis seems correct, however i have some concerns regarding  the presentation and the numerical evaluation. 

1) In the title the word "online" is mentioned but never explained  in the main text. What is this mean? What are the differences compare to the "static" setting? See for example the work of [Tsianos, Rabbat (2016)] for more details on that. What are the related literature on this setting?

2) In the last paragraph of Introduction is highlighted that the algorithm AMB has the optimum performance?  The authors should add an appropriate reference there and explain why this is optimum for their setting. I believe that for the convenience of the reader current Section 5 called "previous work" can move immediately after introduction and more details of AMB with the existing literature should be provided. Probably rename the section "Closely relate work".

3) Section 2 is devoted mostly on the formal presentation of algorithm AMB. I strongly suggest the addition of a pseudocode of the algorithm in the appendix (or even in the main text if there is a space) where the reader can easily understand how the algorithm works.

4) On the Algorithm:  if some nodes are very slow and they do not make any update during the given time T what will happen? How this will affect the performance of the method? In this case does it make sense to increase the value of T.

5) On numerical evaluation:  A comparison of AMB and FMB  is presented both in synthetic and real data showing that AMB can be faster than FMB in terms of wall clock time. 
I am not sure if the performance of the AMB is as good as one should expect especially for the case of synthetic data. Will it be possible to construct a synthetic example with extremely slow nodes where the improvement of the performance is much better than 50%?

In general i find the paper interesting, with nice ideas and I believe that will be appreciated from researchers that are interested on control theory/signal processing and information theory.  Since the paper is focused on convex optimization I am not sure if it will be particularly interesting for a substantial fraction of the ICLR attendees.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1ewb3O10X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>We thank the reviewer for the comments. In the following we address them.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rkzDIiA5YQ&amp;noteId=r1ewb3O10X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper184 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper184 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">1) In our paper, “online” refers to the manner in which the nodes receive data samples: they obtain them one-by-one (or batch-by-batch).  Nodes do not have access to the entire dataset a priori. The online distributed learning problem has been studied extensively, and a few representative works (in addition to [Tsianos, Rabbat (2016)] and those already cited in our submission) are [Duchi et al. (2011)], [Dekel et al. (2012)], [Di Lorenzo, Scutari (2016)], and [Smith et al. (2018)]. Finally, we emphasize that AMB is not restricted to the online scenario.  We added the definition of online in the footnote of the first page. The algorithm can immediately be applied to settings where nodes are allocated predetermined data sets. 

2) The optimum regret bound of O(\sqrt{m}) is well established in the stochastic approximation literature (see, e.g., [Nemirovski, Yudin (1983)] as well as [(Dekel et al. (2011)].   We show that AMB can achieve this bound in our theoretical analysis under appropriate conditions. We have revised our paper to include a reference and to clarify this point.

++We thank the reviewer for the suggestion of moving previous work. We have revised the paper per the reviewer’s suggestion. 

3) We thank the reviewer for their suggestion.  We have included pseudocode in the revised appendix I (cf. Algorithm 1). Due to the space limitations, we did not find it possible to include the pseudocode in the main text.

4) Setting T to be very large is not a good choice as it will result in a large average minibatch size. We set T so that nodes compute the desired minibatch size in expectation. As the reviewer points out, by setting T this way we may sometimes encounter very slow nodes that are able to perform only very few computations. However, AMB is robust to these slow nodes as it supports variable sized minibatches in distinct epochs. The design of anytime minibatch is to have faster nodes compensate for slower nodes to get the minibatch close to the target expected minibatch size. 

5) In response to the reviewer’s request, we have created an example to match the reviewer’s proposed scenario and in which the performance of AMB is even better.  Please see Appendix H.3 and Figures 6(a), 6(b), and 7 for the details and results. We conducted our new experiment on EC2 in which we created extra jobs on some nodes to slow those nodes to mimic stragglers. As is described in Appendix H, our setup of the experiment was as follows.  We first divided the nodes into three groups. In the first group, we ran two “interfering” simulations.  In the second group, we ran one interfering simulation. In the third group, we ran no extra simulations.  The purpose of these interfering simulations was to share the resources of those nodes with other computational tasks so that they are slowed down when running AMB or FMB.  The interfering jobs were matrix multiplications that were repeated continuously throughout the experiment.  As can be seen from the histograms of the compute times taken by the nodes to calculate an FMB gradient and the variable local minibatches processed by the nodes when running AMB presented in Figures 6(a) and 6(b), three distinct clusters of node processing speed are clearly visible.  The compute speeds of nodes in the first group is more than three times slower than those of the nodes in group 3. Figure 7 compares the convergence speed of AMB to FMB in this setting.  As can be seen from the figure, AMB is now more than twice as fast as is FMB.

++We acknowledge the reviewer’s point: the paper does have broad applications beyond representation learning. However, we point out that large-scale stochastic optimization is central to modern techniques for representation learning, including (e.g.) matrix factorization and deep networks, and thus we expect the anytime approach to be of substantial use to the ICLR community. Furthermore, many recent ICLR papers study optimization for machine and representation learning (two notable examples are [Kingma and Ba, (2015)] and [Chen et al., (2016)]. Finally, although the theoretical analysis is given for convex problems, the general approach can be applied to non-convex problems, with a reasonable expectation of similar performance improvements.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>