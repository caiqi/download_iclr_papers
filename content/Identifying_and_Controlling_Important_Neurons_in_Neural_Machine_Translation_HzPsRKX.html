<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Identifying and Controlling Important Neurons in Neural Machine Translation | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Identifying and Controlling Important Neurons in Neural Machine Translation" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=H1z-PsR5KX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Identifying and Controlling Important Neurons in Neural Machine..." />
      <meta name="og:description" content="Neural machine translation (NMT) models learn representations containing substantial linguistic information. However, it is not clear if such information is fully distributed or if some of it can..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_H1z-PsR5KX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Identifying and Controlling Important Neurons in Neural Machine Translation</a> <a class="note_content_pdf" href="/pdf?id=H1z-PsR5KX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019identifying,    &#10;title={Identifying and Controlling Important Neurons in Neural Machine Translation},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=H1z-PsR5KX},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Neural machine translation (NMT) models learn representations containing substantial linguistic information. However, it is not clear if such information is fully distributed or if some of it can be attributed to individual neurons. We develop unsupervised methods for discovering important neurons in NMT models. Our methods rely on the intuition that different models learn similar properties, and do not require any costly external supervision. We show experimentally that translation quality depends on the discovered neurons, and find that many of them capture common linguistic phenomena. Finally, we show how to control NMT translations in predictable ways, by modifying activations of individual neurons.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">neural machine translation, individual neurons, unsupervised, analysis, correlation, translation control, distributivity, localization</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Unsupervised methods for finding, analyzing, and controlling important neurons in NMT</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">7 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Byl0raGq2X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting analysis of the contributions of different neurons in NMT</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1z-PsR5KX&amp;noteId=Byl0raGq2X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper240 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper240 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper presents unsupervised approaches to discover import neurons in
neural machine translation systems. Some linguistic properties controlled by the
discovered neurons are discussed and analyzed.

Strengths:

The paper is well-written and provides valuable information to understand the
behaviour of neural machine translation models.

The ability to control characteristics (such as gender) without training
specialized models is promising, even if the results are not good enough for
immediate use. It would be interesting to see whether controlling neurons
in the decoder would be more effective.

Weaknesses:

Multiple NMT systems are necessary to discover important neurons. The authors
mention that it would be possible to use different checkpoints from a single
model, but don't evaluate how well this would work.

The findings in this paper do not lead to immediate translation performance
improvements.

Questions and other remarks:

In Table 4a, why are there 2 results for "-0.25, -0.125, 0"?

In section 4.3 (Tense), it may be worthwhile to mention that the neuron is
highly activated on the word "Spreads", even if it acts as a noun in this
specific sentence.

Bottom of p. 6: "Our supervised methods" -&gt; "Our unsupervised methods"

To control properties, could SVCCA directions or coefficients be manipulated?

Some parentheses around citations are missing or misplaced.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_B1gWh83lCQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1z-PsR5KX&amp;noteId=B1gWh83lCQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper240 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper240 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your positive and constructive feedback. We are happy that you find our analysis interesting and valuable for understanding the behavior of neural MT models. We answer specific comments below. 

1.  “Controlling neurons in the decoder”
We are also interested in expanding the controlling experiments, both to other properties and to the decoder side, and intend to do so in the future. We will mention this as potential future work. 

2. “use different checkpoints from a single model” 
Thank you for bringing this point up. We have compared all checkpoints from a couple of our models and found highly correlated neurons, especially when correlating later checkpoints. This makes sense as the model converges to a solution. We verified that these top correlated neurons are important for the model performance via an erasure experiment similar to Section 5.1. 
Moreover, the top ranked neurons when comparing the last checkpoint to earlier ones are very similar to the ones found when comparing this last checkpoint to different models, including models trained with different target languages. In particular, for the English-Spanish model, we found that 8 out of 10 and 34 out of 50 top ranked neurons are the same in these two rankings. For the English-Arabic model, we found a similar behavior (7 out of 10 and 33 out of 50 top ranked neurons are the same). This indicates that our method may be applied to different checkpoints as well. We will add these results in the next revision. 

3. “The findings in this paper do not lead to immediate translation performance improvements” 
This is correct. Beyond the scientific value in illuminating how NMT models work, we would also like to mention several potential ideas for improving the systems. First, our experiments for controlling specific characteristics may help mitigate model bias by identifying neurons responsible for sensitive attributes such as gender or politeness. For instance, we might have external knowledge of the gender of person mentioned by an ambiguous profession or title in the source language (e.g., doctor), and may want to encourage the translation to be of the correct gender in the target language (as the Turkish example in Section 6 illustrates). More generally, by identifying neurons that are responsible for common mistakes we may be able to improve the system through similar control experiments. Other directions for improving NMT systems are doing model compression by removing unimportant neurons and guiding neural architecture search by tracking important neurons. 

4. “Table 4a, two results” 
Thank you. We have fixed this typo.

5. Tense neuron activating on “Spreads”
Indeed, this is a “mistake” of the neuron. We will mention this. 

6. "Our supervised methods" → "Our unsupervised methods"
Fixed. Thank you. 

7. “Could SVCCA directions be manipulated”
This is difficult to do, as it requires changing all the dimensions, rather than a small number of dimensions, and we’ve noticed that modifying more dimensions leads to performance degradation. Moreover, SVCCA directions mostly detect specific tokens rather than a linguistic property (see Table 1) so controlling is not very intuitive in this case. 

8. Missing or misplaced parentheses 
We have fixed those. Thank you. 
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rklAXqtrhX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>This paper presents unsupervised methods for ranking neurons in machine translation. Important neurons are thus identified and used to control the MT output.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1z-PsR5KX&amp;noteId=rklAXqtrhX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper240 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">30 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper240 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Strengths:
- even though the methods for detecting important neurons are not novel (as also stated in the paper), their application to MT is novel
- the presentation is very clear
- the choice of methods is well argued and justified
- the experiments are well executed and analysed
- thorough and varied analysis of the experimental findings 

I recommend this paper for the best paper award.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">10: Top 5% of accepted papers, seminal paper</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BklYGw3lRQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1z-PsR5KX&amp;noteId=BklYGw3lRQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper240 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper240 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your very positive review. We are glad that you found the choice of methods justified, and the experiments and analysis thorough and well executed. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_r1l4JK8ijQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Well-written paper applying a method for finding individual influential neurons to MT, but insight is ultimately limited</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1z-PsR5KX&amp;noteId=r1l4JK8ijQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper240 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">22 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper240 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The authors propose a number of methods to identify individual important neurons in a machine translation system. The crucial assumption, drawn from the computer vision literature, is that important neurons are going to be correlated across related models (e.g. models that are trained on different subsets of the data). This hypothesis is validated to some extent: erasing the neurons that scored highly on these measures reduced BLEU score substantially. However, it turns out that most of the activation of the important neurons can be explained using sentence position. Supervised classification experiments on the important neurons revealed neurons that tracked properties such as the span of parentheses or word classes (e.g., auxiliary verbs, plural nouns, etc).

Strengths:
* The paper is very well written and provides solid intuitions for the methods proposed.
* The methods seem promising, and the degree of localist representation is striking.
* The methods may be able to address the question of *how* localist the representations are (though no numerical measure of localism is proposed).
* There is a correlation between the neuron importance metrics proposed in the paper and the effect on BLEU score of erasing those neurons from the network (of course, it’s not clear what particular linguistic properties are affected by this erasure - the decrease BLEU may reflect inability to track specific word tokens more than any higher-level linguistic property).

Weaknesses:
* It wasn't clear to me why the neurons that track particular properties (e.g., being inside a parentheses) couldn't be identified using a supervised classifier to begin with, without first identifying "important" neurons using the unsupervised methods proposed in the paper. The unsupervised methods do show their strength in the more exploratory visualization-based analyses -- as the authors point out (bottom of p. 6), the neuron that activates on numbers but only at the beginning of the sentence does not correspond to a plausible a-priori hypothesis. Still, most of the insight in the paper seems to be derived from the supervised experiments.
* The particular linguistic properties that are being investigated in the classification experiments are fairly limited. Are there neurons that track syntactic dependencies, for example?
* I wasn't sure how the GMMs (Gaussian mixture models) for predicting linguistic properties from neuron activations were set up.
* It's nice to see that individual neurons function as knobs that can change the gender or tense of the output (with varying accuracy). At the same time, I was unable to follow the authors' argument that this technique could be used to reduce gender bias in MT.
* I wasn't sure what insight was gained from the SVCCA analyses -- this method seems to be a bit of a distraction given the general focus on localist vs. distributed representation. In general, I didn’t come away with an understanding of the pros and cons of each of the methods.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_rkeX0uhx0Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 3: part 2/2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1z-PsR5KX&amp;noteId=rkeX0uhx0Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper240 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper240 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">
7. “what insight was gained from the SVCCA analyses” and “the pros and cons of each of the methods” 
Thank you for the feedback. We will improve the discussion in the next revision accordingly. 
The different methods aim to analyze representations at different levels of localism/distributivity. In particular, while MaxCorr and MinCorr target pairwise neuron correlations, LinReg searches for information that’s distributed in the whole representation in one network, but localized in a different network. SVCCA tries to find a middle ground by projecting representations to a lower-dimensional space and then computing correlations. 
Some of the insights are discussed in Section 5.2, where we observe that the more distributed methods (LinReg and SVCCA) give much importance to identifying specific tokens. This means that information about token identity is distributed among many neurons. The fact that MinCorr cares a lot about position suggests that this kind of information is captured in multiple models in a similar way. Table 10 in the appendix shows that in addition to detecting specific tokens, SVCCA directions may also capture some classes like adjectives and verbs.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ByeWhune0X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Response to Reviewer 3: part 1/2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=H1z-PsR5KX&amp;noteId=ByeWhune0X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper240 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">20 Nov 2018</span><span class="item">ICLR 2019 Conference Paper240 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your useful feedback and helpful comments. We are glad that you found our methods promising with good intuitions. We would like to clarify a few points based on your comments.

1. “Most of the activations of the important neurons can be explained using sentence position” 
It is true that many top ranked neurons capture sentence position, especially in the MinCorr method (Table 1). However, other methods reveal important neurons that do not capture position: only 3/10 top ranked neurons by MaxCorr and LinReg are position neurons, and only one top ranked SVCCA direction captures position. These top neurons often capture linguistic properties like morphological features, punctuations, word classes, etc., as analyzed in Section 5.3 and in appendices A and C. We will make this point clearer in the next revision. 

2. “The methods may be able to address the question of *how* localist the representations are (though no numerical measure of localism is proposed)”
We also believe that our methods can shed light on the question of how localist the representations are. We would love to try out any suggestions for numerical measures of localism.  

3.”why the neurons that track particular properties couldn't be identified using a supervised classifier to begin with” and “most of the insight in the paper seems to be derived from supervised experiments”
The advantage of unsupervised methods is that they are not constrained by available supervision in the form of linguistic annotations. Our working process involved visualizations of the important neurons, which led to forming hypotheses about their function. In order to validate our interpretations, we designed supervised experiments whenever we could. While this may give the impression that most insights are derived from the supervised experiments, in practice it would have been difficult to choose specific properties to target without the unsupervised methods + visualizations. 
In addition, we have found properties that do not correspond to plausible a priori hypotheses. The neuron detecting item numbers which you mention is one such case. We also found a neuron that activates positively on the first word in a noun phrase and negatively in the rest of the phrase (Figure 5). Other properties that may not be expected to emerge include year and month neurons (Figure 6), a neuron that activates on verbs and their surrounding words (Table 7), and neurons that capture both punctuation and conjunctions (Tables 6+7; note that this would not be captured by standard part-of-speech tag sets). 
We will improve our presentation according to your feedback. 

4. “are there neurons that track syntactic dependencies, for example?” 
In this work we focused mainly on word-level properties. However, we did investigate parentheses, which require longer-range context, and also noun phrases. Still, we agree that it would be interesting to consider more compositional properties such as dependencies or phrase structure. 

5. “how the GMMs … were set up” 
The GMMs were set up to predict a property from a neuron activation. The number of mixture was chosen as the number of different classes in the prediction task. For instance, for finding parenthesis neurons we used two classes (inside or outside of parentheses/quotes/brackets). We estimated the parameters of the GMMs using the mean and variance of the neuron activation conditioned on each class. We tested the resulting model to see how well it predicts the tag from the neuron activation by computing the posterior probability of each class given an activation using Bayes' rule, and taking the argmax. We will provide more details on the GMM in the next revision. 

6. “argument that this technique could be used to reduce gender bias in MT” 
Our reasoning in the control experiments is that some information about sensitive attributes like gender may be available from other sources, such as metadata. If we know that an entity has a specific gender (say, feminine), but that gender is unmarked in the English language (as in the word “doctor”), then we may encourage the system to output a translation with the correct gender by modifying gender neurons. This is a kind of soft constraint that we may add to the system. We will improve the motivation in the next revision. </span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>