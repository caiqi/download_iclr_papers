<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>The Body is not a Given: Joint Agent Policy Learning and Morphology Evolution | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="The Body is not a Given: Joint Agent Policy Learning and Morphology Evolution" />
        <meta name="citation_author" content="Dylan Banarse" />
        <meta name="citation_author" content="Yoram Bachrach" />
        <meta name="citation_author" content="Siqi Liu" />
        <meta name="citation_author" content="Chrisantha Fernando" />
        <meta name="citation_author" content="Nicolas Heess" />
        <meta name="citation_author" content="Pushmeet Kohli" />
        <meta name="citation_author" content="Guy Lever" />
        <meta name="citation_author" content="Thore Graepel" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=BJgWl3A5YX" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="The Body is not a Given: Joint Agent Policy Learning and Morphology..." />
      <meta name="og:description" content="Reinforcement learning (RL) has proven to be a powerful paradigm for deriving complex behaviors from simple reward signals in a wide range of environments. When applying RL to continuous control..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_BJgWl3A5YX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The Body is not a Given: Joint Agent Policy Learning and Morphology Evolution</a> <a class="note_content_pdf" href="/pdf?id=BJgWl3A5YX" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=dylski%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="dylski@google.com">Dylan Banarse</a>, <a href="/profile?email=yorambac%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="yorambac@google.com">Yoram Bachrach</a>, <a href="/profile?email=guylever%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="guylever@google.com">Siqi Liu</a>, <a href="/profile?email=heess%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="heess@google.com">Chrisantha Fernando</a>, <a href="/profile?email=pushmeet%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="pushmeet@google.com">Nicolas Heess</a>, <a href="/profile?email=liusiqi%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="liusiqi@google.com">Pushmeet Kohli</a>, <a href="/profile?email=chrisantha%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="chrisantha@google.com">Guy Lever</a>, <a href="/profile?email=thore%40google.com" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="thore@google.com">Thore Graepel</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Reinforcement learning (RL) has proven to be a powerful paradigm for deriving complex behaviors from simple reward signals in a wide range of environments. When applying RL to continuous control agents in simulated physics environments, the body is usually considered to be part of the environment. However, during evolution the physical body of biological organisms and their controlling brains are co-evolved, thus exploring a much larger space of actuator/controller configurations. Put differently, the intelligence does not reside only in the agent's mind, but also in the design of their body. 
We propose a method for uncovering strong agents, consisting of a good combination of a body and policy, based on combining RL with an evolutionary procedure. Given the resulting agent, we also propose an approach for identifying the body changes that contributed the most to the agent performance. We use the Shapley value from cooperative game theory to find the fair contribution of individual components, taking into account synergies between components. 
We evaluate our methods in an environment similar to the the recently proposed Robo-Sumo task, where agents in a 3D environment with simulated physics compete in tipping over their opponent or pushing them out of the arena. Our results show that the proposed methods are indeed capable of generating strong agents, significantly outperforming baselines that focus on optimizing the agent policy alone. 

A video is available at: www.youtube.com/watch?v=eei6Rgom3YY</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">Reinforcement Learning, Continuous Control, Evolutionary Computation, Genetic Algorithms, Evolving Morphology, Baldwin Effect, Population Based Training</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">Evolving the shape of the body in RL controlled agents improves their performance (and help learning)</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_BylYX10Pa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good but novelty concerns</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJgWl3A5YX&amp;noteId=BylYX10Pa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1047 AnonReviewer5</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1047 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This review will be short considering the current state of the paper and the reviews. I would just like to note to the authors that a somewhat similar piece of work was done in "Learning Locomotion Skills Using DeepRL: Does the Choice of Action Space Matter?". The authors optimized muscle insertion locations as part of their results. There is also an abstract on the topic that learns how to add links and morphology in "Real-time motion generation for imaginary creatures using hierarchical reinforcement learning".

Best of luck, look forward to seeing more work.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_Byg1aq4vp7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJgWl3A5YX&amp;noteId=Byg1aq4vp7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1047 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1047 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HkxvvqYITQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>More experiments are needed to show the effectiveness and novelty</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJgWl3A5YX&amp;noteId=HkxvvqYITQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1047 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1047 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper discuss the joint evolution of controllers and topologies of agents.
The PEOM algorithm in the paper incorporates Shapley value to accelerate the evolution by identifying contribution of each body part.

Side note: I am an added emergency reviewer due to the missing of reviewer 2.
Sorry for the caused trouble. I am already many days late, so I try to make this review available as soon as possible.
Although I would not say that I understand the paper perfectly,
this review should cover the contents enough for the authors’ rebuttal. And I will probably update the reviews in the following days.

*******    Pros    *******:
---- I really enjoy the idea of identifying the contribution of each body parts. I believe it should be a good direction to go for research on automatic robot design or creature evolution.
---- The topic of joint policy &amp; morphology learning is interesting and deserves more attention.
 
*******    Cons    *******
---- The contribution and novelty of the paper is relatively limited other than the use of Shapley value.
---- The experiment session really undermines the value of this paper. (see details below).
---- It is not clear what information is delivered in the paper, in terms of what components of PEOM are beneficial and what are not. 
---- This paper could benefit from better writing. (see details below)

*******    Detailed comments and suggestions    ******* 
---- Why does the author start with a multi-agent competing environment like sumo?
As the author also pointed out in the paper, "Further, changes in the agent’s own body also contribute to the non-stationary nature of the environment",
it seems to me that the algorithm should be first tested in a single agent environment.
The introduction of multi-agent and Elo rating, seems to me, complicates the problem.
---- It will be great if the author could well introduce and formulate the objective function at the beginning method section.
---- (This is not a criticism) For the training algorithm of the controllers, why do the authors choose SVG?
There are other more stable and well-used algorithms (both model-free and model-based) available, for example TRPO / PPO / PETS algorithm.
---- For agent i to inherit policy network from agent j by cross-over, how does it work exactly?
In the paper it is simply said that “and body configuration parameters are set to the target j’s configuration.” 
If the agent i and agent j has different topologies, then parameters for network i and network j should not be transferable.
---- Shapley value seems to a core idea of PEOM. For the better presentation of this paper, it will be great if a background section (with math) on Shaley value is included.
---- Ablation study on Shapley value is needed.
It seems to me that the introduction of Shapley value also introduce the problem of falling into local-minima (more easily).
My personal understanding of the difficulty of creature evolution is that, it is easily stuck in local-minimas.
For example, (for a two-legged under-evolved ant) evolving only one leg might be bad, as it adds more difficulty to controlling and introduce unbalancing.
But adding two symmetrical leg will boost the performance.
However, Shapley value might discourage evolving a leg.
How does POEM overcome this?
---- More experiments are needed.
Currently only one environment type is tested (the ant). More experiments will greatly increasing the quality of the paper.
---- It seems that POEM does not evolve creatures of different topologies by looking at the youtube video.


---- General comment ---- 

In general, I believe more experiments and better presentation is needed for this paper.

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_S1xXJjvyaX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Compelling idea, limited novelty, insufficient experiments</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJgWl3A5YX&amp;noteId=S1xXJjvyaX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1047 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1047 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper is generally clear and well structured, with the exception of the proposed approach which is not clearly explained.
Although not very novel, the topic considered is fairly interesting as it might tap into important questions about natural evolution. 

Pro:
- The use of multi-agents cooperative game theory is interesting and well fit with the evolution of bodies. 
Cons:
- This cooperative game theory formulation is not really explored in details and has little practical consequences to the experimental setting.
- The contribution of the paper is very limited. The idea of jointly optimizing bodies and controllers has been extensively explored in the existing literature. Compared to this literature, I do not see any significant novelty being introduced.
- The actual algorithm used is unclear to me. Where is SVG being used in algorithm 1?
- The experimental results are weak (more details below).
- The paper is often not sufficiently rigorous in its statements (see details below).

In its current form, the paper does not seem to be a significant contribution to the literature.


Minor points:
- "the the" in the abstract
- In the paragraph at the end of the first page and the beginning of the second page, there is a number of statements about the role of the body and intelligence. In a scientific article, this type of unsupported and hand-wavy statements is unacceptable and must either be supported by previous works or clearly indicated as a hypothesis.
- What is a "physically simulated environment"? I guess that you refer to a software simulator, but this wording is non-sense. A physically simulated environment to me suggests that somebody physically simulates the environment with action-figures or something similar. 
- In the related work, you indicate a number of previous work from the evolutionary community without comparing them to your work. Instead, you briefly conclude that "in contrast, we use an RL algorithm". Since both evolutionary strategies and RL are nothing more than optimizers (and in particular, since your approach is also based on populations and evolution), you need to explain this connection significantly more in details.
- One aspect that I find worthy of further explanations in the paper it is the use of self-play and a population of agents. This could be at the same time beneficial and a limitation. Thinking about and discussing both would be valuable.
- please use a spell checker: "algoithm"
- The experimental results in Sec 3.1 this moment only show that adding parameters of the body increase performances -- which is a fairly well-studied phenomenon in the literature. No comparisons to previous literature are performed (e.g., evolutionary strategies). Moreover, it naturally raises the question of how would an equal-weight fix-configuration robot perform.
- Fig.5A should be in the number of iterations of similar metric. Using time can be misleading and unclear (is it the computational time or equivalent real-time?). Whatever metric you decide to use, you should accurately describe how it is computed.
- It would be nice to have an intuitive explanation of what elo scores correspond to behaviorally.
- statements such as the "in 64% of the runs" would benefit from including a plot of the whole distribution.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">3: Clear rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_HyxSIWgTn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting study, results not convincing, analysis provides a biased picture</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJgWl3A5YX&amp;noteId=HyxSIWgTn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1047 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1047 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper proposes a way to optimize morphology and controller of a simulated robot.

Strong aspects:
- Combination of Evolutionary search over morphology and hyper parameters with RL for controller
- Introducing analysis tools from game theory to the community
- entertaining video
- good presentation/clear writing


Weak aspects:
- authors do not mention that they will make the code public
- presentation is based on the best of 50 runs! They are not directly hiding this fact, but when looking through the results they appear much better than they are.
  Only in 70% of the cases their algorithm is finding a better morphology! Not sure we can call this a success, given that it takes an enormous training effort.   
- Only one particular environment is considered
- The result that controllers from body-improving population is better (in 64% of cases) than with fixed body is suspicious and would have needed a stronger analysis.
 a) in principle the controller optimized for the fixed body should not be out beaten by a controller that was optimized for a different morphology
 b) when considering the process of changing morphology as a source of noise then the controllers in the body-improving sub-population might be able to find a different strategy which is also better on the original body, but was not found. Still, a bit strange.
- The Shapley analysis is not really convincing: It really depends on how the controller is adapted to these changes. For instance if the changes were early during evolution than they are very important because the controller would "forget" how to behave without them. So a temporal aspect would have been important as a baseline. Also, running the same analysis with many agents, not just the one best agent from 50 runs. This is not a generic or particularly useful result. 
- in evolutionary robotics work studying the interplay of learning and evolution, the body would vastly change, in order to obtain surprising and creative solutions. Here is it merely a fine-tuning operation.

- When referring to the Baldwin and Lamarkian evolution, it would be useful to state what this paper studies, i.e. the unbiological Lamarkian version.

What did we really learn from the paper? It not really surprising that an adapted morphology can be better than an unadapted one. I understand that it is not necessarily easy to obtain this result, but that the algorithm is only managing this in 70% of the cases is actually not a good sign. Since the analysis is based on a particular run, how does the worst run look like? Which conclusions would have been made with the given analysis tools? </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">3: The reviewer is fairly confident that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_BylMqYHJa7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>The success rate of the procedure is high (over 95% when allowing multiple independent runs or training for longer); We'll examine the Shapley values distribution across runs and time.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJgWl3A5YX&amp;noteId=BylMqYHJa7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1047 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1047 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Our analysis is indeed based on many (50) runs of our proposed procedure, where each run pits agents from a body-evolving sub-population against agents from a fixed body sub-population. At a high level, our procedure is a random algorithm which searches a high-dimensional parameter space (consisting of the parameters of the body and of the controller) for an effective agent configuration; it is heavily influenced by the initial configuration chosen randomly for the initial agent populations. Indeed, not every run of the procedure yields an agent that outperforms the original body (which we believe is a relatively strong body). 

First, note that when seeking a good agent body, it is enough for even a *single* evolution run to succeed in finding a good configuration to achieve the goal: once a good agent (body and controller) is found in one run, it can be used from that point onward. Similarly to many other random algorithms which have a certain failure probability, when a run fails, we can simply start another run, until eventually one run succeeds. Given that the success of a runs are i.i.d (i.e. each run is an independent Bernoulli variable), even for a success rate of only 70% for a single run, 4 runs are sufficient to achieve a success probability of over 99%. Also, note that the controller in the evolving body population is at a disadvantage, as it has to be robust to changes in the agent's body (as opposed to the learner in the body-fixed population, which need only find a policy that works well for a single body), so it is reasonable for the procedure not to achieve a success probability of 100%. 

Nevertheless, as you pointed out, the computational burden of each such run is considerable. Our experiments have capped the change of every parameter at +/-10% of its original configuration (i.e. the allowed changes were not extreme), and have only trained agents for 10 hours. Clearly, allowing a cap of more than 10% allows more extreme changes of the body, and running the procedure for more time gives the procedure a better chance at finding a better morphology. Indeed, we've launched 50 more runs with caps of +/- 25% of the original configuration, and let the procedure run for 36 hours, resulting in 95% of the runs "succeeding" in finding a superior body. We'll add a discussion of this in the revised paper. 

Ultimately, as we discuss in the paper, we view the high computational cost as the key limitation of our approach. However, running the procedure for 36 hours instead of 10 hours, or alternatively launching 4 runs of the procedure instead of only one run, both result in a success rate of over 95%. Since handling 4 times the compute demand seems reasonable, we believe our proposed procedure is quite effective in finding good agents (body and controller). Given your feedback, we'll add a discussion of this in the paper. 

Turning to the Shapley value analysis, we acknowledge that is interesting to see whether the relative importance of body changes varies across the runs. Indeed, a body change that is critical in one body configuration may not be effective in another, and it would be interesting to see whether there are components that are important in most evolved morphologies. Based on your comment, we'll add a distribution boxplot for the Shapley value of components across runs. We'll also add a temporal analysis, showing how the Shapley values of body parts change over time (training steps). Having looked at a small sample, it seems there is some correlation between Shapley values across runs (and across time), so some body parts are important in many body configurations found. We'll perform the analysis on all runs and update the paper. 

You've also mentioned that you were surprised that a many (64%) of the controllers learned in  body-evolving population turned out to outperform those trained in the body-fixed population. Indeed, we have also found this surprising. As you point out, this may mean that perturbing the body during the learning allowed the learner to uncover a good behavior it missed when training with fixed-bodies. As you suggest, we'll extend this analysis to all runs, to see how often this occurs. If this phenomenon persists persists across runs, it may indicate that one can escape a local policy optima by perturbing the environment during training. Although this is an intriguing effect, this is not the focus of this paper. 

Finally, we wholeheartedly agree that while the current procedure allows changing the parameters of the agent's body, it does not allow complete changes, such as growing new limbs, or forming new actuators (in fact, the Shapley analysis uses the fact that there are no new body components or actuators). We think this is a great direction for future work.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_rJeFkKB1aQ" class="note panel trashed"><div class="title_pdf_row clearfix"><h2 class="note_content_title">  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=BJgWl3A5YX&amp;noteId=rJeFkKB1aQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">[Deleted]</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="item">ICLR 2019 Conference Paper1047 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>