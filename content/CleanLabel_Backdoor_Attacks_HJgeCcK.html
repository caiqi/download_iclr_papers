<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Clean-Label Backdoor Attacks | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Clean-Label Backdoor Attacks" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=HJg6e2CcK7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Clean-Label Backdoor Attacks" />
      <meta name="og:description" content="Deep neural networks have been recently demonstrated to be vulnerable to backdoor attacks. Specifically, by altering a small set of training examples, an adversary is able to install a backdoor..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_HJg6e2CcK7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clean-Label Backdoor Attacks</a> <a class="note_content_pdf" href="/pdf?id=HJg6e2CcK7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 20 Nov 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019clean-label,    &#10;title={Clean-Label Backdoor Attacks},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=HJg6e2CcK7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span><a class="note_content_pdf item" href="/revisions?id=HJg6e2CcK7" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Deep neural networks have been recently demonstrated to be vulnerable to backdoor attacks. Specifically, by altering a small set of training examples, an adversary is able to install a backdoor that can be used during inference to fully control the model’s behavior. While the attack is very powerful, it crucially relies on the adversary being able to introduce arbitrary, often clearly mislabeled, inputs to the training set and can thus be detected even by fairly rudimentary data filtering. In this paper, we introduce a new approach to executing backdoor attacks, utilizing adversarial examples and GAN-generated data. The key feature is that the resulting poisoned inputs appear to be consistent with their label and thus seem benign even upon human inspection.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">data poisoning, backdoor attacks, clean labels, adversarial examples, generative adversarial networks</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We show how to successfully perform backdoor attacks without changing training labels.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">6 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_Bke-NGHlTQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Clean-Label Backdoor Attacks</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJg6e2CcK7&amp;noteId=Bke-NGHlTQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1118 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">07 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1118 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">
This work explores backdoor attacks -- attacks that alter a fraction of training examples which can alter inference -- while ensuring that the poisoned inputs are consisten
t with their labels. These attacks are attained through either a GAN mechanism or using adversarial perturbations.

The ideas proposed (i.e. GAN mechanism and adversarial mechanism) are interesting additions to this literaature. I found the observation of greater effectiveness of adversa
rial mechanism particularly interesting.

The paper also does a good job of investigating effectiveness of the attack under data augmentation and propooses a limited solution.

Main criticism: there are a number of typos that need fixing.
~                                            </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">6: Marginally above acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1xZxO3J0X" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJg6e2CcK7&amp;noteId=r1xZxO3J0X"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1118 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1118 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the kind comments. We have updated the manuscript to fix typos.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rkgXH4Xwn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>I think this paper adds an original and valuable angle to the existing literature on data poisoning attacks</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJg6e2CcK7&amp;noteId=rkgXH4Xwn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1118 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1118 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Overall I am positive about this manuscript:
- I find the motivation is clear and valid. As far as I know, this is a novel contribution (my confidence is not very high on that one though - I might be unaware of related work).
- The paper is well-written and organized.
- Experiments are conducted systematically, although certain parts could be better explained (see my questions below).

I think this paper adds an original and valuable angle to the existing literature on data poisoning attacks. I don't see any major flaws, therefore I think it should be accepted.

A few points which might need clarification:
- How exactly is "attack success" being measured?
- Which model is used to generate the adversarial samples? Is this an (adversarially) pretrained model? (If that's the case, then what is the model architecture?) Or are adversarial samples generated on the fly using the currently trained/poisoned model?
- At the end of Section 4.4: if the images with larger noise rely more on the backdoor, why does this have an adverse effect? Shouldn't it increase the effectiveness of the attack?
- Was the data augmentation (flips, crops etc) performed before or after the poisoning pattern was applied?

Minor comments:
- definition of the encoding at the bottom of page 4: this should be argmax instead of max
- typo in Sec. 5.1: "to evaluate the uat a wide variety"
- repetitive sentence in Sec. 5.2: "we find that images generated with $\tau \leq 0.2$ remain [fairly] plausible"

</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">7: Good paper, accept</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1xefd2J0m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJg6e2CcK7&amp;noteId=H1xefd2J0m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1118 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1118 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the kind comments and helpful suggestions. We will address points raised below:
- The attack success rate (ASR) is computed as the fraction of inputs that are _not_ labeled with the target class but are classified as the target class after the backdoor pattern is applied (Beginning of Section 5). We have edited the manuscript to make this definition appear more prominently earlier in the paper and edited the relevant captions.
- We use adversarially trained models trained with the publicly available code from <a href="https://github.com/MadryLab/cifar10_challenge" target="_blank" rel="nofollow">https://github.com/MadryLab/cifar10_challenge</a> (we train the non-wide variant both with L2 and Linf). The adversarial examples are generated once using this pre-trained network. Since our threat model only allows us to add examples to the training set, we cannot compute these adversarial perturbations on the fly. We have edited the manuscript to incorporate this discussion.
- We were also surprised initially but we believe that there is a fairly simple explanation (outlined in Section 4.4). On noisy images, the classifier learns to predict by relying on the backdoor *in the absence of strong image signal* (since the salient image features are fairly corrupted). However, when evaluated on the test set with a backdoor applied, the image itself will have a strong signal (since it will not be noisy) that can overcome the backdoor pattern. Therefore, it is necessary for the classifier to learn to predict the backdoor even when the salient image characteristics are present. As a result, random noise is not very effective at injecting backdoors. We have updated Section 4.4 to better reflect this argument.
- Since we do not have access to the training procedure, the pattern is applied before any data augmentation. This is the reason why this setting is challenging -- data augmentation might obscure the pattern.

We have updated the manuscript to incorporate the other comments.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_SJgU__OUn7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A nice idea which needs further in-depth exploitation</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJg6e2CcK7&amp;noteId=SJgU__OUn7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1118 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">31 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper1118 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper investigates an interesting problem, backdoor attack against neural networks. The main idea is to add a watermark pattern to the corners of the training images, so that the classifier is guided to leverage the watermark as a discriminative cue as opposed to the real content of the image. At the test stage, one can hence manipulate the classifier’s predictions by adding the watermark to the test images.

This paper is heavily built upon Gu et al. (2017)’s work. It shows that Gu et al. (2017)’s method can be easily defended by a data sanitization algorithm. To improve Gu et al.’s work, the authors propose to add watermark patterns to the adversarial examples or examples interpolated in GAN’s latent space. The intuition is that these examples are adversarial and hard to learn, forcing the classifier to focus on the watermark pattern instead. 

It is an interesting idea and an intuitive improvement over (Gu et al. 2017). However, the implementation of the idea could be improved. This paper does not propose any new attack algorithms. Instead, it investigates an existing adversarial attack method and the GAN based interpolation for the purpose of backdoor attack. As experiments are conducted on small-scale datasets, it is unclear how effective the improved backdoor attack is. Moreover, one of the main disadvantages of the proposed attack method is that simple data augmentation techniques, especially random cropping, can successfully defend against the attack. 

The quality of the paper writing could be improved. I had to read the paper more than twice and check the references now and then in order to understand some claims of the paper. The paper’s lack of clarity was actually also raised by probably one of the coauthors of the paper; see the comment “Dimitris: clarify this point” on Page 11. Please find some concrete suggestions below.
- Figure 1 is visually not appealing at all. Perhaps find better illustrative examples. 
- It is worth considering to add a separate section/paragraph to describe the details of Gu et al. (2017)’s method, given that this paper is heavily built upon Gu et al. (2017)’s work.
- It was unclear what the “reduced amplitude backdoor trigger” means until Section 4. If a context-dependent term has to be used in the introduction, explain it or refer the readers to the right place of the paper. 
- Merge Sections 4.3—4.5 with the experiment section (Section 5). The results of Section 4.3–4.5 are out of context without any explanation about the experiment setups. 

I have some concerns about Section 3, which is the main motivation of this work. As the authors noted in Appendix A that Gu et al.’s method works well with as few as 75 poised examples, the proposed sanitization algorithm would not be able to fail Gu et al.’s method by only identifying 20 out of 100 poised examples. 

How to control the parameter $\tau$ so that the perturbation appears plausible to humans? </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_HJlyuOh1RX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Author response</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=HJg6e2CcK7&amp;noteId=HJlyuOh1RX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper1118 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">19 Nov 2018</span><span class="item">ICLR 2019 Conference Paper1118 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">We thank the reviewer for the thoughtful comments. We will address comments raised below.

- On the novelty of our attacks. We believe that the main conceptual contribution of our work is the formulation of the clean-label attack problem and showing how these attacks can be made successful by modifying samples to be "harder". The two attacks investigated are meant as proof-of-concept that this approach works with existing methods. We agree with the reviewer that designing specialized attacks for this task is a valuable research direction that could lead to even more successful attacks.

- On the scale of our datasets. We do not have the resources to run an equally comprehensive study on ImageNet-scale datasets. Hence, we decided to perform more experiments on a small dataset rather than fewer experiments on a larger dataset. Note that the plots in Figure 3 and 4 involved training 50 models each. Does the reviewer have concrete concerns about the applicability of our approach to large-scale datasets?

- On the resistance of our approach to data augmentation. We have demonstrated (Appendix B) that simply modifying the pattern to appear in all 4 corners is already sufficient to make the attack significantly more resistant to data augmentation. Thus, we don't consider data augmentation to be a fundamental obstacle to our attack. We believe that future work investigating different backdoor triggers can further increase the resistance of our attack to data augmentation.

We thank the reviewer for concrete suggestions on improving our manuscript. We incorporated the following changes:
- We replaced Figure 1 with more illustrative examples.
- We modified the second paragraph of Section 2 to better explain the original Gu et al. (2017) attack. 
- We changed the wording of “reduced amplitude backdoor trigger” to "less conspicuous backdoor trigger" which should be clear without any further context. 
- The goal of Sections 4.3 - 4.5 is to provide a reader with an overview of our results before going into the experimental details. We modified these Sections to be more self-contained. 

- On concerns about Section 3. We do not argue that manual inspection will find all the poisoned examples (or enough to render the attack ineffective). We rather argue that if manual inspection of 300 images reveals 20 *clearly mislabelled* images, then the attack will very likely be detected leading to additional investigation and filtering. This argument illustrates a broader point -- if poisoned inputs appear suspicious upon human inspection, the attack is not truly insidious and can always be detected by more advanced filtering. This is why we believe our proposed attack is powerful: even if the samples are identified as potential outliers, they will not appear suspicious upon human inspection. We modified the text to better explain our argument. 

- We chose the parameter tau by manually inspecting different values of \tau on a 100 images.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>