<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Logically-Constrained Neural Fitted Q-iteration | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="Logically-Constrained Neural Fitted Q-iteration" />
        <meta name="citation_author" content="Mohammadhosein Hasanbeig" />
        <meta name="citation_author" content="Alessandro Abate" />
        <meta name="citation_author" content="Daniel Kroening" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=ryxMjoRcYm" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="Logically-Constrained Neural Fitted Q-iteration" />
      <meta name="og:description" content="This paper proposes a method for efficient training of Q-function for continuous-state Markov Decision Processes (MDP), such that the traces of the resulting policies satisfy a Linear Temporal..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_ryxMjoRcYm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Logically-Constrained Neural Fitted Q-iteration</a> <a class="note_content_pdf" href="/pdf?id=ryxMjoRcYm" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures"><a href="/profile?email=hosein.hasanbeig%40cs.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="hosein.hasanbeig@cs.ox.ac.uk">Mohammadhosein Hasanbeig</a>, <a href="/profile?email=aabate%40cs.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="aabate@cs.ox.ac.uk">Alessandro Abate</a>, <a href="/profile?email=kroening%40cs.ox.ac.uk" class="profile-link" data-toggle="tooltip" data-placement="bottom" title="" data-original-title="kroening@cs.ox.ac.uk">Daniel Kroening</a></span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 17 Nov 2018)</span><span class="item">ICLR 2019 Conference Withdrawn Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><a class="note_content_pdf item" href="/revisions?id=ryxMjoRcYm" target="_blank">Show Revisions</a></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">This paper proposes a method for efficient training of Q-function for continuous-state Markov Decision Processes (MDP), such that the traces of the resulting policies satisfy a Linear Temporal Logic (LTL) property. LTL, a modal logic, can express a wide range of time-dependent logical properties including safety and liveness. We convert the LTL property into a limit deterministic Buchi automaton with which a synchronized product MDP is constructed. The control policy is then synthesised by a reinforcement learning algorithm assuming that no prior knowledge is available from the MDP. The proposed method is evaluated in a numerical study to test the quality of the generated control policy and is compared against conventional methods for policy synthesis such as MDP abstraction (Voronoi quantizer) and approximate dynamic programming (fitted value iteration). </span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">As safety is becoming a critical notion in machine learning we believe that this work can act as a foundation for a number of research directions such as safety-aware learning algorithms.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">9 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_HJgxLlOna7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Submission Withdrawn by the Authors</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxMjoRcYm&amp;noteId=HJgxLlOna7"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper605 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper605 Withdraw Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Withdrawal confirmation: </span><span class="note_content_value">I have read and agree with the withdrawal statement on behalf of myself and my co-authors.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_BJxI5dIc37" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>somewhat unclear, possible modeling contributions</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxMjoRcYm&amp;noteId=BJxI5dIc37"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper605 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">03 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper605 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper considers the problem of solving a continuous-state MDP where the traces of the resulting policy satisfy some Linear Temporal Logic property. The authors use a version of finite-state machines called the Limit Deterministic Büchi automaton to represent the desired logic. The paper then defines a product MDP that extends the state space of the original MDP to incorporate those logic states in the automaton. Finally, to solve this extended MDP, the paper proposes to use neural fitted Q-iteration. Let n be the number of those logic states. They propose to use n neural networks to represent the value function, one Q-network for each logic state. 

I think the problem formulation is a little confusing. It is unclear whether there is a given reward function and the objective is to maximize the discounted cumulative reward, or the objective is to reach some predefined accepting states. It seems to be the latter, but the objective is not clearly stated in the paper. Also, it is unclear to me why the authors choose to define rewards as in equation (4) in their proposed algorithm. There is hardly any explanation on the parameters or justification for their choice. 

I think the main contribution of this paper is on the modeling side, that one may enlarge an MDP to incorporate certain logic information, which may be helpful in defining tasks.  There doesn't seem to be a significant contribution on the algorithmic side.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyeKxTCr6m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Review 1</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxMjoRcYm&amp;noteId=SyeKxTCr6m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper605 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your feedback. 

1) "It is unclear whether there is a given reward function and the objective is to maximize the discounted cumulative reward, or the objective is to reach some predefined accepting states. It seems to be the latter, but the objective is not clearly stated in the paper." 
- As we stated in the paper the objective is to find a policy by which the expected discounted reward gain is maximum. We call this policy, "optimal policy" and in Section 2 (page 2) we clearly mention that "The reward function specifies what the agent needs to achieve but not how to achieve it. Thus, the objective is that the agent itself comes up with an optimal policy". On the other hand, the reward shaping is done by incorporating a temporal logic formula. This means that by following the optimal policy the agent is able to reach the accepting states of the logical formula. Hence, thanks to the structure of the product automaton the notion of "finding an optimal" is equivalent to "finding a policy whose traces satisfies the logical formula". 

2) "... it is unclear to me why the authors choose to define rewards as in equation (4) in their proposed algorithm. There is hardly any explanation on the parameters or justification for their choice ..."
- This is a valid point. We should have clarified the role of parameter "y" which essentially acts as a switch to bypass the rand function. However, after Equation (4) we mention: "The role of the function rand is to break the symmetry in LCNFQ". Therefore, when we employ LCNFQ the switch is on, i.e. y=1, otherwise it is off, i.e. y=0.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_HyeIXypK3m" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Interesting direction but not easy to understand and weak empirical evaluation and weak discussion of related work </a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxMjoRcYm&amp;noteId=HyeIXypK3m"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper605 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper605 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">The paper extends neural fitted q-iteration in such a way that 
the traces of the resulting policies satisfy a Linear Temporal 
Logic (LTL) property. 

The main idea is to define a product MDP of the original 
MDP and a Limit Deterministic Buechi Automaton (LDBA).
Unfortunately, the paper is not well written and presented.  


The part on LTL is hard to grasp for an informed outsider such as the reviewer. 
The meanings of the operators are not introduced in an easy to grasp way. For instance, 
what exactly is L in the Definition of 3.1? What are words in general? Likewise, the product MDP defined is hard to grasp since no intuitive examples are provided. The intuition is that the LDBA is constraining the MDP according to the LTL. However, how 
this is actually done algorithmically is not explained. The paper, as presented, just boils down to "we build the product MDP". In this way, one can also use say some first order logic, since properties of the product MDP are not discussed. For instance, 
can the product MDP explode in size? Likewise the resulting LCNFQ is not really providing more insights. It essentially just says, learn n feed-forward neural networks. Does
the order of the Rprop upates really make no difference? Should be follow the temporal order? Is it really the best to train the networks independently? Why not training a single neural network that take the LDBA state as input (using some embedding)? Remark 4.2
only shows that one particular encoding does not work. This is even more important given that there are relational MDPs in the literature that are unfortunately not discussed, 
see e.g. 

Scott Sanner, Craig Boutilier:
Practical solution techniques for first-order MDPs. 
Artif. Intell. 173(5-6): 748-788 (2009)

and the overview in 

Luc De Raedt, Kristian Kersting, Sriraam Natarajan, David Poole:
Statistical Relational Artificial Intelligence: Logic, Probability, and Computation. Synthesis Lectures on Artificial Intelligence and Machine Learning, Morgan &amp; Claypool Publishers 2016 

The authors should discuss the difference to these approaches. Likewise, 
the authors should connect to the large body of work on safe reinforcement learning, 
where general constraints on the traces of a RL agent are imposed, too. Finally, the experimental evaluation should be improved. Right now, only one domain is considered: 
autonomous Mars rover. To show that this is of general interest, other domains should be considered. 

To summarize, the general direction is really important. However, the paper is hard to follow, the experimental evaluation is not convincing, and there is missing related work.  </span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">4: Ok but not good enough - rejection</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_S1lt0wgL6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Review 2</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxMjoRcYm&amp;noteId=S1lt0wgL6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper605 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your comments.

1) "The meanings of the operators are not introduced in an easy to grasp way. For instance, what exactly is L in the Definition of 3.1? What are words in general? " 
- Section 3 is a fairly standard introduction to LTL logic based on 

Baier, Christel, and Joost-Pieter Katoen. Principles of model checking. MIT Press, 2008.

which is the main reference book in model checking community. The definition of the labelling function L in Definition 3.1 is already given in Definition 2.1. The definition of words also is clearly given in Definition 3.1 based on Definition 2.2.

2) "... the product MDP defined is hard to grasp since no intuitive examples are provided. The intuition is that the LDBA is constraining the MDP according to the LTL. However, how 
this is actually done algorithmically is not explained ... can the product MDP explode in size?"
- This will be amended and an explanation will be added to the appendix (due to the space limitation).  

3) "Does the order of the Rprop updates really make no difference? Should be follow the temporal order? Is it really the best to train the networks independently? Why not training a single neural network that take the LDBA state as input (using some embedding)? Remark 4.2 only shows that one particular encoding does not work."
-As we explain after introducing LCNFQ algorithm in page 5: "In each cycle of LCNFQ (Algorithm 1), the training schedule starts from networks that are associated with accepting states of the automaton and goes backward until it reaches the networks that are associated to the initial states. In this way, we allow the Q-value to back-propagate through the networks" Q-value back-propagation happens between feedforward nets. Hence, each net has to be trained independently and while being trained, the order of Rprop has no role in Q-value back-propagation. Rprop is used to speed up the training. The order of training depends solely on the order of associated states in the automaton, i.e. starting from accepting states and going back to initial states. Further, in an attempt to use a single neural net, as we discuss in Remark 4.2, "integer-encoding" and also "one-hot-encoding" failed to generalize efficiently over the state space.</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div><div class="note_with_children"><div id="note_rklkvEOFhQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Neat idea but somewhat underwhelming proposition</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxMjoRcYm&amp;noteId=rklkvEOFhQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper605 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">02 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper605 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">    The authors of the article propose to use logical propositions to simplify the expression of high-level objectives in reinforcement learning problems. For that, from logical formulas, an automaton is generated and, from this automaton, is deduced a reward function. The reward function is defined in such a way that the agent reaches an acceptable absorbing state. Thus, it is easier to define sub-objectives that should have been encoded by hand in the reward function. The authors formally define the combined framework of MDP with LDBA, then they describe an adaptation of neural fitted iteration, Voronoi quantizer and fitted value iteration in this context. They compared their new methods in 2 different environments with different constraints.


    The paper is well written and easy to read. However, these works do not really overcome the problem of safety or expressing "complex high-level control objectives" because logic rules are only used to define a reward function. Once this function is defined, there is no other applied constraints, so during the learning phase there is no more safety than using conventional RL. Moreover, on the example studied, adding a dimension to the state and defining an adequate reward function with this additional dimension would allow obtaining the same behaviors. The proposed framework would be more convincing if it were illustrated with an example where the reward function is really easier to define with the proposed approach, but difficult to define without it.

    Pros :
    - beautiful clean formalization
    - interesting experimentation

    Cons :
    - policies/algorithms are not directly logically constrained but more guided by constraints
    - the obtained reward function is sparse, e.g., subgoals don't receive any additional rewards
    - experiments are difficult to reproduce (no link to source, missing a full description of neural networks used, etc.)
    - some details are not necessary for the overall understanding, while some essential elements are missing (what happens if a non-accepting absorbing state is reached during the training phase, what is the concrete inputs of your neural networks ? how your discrete actions are handled in your neural networks, what happens if y = 0 for LCNFQ?)
    - satisfiability of the generated policies is only little studied : what their proportions during the learning phase are, ...


    Others questions/remarks :
    - it seems that you do not use any exploration strategy, is it because your environment is already stochastic on the action ?
    In this case, "LCNFQ stops when a satisfying policy is generated." seems to be a bad idea as the algorithm might just have been lucky.
    - it would be better if the definition of the reward function of the general framework does not contain a LCNFQ specific hack
    - In (4), the reward function should also depend on the next state if it is deterministic. Besides, the definition of rand is slightly ambiguous, does it return a new random value every time the reward is evaluated?</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyeBptbITQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Review 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxMjoRcYm&amp;noteId=SyeBptbITQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper605 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thank you for your feedback.

1) "... policies/algorithms are not directly logically constrained but more guided by constraints... Once this function is defined, there is no other applied constraints, so during the learning phase there is no more safety than using conventional RL... what happens if a non-accepting absorbing state is reached during the training phase... satisfiability of the generated policies is only little studied: what their proportions during the learning phase are"
- The confusion lies in the difference between offline and online RL. This is true if we have used an online RL approach. However, the proposed algorithm is offline and as we mention in the paper, the learning phase is separated from the exploration phase. More specifically, once the training samples are gathered, then the learning phase starts on those samples. During the "exploration" phase, if a non-accepting absorbing state is reached, the proposed reward function gives r_n which is a neutral reward. Exploration phase includes several re-initializations or episodes as are called in this work. Notice that both LTL properties studied in this paper have non-accepting states (q_5 in Figure 2.a and q_3 in Figure 2.b).

2) "... the obtained reward function is sparse, e.g., subgoals don't receive any additional rewards"
- Indeed! And we believe this is important if we like to extend this approach to knowledge transfer methods in RL.

3) "... experiments are difficult to reproduce (no link to source, missing a full description of neural networks used, etc.)"
- Will be amended.

4) "... what is the concrete inputs of your neural networks? how your discrete actions are handled in your neural networks, what happens if y = 0 for LCNFQ?"
-  The inputs are neural nets are explicitly defined when we discuss the pattern set P_{q_i} on page 4 and 5. Each action is an input argument for each neural net. Also, y=1 for LCNFQ as we discuss it in Section 7. The parameter y essentially acts as a switch to bypass the rand function. When we employ LCNFQ the switch is on, i.e. y=1, otherwise it is off, i.e. y=0.

5) "... the reward function should also depend on the next state if it is deterministic."
- This is true if we like to apply a DP-based algorithm (e.g. Section 6). However, it is standard in RL to have a reward function that depends on the subsequent state. 

6) "... the definition of rand is slightly ambiguous, does it return a new random value every time the reward is evaluated?"
- Thank you for raising this. As we mentioned, the role of rand is to break the symmetry in neural nets and it has to return a new value each time we evaluate the reward function.

  </span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_r1gG9XoI67" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Potential issue with stochastic rewards</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxMjoRcYm&amp;noteId=r1gG9XoI67"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper605 AnonReviewer3</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">12 Nov 2018</span><span class="item">ICLR 2019 Conference Paper605 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for your reply!

5) It is indeed standard, but the reward function should then be said to be random variable for clarity.

6) If a random value is generated every time, then there may be an issue here:

"Recall that the reward function (4) only returns a positive value when the agent has a transition to an accepting state in the product MDP. Therefore, if accepting states are reachable, by following this reward function the agent is able to come up with a policy \pi that leads to the accepting states. ... Consequently, the optimal policy has the highest expected probability of reaching to the accepting set, i.e. satisfying the LTL property." 

    This sentence seems false when y=1. With the parameters you propose (M=1, m=0.05, gamma=0.9), 
    if the trajectory length is too high the reward function can become contradictory 
    trajectory 1 (unlucky draw + reaching accepting state) : 0 + 0.9*0 + 0.9^2 * 0 + ... + gamma^n * M ~= 0,205 (n=15)
    trajectory 2 (lucky draw + reaching non-accepting state): 0.05 + 0.9*0.05 + 0.9^2*0.05 + ... + gamma^n * m ~= 0,407 (n=15) the trajectory 2 is valued better than 1.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_H1ld2zVDaQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Reply to Review 3</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=ryxMjoRcYm&amp;noteId=H1ld2zVDaQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">13 Nov 2018</span><span class="item">ICLR 2019 Conference Paper605 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Thanks for the insightful comment!

6) We would like to emphasize again that our algorithm is an offline learning algorithm. Hence, the sample generation phase is done initially and then learning phase starts. Furthermore, the optimal policy is generated by looking at the optimal Q-function when the training phase is finished. Last, but not least, the underlying MDP has continuous state space. In this sense, in the sample generation phase, since the MDP is continuous-state and the learning has not started yet, picking a single trajectory is meaningless; let alone that trajectory 1 is very unlikely to happen. Let us go through it step-by-step:
i) As we mentioned in the paper, the sample generation phase is episodic which lets the agent re-initialize itself and gather samples over and over again. Over thousands of episodes, the agent samples the MDP multiple times and from different states. The probability of a single trajectory to appear multiple times in this phase is zero due to the continuity of the MDP state space.
ii) Let's assume that state-action-state-reward tuples of trajectory 1 and trajectory 2 in your example appear in the sample set and we start training based on a sample set that includes these state-action-state-reward tuples. Notice that the role of each feedforward net is to generalize over the state space. Thus, the effect of tuples of trajectory 1 and trajectory 2 in the sample set will be weakened by other (less unlucky) tuples from other trajectories gathered over thousands of episodes. The final policy is generated via the optimal trained Q-function. Hence, we have to wait until the backpropagation is completed and then evaluate the expected accumulated reward (as you did for 15 steps) by evaluating the trained Q-function. 

5) It will be amended in the final draft.
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>