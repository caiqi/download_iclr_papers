<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" class="no-js"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>A PRIVACY-PRESERVING IMAGE CLASSIFICATION FRAMEWORK WITH A LEARNABLE OBFUSCATOR | OpenReview</title>
  <meta name="description" content="" />

      <meta name="citation_title" content="A PRIVACY-PRESERVING IMAGE CLASSIFICATION FRAMEWORK WITH A LEARNABLE OBFUSCATOR" />
        <meta name="citation_author" content="Anonymous" />
      <meta name="citation_publication_date" content="2018/09/27" />
      <meta name="citation_online_date" content="2018/09/27" />
      <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=rklvnjRqY7" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:site" content="@openreviewnet" />
      <meta name="og:title" content="A PRIVACY-PRESERVING IMAGE CLASSIFICATION FRAMEWORK WITH A..." />
      <meta name="og:description" content="Real world images often contain large amounts of private / sensitive information that should be carefully protected without reducing their utilities. In this paper, we propose a privacy-preserving..." />
      <meta name="og:image" content="https://openreview.net/static/images/openreview_logo_512.png" />

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans:400,400i,700,700i" />

  <link rel="stylesheet" href="/static/css/bootstrap.min.css" />
  <link rel="stylesheet" href="/static/css/jquery-ui.min.css" />
  <link rel="stylesheet" href="/static/css/main.min.css" />
</head>

<body class="forum">
  <nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
  
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand home push-link" href="/"><strong>OpenReview</strong>.net</a>
      </div>
  
      <div id="navbar" class="navbar-collapse collapse">
        <form class="navbar-form navbar-left profile-search" role="search">
          <div class="form-group has-feedback">
            <input id="search_input" type="text" class="form-control ui-autocomplete-input" placeholder="Search ICLR 2019 Conference" autocomplete="off" />
            <span class="glyphicon glyphicon-search form-control-feedback" aria-hidden="true"></span>
          </div>
  
          <input id="search_group" type="hidden" value="ICLR.cc/2019/Conference" />
          <input id="search_content" type="hidden" value="all" />
        <ul id="ui-id-1" tabindex="0" class="ui-menu ui-widget ui-widget-content ui-autocomplete ui-front" style="display: none;"></ul></form>
  
        <ul class="nav navbar-nav navbar-right">
        
            <li id="user-menu"><a href="/login">Login</a></li>
        </ul>
      </div>
  
    </div>
  </nav>

  <div id="or-banner" class="banner" style="">
  <div class="container">
    <div class="row">
      <div class="col-xs-12"><a href="/group?id=ICLR.cc/2019/Conference" title="Venue Homepage"><img class="icon" src="/static/images/arrow_left.svg" /> Go to <strong>ICLR 2019 Conference</strong> homepage</a></div>
    </div>
  </div>
</div>
<div id="flash-message-container" class="alert alert-danger" role="alert" style="display: none;">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <div class="alert-content">
          <button type="button" class="close" aria-label="Close"><span aria-hidden="true">×</span></button>
        </div>
      </div>
    </div>
  </div>
</div>

<div class="container">
  <div class="row">
    <div class="col-xs-12">
      <main id="content" class="clearfix openbanner-visible legacy-styles"><div id="note_rklvnjRqY7" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>A PRIVACY-PRESERVING IMAGE CLASSIFICATION FRAMEWORK WITH A LEARNABLE OBFUSCATOR</a> <a class="note_content_pdf" href="/pdf?id=rklvnjRqY7" title="Download PDF" target="_blank"><img src="/static/images/pdf_icon_blue.svg" /></a> </h2></div><div class="meta_row"><span class="signatures">Anonymous</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">28 Sep 2018 (modified: 11 Oct 2018)</span><span class="item">ICLR 2019 Conference Blind Submission</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span><span class="item"><a class="action-bibtex-modal" data-bibtex="@inproceedings{    &#10;anonymous2019a,    &#10;title={A PRIVACY-PRESERVING IMAGE CLASSIFICATION FRAMEWORK WITH A LEARNABLE OBFUSCATOR},    &#10;author={Anonymous},    &#10;booktitle={Submitted to International Conference on Learning Representations},    &#10;year={2019},    &#10;url={https://openreview.net/forum?id=rklvnjRqY7},    &#10;note={under review}    &#10;}">Show Bibtex</a></span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Abstract: </span><span class="note_content_value">Real world images often contain large amounts of private / sensitive information that should be carefully protected without reducing their utilities. In this paper, we propose a privacy-preserving deep learning framework with a learnable ob- fuscator for the image classification task. Our framework consists of three mod- els: learnable obfuscator, classifier and reconstructor. The learnable obfuscator is used to remove the sensitive information in the images and extract the feature maps from them. The reconstructor plays the role as an attacker, which tries to recover the image from the feature maps extracted by the obfuscator. In order to best protect users’ privacy in images, we design an adversarial training methodol- ogy for our framework to optimize the obfuscator. Through extensive evaluations on real world datasets, both the numerical metrics and the visualization results demonstrate that our framework is qualified to protect users’ privacy and achieve a relatively high accuracy on the image classification task.</span></div><div class="note_contents"><span class="note_content_field">Keywords: </span><span class="note_content_value">privacy-preserving, image classification, adversarial training, learnable obfuscator</span></div><div class="note_contents"><span class="note_content_field">TL;DR: </span><span class="note_content_value">We proposed a novel deep learning image classification framework that can both accurately classify images and protect users' privacy.</span></div><div class="reply_row clearfix"><div class="item" id="reply_count">5 Replies</div></div></div><hr class="small" /><div id="note_children"><div class="note_with_children"><div id="note_SkgIh8JT6Q" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Good demonstration of adversarial training but evaluation doesn't seem very promising</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklvnjRqY7&amp;noteId=SkgIh8JT6Q"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper718 AnonReviewer4</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">17 Nov 2018</span><span class="item">ICLR 2019 Conference Paper718 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposes a framework which preserves the private information in the image and at the same time doesn’t compromise the usability of the image. This framework is composed of three models - the obfuscator, the classifier, and the reconstructor. While the obfuscator’s objective is to remove the sensitive information from the image and extract feature maps that still have it’s discriminative features, the classifier uses these feature maps to do the classification learning task. On the other hand, the reconstructor is like an attacker which tries to expose the private information by restoring the image. The framework thus uses adversarial training since the obfuscator and the reconstructor have opposite goals. The paper shows that the reconstructed images don’t seem to have crucial sensitive information of the raw image yet it still manages to achieve high classification accuracy. 

Pros:
- While we focus on achieving better and better results with Machine Learning, we tend to forget about the private nature of the data we often have to work with. I believe it is a very important issue to address and a great idea to work on.
- The idea of using the reconstructor as an adversary in the framework seems so intuitive. 
The visualizations at the end comparing the reconstructed images with the raw images reveal how this training procedure doesn’t allow sensitive information to leak.

Cons:
- I am not very convinced with choosing the image reconstruction quality as a measure for the privacy preservation. I am unable to see how those two can be so strongly related. There can be scenarios I believe, where the image doesn’t reconstruct well but can still retain the sensitive features of the image. 
- I am just curious about a few aspects of the design of the network. Firstly, I would have expected to a deeper network required to remove the private information of the image. I think a visualization on how the feature maps look after 3rd layer when they lose their private traits, with possibly a comparison with an early, say 1st layer would help understand it better. 
- When I try to think of a data with private information, it seems to be pretty different than the MNIST or CIFAR data. I can’t seem to imagine any sort of privacy linked to these datasets (with a digit for example). It would be more helpful to look at the performance of the framework with images having some private content as we have from social networking sites or medical images which contains sensitive details. 

Questions:
- Are you using any sort of pretrained weights from the VGG network and fine-tuning on top of it or are you just borrowing the architecture and training it from scratch in your experiments?
- Table 1 shows a sigmoid layer in the last layer of the reconstruction networks. It was a little confusing since we are expecting an image as an output of the network.
</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SkeNOmOpnm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Nice idea. Need more justification for privacy metric.</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklvnjRqY7&amp;noteId=SkeNOmOpnm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper718 AnonReviewer2</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">05 Nov 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper718 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">This paper proposed a deep learning framework with three components: an obfuscator, a classifier, and a reconstructor, for privacy-preserving image classification. To protect the privacy of the image data, this paper proposed an adversarial training process to optimize the obfuscator such that it achieves high classification accuracy while the "best" reconstructor is not able to accurately reconstruct the sensitive input image from the output features. Experiment results showed relatively high accuracy with no sign that an input image can be reconstructed by state-of-the-art reconstruction models.

The presentation of the paper is pretty good. The problem of learning without leaking sensitive information in data is an important issue, and the paper proposed an interesting framework for it. However, I think some technical aspects need to be more addressed:
It seems that the obfuscator+classifier models are obtained by dividing an existing deep CNN architecture, but the paper did not justify the choice of the model or the choice of the layer as the dividing point. These seem to be important decisions in the framework, so I think more intuitions, or some heuristics for choosing them need to be provided.
It is not clear to me how reconstruction accuracy (measured by human’s perception) and data privacy are related. I think in general high reconstruction error does not automatically imply that the sensitive information is removed. It is still possible for a machine learning model to extract sensitive information from images that are not readable for human. So I think justifications for this choice of privacy metric need to be provided.

A few things about presentation:
1. The first sentence of Section 4: "the our proposed framework" --&gt; "the proposed framework of"
2. Figure 3: all lines are overlapping. I suggest explaining why and color-code the lines for better visibility.</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">4: The reviewer is confident but not absolutely certain that the evaluation is correct</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_SyxDx_OeiQ" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Important topic to study but deeper investigation is suggested</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklvnjRqY7&amp;noteId=SyxDx_OeiQ"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper718 AnonReviewer1</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">14 Oct 2018 (modified: 07 Nov 2018)</span><span class="item">ICLR 2019 Conference Paper718 Official Review</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Review: </span><span class="note_content_value">Privacy issues are a burning issue in ML. Elevated awareness to privacy issues, as expressed, for example, by GDPR regulations, might make it hard to collect large datasets for training machine learning models. This current work suggests using adversarial networks to obfuscate images and thus allow collecting them without privacy concerns. 
The problemed studied in this work is of great interest and there many researchers are studying it today. The popularity of this topic makes it hard to present a good background on the state of current research. However, I fined the method proposed here naïve compared to the methods proposed by previous researchers, even the ones that are cited in the related work section: while methods that use differential privacy, homomorphic encryptions or secure-multi-party-computation provide theoretical guaranties on the privacy provided, the work presented here shows only that the obfuscated data does not allow reconstructing the image using several networks with which such reconstruction was attempted.  

The problem discussed above breaks into two questions that are left un-answered. First, what is the trust model? That is, under what assumptions can we prove that the private data is protected? The authors here propose using the reconstruction quality is a measure of privacy. The advantage of this measure is that it is easy to evaluate, and it is generic. However, this leads to a second question, why is reconstruction error a good measure of privacy? For example, even a well distorted image may allow identifying the identity of a person in an image.

In the following there are some detailed comments on specific sections in this work:
•	Introduction 
o	 “GDPR stipulates that personal data cannot be stored for long periods of time … In other words, this regulation prevents long-term storage of video/image data” – this is not accurate. GDPR allows for pseudo-anonymization of data. Therefore, for example, encrypted data can be stored for long periods of time.
o	Does the obfuscated image qualify as anonymization/pseudo-anonymization according to GDPR?
o	“Thus the obfuscated intermediate representation can be stored indefinitely” – is this representation not subject to GDPR deletion requests?
o	“we choose image reconstruction quality as a general measure for privacy preservation” – why is it a good measure of privacy preservation? For example, could it be that the reconstruction quality is low, but it is still possible to identify the person in the image?
o	“To the best of our knowledge, this is the first study of using the adversarial training methodology for privacy-preserving image classification” – there have been studies in the past of using adversarial networks for privacy tasks. For example, Abadi &amp; Andersen, 2016 studied using adversarial networks as an encryption mechanism without specifying whether the data is an image or not. The work of Yu, Zhang, Kuang, Lin &amp; Fan, 2017 is also very close in nature to the current one. One of the contributions Yu et al. propose is image privacy protection by blurring private sensitive objects from the image.
•	Related Work
o	The main limitation of CryptoNets in the context of this work is that it only handles inference and does not handle training of neural networks. There are some attempts to perform training using homomorphic encryptions, for example see Kim, Song, Wang, Xia, and Jiang, 2018.
o	It is not accurate to say about DeepSecure that it is limited to cases in which each client sends less than 2600 examples. Instead, what is true about DeepSecure and related methods such as SecureML is that there is a big computation cost.
o	There are methods that use Intel’s SGX for training neural networks which provide privacy but have better computation performance, for example see Ohrimenko, Schuste, Fournet, Mehta, Nowozin, Vaswani and Costa, 2016
o	When discussing privacy methods, you should also mention the trust model – that is, what is an adversary allowed to do to try to capture private information. For example, DeepSecure, assumes the non-collusion &amp; semi-honest model which means that the data is protected by splitting it on several servers, each server will try to use the information it can see to infer the private information but multiple parties do not collaborate in trying to get to private information and moreover, each party follows the designed protocol. Other methods use different trust models.
o	On the work of Osia et al you say that it can’t be used for training because of the communication costs. The large communication costs are common to techniques that use multi-parties to secure computation, this includes DeepSecure and SecureML. Note, however, that the communication costs do not make it impossible to train, instead, it makes it slow and costly.
•	Security Analysis 
o	is it necessary the case that there is a single data provider? If there are more than a single data provider, then you may have more that 3 entities in the system. In this case you may say that there are 3 entity types.
o	What are the restrictions on the power of the attacker? According to the description the attacker can be an internal staff which may allow the attacker to obtain access to the data before obfuscation
o	Besides the nice acronym, why is “Chosen Image Attack” a proper name for an attack that modifies the representation of images?
o	Why is the Chosen Image Attack the “most natural and convenient way of launching an attack”? In many cases, privacy leaks because of joining data from multiple sources, see for example the attacks on the Netflix challenge data, see for example Ohm, 2009.
o	
•	 Obfuscator-Adversary Framework
o	A similar model to the one proposed here has been suggested by Moore, Pfeiffer, Wei, Iyer, Charles, Gilad-Bachrach, Boyles and Manavoglu, 2018 in the context of removing biases from datasets.
•	Experiments
o	The images used in this experiment are very different from the images on which VGG and ResNet were designed to work. The images here are very small around 30x30 pixels while VGG and ResNet were designed to work on large images, for example 200x300 pixels. 
o	It may be interesting to conduct an experiment on large images (ImageNet, Caltech dataset) and see how the reconstruction error differs between the proposed representation compared to standard representations such as the ones generated by AlexNet, VGG, Inception and others.


Therefore, a lot of important and interesting work is reported in this report, however, some additional thinking is required. I advise the authors to keep working in this direction.


</span></div><div class="note_contents"><span class="note_content_field">Rating: </span><span class="note_content_value">5: Marginally below acceptance threshold</span></div><div class="note_contents"><span class="note_content_field">Confidence: </span><span class="note_content_value">5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div><div class="note_with_children"><div id="note_ryx8jHiQcX" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Related work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklvnjRqY7&amp;noteId=ryx8jHiQcX"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">(anonymous)</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">04 Oct 2018</span><span class="item">ICLR 2019 Conference Paper718 Public Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Have you seen this work: <a href="https://arxiv.org/abs/1805.12024" target="_blank" rel="nofollow">https://arxiv.org/abs/1805.12024</a> ? It has the same obfuscator, classifier, reconstructor architecture to remove privacy sensitive information from pictures before sending them to the cloud.</span></div><div class="reply_row clearfix"></div></div><div class="children"><div class="note_with_children"><div id="note_SyxQsU93qm" class="note panel"><div class="title_pdf_row clearfix"><h2 class="note_content_title"><a>Differences between our work and the mentioned related work</a>  <button class="btn btn-xs btn-default permalink-button" title="Link to this comment" data-permalink-url="https://openreview.net/forum?id=rklvnjRqY7&amp;noteId=SyxQsU93qm"><span class="glyphicon glyphicon-link" aria-hidden="true"></span></button></h2></div><div class="meta_row"><span class="signatures">ICLR 2019 Conference Paper718 Authors</span></div><div class="clearfix"><div class="meta_row pull-left"><span class="date item">11 Oct 2018</span><span class="item">ICLR 2019 Conference Paper718 Official Comment</span><span class="item">Readers: <span class="readers-icon glyphicon glyphicon-globe"></span> Everyone</span></div><div class="meta_row meta_actions"></div></div><div class="note_contents"><span class="note_content_field">Comment: </span><span class="note_content_value">Dear Sir or Madam,
 
Thanks for the reference to “Privacy Aware Offloading of Deep Networks” by Leroux, et al. (2018). We were not aware of this paper, since it appeared recently in an ICML2018 workshop in July, which was after our initial literature review.  We will include this new reference in the revised version of our paper.
 
The Leroux, et al. (2018) proposes an image obfuscator based on an autoencoder, where the image is obfuscated and then sent to a pre-trained classifier. Essentially, the obfuscator exploits structural deficiencies in the CNN feature extractor to fool it to make a particular prediction (analogous to adversarial images). Their motivation is to obfuscate the images to be sent to a cloud service (the output is still an image). In contrast, the motivation of our paper is to create an intermediate obfuscated representation (a feature map) that anonymizes the image so that it can be stored without revealing privacy, and is also useful for later re-training of the network. Hence, a key methodology difference between the two papers is that we train both the obfuscator and classifier together, while Leroux, et al. (2018) only trains the obfuscator while keeping the classifier fixed.  As a result our intermediate representation is both discriminative and privacy-preserving, whereas the obfuscated images from Leroux, et al. (2018) are not likely to contain useful discriminative information for training, since it exploits structural deficiencies in the CNN.   There are also other minor differences, such as we adopt reconstruction error based on PSNR, which is more similar to human perception, whereas their paper uses MSE, and we also propose an adversarial training strategy. 

Besides the methodology difference, our paper also has a more complete set of experiments than Leroux, et al. (2018).  In particular, we demonstrate experimentally the robustness of our approach to “brute-force” attack, where we simulate an attacker attempting to attack an obfuscator by training different types of reconstructors to recover the images. Our results (Table 3) show that the obfuscator can resist a “brute-force” attack from other types of reconstructors that are different from the one used for training the obfuscator.  We also conduct an ablation study that shows that adding the adversarial reconstructor increases the reconstruction error, compared to a standard VGG16 network. We also assess the reconstruction ability quantitatively, whereas Leroux, et al. (2018) only showed qualitative examples. And we will add the quantitative comparison between our framework and Leroux’s work in the revised version of our paper.

Considering the above points, we believe that our work contains both methodological contribution (an anonymous intermediate representation) and more complete experiments to assess it (brute force attack, ablation studies, and reconstruction evaluation). 

Regards,
The authors
</span></div><div class="reply_row clearfix"></div></div><div class="children"></div></div></div></div></div></main></div>
  </div>
</div>


    <!-- Footer -->
    <footer class="sitemap">
      <div class="container">
        <div class="row hidden-xs">
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/venues">All Venues</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="/contact">Contact</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-sm-4">
            <ul class="list-unstyled">
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
    
        <div class="row visible-xs-block">
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/" class="home">Home</a></li>
              <li><a href="/about">About OpenReview</a></li>
              <li><a href="#" data-toggle="modal" data-target="#feedback-modal">Feedback</a></li>
            </ul>
          </div>
    
          <div class="col-xs-6">
            <ul class="list-unstyled">
              <li><a href="/contact">Contact</a></li>
              <li><a href="/terms">Terms of Service</a></li>
              <li><a href="/privacy">Privacy Policy</a></li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    
    <footer class="sponsor">
      <div class="container">
        <div class="row">
          <div class="col-sm-10 col-sm-offset-1">
            <p class="text-center">
              OpenReview is created by the <a href="http://www.iesl.cs.umass.edu/" target="_blank">Information Extraction and Synthesis Laboratory</a>, College of Information and Computer Science, University of Massachusetts Amherst. We gratefully acknowledge the support of the OpenReview sponsors:  Google,  Facebook, NSF, the University of Massachusetts Amherst Center for Data Science, and Center for Intelligent Information Retrieval, as well as the Google Cloud Platform for donating the computing and networking services on which OpenReview.net runs.
            </p>
          </div>
        </div>
      </div>
    </footer>

  <div id="feedback-modal" class="modal fade" tabindex="-1" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
          <h3 class="modal-title">Send Feedback</h3>
        </div>
        <div class="modal-body">
          <p>Enter your feedback below and we'll get back to you as soon as possible.</p>
          <form action="/feedback" method="POST">
            <div class="form-group">
              <input type="email" name="from" class="form-control" placeholder="Email" />
            </div>
            <div class="form-group">
              <input type="text" name="subject" class="form-control" placeholder="Subject" />
            </div>
            <div class="form-group">
              <textarea name="message" class="form-control feedback-input" rows="5" placeholder="Message" required=""></textarea>
            </div>
          <ul id="ui-id-2" tabindex="0" class="ui-menu ui-widget ui-widget-content" style="display: none;"></ul></form>
        </div>
        <div class="modal-footer">
          <button type="button" class="btn btn-default" data-dismiss="modal">Cancel</button>
          <button type="button" class="btn btn-primary">Send</button>
        </div>
      </div>
    </div>
  </div>

  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script>window.jQuery || document.write('&lt;script src="/static/js/vendor/jquery-2.2.4.min.js"&gt;&lt;\/script&gt;')</script>

  <script src="/static/dist/vendor.min.js"></script>
  <script src="/static/dist/templates.min.js"></script>
  <script src="/static/dist/openreview.min.js"></script>

    <script>window.legacyScripts = true;</script>


    <script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-108703919-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() { dataLayer.push(arguments); }
      gtag('js', new Date());
      gtag('config', 'UA-108703919-1');
    </script>


<div role="status" aria-live="assertive" aria-relevant="additions" class="ui-helper-hidden-accessible"></div></body></html>